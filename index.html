<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2023-11-22T00:00:00Z">2023-11-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">36</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PaSS: Parallel Speculative Sampling <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Monea, Armand Joulin, Edouard Grave
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling the size of language models to tens of billions of parameters has led
to impressive performance on a wide range of tasks. At generation, these models
are used auto-regressively, requiring a forward pass for each generated token,
and thus reading the full set of parameters from memory. This memory access
forms the primary bottleneck for generation and it worsens as the model size
increases. Moreover, executing a forward pass for multiple tokens in parallel
often takes nearly the same time as it does for just one token. These two
observations lead to the development of speculative sampling, where a second
smaller model is used to draft a few tokens, that are then validated or
rejected using a single forward pass of the large model. Unfortunately, this
method requires two models that share the same tokenizer and thus limits its
adoption. As an alternative, we propose to use parallel decoding as a way to
draft multiple tokens from a single model with no computational cost, nor the
need for a second model. Our approach only requires an additional input token
that marks the words that will be generated simultaneously. We show promising
performance (up to $30\%$ speed-up) while requiring only as few as $O(d_{emb})$
additional parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 3rd workshop on Efficient Natural Language and Speech
  Processing (ENLSP, NeurIPS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Drilling Down into the Discourse Structure with LLMs for Long Document
  Question Answering <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inderjeet Nair, Shwetha Somasundaram, Apoorv Saxena, Koustava Goswami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the task of evidence retrieval for long document question
answering, which involves locating relevant paragraphs within a document to
answer a question. We aim to assess the applicability of large language models
(LLMs) in the task of zero-shot long document evidence retrieval, owing to
their unprecedented performance across various NLP tasks. However, currently
the LLMs can consume limited context lengths as input, thus providing document
chunks as inputs might overlook the global context while missing out on
capturing the inter-segment dependencies. Moreover, directly feeding the large
input sets can incur significant computational costs, particularly when
processing the entire document (and potentially incurring monetary expenses
with enterprise APIs like OpenAI's GPT variants). To address these challenges,
we propose a suite of techniques that exploit the discourse structure commonly
found in documents. By utilizing this structure, we create a condensed
representation of the document, enabling a more comprehensive understanding and
analysis of relationships between different parts. We retain $99.6\%$ of the
best zero-shot approach's performance, while processing only $26\%$ of the
total tokens used by the best approach in the information seeking evidence
retrieval setup. We also show how our approach can be combined with
\textit{self-ask} reasoning agent to achieve best zero-shot performance in
complex multi-hop question answering, just $\approx 4\%$ short of zero-shot
performance using gold evidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Findings of EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LM-Cocktail: Resilient Tuning of Language Models via Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shitao Xiao, Zheng Liu, Peitian Zhang, Xingrun Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pre-trained language models are continually fine-tuned to better support
downstream applications. However, this operation may result in significant
performance degeneration on general tasks beyond the targeted domain. To
overcome this problem, we propose a novel method which enables the fine-tuned
model to stay resilient in general perspectives. Our method is conducted in the
form of model merging (namely LM-Cocktail), where the fine-tuned language model
is merged with the pre-trained base model or the peer models from other domains
through weighted average. Despite simplicity, LM-Cocktail is surprisingly
effective: the resulted model is able to achieve a strong empirical performance
in the whole scope of general tasks while preserving a superior capacity in its
targeted domain. We conduct comprehensive experiments with LLama and BGE model
on popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the
efficacy of our proposed method. The code and checkpoints are available at
https://github.com/FlagOpen/FlagEmbedding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Current Topological and Machine Learning Applications for Bias Detection
  in Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colleen Farrelly, Yashbir Singh, Quincy A. Hathaway, Gunnar Carlsson, Ashok Choudhary, Rahul Paul, Gianfranco Doretto, Yassine Himeur, Shadi Atalls, Wathiq Mansoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Institutional bias can impact patient outcomes, educational attainment, and
legal system navigation. Written records often reflect bias, and once bias is
identified; it is possible to refer individuals for training to reduce bias.
Many machine learning tools exist to explore text data and create predictive
models that can search written records to identify real-time bias. However, few
previous studies investigate large language model embeddings and geometric
models of biased text data to understand geometry's impact on bias modeling
accuracy. To overcome this issue, this study utilizes the RedditBias database
to analyze textual biases. Four transformer models, including BERT and RoBERTa
variants, were explored. Post-embedding, t-SNE allowed two-dimensional
visualization of data. KNN classifiers differentiated bias types, with lower
k-values proving more effective. Findings suggest BERT, particularly mini BERT,
excels in bias classification, while multilingual models lag. The
recommendation emphasizes refining monolingual models and exploring
domain-specific biases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Translation to Control Formality Features in the Target Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshita Tyagi, Prashasta Jung, Hyowon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formality plays a significant role in language communication, especially in
low-resource languages such as Hindi, Japanese and Korean. These languages
utilise formal and informal expressions to convey messages based on social
contexts and relationships. When a language translation technique is used to
translate from a source language that does not pertain the formality (e.g.
English) to a target language that does, there is a missing information on
formality that could be a challenge in producing an accurate outcome. This
research explores how this issue should be resolved when machine learning
methods are used to translate from English to languages with formality, using
Hindi as the example data. This was done by training a bilingual model in a
formality-controlled setting and comparing its performance with a pre-trained
multilingual model in a similar setting. Since there are not a lot of training
data with ground truth, automated annotation techniques were employed to
increase the data size. The primary modeling approach involved leveraging
transformer models, which have demonstrated effectiveness in various natural
language processing tasks. We evaluate the official formality accuracy(ACC) by
comparing the predicted masked tokens with the ground truth. This metric
provides a quantitative measure of how well the translations align with the
desired outputs. Our study showcases a versatile translation strategy that
considers the nuances of formality in the target language, catering to diverse
language communication needs and scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, based on DCU MCM Practicum 2022/2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complexity-Guided Curriculum Learning for Text Graphs <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nidhi Vakil, Hadi Amiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curriculum learning provides a systematic approach to training. It refines
training progressively, tailors training to task requirements, and improves
generalization through exposure to diverse examples. We present a curriculum
learning approach that builds on existing knowledge about text and graph
complexity formalisms for training with text graph data. The core part of our
approach is a novel data scheduler, which employs "spaced repetition" and
complexity formalisms to guide the training process. We demonstrate the
effectiveness of the proposed approach on several text graph tasks and graph
neural network architectures. The proposed model gains more and uses less data;
consistently prefers text over graph complexity indices throughout training,
while the best curricula derived from text and graph complexity indices are
equally effective; and it learns transferable curricula across GNN models and
datasets. In addition, we find that both node-level (local) and graph-level
(global) graph complexity indices, as well as shallow and traditional text
complexity indices play a crucial role in effective curriculum learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long Paper Accepted at EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generation of Explanations for Logic Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyi Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis delves into a fortiori arguments in deductive reasoning,
underscoring their relevance in various domains such as law, philosophy, and
artificial intelligence. The research is centred on employing GPT-3.5-turbo to
automate the analysis of these arguments, with a focus on understanding
intricate reasoning processes, generating clear and coherent explanations, and
creating novel arguments. The methodology encompasses a series of tasks
including detailed reasoning, interpretation, and the augmentation of a
fortiori arguments. It involves meticulously identifying these arguments in
diverse contexts, differentiating comparative elements, and categorizing them
based on their logical structure.
  Extensive experiments reveals the challenges encountered by GPT-3.5-turbo in
accurately detecting and classifying a fortiori arguments. Nevertheless, the
model demonstrates a performance that rivals specialized models, particularly
in extracting key components and interpreting underlying properties. The
integration of external information into the model's processing significantly
elevates the quality of the generated explanations. Additionally, the model
exhibits a noteworthy capability in augmenting arguments, thus contributing to
the enrichment of the data set.
  Despite facing certain limitations, this thesis makes significant
contributions to the fields of artificial intelligence and logical reasoning.
It introduces novel methodologies, establishes a rigorous evaluation framework,
and provides deep insights that set the stage for future advancements in
automated logical reasoning. The findings and methodologies presented herein
not only underscore the potential of AI in complex reasoning tasks but also
highlight areas for future research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>78 Pages, 16 Figures, Thesis Presentation is available at
  https://drive.google.com/file/d/1wLIBsjfLvO11PjCS6qx4Y9UgRBUfq3wQ/view?usp=sharing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact-based Court Judgment Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Kumar Nigam, Aniket Deroy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This extended abstract extends the research presented in "ILDC for CJPE:
Indian Legal Documents Corpus for Court Judgment Prediction and Explanation"
\cite{malik-etal-2021-ildc}, focusing on fact-based judgment prediction within
the context of Indian legal documents. We introduce two distinct problem
variations: one based solely on facts, and another combining facts with rulings
from lower courts (RLC). Our research aims to enhance early-phase case outcome
prediction, offering significant benefits to legal professionals and the
general public. The results, however, indicated a performance decline compared
to the original ILDC for CJPE study, even after implementing various weightage
schemes in our DELSumm algorithm. Additionally, using only facts for legal
judgment prediction with different transformer models yielded results inferior
to the state-of-the-art outcomes reported in the "ILDC for CJPE" study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Large Language Model Hallucinations via Autonomous Knowledge
  Graph-based Retrofitting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyan Guan, Yanjiang Liu, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating factual knowledge in knowledge graph is regarded as a promising
approach for mitigating the hallucination of large language models (LLMs).
Existing methods usually only use the user's input to query the knowledge
graph, thus failing to address the factual hallucination generated by LLMs
during its reasoning process. To address this problem, this paper proposes
Knowledge Graph-based Retrofitting (KGR), a new framework that incorporates
LLMs with KGs to mitigate factual hallucination during the reasoning process by
retrofitting the initial draft responses of LLMs based on the factual knowledge
stored in KGs. Specifically, KGR leverages LLMs to extract, select, validate,
and retrofit factual statements within the model-generated responses, which
enables an autonomous knowledge verifying and refining procedure without any
additional manual efforts. Experiments show that KGR can significantly improve
the performance of LLMs on factual QA benchmarks especially when involving
complex reasoning processes, which demonstrates the necessity and effectiveness
of KGR in mitigating hallucination and enhancing the reliability of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Radiology Report Generation via Causal Reasoning and
  Counterfactual Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Song, Jiafan Liu, Yun Li, Wenbin Lei, Ruxin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology Report Generation (RRG) draws attention as an interaction between
vision and language fields. Previous works inherited the ideology of
vision-to-language generation tasks,aiming to generate paragraphs with high
consistency as reports. However, one unique characteristic of RRG, the
independence between diseases, was neglected, leading to the injection of the
spurious confounder, i.e., the disease co-occurrence. Unfortunately, this
confounder confuses the process of report generation worse because of the
biased RRG data distribution. In this paper, to rethink this issue thoroughly,
we reason about its causes and effects from a novel perspective of statistics
and causality, where the Joint Vision Coupling and the Conditional Sentence
Coherence Coupling are two aspects prone to implicitly decrease the accuracy of
reports. Then, a counterfactual augmentation strategy that contains the
Counterfactual Sample Synthesis and the Counterfactual Report Reconstruction
sub-methods is proposed to break these two aspects of spurious effects.
Experimental results and further analyses on two widely used datasets justify
our reasoning and proposed methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intention and Context Elicitation with Large Language Models in the
  Legal Aid Intake Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Goodson, Rongfei Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) and chatbots show significant promise in
streamlining the legal intake process. This advancement can greatly reduce the
workload and costs for legal aid organizations, improving availability while
making legal assistance more accessible to a broader audience. However, a key
challenge with current LLMs is their tendency to overconfidently deliver an
immediate 'best guess' to a client's question based on the output distribution
learned over the training data. This approach often overlooks the client's
actual intentions or the specifics of their legal situation. As a result,
clients may not realize the importance of providing essential additional
context or expressing their underlying intentions, which are crucial for their
legal cases. Traditionally, logic based decision trees have been used to
automate intake for specific access to justice issues, such as immigration and
eviction. But those solutions lack scalability. We demonstrate a
proof-of-concept using LLMs to elicit and infer clients' underlying intentions
and specific legal circumstances through free-form, language-based
interactions. We also propose future research directions to use supervised
fine-tuning or offline reinforcement learning to automatically incorporate
intention and context elicitation in chatbots without explicit prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Summarization Performance through <span class="highlight-title">Transformer</span>-Based <span class="highlight-title">Prompt</span>
  Engineering in Automated Medical Reporting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daphne van Zandvoort, Laura Wiersema, Tom Huibers, Sandra van Dulmen, Sjaak Brinkkemper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customized medical prompts enable Large Language Models (LLM) to effectively
address medical dialogue summarization. The process of medical reporting is
often time-consuming for healthcare professionals. Implementing medical
dialogue summarization techniques presents a viable solution to alleviate this
time constraint by generating automated medical reports. The effectiveness of
LLMs in this process is significantly influenced by the formulation of the
prompt, which plays a crucial role in determining the quality and relevance of
the generated reports. In this research, we used a combination of two distinct
prompting strategies, known as shot prompting and pattern prompting to enhance
the performance of automated medical reporting. The evaluation of the automated
medical reports is carried out using the ROUGE score and a human evaluation
with the help of an expert panel. The two-shot prompting approach in
combination with scope and domain context outperforms other methods and
achieves the highest score when compared to the human reference set by a
general practitioner. However, the automated reports are approximately twice as
long as the human references, due to the addition of both redundant and
relevant statements that are added to the report.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, submitted to Healthinf 2024, author roles:
  research conducted and written by Daphne van Zandvoort and Laura Wiersema,
  research suggested and used software created by Tom Huibers, data provided
  and feedback provided by Sandra van Dulmen, supervision and feedback provided
  by Sjaak Brinkkemper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Experimentation of Accuracy Metrics in Automated Medical
  Reporting: The Case of Otitis Consultations <span class="chip">ALT</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wouter Faber, Renske Eline Bootsma, Tom Huibers, Sandra van Dulmen, Sjaak Brinkkemper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Artificial Intelligence (AI) can be used to automatically generate
medical reports based on transcripts of medical consultations. The aim is to
reduce the administrative burden that healthcare professionals face. The
accuracy of the generated reports needs to be established to ensure their
correctness and usefulness. There are several metrics for measuring the
accuracy of AI generated reports, but little work has been done towards the
application of these metrics in medical reporting. A comparative
experimentation of 10 accuracy metrics has been performed on AI generated
medical reports against their corresponding General Practitioner's (GP) medical
reports concerning Otitis consultations. The number of missing, incorrect, and
additional statements of the generated reports have been correlated with the
metric scores. In addition, we introduce and define a Composite Accuracy Score
which produces a single score for comparing the metrics within the field of
automated medical reporting. Findings show that based on the correlation study
and the Composite Accuracy Score, the ROUGE-L and Word Mover's Distance metrics
are the preferred metrics, which is not in line with previous work. These
findings help determine the accuracy of an AI generated medical report, which
aids the development of systems that generate medical reports for GPs to reduce
the administrative burden.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure, submitted to HEALTHINF 2024, Author
  contributions: Wouter Faber and Renske Eline Bootsma performed research and
  wrote paper, Tom Huibers provided needed software and research inspiration,
  Sandra van Dulmen provided the data and feedback on paper, Sjaak Brinkkemper
  supervised the project and provided continuous feedback</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided
  Code-Vision Representation <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyi Chen, Xingyao Wang, Manling Li, Derek Hoiem, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art vision-language models (VLMs) still have limited performance
in structural knowledge extraction, such as relations between objects. In this
work, we present ViStruct, a training framework to learn VLMs for effective
visual structural knowledge extraction. Two novel designs are incorporated.
First, we propose to leverage the inherent structure of programming language to
depict visual structural information. This approach enables explicit and
consistent representation of visual structural information of multiple
granularities, such as concepts, relations, and events, in a well-organized
structured format. Second, we introduce curriculum-based learning for VLMs to
progressively comprehend visual structures, from fundamental visual concepts to
intricate event structures. Our intuition is that lower-level knowledge may
contribute to complex visual structure understanding. Furthermore, we compile
and release a collection of datasets tailored for visual structural knowledge
extraction. We adopt a weakly-supervised approach to directly generate visual
event structures from captions for ViStruct training, capitalizing on abundant
image-caption pairs from the web. In experiments, we evaluate ViStruct on
visual structure prediction tasks, demonstrating its effectiveness in improving
the understanding of visual structures. The code is public at
\url{https://github.com/Yangyi-Chen/vi-struct}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Instruction Optimization for Open-source LLM Instruction
  Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Liu, Shimin Tao, Xiaofeng Zhao, Ming Zhu, Wenbing Ma, Junhao Zhu, Chang Su, Yutai Hou, Miao Zhang, Min Zhang, Hongxia Ma, Li Zhang, Hao Yang, Yanfei Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning is crucial for enabling Language Learning Models (LLMs) in
responding to human instructions. The quality of instruction pairs used for
tuning greatly affects the performance of LLMs. However, the manual creation of
high-quality instruction datasets is costly, leading to the adoption of
automatic generation of instruction pairs by LLMs as a popular alternative in
the training of open-source LLMs. To ensure the high quality of LLM-generated
instruction datasets, several approaches have been proposed. Nevertheless,
existing methods either compromise dataset integrity by filtering a large
proportion of samples, or are unsuitable for industrial applications. In this
paper, instead of discarding low-quality samples, we propose CoachLM, a novel
approach to enhance the quality of instruction datasets through automatic
revisions on samples in the dataset. CoachLM is trained from the samples
revised by human experts and significantly increases the proportion of
high-quality samples in the dataset from 17.7% to 78.9%. The effectiveness of
CoachLM is further assessed on various real-world instruction test sets. The
results show that CoachLM improves the instruction-following capabilities of
the instruction-tuned LLM by an average of 29.9%, which even surpasses larger
LLMs with nearly twice the number of parameters. Furthermore, CoachLM is
successfully deployed in a data management system for LLMs at Huawei, resulting
in an efficiency improvement of up to 20% in the cleaning of 40k real-world
instruction pairs. We release the training data and code of CoachLM
(https://github.com/lunyiliu/CoachLM).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Calibration of Large Language Models and Alignment <span class="chip">EMNLP-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiwei Zhu, Benfeng Xu, Quan Wang, Yongdong Zhang, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models attract increasing attention and find widespread
application, concurrent challenges of reliability also arise at the same time.
Confidence calibration, an effective analysis method for gauging the
reliability of deep models, serves as a crucial tool for assessing and
improving their reliability. However, such investigation has been comparatively
underexplored. In this work, we conduct a systematic examination of the
calibration of aligned language models throughout the entire construction
process, including pretraining and alignment training. At each stage, we
investigate how different training settings, such as parameter scales and
training data, affect model calibration. To thoroughly assess model
calibration, we evaluate models on three most concerned aspects: generation,
factuality and understanding. Our work sheds light on whether popular LLMs are
well-calibrated and how the training process influences model calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in findings of EMNLP-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang, Luoyi Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have gained significant popularity for their
impressive performance across diverse fields. However, LLMs are prone to
hallucinate untruthful or nonsensical outputs that fail to meet user
expectations in many real-world applications. Existing works for detecting
hallucinations in LLMs either rely on external knowledge for reference
retrieval or require sampling multiple responses from the LLM for consistency
verification, making these methods costly and inefficient. In this paper, we
propose a novel reference-free, uncertainty-based method for detecting
hallucinations in LLMs. Our approach imitates human focus in factuality
checking from three aspects: 1) focus on the most informative and important
keywords in the given text; 2) focus on the unreliable tokens in historical
context which may lead to a cascade of hallucinations; and 3) focus on the
token properties such as token type and token frequency. Experimental results
on relevant datasets demonstrate the effectiveness of our proposed method,
which achieves state-of-the-art performance across all the evaluation metrics
and eliminates the need for additional information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2023 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AS-LLM: When Algorithm Selection Meets Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Wu, Yan Zhong, Jibin Wu, Kay Chen Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithm selection aims to identify the most suitable algorithm for solving
a specific problem before execution, which has become a critical process of the
AutoML. Current mainstream algorithm selection techniques rely heavily on
feature representations of various problems and employ the performance of each
algorithm as supervised information. However, there is a significant research
gap concerning the consideration of algorithm features. This gap is primarily
attributed to the inherent complexity of algorithms, making it particularly
challenging to find a universally effective feature extraction method that is
applicable across a diverse range of algorithms. Unfortunately, neglecting this
aspect undoubtedly impacts the accuracy of algorithm selection and indirectly
necessitates an increased volume of problem data for training purposes. This
paper takes a significant stride towards addressing this gap by proposing an
approach that integrates algorithm representation into the algorithm selection
process. Specifically, our proposed model employs distinct modules to extract
representations of both problems and algorithms, where the algorithm
representation leverages the capabilities of pre-trained LLMs in the realm of
code comprehension. Following the extraction of embedding vectors for both
algorithms and problems, the most suitable algorithm is determined through
calculations of matching degrees. Our experiments not only validate the
effectiveness of the proposed model but also showcase the performance of
different embedded pre-trained LLMs, which suggests that the proposed algorithm
selection framework holds the potential to serve as a baseline task for
evaluating the code representation capabilities of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComPEFT: Compression for Communicating Parameter Efficient Updates via
  Sparsification and Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Yadav, Leshem Choshen, Colin Raffel, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) techniques make it possible to
efficiently adapt a language model to create "expert" models that specialize to
new tasks or domains. Recent techniques in model merging and compositional
generalization leverage these expert models by dynamically composing modules to
improve zero/few-shot generalization. Despite the efficiency of PEFT methods,
the size of expert models can make it onerous to retrieve expert models per
query over high-latency networks like the Internet or serve multiple experts on
a single GPU. To address these issues, we present ComPEFT, a novel method for
compressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT
employs sparsification and ternary quantization to reduce the size of the PEFT
module without performing any additional retraining while preserving or
enhancing model performance. In extensive evaluation across T5, T0, and
LLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression
ratios of 8x - 50x. In particular, we show that ComPEFT improves with scale -
stronger models exhibit higher compressibility and better performance. For
example, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on
MMLU with a storage size reduction of up to 26x. In addition, we show that the
compressed experts produced by ComPEFT maintain few-shot compositional
generalization capabilities, facilitate efficient communication and
computation, and exhibit enhanced performance when merged. Lastly, we provide
an analysis of different method components, compare it with other PEFT methods,
and test ComPEFT's efficacy for compressing the residual of full-finetuning.
Our code is available at https://github.com/prateeky2806/compeft.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 Pages, 6 Figures, 16 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditi Jha, Sam Havens, Jeremey Dohmann, Alex Trott, Jacob Portes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models are traditionally finetuned on large instruction
datasets. However recent studies suggest that small, high-quality datasets can
suffice for general purpose instruction following. This lack of consensus
surrounding finetuning best practices is in part due to rapidly diverging
approaches to LLM evaluation. In this study, we ask whether a small amount of
diverse finetuning samples can improve performance on both traditional
perplexity-based NLP benchmarks, and on open-ended, model-based evaluation. We
finetune open-source MPT-7B and MPT-30B models on instruction finetuning
datasets of various sizes ranging from 1k to 60k samples. We find that subsets
of 1k-6k instruction finetuning samples are sufficient to achieve good
performance on both (1) traditional NLP benchmarks and (2) model-based
evaluation. Finally, we show that mixing textbook-style and open-ended QA
finetuning datasets optimizes performance on both evaluation paradigms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 12 figures, NeurIPS 2023 Workshop on Instruction Tuning and
  Instruction Following</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Better Parameter-Efficient Fine-Tuning for Large Language
  Models: A Position Paper 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyu Wang, Junbing Yan, Wei Zhang, Jun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper delves into the pressing need in Parameter-Efficient Fine-Tuning
(PEFT) for Large Language Models (LLMs). While LLMs possess remarkable
capabilities, their extensive parameter requirements and associated
computational demands hinder their practicality and scalability for real-world
applications. Our position paper highlights current states and the necessity of
further studying into the topic, and recognizes significant challenges and open
issues that must be addressed to fully harness the powerful abilities of LLMs.
These challenges encompass novel efficient PEFT architectures, PEFT for
different learning settings, PEFT combined with model compression techniques,
and the exploration of PEFT for multi-modal LLMs. By presenting this position
paper, we aim to stimulate further research and foster discussions surrounding
more efficient and accessible PEFT for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combatting Human Trafficking in the Cyberspace: A Natural Language
  Processing-Based Methodology to Analyze the Language in Online Advertisements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Rodriguez Perez, Pablo Rivas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project tackles the pressing issue of human trafficking in online C2C
marketplaces through advanced Natural Language Processing (NLP) techniques. We
introduce a novel methodology for generating pseudo-labeled datasets with
minimal supervision, serving as a rich resource for training state-of-the-art
NLP models. Focusing on tasks like Human Trafficking Risk Prediction (HTRP) and
Organized Activity Detection (OAD), we employ cutting-edge Transformer models
for analysis. A key contribution is the implementation of an interpretability
framework using Integrated Gradients, providing explainable insights crucial
for law enforcement. This work not only fills a critical gap in the literature
but also offers a scalable, machine learning-driven approach to combat human
exploitation online. It serves as a foundation for future research and
practical applications, emphasizing the role of machine learning in addressing
complex social issues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ White-Box <span class="highlight-title">Transformer</span>s via Sparse Rate Reduction: Compression Is All
  There Is? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Hao Bai, Yuexiang Zhai, Benjamin D. Haeffele, Yi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we contend that a natural objective of representation learning
is to compress and transform the distribution of the data, say sets of tokens,
towards a low-dimensional Gaussian mixture supported on incoherent subspaces.
The goodness of such a representation can be evaluated by a principled measure,
called sparse rate reduction, that simultaneously maximizes the intrinsic
information gain and extrinsic sparsity of the learned representation. From
this perspective, popular deep network architectures, including transformers,
can be viewed as realizing iterative schemes to optimize this measure.
Particularly, we derive a transformer block from alternating optimization on
parts of this objective: the multi-head self-attention operator compresses the
representation by implementing an approximate gradient descent step on the
coding rate of the features, and the subsequent multi-layer perceptron
sparsifies the features. This leads to a family of white-box transformer-like
deep network architectures, named CRATE, which are mathematically fully
interpretable. We show, by way of a novel connection between denoising and
compression, that the inverse to the aforementioned compressive encoding can be
realized by the same class of CRATE architectures. Thus, the so-derived
white-box architectures are universal to both encoders and decoders.
Experiments show that these networks, despite their simplicity, indeed learn to
compress and sparsify representations of large-scale real-world image and text
datasets, and achieve performance very close to highly engineered
transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the
proposed computational framework demonstrates great potential in bridging the
gap between theory and practice of deep learning, from a unified perspective of
data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper integrates the works arXiv:2306.01129 and
  arXiv:2308.16271, as well as this under-review work:
  https://openreview.net/forum?id=PvyOYleymy into a complete story. In this
  paper, we improve the writing and organization, and also add conceptual,
  empirical, and theoretical improvements over the previous work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceptual Structure in the Absence of Grounding for LLMs: The Impact of
  Abstractedness and Subjectivity in Color Language <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Loyola, Edison Marrese-Taylor, Andres Hoyos-Idobro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need for grounding in language understanding is an active research topic.
Previous work has suggested that color perception and color language appear as
a suitable test bed to empirically study the problem, given its cognitive
significance and showing that there is considerable alignment between a defined
color space and the feature space defined by a language model. To further study
this issue, we collect a large scale source of colors and their descriptions,
containing almost a 1 million examples , and perform an empirical analysis to
compare two kinds of alignments: (i) inter-space, by learning a mapping between
embedding space and color space, and (ii) intra-space, by means of prompting
comparatives between color descriptions. Our results show that while color
space alignment holds for monolexemic, highly pragmatic color descriptions,
this alignment drops considerably in the presence of examples that exhibit
elements of real linguistic usage such as subjectivity and abstractedness,
suggesting that grounding may be required in such cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting out-of-distribution text using topological features of
  <span class="highlight-title">transformer</span>-based language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andres Pollano, Anupam Chaudhuri, Anj Simmons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We attempt to detect out-of-distribution (OOD) text samples though applying
Topological Data Analysis (TDA) to attention maps in transformer-based language
models. We evaluate our proposed TDA-based approach for out-of-distribution
detection on BERT, a transformer-based language model, and compare the to a
more traditional OOD approach based on BERT CLS embeddings. We found that our
TDA approach outperforms the CLS embedding approach at distinguishing
in-distribution data (politics and entertainment news articles from HuffPost)
from far out-of-domain samples (IMDB reviews), but its effectiveness
deteriorates with near out-of-domain (CNN/Dailymail) or same-domain (business
news articles from HuffPost) datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Logical Reasoning in Large Language Models to Facilitate Legal
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ha-Thanh Nguyen, Wachara Fungwacharakorn, Ken Satoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language serves as a vehicle for conveying thought, enabling communication
among individuals. The ability to distinguish between diverse concepts,
identify fairness and injustice, and comprehend a range of legal notions
fundamentally relies on logical reasoning. Large Language Models (LLMs) attempt
to emulate human language understanding and generation, but their competency in
logical reasoning remains limited. This paper seeks to address the
philosophical question: How can we effectively teach logical reasoning to LLMs
while maintaining a deep understanding of the intricate relationship between
language and logic? By focusing on bolstering LLMs' capabilities in logical
reasoning, we aim to expand their applicability in law and other
logic-intensive disciplines. To this end, we propose a Reinforcement Learning
from Logical Feedback (RLLF) approach, which serves as a potential framework
for refining LLMs' reasoning capacities. Through RLLF and a revised evaluation
methodology, we explore new avenues for research in this domain and contribute
to the development of LLMs capable of handling complex legal reasoning tasks
while acknowledging the fundamental connection between language and logic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ALP@JURIX2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphCFC: A Directed Graph Based Cross-Modal Feature Complementation
  Approach for Multimodal Conversational Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12261v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12261v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Li, Xiaoping Wang, Guoqing Lv, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation (ERC) plays a significant part in
Human-Computer Interaction (HCI) systems since it can provide empathetic
services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.
Recently, Graph Neural Networks (GNNs) have been widely used in a variety of
fields due to their superior performance in relation modeling. In multimodal
ERC, GNNs are capable of extracting both long-distance contextual information
and inter-modal interactive information. Unfortunately, since existing methods
such as MMGCN directly fuse multiple modalities, redundant information may be
generated and diverse information may be lost. In this work, we present a
directed Graph based Cross-modal Feature Complementation (GraphCFC) module that
can efficiently model contextual and interactive information. GraphCFC
alleviates the problem of heterogeneity gap in multimodal fusion by utilizing
multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)
strategy. We extract various types of edges from the constructed graph for
encoding, thus enabling GNNs to extract crucial contextual and interactive
information more accurately when performing message passing. Furthermore, we
design a GNN structure called GAT-MLP, which can provide a new unified network
framework for multimodal learning. The experimental results on two benchmark
datasets show that our GraphCFC outperforms the state-of-the-art (SOTA)
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Multimedia (TMM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating <span class="highlight-title">Pre-train</span>ed Language Model into Neural Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19680v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19680v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soon-Jae Hwang, Chang-Sung Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Machine Translation (NMT) has become a significant technology in
natural language processing through extensive research and development.
However, the deficiency of high-quality bilingual language pair data still
poses a major challenge to improving NMT performance. Recent studies have been
exploring the use of contextual information from pre-trained language model
(PLM) to address this problem. Yet, the issue of incompatibility between PLM
and NMT model remains unresolved. This study proposes PLM-integrated NMT
(PiNMT) model to overcome the identified problems. PiNMT model consists of
three critical components, PLM Multi Layer Converter, Embedding Fusion, and
Cosine Alignment, each playing a vital role in providing effective PLM
information to NMT. Furthermore, two training strategies, Separate Learning
Rates and Dual Step Training, are also introduced in this paper. By
implementing the proposed PiNMT model and training strategy, we achieve
state-of-the-art performance on the IWSLT'14 En$\leftrightarrow$De dataset.
This study's outcomes are noteworthy as they demonstrate a novel approach for
efficiently integrating PLM with NMT to overcome incompatibility and enhance
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Dual-Stream Recurrence-Attention Network With Global-Local Awareness
  for Emotion Recognition in Textual Dialog 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Li, Xiaoping Wang, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world dialog systems, the ability to understand the user's emotions
and interact anthropomorphically is of great significance. Emotion Recognition
in Conversation (ERC) is one of the key ways to accomplish this goal and has
attracted growing attention. How to model the context in a conversation is a
central aspect and a major challenge of ERC tasks. Most existing approaches
struggle to adequately incorporate both global and local contextual
information, and their network structures are overly sophisticated. For this
reason, we propose a simple and effective Dual-stream Recurrence-Attention
Network (DualRAN), which is based on Recurrent Neural Network (RNN) and
Multi-head ATtention network (MAT). DualRAN eschews the complex components of
current methods and focuses on combining recurrence-based methods with
attention-based ones. DualRAN is a dual-stream structure mainly consisting of
local- and global-aware modules, modeling a conversation simultaneously from
distinct perspectives. In addition, we develop two single-stream network
variants for DualRAN, i.e., SingleRANv1 and SingleRANv2. According to the
experimental findings, DualRAN boosts the weighted F1 scores by 1.43% and 0.64%
on the IEMOCAP and MELD datasets, respectively, in comparison to the strongest
baseline. On two other datasets (i.e., EmoryNLP and DailyDialog), our method
also attains competitive results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Engineering Applications of Artificial Intelligence
  (EAAI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning Principles for In-Context Learning with Large Language
  Models <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katerina Margatina, Timo Schick, Nikolaos Aletras, Jane Dwivedi-Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable advancements in large language models (LLMs) have
significantly enhanced the performance in few-shot learning settings. By using
only a small number of labeled examples, referred to as demonstrations, LLMs
can effectively grasp the task at hand through in-context learning. However,
the process of selecting appropriate demonstrations has received limited
attention in prior work. This paper addresses the issue of identifying the most
informative demonstrations for few-shot learning by approaching it as a
pool-based Active Learning (AL) problem over a single iteration. Our objective
is to investigate how AL algorithms can serve as effective demonstration
selection methods for in-context learning. We compare various standard AL
algorithms based on uncertainty, diversity, and similarity, and consistently
observe that the latter outperforms all other methods, including random
sampling. Notably, uncertainty sampling, despite its success in conventional
supervised learning scenarios, performs poorly in this context. Our extensive
experimentation involving a diverse range of GPT and OPT models across $24$
classification and multi-choice tasks, coupled with thorough analysis,
unambiguously demonstrates that in-context example selection through AL
prioritizes high-quality examples that exhibit low uncertainty and bear
similarity to the test examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at Findings of EMNLP (Camera Ready version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongjin Yang, Joonkee Kim, Yujin Kim, Namgyu Ho, James Thorne, Se-young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the proliferation of social media, accurate detection of hate speech has
become critical to ensure safety online. To combat nuanced forms of hate
speech, it is important to identify and thoroughly explain hate speech to help
users understand its harmful effects. Recent benchmarks have attempted to
tackle this issue by training generative models on free-text annotations of
implications in hateful text. However, we find significant reasoning gaps in
the existing annotations schemes, which may hinder the supervision of detection
models. In this paper, we introduce a hate speech detection framework, HARE,
which harnesses the reasoning capabilities of large language models (LLMs) to
fill these gaps in explanations of hate speech, thus enabling effective
supervision of detection models. Experiments on SBIC and Implicit Hate
benchmarks show that our method, using model-generated data, consistently
outperforms baselines, using existing free-text human annotations. Analysis
demonstrates that our method enhances the explanation quality of trained models
and improves generalization to unseen datasets. Our code is available at
https://github.com/joonkeekim/hare-hate-speech.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2023; The first three authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Learning Functions with Varying Number of Minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Oniani, Yanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have proven effective at In-Context Learning
(ICL), an ability that allows them to create predictors from labeled examples.
Few studies have explored the interplay between ICL and specific properties of
functions it attempts to approximate. In our study, we use a formal framework
to explore ICL and propose a new task of approximating functions with varying
number of minima. We implement a method that allows for producing functions
with given inputs as minima. We find that increasing the number of minima
degrades ICL performance. At the same time, our evaluation shows that ICL
outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster
than 2NN in all settings. We validate the findings through a set of few-shot
experiments across various hyperparameter configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faithful Explanations of Black-box NLP Models Using LLM-generated
  Counterfactuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal explanations of the predictions of NLP systems are essential to ensure
safety and establish trust. Yet, existing methods often fall short of
explaining model predictions effectively or efficiently and are often
model-specific. In this paper, we address model-agnostic explanations,
proposing two approaches for counterfactual (CF) approximation. The first
approach is CF generation, where a large language model (LLM) is prompted to
change a specific text concept while keeping confounding concepts unchanged.
While this approach is demonstrated to be very effective, applying LLM at
inference-time is costly. We hence present a second approach based on matching,
and propose a method that is guided by an LLM at training-time and learns a
dedicated embedding space. This space is faithful to a given causal graph and
effectively serves to identify matches that approximate CFs. After showing
theoretically that approximating CFs is required in order to construct faithful
explanations, we benchmark our approaches and explain several models, including
LLMs with billions of parameters. Our empirical results demonstrate the
excellent performance of CF generation models as model-agnostic explainers.
Moreover, our matching approach, which requires far less test-time resources,
also provides effective explanations, surpassing many baselines. We also find
that Top-K techniques universally improve every tested method. Finally, we
showcase the potential of LLMs in constructing new benchmarks for model
explanation and subsequently validate our conclusions. Our work illuminates new
pathways for efficient and accurate approaches to interpreting NLP systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FreshLLMs: Refreshing Large Language Models with Search Engine
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, Thang Luong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most large language models (LLMs) are trained once and never updated; thus,
they lack the ability to dynamically adapt to our ever-changing world. In this
work, we perform a detailed study of the factuality of LLM-generated text in
the context of answering questions that test current world knowledge.
Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a
diverse range of question and answer types, including questions that require
fast-changing world knowledge as well as questions with false premises that
need to be debunked. We benchmark a diverse array of both closed and
open-source LLMs under a two-mode evaluation procedure that allows us to
measure both correctness and hallucination. Through human evaluations involving
more than 50K judgments, we shed light on limitations of these models and
demonstrate significant room for improvement: for instance, all models
(regardless of model size) struggle on questions that involve fast-changing
knowledge and false premises. Motivated by these results, we present
FreshPrompt, a simple few-shot prompting method that substantially boosts the
performance of an LLM on FreshQA by incorporating relevant and up-to-date
information retrieved from a search engine into the prompt. Our experiments
show that FreshPrompt outperforms both competing search engine-augmented
prompting methods such as Self-Ask (Press et al., 2022) as well as commercial
systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that
both the number of retrieved evidences and their order play a key role in
influencing the correctness of LLM-generated answers. Additionally, instructing
the LLM to generate concise and direct answers helps reduce hallucination
compared to encouraging more verbose answers. To facilitate future work, we
release FreshQA at github.com/freshllms/freshqa and commit to updating it at
regular intervals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, 26 pages, 10 figures, 5 tables; Added FreshEval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lifelong Sequence Generation with Dynamic Module Expansion and
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09886v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09886v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwei Qin, Chen Chen, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lifelong sequence generation (LSG), a problem in continual learning, aims to
continually train a model on a sequence of generation tasks to learn constantly
emerging new generation patterns while avoiding the forgetting of previous
knowledge. Existing LSG methods mainly focus on maintaining old knowledge while
paying little attention to knowledge transfer across tasks. In contrast, humans
can better learn new tasks by leveraging previously acquired knowledge from
similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic
Module Expansion and Adaptation (DMEA), which enables the model to dynamically
determine the architecture for acquiring new knowledge based on task
correlation and select the most similar previous tasks to facilitate adaptation
to new tasks. In addition, as the learning process can easily be biased towards
the current task which might cause more severe forgetting of previously learned
knowledge, we propose dynamic gradient scaling to balance the learning of the
current task and replayed tasks. With extensive experiments, we demonstrate
that DMEA can consistently outperform existing methods in different LSG
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Representational Capacity of Recurrent Neural Language Models <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12942v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12942v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franz Nowak, Anej Svete, Li Du, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates the computational expressivity of language models
(LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992)
famously showed that RNNs with rational weights and hidden states and unbounded
computation time are Turing complete. However, LMs define weightings over
strings in addition to just (unweighted) language membership and the analysis
of the computational power of RNN LMs (RLMs) should reflect this. We extend the
Turing completeness result to the probabilistic case, showing how a rationally
weighted RLM with unbounded computation time can simulate any deterministic
probabilistic Turing machine (PTM) with rationally weighted transitions. Since,
in practice, RLMs work in real-time, processing a symbol at every time step, we
treat the above result as an upper bound on the expressivity of RLMs. We also
provide a lower bound by showing that under the restriction to real-time
computation, such models can simulate deterministic real-time rational PTMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published at EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">103</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Augmented Layout <span class="highlight-title">Transformer</span> for Content-Aware Layout
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daichi Horita, Naoto Inoue, Kotaro Kikuchi, Kota Yamaguchi, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content-aware graphic layout generation aims to automatically arrange visual
elements along with a given content, such as an e-commerce product image. In
this paper, we argue that the current layout generation approaches suffer from
the limited training data for the high-dimensional layout structure. We show
that a simple retrieval augmentation can significantly improve the generation
quality. Our model, which is named Retrieval-Augmented Layout Transformer
(RALF), retrieves nearest neighbor layout examples based on an input image and
feeds these results into an autoregressive generator. Our model can apply
retrieval augmentation to various controllable generation tasks and yield
high-quality layouts within a unified architecture. Our extensive experiments
show that RALF successfully generates content-aware layouts in both constrained
and unconstrained settings and significantly outperforms the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Webpage: https://udonda.github.io/RALF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual In-Context <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Chunyuan Li, Jianwei Yang, Lei Zhang, Jianfeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context prompting in large language models (LLMs) has become a prevalent
approach to improve zero-shot capabilities, but this idea is less explored in
the vision domain. Existing visual prompting methods focus on referring
segmentation to segment the most relevant object, falling short of addressing
many generic vision tasks like open-set segmentation and detection. In this
paper, we introduce a universal visual in-context prompting framework for both
tasks. In particular, we build on top of an encoder-decoder architecture, and
develop a versatile prompt encoder to support a variety of prompts like
strokes, boxes, and points. We further enhance it to take an arbitrary number
of reference image segments as the context. Our extensive explorations show
that the proposed visual in-context prompting elicits extraordinary referring
and generic segmentation capabilities to refer and detect, yielding competitive
performance to close-set in-domain datasets and showing promising results on
many open-set segmentation datasets. By joint training on COCO and SA-1B, our
model achieves $57.7$ PQ on COCO and $23.2$ PQ on ADE20K. Code will be
available at https://github.com/UX-Decoder/DINOv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, Varun Jampani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods for finetuning generative models for concept-driven personalization
generally achieve strong results for subject-driven or style-driven generation.
Recently, low-rank adaptations (LoRA) have been proposed as a
parameter-efficient way of achieving concept-driven personalization. While
recent work explores the combination of separate LoRAs to achieve joint
generation of learned styles and subjects, existing techniques do not reliably
address the problem; they often compromise either subject fidelity or style
fidelity. We propose ZipLoRA, a method to cheaply and effectively merge
independently trained style and subject LoRAs in order to achieve generation of
any user-provided subject in any user-provided style. Experiments on a wide
range of subject and style combinations show that ZipLoRA can generate
compelling results with meaningful improvements over baselines in subject and
style fidelity while preserving the ability to recontextualize. Project page:
https://ziplora.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ziplora.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T-Rex: Counting by Visual <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Jiang, Feng Li, Tianhe Ren, Shilong Liu, Zhaoyang Zeng, Kent Yu, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce T-Rex, an interactive object counting model designed to first
detect and then count any objects. We formulate object counting as an open-set
object detection task with the integration of visual prompts. Users can specify
the objects of interest by marking points or boxes on a reference image, and
T-Rex then detects all objects with a similar pattern. Guided by the visual
feedback from T-Rex, users can also interactively refine the counting results
by prompting on missing or falsely-detected objects. T-Rex has achieved
state-of-the-art performance on several class-agnostic counting benchmarks. To
further exploit its potential, we established a new counting benchmark
encompassing diverse scenarios and challenges. Both quantitative and
qualitative results show that T-Rex possesses exceptional zero-shot counting
capabilities. We also present various practical application scenarios for
T-Rex, illustrating its potential in the realm of visual prompting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XAGen: 3D Expressive Human Avatars Generation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Jiashi Feng, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in 3D-aware GAN models have enabled the generation of
realistic and controllable human body images. However, existing methods focus
on the control of major body joints, neglecting the manipulation of expressive
attributes, such as facial expressions, jaw poses, hand poses, and so on. In
this work, we present XAGen, the first 3D generative model for human avatars
capable of expressive control over body, face, and hands. To enhance the
fidelity of small-scale regions like face and hands, we devise a multi-scale
and multi-part 3D representation that models fine details. Based on this
representation, we propose a multi-part rendering technique that disentangles
the synthesis of body, face, and hands to ease model training and enhance
geometric quality. Furthermore, we design multi-part discriminators that
evaluate the quality of the generated avatars with respect to their appearance
and fine-grained control capabilities. Experiments show that XAGen surpasses
state-of-the-art methods in terms of realism, diversity, and expressive control
abilities. Code and data will be made available at
https://showlab.github.io/xagen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023, Project Page at
  https://showlab.github.io/xagen</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katja Schwarz, Seung Wook Kim, Jun Gao, Sanja Fidler, Andreas Geiger, Karsten Kreis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern learning-based approaches to 3D-aware image synthesis achieve high
photorealism and 3D-consistent viewpoint changes for the generated images.
Existing approaches represent instances in a shared canonical space. However,
for in-the-wild datasets a shared canonical system can be difficult to define
or might not even exist. In this work, we instead model instances in view
space, alleviating the need for posed images and learned camera distributions.
We find that in this setting, existing GAN-based methods are prone to
generating flat geometry and struggle with distribution coverage. We hence
propose WildFusion, a new approach to 3D-aware image synthesis based on latent
diffusion models (LDMs). We first train an autoencoder that infers a compressed
latent representation, which additionally captures the images' underlying 3D
structure and enables not only reconstruction but also novel view synthesis. To
learn a faithful 3D representation, we leverage cues from monocular depth
prediction. Then, we train a diffusion model in the 3D-aware latent space,
thereby enabling synthesis of high-quality 3D-consistent image samples,
outperforming recent state-of-the-art GAN-based methods. Importantly, our
3D-aware LDM is trained without any direct supervision from multiview images or
3D geometry and does not require posed images or learned pose or camera
distributions. It directly learns a 3D representation without relying on
canonical camera coordinates. This opens up promising research avenues for
scalable 3D-aware image synthesis and 3D content creation from in-the-wild
image data. See https://katjaschwarz.github.io/wildfusion for videos of our 3D
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soulstyler: Using Large Language Model to Guide Image Style Transfer for
  Target Object <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Chen, Peng Rong, Jingbo Sun, Chao Li, Xiang Li, Hongwu Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image style transfer occupies an important place in both computer graphics
and computer vision. However, most current methods require reference to
stylized images and cannot individually stylize specific objects. To overcome
this limitation, we propose the "Soulstyler" framework, which allows users to
guide the stylization of specific objects in an image through simple textual
descriptions. We introduce a large language model to parse the text and
identify stylization goals and specific styles. Combined with a CLIP-based
semantic visual embedding encoder, the model understands and matches text and
image content. We also introduce a novel localized text-image block matching
loss that ensures that style transfer is performed only on specified target
objects, while non-target regions remain in their original style. Experimental
results demonstrate that our model is able to accurately perform style transfer
on target objects according to textual descriptions without affecting the style
of background regions. Our code will be available at
https://github.com/yisuanwang/Soulstyler.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages,3 figures,ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning-based Real-time Handgun Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Elmir, Sid Ahmed Laouar, Larbi Hamdaoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional surveillance systems rely on human attention, limiting their
effectiveness. This study employs convolutional neural networks and transfer
learning to develop a real-time computer vision system for automatic handgun
detection. Comprehensive analysis of online handgun detection methods is
conducted, emphasizing reducing false positives and learning time. Transfer
learning is demonstrated as an effective approach. Despite technical
challenges, the proposed system achieves a precision rate of 84.74%,
demonstrating promising performance comparable to related works, enabling
faster learning and accurate automatic handgun detection for enhanced security.
This research advances security measures by reducing human monitoring
dependence, showcasing the potential of transfer learning-based approaches for
efficient and reliable handgun detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADriver-I: A General World Model for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Yuqing Wen, Chi Zhang, Xiangyu Zhang, Tiancai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Typically, autonomous driving adopts a modular design, which divides the full
stack into perception, prediction, planning and control parts. Though
interpretable, such modular design tends to introduce a substantial amount of
redundancy. Recently, multimodal large language models (MLLM) and diffusion
techniques have demonstrated their superior performance on comprehension and
generation ability. In this paper, we first introduce the concept of
interleaved vision-action pair, which unifies the format of visual features and
control signals. Based on the vision-action pairs, we construct a general world
model based on MLLM and diffusion model for autonomous driving, termed
ADriver-I. It takes the vision-action pairs as inputs and autoregressively
predicts the control signal of the current frame. The generated control signals
together with the historical vision-action pairs are further conditioned to
predict the future frames. With the predicted next frame, ADriver-I performs
further control signal prediction. Such a process can be repeated infinite
times, ADriver-I achieves autonomous driving in the world created by itself.
Extensive experiments are conducted on nuScenes and our large-scale private
datasets. ADriver-I shows impressive performance compared to several
constructed baselines. We hope our ADriver-I can provide some new insights for
future autonomous driving and embodied intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Image Retrieval Using <span class="highlight-title">Pretrain</span>ed Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnaz Khun Jush, Tuan Truong, Steffen Vogler, Matthias Lenga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A wide range of imaging techniques and data formats available for medical
images make accurate retrieval from image databases challenging.
  Efficient retrieval systems are crucial in advancing medical research,
enabling large-scale studies and innovative diagnostic tools. Thus, addressing
the challenges of medical image retrieval is essential for the continued
enhancement of healthcare and research.
  In this study, we evaluated the feasibility of employing four
state-of-the-art pretrained models for medical image retrieval at modality,
body region, and organ levels and compared the results of two similarity
indexing approaches. Since the employed networks take 2D images, we analyzed
the impacts of weighting and sampling strategies to incorporate 3D information
during retrieval of 3D volumes. We showed that medical image retrieval is
feasible using pretrained networks without any additional training or
fine-tuning steps. Using pretrained embeddings, we achieved a recall of 1 for
various tasks at modality, body region, and organ level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffusionMat: Alpha Matting as Sequential Refinement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyang Xu, Shengfeng He, Wenqi Shao, Kwan-Yee K. Wong, Yu Qiao, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce DiffusionMat, a novel image matting framework
that employs a diffusion model for the transition from coarse to refined alpha
mattes. Diverging from conventional methods that utilize trimaps merely as
loose guidance for alpha matte prediction, our approach treats image matting as
a sequential refinement learning process. This process begins with the addition
of noise to trimaps and iteratively denoises them using a pre-trained diffusion
model, which incrementally guides the prediction towards a clean alpha matte.
The key innovation of our framework is a correction module that adjusts the
output at each denoising step, ensuring that the final result is consistent
with the input image's structures. We also introduce the Alpha Reliability
Propagation, a novel technique designed to maximize the utility of available
guidance by selectively enhancing the trimap regions with confident alpha
information, thus simplifying the correction task. To train the correction
module, we devise specialized loss functions that target the accuracy of the
alpha matte's edges and the consistency of its opaque and transparent regions.
We evaluate our model across several image matting benchmarks, and the results
indicate that DiffusionMat consistently outperforms existing methods. Project
page at~\url{https://cnnlstm.github.io/DiffusionMat
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging CNNs and Ensemble Learning for Automated Disaster Image
  Classification <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archit Rathod, Veer Pariawala, Mokshit Surana, Kumkum Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural disasters act as a serious threat globally, requiring effective and
efficient disaster management and recovery. This paper focuses on classifying
natural disaster images using Convolutional Neural Networks (CNNs). Multiple
CNN architectures were built and trained on a dataset containing images of
earthquakes, floods, wildfires, and volcanoes. A stacked CNN ensemble approach
proved to be the most effective, achieving 95% accuracy and an F1 score going
up to 0.96 for individual classes. Tuning hyperparameters of individual models
for optimization was critical to maximize the models' performance. The stacking
of CNNs with XGBoost acting as the meta-model utilizes the strengths of the CNN
and ResNet models to improve the overall accuracy of the classification.
Results obtained from the models illustrated the potency of CNN-based models
for automated disaster image classification. This lays the foundation for
expanding these techniques to build robust systems for disaster response,
damage assessment, and recovery management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures, 4 tables, ICSISCET 2023 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Whale-Mud-Ring Optimization for Precise Color Skin Cancer Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Hamza, Badis Lekouaghet, Yassine Himeur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timely identification and treatment of rapidly progressing skin cancers can
significantly contribute to the preservation of patients' health and
well-being. Dermoscopy, a dependable and accessible tool, plays a pivotal role
in the initial stages of skin cancer detection. Consequently, the effective
processing of digital dermoscopy images holds significant importance in
elevating the accuracy of skin cancer diagnoses. Multilevel thresholding is a
key tool in medical imaging that extracts objects within the image to
facilitate its analysis. In this paper, an enhanced version of the Mud Ring
Algorithm hybridized with the Whale Optimization Algorithm, named WMRA, is
proposed. The proposed approach utilizes bubble-net attack and mud ring
strategy to overcome stagnation in local optima and obtain optimal thresholds.
The experimental results show that WMRA is powerful against a cluster of recent
methods in terms of fitness, Peak Signal to Noise Ratio (PSNR), and Mean Square
Error (MSE).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep-learning-based acceleration of MRI for radiotherapy planning of
  pediatric patients with brain tumors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahinur Alam, Jinsoo Uh, Alexander Dresner, Chia-ho Hua, Khaled Khairy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic Resonance Imaging (MRI) is a non-invasive diagnostic and
radiotherapy (RT) planning tool, offering detailed insights into the anatomy of
the human body. The extensive scan time is stressful for patients, who must
remain motionless in a prolonged imaging procedure that prioritizes reduction
of imaging artifacts. This is challenging for pediatric patients who may
require measures for managing voluntary motions such as anesthesia. Several
computational approaches reduce scan time (fast MRI), by recording fewer
measurements and digitally recovering full information via post-acquisition
reconstruction. However, most fast MRI approaches were developed for diagnostic
imaging, without addressing reconstruction challenges specific to RT planning.
In this work, we developed a deep learning-based method (DeepMRIRec) for MRI
reconstruction from undersampled data acquired with RT-specific receiver coil
arrangements. We evaluated our method against fully sampled data of T1-weighted
MR images acquired from 73 children with brain tumors/surgical beds using loop
and posterior coils (12 channels), with and without applying virtual
compression of coil elements. DeepMRIRec reduced scanning time by a factor of
four producing a structural similarity score surpassing the evaluated
state-of-the-art method (0.960 vs 0.896), thereby demonstrating its potential
for accelerating MRI scanning for RT planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SkeletonGait: Gait Recognition Using Skeleton Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Fan, Jingzhe Ma, Dongyang Jin, Chuanfu Shen, Shiqi Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The choice of the representations is essential for deep gait recognition
methods. The binary silhouettes and skeletal coordinates are two dominant
representations in recent literature, achieving remarkable advances in many
scenarios. However, inherent challenges remain, in which silhouettes are not
always guaranteed in unconstrained scenes, and structural cues have not been
fully utilized from skeletons. In this paper, we introduce a novel skeletal
gait representation named Skeleton Map, together with SkeletonGait, a
skeleton-based method to exploit structural information from human skeleton
maps. Specifically, the skeleton map represents the coordinates of human joints
as a heatmap with Gaussian approximation, exhibiting a silhouette-like image
devoid of exact body structure. Beyond achieving state-of-the-art performances
over five popular gait datasets, more importantly, SkeletonGait uncovers novel
insights about how important structural features are in describing gait and
when do they play a role. Furthermore, we propose a multi-branch architecture,
named SkeletonGait++, to make use of complementary features from both skeletons
and silhouettes. Experiments indicate that SkeletonGait++ outperforms existing
state-of-the-art methods by a significant margin in various scenarios. For
instance, it achieves an impressive rank-1 accuracy of over $85\%$ on the
challenging GREW dataset. All the source code will be available at
https://github.com/ShiqiYu/OpenGait.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Flows for Generative Modeling and Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinqing Zheng, Matt Le, Neta Shaul, Yaron Lipman, Aditya Grover, Ricky T. Q. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifier-free guidance is a key component for improving the performance of
conditional generative models for many downstream tasks. It drastically
improves the quality of samples produced, but has so far only been used for
diffusion models. Flow Matching (FM), an alternative simulation-free approach,
trains Continuous Normalizing Flows (CNFs) based on regressing vector fields.
It remains an open question whether classifier-free guidance can be performed
for Flow Matching models, and to what extent does it improve performance. In
this paper, we explore the usage of Guided Flows for a variety of downstream
applications involving conditional image generation, speech synthesis, and
reinforcement learning. In particular, we are the first to apply flow models to
the offline reinforcement learning setting. We also show that Guided Flows
significantly improves the sample quality in image generation and zero-shot
text-to-speech synthesis, and can make use of drastically low amounts of
computation without affecting the agent's overall performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PG-Video-LLaVA: Pixel Grounding Large Video-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, Fahad Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extending image-based Large Multimodal Models (LMM) to videos is challenging
due to the inherent complexity of video data. The recent approaches extending
image-based LMM to videos either lack the grounding capabilities (e.g.,
VideoChat, Video-ChatGPT, Video-LLaMA) or do not utilize the audio-signals for
better video understanding (e.g., Video-ChatGPT). Addressing these gaps, we
propose Video-LLaVA, the first LMM with pixel-level grounding capability,
integrating audio cues by transcribing them into text to enrich video-context
understanding. Our framework uses an off-the-shelf tracker and a novel
grounding module, enabling it to spatially and temporally localize objects in
videos following user instructions. We evaluate Video-LLaVA using video-based
generative and question-answering benchmarks and introduce new benchmarks
specifically designed to measure prompt-based object grounding performance in
videos. Further, we propose the use of Vicuna over GPT-3.5, as utilized in
Video-ChatGPT, for video-based conversation benchmarking, ensuring
reproducibility of results which is a concern with the proprietary nature of
GPT-3.5. Our framework builds on SoTA image-based LLaVA model and extends its
advantages to the video domain, delivering promising gains on video-based
conversation and grounding tasks. Project Page:
https://github.com/mbzuai-oryx/Video-LLaVA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CompenHR: Efficient Full Compensation for High-resolution Projector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Wang, Haibin Ling, Bingyao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Full projector compensation is a practical task of projector-camera systems.
It aims to find a projector input image, named compensation image, such that
when projected it cancels the geometric and photometric distortions due to the
physical environment and hardware. State-of-the-art methods use deep learning
to address this problem and show promising performance for low-resolution
setups. However, directly applying deep learning to high-resolution setups is
impractical due to the long training time and high memory cost. To address this
issue, this paper proposes a practical full compensation solution. Firstly, we
design an attention-based grid refinement network to improve geometric
correction quality. Secondly, we integrate a novel sampling scheme into an
end-to-end compensation network to alleviate computation and introduce
attention blocks to preserve key features. Finally, we construct a benchmark
dataset for high-resolution projector full compensation. In experiments, our
method demonstrates clear advantages in both efficiency and quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Animatable 3D Gaussians for High-fidelity Synthesis of Human Motions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyang Ye, Tianjia Shao, Kun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel animatable 3D Gaussian model for rendering high-fidelity
free-view human motions in real time. Compared to existing NeRF-based methods,
the model owns better capability in synthesizing high-frequency details without
the jittering problem across video frames. The core of our model is a novel
augmented 3D Gaussian representation, which attaches each Gaussian with a
learnable code. The learnable code serves as a pose-dependent appearance
embedding for refining the erroneous appearance caused by geometric
transformation of Gaussians, based on which an appearance refinement model is
learned to produce residual Gaussian properties to match the appearance in
target pose. To force the Gaussians to learn the foreground human only without
background interference, we further design a novel alpha loss to explicitly
constrain the Gaussians within the human body. We also propose to jointly
optimize the human joint parameters to improve the appearance accuracy. The
animatable 3D Gaussian model can be learned with shallow MLPs, so new human
motions can be synthesized in real time (66 fps on avarage). Experiments show
that our model has superior performance over NeRF-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyoung Chung, Jeongtaek Oh, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a method to optimize Gaussian splatting with a
limited number of images while avoiding overfitting. Representing a 3D scene by
combining numerous Gaussian splats has yielded outstanding visual quality.
However, it tends to overfit the training views when only a small number of
images are available. To address this issue, we introduce a dense depth map as
a geometry guide to mitigate overfitting. We obtained the depth map using a
pre-trained monocular depth estimation model and aligning the scale and offset
using sparse COLMAP feature points. The adjusted depth aids in the color-based
optimization of 3D Gaussian splatting, mitigating floating artifacts, and
ensuring adherence to geometric constraints. We verify the proposed method on
the NeRF-LLFF dataset with varying numbers of few images. Our approach
demonstrates robust geometry compared to the original method that relies solely
on images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegVol: Universal and Interactive Volumetric Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Du, Fan Bai, Tiejun Huang, Bo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise image segmentation provides clinical study with meaningful and
well-structured information. Despite the remarkable progress achieved in
medical image segmentation, there is still an absence of foundation
segmentation model that can segment a wide range of anatomical categories with
easy user interaction. In this paper, we propose a universal and interactive
volumetric medical image segmentation model, named SegVol. By training on 90k
unlabeled Computed Tomography (CT) volumes and 6k labeled CTs, this foundation
model supports the segmentation of over 200 anatomical categories using
semantic and spatial prompts. Extensive experiments verify that SegVol
outperforms the state of the art by a large margin on multiple segmentation
benchmarks. Notably, on three challenging lesion datasets, our method achieves
around 20% higher Dice score than nnU-Net. The model and data are publicly
available at: https://github.com/BAAI-DCAI/SegVol.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread usage of VR devices and contents, demands for 3D scene
generation techniques become more popular. Existing 3D scene generation models,
however, limit the target scene to specific domain, primarily due to their
training strategies using 3D scan dataset that is far from the real-world. To
address such limitation, we propose LucidDreamer, a domain-free scene
generation pipeline by fully leveraging the power of existing large-scale
diffusion-based generative model. Our LucidDreamer has two alternate steps:
Dreaming and Alignment. First, to generate multi-view consistent images from
inputs, we set the point cloud as a geometrical guideline for each image
generation. Specifically, we project a portion of point cloud to the desired
view and provide the projection as a guidance for inpainting using the
generative model. The inpainted images are lifted to 3D space with estimated
depth maps, composing a new points. Second, to aggregate the new points into
the 3D scene, we propose an aligning algorithm which harmoniously integrates
the portions of newly generated 3D scenes. The finally obtained 3D scene serves
as initial points for optimizing Gaussian splats. LucidDreamer produces
Gaussian splats that are highly-detailed compared to the previous 3D scene
generation methods, with no constraint on domain of the target scene.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point Projection Mapping System for Tracking, Registering, Labeling and
  Validating Optical Tissue Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianne Feenstra, Stefan D. van der Stel, Marcos Da Silva Guimaraes, Theo J. M Ruers, Behdad Dashtbozorg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Validation of newly developed optical tissue sensing techniques for tumor
detection during cancer surgery requires an accurate correlation with
histological results. Additionally, such accurate correlation facilitates
precise data labeling for developing high-performance machine-learning tissue
classification models. In this paper, a newly developed Point Projection
Mapping system will be introduced, which allows non-destructive tracking of the
measurement locations on tissue specimens. Additionally, a framework for
accurate registration, validation, and labeling with histopathology results is
proposed and validated on a case study. The proposed framework provides a more
robust and accurate method for tracking and validation of optical tissue
sensing techniques, which saves time and resources compared to conventional
techniques available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MRGazer: Decoding Eye Gaze Points from Functional Magnetic Resonance
  Imaging in Individual Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuwen Wu, Rongjie Hu, Jie Liang, Yanming Wang, Bensheng Qiu, Xiaoxiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eye-tracking research has proven valuable in understanding numerous cognitive
functions. Recently, Frey et al. provided an exciting deep learning method for
learning eye movements from fMRI data. However, it needed to co-register fMRI
into standard space to obtain eyeballs masks, and thus required additional
templates and was time consuming. To resolve this issue, in this paper, we
propose a framework named MRGazer for predicting eye gaze points from fMRI in
individual space. The MRGazer consisted of eyeballs extraction module and a
residual network-based eye gaze prediction. Compared to the previous method,
the proposed framework skips the fMRI co-registration step, simplifies the
processing protocol and achieves end-to-end eye gaze regression. The proposed
method achieved superior performance in a variety of eye movement tasks than
the co-registration-based method, and delivered objective results within a
shorter time (~ 0.02 Seconds for each volume) than prior method (~0.3 Seconds
for each volume).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Classification and Rejection: A One-versus-All Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifying patterns of known classes and rejecting ambiguous and novel (also
called as out-of-distribution (OOD)) inputs are involved in open world pattern
recognition. Deep neural network models usually excel in closed-set
classification while performing poorly in rejecting OOD. To tackle this
problem, numerous methods have been designed to perform open set recognition
(OSR) or OOD rejection/detection tasks. Previous methods mostly take
post-training score transformation or hybrid models to ensure low scores on OOD
inputs while separating known classes. In this paper, we attempt to build a
unified framework for building open set classifiers for both classification and
OOD rejection. We formulate the open set recognition of $ K $-known-class as a
$ (K + 1) $-class classification problem with model trained on known-class
samples only. By decomposing the $ K $-class problem into $ K $ one-versus-all
(OVA) binary classification tasks and binding some parameters, we show that
combining the scores of OVA classifiers can give $ (K + 1) $-class posterior
probabilities, which enables classification and OOD rejection in a unified
framework. To maintain the closed-set classification accuracy of the OVA
trained classifier, we propose a hybrid training strategy combining OVA loss
and multi-class cross-entropy loss. We implement the OVA framework and hybrid
training strategy on the recently proposed convolutional prototype network.
Experiments on popular OSR and OOD detection datasets demonstrate that the
proposed framework, using a single multi-class classifier, yields competitive
performance in closed-set classification, OOD detection, and misclassification
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Quality Face Caricature via Style Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lamyanba Laishram, Muhammad Shaheryar, Jong Taek Lee, Soon Ki Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Caricature is an exaggerated form of artistic portraiture that accentuates
unique yet subtle characteristics of human faces. Recently, advancements in
deep end-to-end techniques have yielded encouraging outcomes in capturing both
style and elevated exaggerations in creating face caricatures. Most of these
approaches tend to produce cartoon-like results that could be more practical
for real-world applications. In this study, we proposed a high-quality,
unpaired face caricature method that is appropriate for use in the real world
and uses computer vision techniques and GAN models. We attain the exaggeration
of facial features and the stylization of appearance through a two-step
process: Face caricature generation and face caricature projection. The face
caricature generation step creates new caricature face datasets from real
images and trains a generative model using the real and newly created
caricature datasets. The Face caricature projection employs an encoder trained
with real and caricature faces with the pretrained generator to project real
and caricature faces. We perform an incremental facial exaggeration from the
real image to the caricature faces using the encoder and generator's latent
space. Our projection preserves the facial identity, attributes, and
expressions from the input image. Also, it accounts for facial occlusions, such
as reading glasses or sunglasses, to enhance the robustness of our model.
Furthermore, we conducted a comprehensive comparison of our approach with
various state-of-the-art face caricature methods, highlighting our process's
distinctiveness and exceptional realism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum learning and essential cognition under the traction of
  meta-characteristics in an open world 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Wang, Changlin Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has made significant progress in the Close World
problem, being able to accurately recognize old knowledge through training and
classification. However, AI faces significant challenges in the Open World
problem, as it involves a new and unknown exploration journey. AI is not
inherently proactive in exploration, and its challenge lies in not knowing how
to approach and adapt to the unknown world. How do humans acquire knowledge of
the unknown world. Humans identify new knowledge through intrinsic cognition.
In the process of recognizing new colors, the cognitive cues are different from
known color features and involve hue, saturation, brightness, and other
characteristics. When AI encounters objects with different features in the new
world, it faces another challenge: where are the distinguishing features
between influential features of new and old objects? AI often mistakes a new
world's brown bear for a known dog because it has not learned the differences
in feature distributions between knowledge systems. This is because things in
the new and old worlds have different units and dimensions for their features.
This paper proposes an open-world model and elemental feature system that
focuses on fundamentally recognizing the distribution differences in objective
features between the new and old worlds. The quantum tunneling effect of
learning ability in the new and old worlds is realized through the tractive
force of meta-characteristic. The outstanding performance of the model system
in learning new knowledge (using pedestrian re-identification datasets as an
example) demonstrates that AI has acquired the ability to recognize the new
world with an accuracy of $96.71\%$ at most and has gained the capability to
explore new knowledge, similar to humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Supervision for Continual Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Marczak, Sebastian Cygert, Tomasz Trzciński, Bartłomiej Twardowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of continual learning, models are designed to learn tasks one
after the other. While most research has centered on supervised continual
learning, recent studies have highlighted the strengths of self-supervised
continual representation learning. The improved transferability of
representations built with self-supervised methods is often associated with the
role played by the multi-layer perceptron projector. In this work, we depart
from this observation and reexamine the role of supervision in continual
representation learning. We reckon that additional information, such as human
annotations, should not deteriorate the quality of representations. Our
findings show that supervised models when enhanced with a multi-layer
perceptron head, can outperform self-supervised models in continual
representation learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Vascular Segmentation and Applications in Phase
  Contrast Tomography Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekin Yagis, Shahab Aslani, Yashvardhan Jain, Yang Zhou, Shahrokh Rahmani, Joseph Brunet, Alexandre Bellier, Christopher Werlein, Maximilian Ackermann, Danny Jonigk, Paul Tafforeau, Peter D Lee, Claire Walsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated blood vessel segmentation is vital for biomedical imaging, as
vessel changes indicate many pathologies. Still, precise segmentation is
difficult due to the complexity of vascular structures, anatomical variations
across patients, the scarcity of annotated public datasets, and the quality of
images. We present a thorough literature review, highlighting the state of
machine learning techniques across diverse organs. Our goal is to provide a
foundation on the topic and identify a robust baseline model for application to
vascular segmentation in a new imaging modality, Hierarchical Phase Contrast
Tomography (HiP CT). Introduced in 2020 at the European Synchrotron Radiation
Facility, HiP CT enables 3D imaging of complete organs at an unprecedented
resolution of ca. 20mm per voxel, with the capability for localized zooms in
selected regions down to 1mm per voxel without sectioning. We have created a
training dataset with double annotator validated vascular data from three
kidneys imaged with HiP CT in the context of the Human Organ Atlas Project.
Finally, utilising the nnU Net model, we conduct experiments to assess the
models performance on both familiar and unseen samples, employing vessel
specific metrics. Our results show that while segmentations yielded reasonably
high scores such as clDice values ranging from 0.82 to 0.88, certain errors
persisted. Large vessels that collapsed due to the lack of hydrostatic pressure
(HiP CT is an ex vivo technique) were segmented poorly. Moreover, decreased
connectivity in finer vessels and higher segmentation errors at vessel
boundaries were observed. Such errors obstruct the understanding of the
structures by interrupting vascular tree connectivity. Through our review and
outputs, we aim to set a benchmark for subsequent model evaluations using
various modalities, especially with the HiP CT imaging database.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recognition-Guided Diffusion Model for Scene Text Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhou, Liangcai Gao, Zhi Tang, Baole Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene Text Image Super-Resolution (STISR) aims to enhance the resolution and
legibility of text within low-resolution (LR) images, consequently elevating
recognition accuracy in Scene Text Recognition (STR). Previous methods
predominantly employ discriminative Convolutional Neural Networks (CNNs)
augmented with diverse forms of text guidance to address this issue.
Nevertheless, they remain deficient when confronted with severely blurred
images, due to their insufficient generation capability when little structural
or semantic information can be extracted from original images. Therefore, we
introduce RGDiffSR, a Recognition-Guided Diffusion model for scene text image
Super-Resolution, which exhibits great generative diversity and fidelity even
in challenging scenarios. Moreover, we propose a Recognition-Guided Denoising
Network, to guide the diffusion model generating LR-consistent results through
succinct semantic guidance. Experiments on the TextZoom dataset demonstrate the
superiority of RGDiffSR over prior state-of-the-art methods in both text
recognition accuracy and image fidelity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Radiology Report Generation via Causal Reasoning and
  Counterfactual Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Song, Jiafan Liu, Yun Li, Wenbin Lei, Ruxin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology Report Generation (RRG) draws attention as an interaction between
vision and language fields. Previous works inherited the ideology of
vision-to-language generation tasks,aiming to generate paragraphs with high
consistency as reports. However, one unique characteristic of RRG, the
independence between diseases, was neglected, leading to the injection of the
spurious confounder, i.e., the disease co-occurrence. Unfortunately, this
confounder confuses the process of report generation worse because of the
biased RRG data distribution. In this paper, to rethink this issue thoroughly,
we reason about its causes and effects from a novel perspective of statistics
and causality, where the Joint Vision Coupling and the Conditional Sentence
Coherence Coupling are two aspects prone to implicitly decrease the accuracy of
reports. Then, a counterfactual augmentation strategy that contains the
Counterfactual Sample Synthesis and the Counterfactual Report Reconstruction
sub-methods is proposed to break these two aspects of spurious effects.
Experimental results and further analyses on two widely used datasets justify
our reasoning and proposed methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retargeting Visual Data with Deformation Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Elsner, Julia Berger, Tong Wu, Victor Czech, Lin Gao, Leif Kobbelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seam carving is an image editing method that enable content-aware resizing,
including operations like removing objects. However, the seam-finding strategy
based on dynamic programming or graph-cut limits its applications to broader
visual data formats and degrees of freedom for editing. Our observation is that
describing the editing and retargeting of images more generally by a
displacement field yields a generalisation of content-aware deformations. We
propose to learn a deformation with a neural network that keeps the output
plausible while trying to deform it only in places with low information
content. This technique applies to different kinds of visual data, including
images, 3D scenes given as neural radiance fields, or even polygon meshes.
Experiments conducted on different visual data show that our method achieves
better content-aware retargeting compared to previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedFN: Feature Normalization for Alleviating Data Heterogeneity Problem
  in Federated Learning <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongyoon Kim, Gihun Lee, Jaehoon Oh, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a collaborative method for training models while
preserving data privacy in decentralized settings. However, FL encounters
challenges related to data heterogeneity, which can result in performance
degradation. In our study, we observe that as data heterogeneity increases,
feature representation in the FedAVG model deteriorates more significantly
compared to classifier weight. Additionally, we observe that as data
heterogeneity increases, the gap between higher feature norms for observed
classes, obtained from local models, and feature norms of unobserved classes
widens, in contrast to the behavior of classifier weight norms. This widening
gap extends to encompass the feature norm disparities between local and the
global models. To address these issues, we introduce Federated Averaging with
Feature Normalization Update (FedFN), a straightforward learning method. We
demonstrate the superior performance of FedFN through extensive experiments,
even when applied to pretrained ResNet18. Subsequently, we confirm the
applicability of FedFN to foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS Workshop: "Federated Learning in the Age of Foundation
  Models" 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMFDFormer: <span class="highlight-title">Transformer</span>-based Copy-Move Forgery Detection with Continual
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaqi Liu, Chao Xia, Song Xiao, Qingxiao Guan, Wenqian Dong, Yifan Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Copy-move forgery detection aims at detecting duplicated regions in a
suspected forged image, and deep learning based copy-move forgery detection
methods are in the ascendant. These deep learning based methods heavily rely on
synthetic training data, and the performance will degrade when facing new
tasks. In this paper, we propose a Transformer-style copy-move forgery
detection network named as CMFDFormer, and provide a novel PCSD (Pooled Cube
and Strip Distillation) continual learning framework to help CMFDFormer handle
new tasks. CMFDFormer consists of a MiT (Mix Transformer) backbone network and
a PHD (Pluggable Hybrid Decoder) mask prediction network. The MiT backbone
network is a Transformer-style network which is adopted on the basis of
comprehensive analyses with CNN-style and MLP-style backbones. The PHD network
is constructed based on self-correlation computation, hierarchical feature
integration, a multi-scale cycle fully-connected block and a mask
reconstruction block. The PHD network is applicable to feature extractors of
different styles for hierarchical multi-scale information extraction, achieving
comparable performance. Last but not least, we propose a PCSD continual
learning framework to improve the forgery detectability and avoid catastrophic
forgetting when handling new tasks. Our continual learning framework restricts
intermediate features from the PHD network, and takes advantage of both cube
pooling and strip pooling. Extensive experiments on publicly available datasets
demonstrate the good performance of CMFDFormer and the effectiveness of the
PCSD continual learning framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Immunohistochemistry guided segmentation of benign epithelial cells, in
  situ lesions, and invasive epithelial cells in breast cancer slides 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maren Høibø, André Pedersen, Vibeke Grotnes Dale, Sissel Marie Berget, Borgny Ytterhus, Cecilia Lindskog, Elisabeth Wik, Lars A. Akslen, Ingerid Reinertsen, Erik Smistad, Marit Valla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital pathology enables automatic analysis of histopathological sections
using artificial intelligence (AI). Automatic evaluation could improve
diagnostic efficiency and help find associations between morphological features
and clinical outcome. For development of such prediction models, identifying
invasive epithelial cells, and separating these from benign epithelial cells
and in situ lesions would be the first step. In this study, we aimed to develop
an AI model for segmentation of epithelial cells in sections from breast
cancer. We generated epithelial ground truth masks by restaining hematoxylin
and eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists'
annotations. HE/CK image pairs were used to train a convolutional neural
network, and data augmentation was used to make the model more robust. Tissue
microarrays (TMAs) from 839 patients, and whole slide images from two patients
were used for training and evaluation of the models. The sections were derived
from four cohorts of breast cancer patients. TMAs from 21 patients from a fifth
cohort was used as a second test set. In quantitative evaluation, a mean Dice
score of 0.70, 0.79, and 0.75 for invasive epithelial cells, benign epithelial
cells, and in situ lesions, respectively, were achieved. In qualitative scoring
(0-5) by pathologists, results were best for all epithelium and invasive
epithelium, with scores of 4.7 and 4.4. Scores for benign epithelium and in
situ lesions were 3.7 and 2.0. The proposed model segmented epithelial cells in
HE stained breast cancer slides well, but further work is needed for accurate
division between the classes. Immunohistochemistry, together with pathologists'
annotations, enabled the creation of accurate ground truths. The model is made
freely available in FastPathology and the code is available at
https://github.com/AICAN-Research/breast-epithelium-segmentation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures. Submitted to a scientific journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided
  Code-Vision Representation <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyi Chen, Xingyao Wang, Manling Li, Derek Hoiem, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art vision-language models (VLMs) still have limited performance
in structural knowledge extraction, such as relations between objects. In this
work, we present ViStruct, a training framework to learn VLMs for effective
visual structural knowledge extraction. Two novel designs are incorporated.
First, we propose to leverage the inherent structure of programming language to
depict visual structural information. This approach enables explicit and
consistent representation of visual structural information of multiple
granularities, such as concepts, relations, and events, in a well-organized
structured format. Second, we introduce curriculum-based learning for VLMs to
progressively comprehend visual structures, from fundamental visual concepts to
intricate event structures. Our intuition is that lower-level knowledge may
contribute to complex visual structure understanding. Furthermore, we compile
and release a collection of datasets tailored for visual structural knowledge
extraction. We adopt a weakly-supervised approach to directly generate visual
event structures from captions for ViStruct training, capitalizing on abundant
image-caption pairs from the web. In experiments, we evaluate ViStruct on
visual structure prediction tasks, demonstrating its effectiveness in improving
the understanding of visual structures. The code is public at
\url{https://github.com/Yangyi-Chen/vi-struct}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DA-STC: Domain Adaptive Video Semantic Segmentation via Spatio-Temporal
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Zhang, Gaochang Wu, Jing Zhang, Chunhua Shen, Dacheng Tao, Tianyou Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video semantic segmentation is a pivotal aspect of video representation
learning. However, significant domain shifts present a challenge in effectively
learning invariant spatio-temporal features across the labeled source domain
and unlabeled target domain for video semantic segmentation. To solve the
challenge, we propose a novel DA-STC method for domain adaptive video semantic
segmentation, which incorporates a bidirectional multi-level spatio-temporal
fusion module and a category-aware spatio-temporal feature alignment module to
facilitate consistent learning for domain-invariant features. Firstly, we
perform bidirectional spatio-temporal fusion at the image sequence level and
shallow feature level, leading to the construction of two fused intermediate
video domains. This prompts the video semantic segmentation model to
consistently learn spatio-temporal features of shared patch sequences which are
influenced by domain-specific contexts, thereby mitigating the feature gap
between the source and target domain. Secondly, we propose a category-aware
feature alignment module to promote the consistency of spatio-temporal
features, facilitating adaptation to the target domain. Specifically, we
adaptively aggregate the domain-specific deep features of each category along
spatio-temporal dimensions, which are further constrained to achieve
cross-domain intra-class feature alignment and inter-class feature separation.
Extensive experiments demonstrate the effectiveness of our method, which
achieves state-of-the-art mIOUs on multiple challenging benchmarks.
Furthermore, we extend the proposed DA-STC to the image domain, where it also
exhibits superior performance for domain adaptive semantic segmentation. The
source code and models will be made available at
\url{https://github.com/ZHE-SAPI/DA-STC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Hetero-Client Federated Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Lu, Suizhi Huang, Yuwen Yang, Shalayiding Sirejiding, Yue Ding, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables joint training across distributed clients
using their local data privately. Federated Multi-Task Learning (FMTL) builds
on FL to handle multiple tasks, assuming model congruity that identical model
architecture is deployed in each client. To relax this assumption and thus
extend real-world applicability, we introduce a novel problem setting,
Hetero-Client Federated Multi-Task Learning (HC-FMTL), to accommodate diverse
task setups. The main challenge of HC-FMTL is the model incongruity issue that
invalidates conventional aggregation methods. It also escalates the
difficulties in accurate model aggregation to deal with data and task
heterogeneity inherent in FMTL. To address these challenges, we propose the
FedHCA$^2$ framework, which allows for federated training of personalized
models by modeling relationships among heterogeneous clients. Drawing on our
theoretical insights into the difference between multi-task and federated
optimization, we propose the Hyper Conflict-Averse Aggregation scheme to
mitigate conflicts during encoder updates. Additionally, inspired by task
interaction in MTL, the Hyper Cross Attention Aggregation scheme uses
layer-wise cross attention to enhance decoder interactions while alleviating
model incongruity. Moreover, we employ learnable Hyper Aggregation Weights for
each client to customize personalized parameter updates. Extensive experiments
demonstrate the superior performance of FedHCA$^2$ in various HC-FMTL scenarios
compared to representative methods. Our code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry
  Guided <span class="highlight-title">Transformer</span> <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huimin Xiong, Kunle Li, Kaiyuan Tan, Yang Feng, Joey Tianyi Zhou, Jin Hao, Haochao Ying, Jian Wu, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical Intraoral Scanners (IOS) are widely used in digital dentistry to
provide detailed 3D information of dental crowns and the gingiva. Accurate 3D
tooth segmentation in IOSs is critical for various dental applications, while
previous methods are error-prone at complicated boundaries and exhibit
unsatisfactory results across patients. In this paper, we propose TSegFormer
which captures both local and global dependencies among different teeth and the
gingiva in the IOS point clouds with a multi-task 3D transformer architecture.
Moreover, we design a geometry-guided loss based on a novel point curvature to
refine boundaries in an end-to-end manner, avoiding time-consuming
post-processing to reach clinically applicable segmentation. In addition, we
create a dataset with 16,000 IOSs, the largest ever IOS dataset to the best of
our knowledge. The experimental results demonstrate that our TSegFormer
consistently surpasses existing state-of-the-art baselines. The superiority of
TSegFormer is corroborated by extensive analysis, visualizations and real-world
clinical applicability tests. Our code is available at
https://github.com/huiminxiong/TSegFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023, STAR(Student Travel) award. 11 pages, 3 figures, 5
  tables. arXiv admin note: text overlap with arXiv:2210.16627</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Human Feedback to Fine-tune Diffusion Models without Any Reward
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using reinforcement learning with human feedback (RLHF) has shown significant
promise in fine-tuning diffusion models. Previous methods start by training a
reward model that aligns with human preferences, then leverage RL techniques to
fine-tune the underlying models. However, crafting an efficient reward model
demands extensive datasets, optimal architecture, and manual hyperparameter
tuning, making the process both time and cost-intensive. The direct preference
optimization (DPO) method, effective in fine-tuning large language models,
eliminates the necessity for a reward model. However, the extensive GPU memory
requirement of the diffusion model's denoising process hinders the direct
application of the DPO method. To address this issue, we introduce the Direct
Preference for Denoising Diffusion Policy Optimization (D3PO) method to
directly fine-tune diffusion models. The theoretical analysis demonstrates that
although D3PO omits training a reward model, it effectively functions as the
optimal reward model trained using human feedback data to guide the learning
process. This approach requires no training of a reward model, proving to be
more direct, cost-effective, and minimizing computational overhead. In
experiments, our method uses the relative scale of objectives as a proxy for
human preference, delivering comparable results to methods using ground-truth
rewards. Moreover, D3PO demonstrates the ability to reduce image distortion
rates and generate safer images, overcoming challenges lacking robust reward
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Detecting, Recognizing, and Parsing the Address Information from
  Bangla Signboard: A Deep Learning-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Murad, Mohammed Eunus Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieving textual information from natural scene images is an active
research area in the field of computer vision with numerous practical
applications. Detecting text regions and extracting text from signboards is a
challenging problem due to special characteristics like reflecting lights,
uneven illumination, or shadows found in real-life natural scene images. With
the advent of deep learning-based methods, different sophisticated techniques
have been proposed for text detection and text recognition from the natural
scene. Though a significant amount of effort has been devoted to extracting
natural scene text for resourceful languages like English, little has been done
for low-resource languages like Bangla. In this research work, we have proposed
an end-to-end system with deep learning-based models for efficiently detecting,
recognizing, correcting, and parsing address information from Bangla
signboards. We have created manually annotated datasets and synthetic datasets
to train signboard detection, address text detection, address text recognition,
address text correction, and address text parser models. We have conducted a
comparative study among different CTC-based and Encoder-Decoder model
architectures for Bangla address text recognition. Moreover, we have designed a
novel address text correction model using a sequence-to-sequence
transformer-based network to improve the performance of Bangla address text
recognition model by post-correction. Finally, we have developed a Bangla
address text parser using the state-of-the-art transformer-based pre-trained
language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-time Adaptive Vision-and-Language Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Gao, Xuan Yao, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) has witnessed significant advancements
in recent years, largely attributed to meticulously curated datasets and
proficiently trained models. Nevertheless, when tested in diverse environments,
the trained models inevitably encounter significant shifts in data
distribution, highlighting that relying solely on pre-trained and fixed
navigation models is insufficient. To enhance models' generalization ability,
test-time adaptation (TTA) demonstrates significant potential in the computer
vision field by leveraging unlabeled test samples for model updates. However,
simply applying existing TTA methods to the VLN task cannot well handle the
adaptability-stability dilemma of VLN models, i.e., frequent updates can result
in drastic changes in model parameters, while occasional updates can make the
models ill-equipped to handle dynamically changing environments. Therefore, we
propose a Fast-Slow Test-Time Adaptation (FSTTA) approach for VLN by performing
decomposition-accumulation analysis for both gradients and parameters in a
unified framework. Specifically, in the fast update phase, gradients generated
during the recent multi-step navigation process are decomposed into components
with varying levels of consistency. Then, these components are adaptively
accumulated to pinpoint a concordant direction for fast model adaptation. In
the slow update phase, historically recorded parameters are gathered, and a
similar decomposition-accumulation analysis is conducted to revert the model to
a stable state. Extensive experiments show that our method obtains impressive
performance gains on four popular benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-guided Few-shot Semantic Segmentation for Remote Sensing Imagery
  Based on Large Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyu Qi, Yifan Wu, Yongqiang Mao, Wenhui Zhang, Yidan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) exhibits remarkable versatility and
zero-shot learning abilities, owing largely to its extensive training data
(SA-1B). Recognizing SAM's dependency on manual guidance given its
category-agnostic nature, we identified unexplored potential within few-shot
semantic segmentation tasks for remote sensing imagery. This research
introduces a structured framework designed for the automation of few-shot
semantic segmentation. It utilizes the SAM model and facilitates a more
efficient generation of semantically discernible segmentation outcomes. Central
to our methodology is a novel automatic prompt learning approach, leveraging
prior guided masks to produce coarse pixel-wise prompts for SAM. Extensive
experiments on the DLRSD datasets underline the superiority of our approach,
outperforming other available few-shot methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRIFu: Differentiable Rendering and Implicit Function-based Single-View
  3D Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Kuang, Lihang Ying, Shi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Differentiable Rendering and Implicit Function-based model (DRIFu) draws
its roots from the Pixel-aligned Implicit Function (PIFU), a pioneering 3D
digitization technique initially designed for clothed human bodies. PIFU excels
in capturing nuanced body shape variations within a low-dimensional space and
has been extensively trained on human 3D scans. However, the application of
PIFU to live animals poses significant challenges, primarily due to the
inherent difficulty in obtaining the cooperation of animals for 3D scanning. In
response to this challenge, we introduce the DRIFu model, specifically tailored
for animal digitization. To train DRIFu, we employ a curated set of synthetic
3D animal models, encompassing diverse shapes, sizes, and even accounting for
variations such as baby birds. Our innovative alignment tools play a pivotal
role in mapping these diverse synthetic animal models onto a unified template,
facilitating precise predictions of animal shape and texture. Crucially, our
template alignment strategy establishes a shared shape space, allowing for the
seamless sampling of new animal shapes, posing them realistically, animating
them, and aligning them with real-world data. This groundbreaking approach
revolutionizes our capacity to comprehensively understand and represent avian
forms. For further details and access to the project, the project website can
be found at https://github.com/kuangzijian/drifu-for-animals
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:1905.05172 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DoubleAUG: Single-domain Generalized Object Detector in Urban via Color
  Perturbation and Dual-style Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Qi, Peng Dong, Tan Xiong, Hui Xue, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection in urban scenarios is crucial for autonomous driving in
intelligent traffic systems. However, unlike conventional object detection
tasks, urban-scene images vary greatly in style. For example, images taken on
sunny days differ significantly from those taken on rainy days. Therefore,
models trained on sunny day images may not generalize well to rainy day images.
In this paper, we aim to solve the single-domain generalizable object detection
task in urban scenarios, meaning that a model trained on images from one
weather condition should be able to perform well on images from any other
weather conditions. To address this challenge, we propose a novel Double
AUGmentation (DoubleAUG) method that includes image- and feature-level
augmentation schemes. In the image-level augmentation, we consider the
variation in color information across different weather conditions and propose
a Color Perturbation (CP) method that randomly exchanges the RGB channels to
generate various images. In the feature-level augmentation, we propose to
utilize a Dual-Style Memory (DSM) to explore the diverse style information on
the entire dataset, further enhancing the model's generalization capability.
Extensive experiments demonstrate that our proposed method outperforms
state-of-the-art methods. Furthermore, ablation studies confirm the
effectiveness of each module in our proposed method. Moreover, our method is
plug-and-play and can be integrated into existing methods to further improve
model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Transactions on Multimedia Computing, Communications,
  and Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Improving Document Understanding: An Exploration on
  Text-Grounding via MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of document understanding, significant advances have been made
in the fine-tuning of Multimodal Large Language Models (MLLMs) with
instruction-following data. Nevertheless, the potential of text-grounding
capability within text-rich scenarios remains underexplored. In this paper, we
present a text-grounding document understanding model, termed TGDoc, which
addresses this deficiency by enhancing MLLMs with the ability to discern the
spatial positioning of text within images. Empirical evidence suggests that
text-grounding improves the model's interpretation of textual content, thereby
elevating its proficiency in comprehending text-rich images. Specifically, we
compile a dataset containing 99K PowerPoint presentations sourced from the
internet. We formulate instruction tuning tasks including text detection,
recognition, and spotting to facilitate the cohesive alignment between the
visual encoder and large language model. Moreover, we curate a collection of
text-rich images and prompt the text-only GPT-4 to generate 12K high-quality
conversations, featuring textual locations within text-rich scenarios. By
integrating text location data into the instructions, TGDoc is adept at
discerning text locations during the visual question process. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
across multiple text-rich benchmarks, validating the effectiveness of our
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeISF: Neural Incident Stokes Field for Geometry and Material Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhao Li, Taishi Ono, Takeshi Uemori, Hajime Mihara, Alexander Gatto, Hajime Nagahara, Yuseke Moriuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view inverse rendering is the problem of estimating the scene
parameters such as shapes, materials, or illuminations from a sequence of
images captured under different viewpoints. Many approaches, however, assume
single light bounce and thus fail to recover challenging scenarios like
inter-reflections. On the other hand, simply extending those methods to
consider multi-bounced light requires more assumptions to alleviate the
ambiguity. To address this problem, we propose Neural Incident Stokes Fields
(NeISF), a multi-view inverse rendering framework that reduces ambiguities
using polarization cues. The primary motivation for using polarization cues is
that it is the accumulation of multi-bounced light, providing rich information
about geometry and material. Based on this knowledge, the proposed incident
Stokes field efficiently models the accumulated polarization effect with the
aid of an original physically-based differentiable polarimetric renderer.
Lastly, experimental results show that our method outperforms the existing
works in synthetic and real scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applications of Spiking Neural Networks in Visual Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somayeh Hussaini, Michael Milford, Tobias Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for
their largely-unrealized potential energy efficiency and low latency
particularly when implemented on neuromorphic hardware. Our paper highlights
three advancements for SNNs in Visual Place Recognition (VPR). First, we
propose Modular SNNs, where each SNN represents a set of non-overlapping
geographically distinct places, enabling scalable networks for large
environments. Secondly, we present Ensembles of Modular SNNs, where multiple
networks represent the same place, significantly enhancing accuracy compared to
single-network models. Our SNNs are compact and small, comprising only 1500
neurons and 474k synapses, which makes them ideally suited for ensembling due
to this small size. Lastly, we investigate the role of sequence matching in
SNN-based VPR, a technique where consecutive images are used to refine place
recognition. We analyze the responsiveness of SNNs to ensembling and sequence
matching compared to other VPR techniques. Our contributions highlight the
viability of SNNs for VPR, offering scalable and robust solutions, paving the
way for their application in various energy-sensitive robotic tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Radio Frequency Ray Tracing for Millimeter-Wave Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Chen, Xinyu Zhang, Qiyue Xia, Xinmin Fang, Chris Xiaoxuan Lu, Zhengxiong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Millimeter wave (mmWave) sensing is an emerging technology with applications
in 3D object characterization and environment mapping. However, realizing
precise 3D reconstruction from sparse mmWave signals remains challenging.
Existing methods rely on data-driven learning, constrained by dataset
availability and difficulty in generalization. We propose DiffSBR, a
differentiable framework for mmWave-based 3D reconstruction. DiffSBR
incorporates a differentiable ray tracing engine to simulate radar point clouds
from virtual 3D models. A gradient-based optimizer refines the model parameters
to minimize the discrepancy between simulated and real point clouds.
Experiments using various radar hardware validate DiffSBR's capability for
fine-grained 3D reconstruction, even for novel objects unseen by the radar
previously. By integrating physics-based simulation with gradient optimization,
DiffSBR transcends the limitations of data-driven approaches and pioneers a new
paradigm for mmWave sensing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Volumetric Reconstruction Resolves Off-Resonance Artifacts in Static and
  Dynamic PROPELLER MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annesha Ghosh, Gordon Wetzstein, Mert Pilanci, Sara Fridovich-Keil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Off-resonance artifacts in magnetic resonance imaging (MRI) are visual
distortions that occur when the actual resonant frequencies of spins within the
imaging volume differ from the expected frequencies used to encode spatial
information. These discrepancies can be caused by a variety of factors,
including magnetic field inhomogeneities, chemical shifts, or susceptibility
differences within the tissues. Such artifacts can manifest as blurring,
ghosting, or misregistration of the reconstructed image, and they often
compromise its diagnostic quality. We propose to resolve these artifacts by
lifting the 2D MRI reconstruction problem to 3D, introducing an additional
"spectral" dimension to model this off-resonance. Our approach is inspired by
recent progress in modeling radiance fields, and is capable of reconstructing
both static and dynamic MR images as well as separating fat and water, which is
of independent clinical interest. We demonstrate our approach in the context of
PROPELLER (Periodically Rotated Overlapping ParallEL Lines with Enhanced
Reconstruction) MRI acquisitions, which are popular for their robustness to
motion artifacts. Our method operates in a few minutes on a single GPU, and to
our knowledge is the first to correct for chemical shift in gradient echo
PROPELLER MRI reconstruction without additional measurements or pretraining
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/sarafridov/volumetric-propeller</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Complement with Multiple Humans (LECOMH): Integrating
  Multi-rater and Noisy-Label Learning into Human-AI Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhang, Kevin Wells, Gustavo Carneiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of learning with noisy labels (LNL), multi-rater learning, and
human-AI collaboration has revolutionised the development of robust
classifiers, enabling them to address the challenges posed by different types
of data imperfections and complex decision processes commonly encountered in
real-world applications. While each of these methodologies has individually
made significant strides in addressing their unique challenges, the development
of techniques that can simultaneously tackle these three problems remains
underexplored. This paper addresses this research gap by integrating
noisy-label learning, multi-rater learning, and human-AI collaboration with new
benchmarks and the innovative Learning to Complement with Multiple Humans
(LECOMH) approach. LECOMH optimises the level of human collaboration during
testing, aiming to optimise classification accuracy while minimising
collaboration costs that vary from 0 to M, where M is the maximum number of
human collaborators. We quantitatively compare LECOMH with leading human-AI
collaboration methods using our proposed benchmarks. LECOMH consistently
outperforms the competition, with accuracy improving as collaboration costs
increase. Notably, LECOMH is the only method enhancing human labeller
performance across all benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Face Style Transfer with a Hybrid Solution of NeRF and Mesh
  Rasterization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Feng, Prateek Singhal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Style transfer for human face has been widely researched in recent years.
Majority of the existing approaches work in 2D image domain and have 3D
inconsistency issue when applied on different viewpoints of the same face. In
this paper, we tackle the problem of 3D face style transfer which aims at
generating stylized novel views of a 3D human face with multi-view consistency.
We propose to use a neural radiance field (NeRF) to represent 3D human face and
combine it with 2D style transfer to stylize the 3D face. We find that directly
training a NeRF on stylized images from 2D style transfer brings in 3D
inconsistency issue and causes blurriness. On the other hand, training a NeRF
jointly with 2D style transfer objectives shows poor convergence due to the
identity and head pose gap between style image and content image. It also poses
challenge in training time and memory due to the need of volume rendering for
full image to apply style transfer loss functions. We therefore propose a
hybrid framework of NeRF and mesh rasterization to combine the benefits of high
fidelity geometry reconstruction of NeRF and fast rendering speed of mesh. Our
framework consists of three stages: 1. Training a NeRF model on input face
images to learn the 3D geometry; 2. Extracting a mesh from the trained NeRF
model and optimizing it with style transfer objectives via differentiable
rasterization; 3. Training a new color network in NeRF conditioned on a style
embedding to enable arbitrary style transfer to the 3D face. Experiment results
show that our approach generates high quality face style transfer with great 3D
consistency, while also enabling a flexible style control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-Time Augmentation for 3D Point Cloud Classification and
  Segmentation <span class="chip">3DV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan-Anh Vu, Srinjay Sarkar, Zhiyuan Zhang, Binh-Son Hua, Sai-Kit Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is a powerful technique to enhance the performance of a
deep learning task but has received less attention in 3D deep learning. It is
well known that when 3D shapes are sparsely represented with low point density,
the performance of the downstream tasks drops significantly. This work explores
test-time augmentation (TTA) for 3D point clouds. We are inspired by the recent
revolution of learning implicit representation and point cloud upsampling,
which can produce high-quality 3D surface reconstruction and
proximity-to-surface, respectively. Our idea is to leverage the implicit field
reconstruction or point cloud upsampling techniques as a systematic way to
augment point cloud data. Mainly, we test both strategies by sampling points
from the reconstructed results and using the sampled point cloud as test-time
augmented data. We show that both strategies are effective in improving
accuracy. We observed that point cloud upsampling for test-time augmentation
can lead to more significant performance improvement on downstream tasks such
as object classification and segmentation on the ModelNet40, ShapeNet,
ScanObjectNN, and SemanticKITTI datasets, especially for sparse point clouds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted in 3DV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Image Compressed Sensing MRI via a <span class="highlight-title">Self-Supervised</span> Deep Denoising
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlon Bran Lorenzana, Feng Liu, Shekhar S. Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Popular methods in compressed sensing (CS) are dependent on deep learning
(DL), where large amounts of data are used to train non-linear reconstruction
models. However, ensuring generalisability over and access to multiple datasets
is challenging to realise for real-world applications. To address these
concerns, this paper proposes a single image, self-supervised (SS) CS-MRI
framework that enables a joint deep and sparse regularisation of CS artefacts.
The approach effectively dampens structured CS artefacts, which can be
difficult to remove assuming sparse reconstruction, or relying solely on the
inductive biases of CNN to produce noise-free images. Image quality is thereby
improved compared to either approach alone. Metrics are evaluated using
Cartesian 1D masks on a brain and knee dataset, with PSNR improving by 2-4dB on
average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, 2 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion360: Seamless 360 Degree Panoramic Image Generation based on
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyang Feng, Jinlin Liu, Miaomiao Cui, Xuansong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is a technical report on the 360-degree panoramic image generation task
based on diffusion models. Unlike ordinary 2D images, 360-degree panoramic
images capture the entire $360^\circ\times 180^\circ$ field of view. So the
rightmost and the leftmost sides of the 360 panoramic image should be
continued, which is the main challenge in this field. However, the current
diffusion pipeline is not appropriate for generating such a seamless 360-degree
panoramic image. To this end, we propose a circular blending strategy on both
the denoising and VAE decoding stages to maintain the geometry continuity.
Based on this, we present two models for \textbf{Text-to-360-panoramas} and
\textbf{Single-Image-to-360-panoramas} tasks. The code has been released as an
open-source project at
\href{https://github.com/ArcherFMY/SD-T2I-360PanoImage}{https://github.com/ArcherFMY/SD-T2I-360PanoImage}
and
\href{https://www.modelscope.cn/models/damo/cv_diffusion_text-to-360panorama-image_generation/summary}{ModelScope}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages, 8 figures, Tech. Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight High-Speed Photography Built on Coded Exposure and Implicit
  Neural Representation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihong Zhang, Runzhao Yang, Jinli Suo, Yuxiao Cheng, Qionghai Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The compact cameras recording high-speed scenes with high resolution are
highly demanded, but the required high bandwidth often leads to bulky, heavy
systems, which limits their applications on low-capacity platforms. Adopting a
coded exposure setup to encode a frame sequence into a blurry snapshot and
retrieve the latent sharp video afterward can serve as a lightweight solution.
However, restoring motion from blur is quite challenging due to the high
ill-posedness of motion blur decomposition, intrinsic ambiguity in motion
direction, and diverse motions in natural videos. In this work, by leveraging
classical coded exposure imaging technique and emerging implicit neural
representation for videos, we tactfully embed the motion direction cues into
the blurry image during the imaging process and develop a novel self-recursive
neural network to sequentially retrieve the latent video sequence from the
blurry image utilizing the embedded motion direction cues. To validate the
effectiveness and efficiency of the proposed framework, we conduct extensive
experiments on benchmark datasets and real-captured blurry images. The results
demonstrate that our proposed framework significantly outperforms existing
methods in quality and flexibility. The code for our work is available at
https://github.com/zhihongz/BDINR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ P2RBox: A Single Point is All You Need for Oriented Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Cao, Xuehui Yu, Wenwen Yu, Xumeng Han, Xue Yang, Guorong Li, Jianbin Jiao, Zhenjun Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Oriented object detection, a specialized subfield in computer vision, finds
applications across diverse scenarios, excelling particularly when dealing with
objects of arbitrary orientations. Conversely, point annotation, which treats
objects as single points, offers a cost-effective alternative to rotated and
horizontal bounding boxes but sacrifices performance due to the loss of size
and orientation information. In this study, we introduce the P2RBox network,
which leverages point annotations and a mask generator to create mask
proposals, followed by filtration through our Inspector Module and Constrainer
Module. This process selects high-quality masks, which are subsequently
converted into rotated box annotations for training a fully supervised
detector. Specifically, we've thoughtfully crafted an Inspector Module rooted
in multi-instance learning principles to evaluate the semantic score of masks.
We've also proposed a more robust mask quality assessment in conjunction with
the Constrainer Module. Furthermore, we've introduced a Symmetry Axis
Estimation (SAE) Module inspired by the spectral theorem for symmetric matrices
to transform the top-performing mask proposal into rotated bounding boxes.
P2RBox performs well with three fully supervised rotated object detectors:
RetinaNet, Rotated FCOS, and Oriented R-CNN. By combining with Oriented R-CNN,
P2RBox achieves 62.26% on DOTA-v1.0 test dataset. As far as we know, this is
the first attempt at training an oriented object detector with point
supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Robust Imperceptible Perturbation against Unauthorized
  Text-to-image Diffusion-based Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Liu, Chenrui Fan, Yutong Dai, Xun Chen, Pan Zhou, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models allow seamless generation of personalized
images from scant reference photos. Yet, these tools, in the wrong hands, can
fabricate misleading or harmful content, endangering individuals. To address
this problem, existing poisoning-based approaches perturb user images in an
imperceptible way to render them "unlearnable" from malicious uses. We identify
two limitations of these defending approaches: i) sub-optimal due to the
hand-crafted heuristics for solving the intractable bilevel optimization and
ii) lack of robustness against simple data transformations like Gaussian
filtering. To solve these challenges, we propose MetaCloak, which solves the
bi-level poisoning problem with a meta-learning framework with an additional
transformation sampling process to craft transferable and robust perturbation.
Specifically, we employ a pool of surrogate diffusion models to craft
transferable and model-agnostic perturbation. Furthermore, by incorporating an
additional transformation process, we design a simple denoising-error
maximization loss that is sufficient for causing transformation-robust semantic
distortion and degradation in a personalized generation. Extensive experiments
on the VGGFace2 and CelebA-HQ datasets show that MetaCloak outperforms existing
approaches. Notably, MetaCloak can successfully fool online training services
like Replicate, in a black-box manner, demonstrating the effectiveness of
MetaCloak in real-world scenarios. Our code is available at
https://github.com/liuyixin-louis/MetaCloak.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 15 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAE-Net: Deforming Auto-Encoder for fine-grained shape co-segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqin Chen, Qimin Chen, Hang Zhou, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an unsupervised 3D shape co-segmentation method which learns a set
of deformable part templates from a shape collection. To accommodate structural
variations in the collection, our network composes each shape by a selected
subset of template parts which are affine-transformed. To maximize the
expressive power of the part templates, we introduce a per-part deformation
network to enable the modeling of diverse parts with substantial geometry
variations, while imposing constraints on the deformation capacity to ensure
fidelity to the originally represented parts. We also propose a training scheme
to effectively overcome local minima. Architecturally, our network is a
branched autoencoder, with a CNN encoder taking a voxel shape as input and
producing per-part transformation matrices, latent codes, and part existence
scores, and the decoder outputting point occupancies to define the
reconstruction loss. Our network, coined DAE-Net for Deforming Auto-Encoder,
can achieve unsupervised 3D shape co-segmentation that yields fine-grained,
compact, and meaningful parts that are consistent across diverse shapes. We
conduct extensive experiments on the ShapeNet Part dataset, DFAUST, and an
animal subset of Objaverse to show superior performance over prior methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/czq142857/DAE-Net</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal In-Context Learning Makes an Ego-evolving Scene Text
  Recognizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Zhao, Can Huang, Binghong Wu, Chunhui Lin, Hao Liu, Zhizhong Zhang, Xin Tan, Jingqun Tang, Yuan Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene text recognition (STR) in the wild frequently encounters challenges
when coping with domain variations, font diversity, shape deformations, etc. A
straightforward solution is performing model fine-tuning tailored to a specific
scenario, but it is computationally intensive and requires multiple model
copies for various scenarios. Recent studies indicate that large language
models (LLMs) can learn from a few demonstration examples in a training-free
manner, termed "In-Context Learning" (ICL). Nevertheless, applying LLMs as a
text recognizer is unacceptably resource-consuming. Moreover, our pilot
experiments on LLMs show that ICL fails in STR, mainly attributed to the
insufficient incorporation of contextual information from diverse samples in
the training stage. To this end, we introduce E$^2$STR, a STR model trained
with context-rich scene text sequences, where the sequences are generated via
our proposed in-context training strategy. E$^2$STR demonstrates that a
regular-sized model is sufficient to achieve effective ICL capabilities in STR.
Extensive experiments show that E$^2$STR exhibits remarkable training-free
adaptation in various scenarios and outperforms even the fine-tuned
state-of-the-art approaches on public benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ White-Box <span class="highlight-title">Transformer</span>s via Sparse Rate Reduction: Compression Is All
  There Is? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Hao Bai, Yuexiang Zhai, Benjamin D. Haeffele, Yi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we contend that a natural objective of representation learning
is to compress and transform the distribution of the data, say sets of tokens,
towards a low-dimensional Gaussian mixture supported on incoherent subspaces.
The goodness of such a representation can be evaluated by a principled measure,
called sparse rate reduction, that simultaneously maximizes the intrinsic
information gain and extrinsic sparsity of the learned representation. From
this perspective, popular deep network architectures, including transformers,
can be viewed as realizing iterative schemes to optimize this measure.
Particularly, we derive a transformer block from alternating optimization on
parts of this objective: the multi-head self-attention operator compresses the
representation by implementing an approximate gradient descent step on the
coding rate of the features, and the subsequent multi-layer perceptron
sparsifies the features. This leads to a family of white-box transformer-like
deep network architectures, named CRATE, which are mathematically fully
interpretable. We show, by way of a novel connection between denoising and
compression, that the inverse to the aforementioned compressive encoding can be
realized by the same class of CRATE architectures. Thus, the so-derived
white-box architectures are universal to both encoders and decoders.
Experiments show that these networks, despite their simplicity, indeed learn to
compress and sparsify representations of large-scale real-world image and text
datasets, and achieve performance very close to highly engineered
transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the
proposed computational framework demonstrates great potential in bridging the
gap between theory and practice of deep learning, from a unified perspective of
data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper integrates the works arXiv:2306.01129 and
  arXiv:2308.16271, as well as this under-review work:
  https://openreview.net/forum?id=PvyOYleymy into a complete story. In this
  paper, we improve the writing and organization, and also add conceptual,
  empirical, and theoretical improvements over the previous work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Measurement of Pericoronary Adipose Tissue Attenuation and
  Volume in CT Angiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew M. Nguyen, Tejas Sudharshan Mathai, Liangchen Liu, Jianfei Liu, Ronald M. Summers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pericoronary adipose tissue (PCAT) is the deposition of fat in the vicinity
of the coronary arteries. It is an indicator of coronary inflammation and
associated with coronary artery disease. Non-invasive coronary CT angiography
(CCTA) is presently used to obtain measures of the thickness, volume, and
attenuation of fat deposition. However, prior works solely focus on measuring
PCAT using semi-automated approaches at the right coronary artery (RCA) over
the left coronary artery (LCA). In this pilot work, we developed a fully
automated approach for the measurement of PCAT mean attenuation and volume in
the region around both coronary arteries. First, we used a large subset of
patients from the public ImageCAS dataset (n = 735) to train a 3D full
resolution nnUNet to segment LCA and RCA. Then, we automatically measured PCAT
in the surrounding arterial regions. We evaluated our method on a held-out test
set of patients (n = 183) from the same dataset. A mean Dice score of 83% and
PCAT attenuation of -73.81 $\pm$ 12.69 HU was calculated for the RCA, while a
mean Dice score of 81% and PCAT attenuation of -77.51 $\pm$ 7.94 HU was
computed for the LCA. To the best of our knowledge, we are the first to develop
a fully automated method to measure PCAT attenuation and volume at both the RCA
and LCA. Our work underscores how automated PCAT measurement holds promise as a
biomarker for identification of inflammation and cardiac disease.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, IEE ISBI2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Feng, Yintong Shang, Xuan Li, Tianjia Shao, Chenfanfu Jiang, Yin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that physics-based simulations can be seamlessly integrated with NeRF
to generate high-quality elastodynamics of real-world objects. Unlike existing
methods, we discretize nonlinear hyperelasticity in a meshless way, obviating
the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh
or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed
to capture nonlinear dynamics and large deformation on the implicit model. Such
meshless integration enables versatile simulations of complex and codimensional
shapes. We adaptively place the least-square kernels according to the NeRF
density field to significantly reduce the complexity of the nonlinear
simulation. As a result, physically realistic animations can be conveniently
synthesized using our method for a wide range of hyperelastic materials at an
interactive rate. For more information, please visit our project page at
https://fytalon.github.io/pienerf/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Unlearnable Example: Enhancing the Robustness of Unlearnable
  Examples via Stable Error-Minimizing Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Liu, Kaidi Xu, Xun Chen, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The open source of large amounts of image data promotes the development of
deep learning techniques. Along with this comes the privacy risk of these
open-source image datasets being exploited by unauthorized third parties to
train deep learning models for commercial or illegal purposes. To avoid the
abuse of public data, a poisoning-based technique, the unlearnable example, is
proposed to significantly degrade the generalization performance of models by
adding a kind of imperceptible noise to the data. To further enhance its
robustness against adversarial training, existing works leverage iterative
adversarial training on both the defensive noise and the surrogate model.
However, it still remains unknown whether the robustness of unlearnable
examples primarily comes from the effect of enhancement in the surrogate model
or the defensive noise. Observing that simply removing the adversarial noise on
the training process of the defensive noise can improve the performance of
robust unlearnable examples, we identify that solely the surrogate model's
robustness contributes to the performance. Furthermore, we found a negative
correlation exists between the robustness of defensive noise and the protection
performance, indicating defensive noise's instability issue. Motivated by this,
to further boost the robust unlearnable example, we introduce stable
error-minimizing noise (SEM), which trains the defensive noise against random
perturbation instead of the time-consuming adversarial perturbation to improve
the stability of defensive noise. Through extensive experiments, we demonstrate
that SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100,
and ImageNet Subset in terms of both effectiveness and efficiency. The code is
available at https://github.com/liuyixin-louis/Stable-Unlearnable-Example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Limitation of Diffusion Models for Synthesizing Training <span class="highlight-title">Dataset</span>s <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shin'ya Yamaguchi, Takuma Fukuda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic samples from diffusion models are promising for leveraging in
training discriminative models as replications of real training datasets.
However, we found that the synthetic datasets degrade classification
performance over real datasets even when using state-of-the-art diffusion
models. This means that modern diffusion models do not perfectly represent the
data distribution for the purpose of replicating datasets for training
discriminative tasks. This paper investigates the gap between synthetic and
real samples by analyzing the synthetic samples reconstructed from real samples
through the diffusion and reverse process. By varying the time steps starting
the reverse process in the reconstruction, we can control the trade-off between
the information in the original real data and the information added by
diffusion models. Through assessing the reconstructed samples and trained
models, we found that the synthetic data are concentrated in modes of the
training data distribution as the reverse step increases, and thus, they are
difficult to cover the outer edges of the distribution. Our findings imply that
modern diffusion models are insufficient to replicate training data
distribution perfectly, and there is room for the improvement of generative
modeling in the replication of training datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 SyntheticData4ML Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FusionFrames: Efficient Architectural Aspects for Text-to-Video
  Generation Pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Arkhipkin, Zein Shaheen, Viacheslav Vasilev, Elizaveta Dakhova, Andrey Kuznetsov, Denis Dimitrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia generation approaches occupy a prominent place in artificial
intelligence research. Text-to-image models achieved high-quality results over
the last few years. However, video synthesis methods recently started to
develop. This paper presents a new two-stage latent diffusion text-to-video
generation architecture based on the text-to-image diffusion model. The first
stage concerns keyframes synthesis to figure the storyline of a video, while
the second one is devoted to interpolation frames generation to make movements
of the scene and objects smooth. We compare several temporal conditioning
approaches for keyframes generation. The results show the advantage of using
separate temporal blocks over temporal layers in terms of metrics reflecting
video generation quality aspects and human preference. The design of our
interpolation model significantly reduces computational costs compared to other
masked frame interpolation approaches. Furthermore, we evaluate different
configurations of MoVQ-based video decoding scheme to improve consistency and
achieve higher PSNR, SSIM, MSE, and LPIPS scores. Finally, we compare our
pipeline with existing solutions and achieve top-2 scores overall and top-1
among open-source solutions: CLIPSIM = 0.2976 and FVD = 433.054. Project page:
https://ai-forever.github.io/kandinsky-video/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ai-forever.github.io/kandinsky-video/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FuseNet: <span class="highlight-title">Self-Supervised</span> Dual-Path Network for Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Kazerouni, Sanaz Karimijafarbigloo, Reza Azad, Yury Velichko, Ulas Bagci, Dorit Merhof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation, a crucial task in computer vision, often relies on
labor-intensive and costly annotated datasets for training. In response to this
challenge, we introduce FuseNet, a dual-stream framework for self-supervised
semantic segmentation that eliminates the need for manual annotation. FuseNet
leverages the shared semantic dependencies between the original and augmented
images to create a clustering space, effectively assigning pixels to
semantically related clusters, and ultimately generating the segmentation map.
Additionally, FuseNet incorporates a cross-modal fusion technique that extends
the principles of CLIP by replacing textual data with augmented images. This
approach enables the model to learn complex visual representations, enhancing
robustness against variations similar to CLIP's text invariance. To further
improve edge alignment and spatial consistency between neighboring pixels, we
introduce an edge refinement loss. This loss function considers edge
information to enhance spatial coherence, facilitating the grouping of nearby
pixels with similar visual features. Extensive experiments on skin lesion and
lung segmentation datasets demonstrate the effectiveness of our method.
\href{https://github.com/xmindflow/FuseNet}{Codebase.}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Weight-Perturbed Deep Neural Networks With Application in
  Iris Presentation Attack Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renu Sharma, Redwan Sony, Arun Ross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) exhibit superior performance in various machine
learning tasks, e.g., image classification, speech recognition, biometric
recognition, object detection, etc. However, it is essential to analyze their
sensitivity to parameter perturbations before deploying them in real-world
applications. In this work, we assess the sensitivity of DNNs against
perturbations to their weight and bias parameters. The sensitivity analysis
involves three DNN architectures (VGG, ResNet, and DenseNet), three types of
parameter perturbations (Gaussian noise, weight zeroing, and weight scaling),
and two settings (entire network and layer-wise). We perform experiments in the
context of iris presentation attack detection and evaluate on two publicly
available datasets: LivDet-Iris-2017 and LivDet-Iris-2020. Based on the
sensitivity analysis, we propose improved models simply by perturbing
parameters of the network without undergoing training. We further combine these
perturbed models at the score-level and at the parameter-level to improve the
performance over the original model. The ensemble at the parameter-level shows
an average improvement of 43.58% on the LivDet-Iris-2017 dataset and 9.25% on
the LivDet-Iris-2020 dataset. The source code is available at
https://github.com/redwankarimsony/WeightPerturbation-MSU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Good Feature Extractor Is All You Need for Weakly Supervised Learning
  in Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Wölflein, Dyke Ferber, Asier Rabasco Meneghetti, Omar S. M. El Nahhas, Daniel Truhn, Zunamys I. Carrero, David J. Harrison, Ognjen Arandjelović, Jakob N. Kather
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is revolutionising pathology, offering novel opportunities in
disease prognosis and personalised treatment. Historically, stain normalisation
has been a crucial preprocessing step in computational pathology pipelines, and
persists into the deep learning era. Yet, with the emergence of feature
extractors trained using self-supervised learning (SSL) on diverse pathology
datasets, we call this practice into question. In an empirical evaluation of
publicly available feature extractors, we find that omitting stain
normalisation and image augmentations does not compromise downstream
performance, while incurring substantial savings in memory and compute.
Further, we show that the top-performing feature extractors are remarkably
robust to variations in stain and augmentations like rotation in their latent
space. Contrary to previous patch-level benchmarking studies, our approach
emphasises clinical relevance by focusing on slide-level prediction tasks in a
weakly supervised setting with external validation cohorts. This work
represents the most comprehensive robustness evaluation of public pathology SSL
feature extractors to date, involving more than 6,000 training runs across nine
tasks, five datasets, three downstream architectures, and various preprocessing
setups. Our findings stand to streamline digital pathology workflows by
minimising preprocessing needs and informing the selection of feature
extractors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval
  Score Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, Yingcong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in text-to-3D generation mark a significant milestone
in generative models, unlocking new possibilities for creating imaginative 3D
assets across various real-world scenarios. While recent advancements in
text-to-3D generation have shown promise, they often fall short in rendering
detailed and high-quality 3D models. This problem is especially prevalent as
many methods base themselves on Score Distillation Sampling (SDS). This paper
identifies a notable deficiency in SDS, that it brings inconsistent and
low-quality updating direction for the 3D model, causing the over-smoothing
effect. To address this, we propose a novel approach called Interval Score
Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes
interval-based score matching to counteract over-smoothing. Furthermore, we
incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline.
Extensive experiments show that our model largely outperforms the
state-of-the-art in quality and training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. Our code will
  be available at: https://github.com/EnVision-Research/LucidDreamer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory
  Prediction in Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mozhgan Pourkeshavarz, Mohammad Sabokrou, Amir Rasouli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, behavior prediction is fundamental for safe motion
planning, hence the security and robustness of prediction models against
adversarial attacks are of paramount importance. We propose a novel adversarial
backdoor attack against trajectory prediction models as a means of studying
their potential vulnerabilities. Our attack affects the victim at training time
via naturalistic, hence stealthy, poisoned samples crafted using a novel
two-step approach. First, the triggers are crafted by perturbing the trajectory
of attacking vehicle and then disguised by transforming the scene using a
bi-level optimization technique. The proposed attack does not depend on a
particular model architecture and operates in a black-box manner, thus can be
effective without any knowledge of the victim model. We conduct extensive
empirical studies using state-of-the-art prediction models on two benchmark
datasets using metrics customized for trajectory prediction. We show that the
proposed attack is highly effective, as it can significantly hinder the
performance of prediction models, unnoticeable by the victims, and efficient as
it forces the victim to generate malicious behavior even under constrained
conditions. Via ablative studies, we analyze the impact of different attack
design choices followed by an evaluation of existing defence mechanisms against
the proposed attack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pelvic floor MRI segmentation based on semi-supervised deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Zuo, Fei Feng, Zhuhui Wang, James A. Ashton-Miller, John O. L. Delancey, Jiajia Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The semantic segmentation of pelvic organs via MRI has important clinical
significance. Recently, deep learning-enabled semantic segmentation has
facilitated the three-dimensional geometric reconstruction of pelvic floor
organs, providing clinicians with accurate and intuitive diagnostic results.
However, the task of labeling pelvic floor MRI segmentation, typically
performed by clinicians, is labor-intensive and costly, leading to a scarcity
of labels. Insufficient segmentation labels limit the precise segmentation and
reconstruction of pelvic floor organs. To address these issues, we propose a
semi-supervised framework for pelvic organ segmentation. The implementation of
this framework comprises two stages. In the first stage, it performs
self-supervised pre-training using image restoration tasks. Subsequently,
fine-tuning of the self-supervised model is performed, using labeled data to
train the segmentation model. In the second stage, the self-supervised
segmentation model is used to generate pseudo labels for unlabeled data.
Ultimately, both labeled and unlabeled data are utilized in semi-supervised
training. Upon evaluation, our method significantly enhances the performance in
the semantic segmentation and geometric reconstruction of pelvic organs, Dice
coefficient can increase by 2.65% averagely. Especially for organs that are
difficult to segment, such as the uterus, the accuracy of semantic segmentation
can be improved by up to 3.70%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention-based Adversarial Appearance Learning of Augmented Pedestrians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.02673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.02673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Strauss, Artem Savkin, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data became already an essential component of machine
learning-based perception in the field of autonomous driving. Yet it still
cannot replace real data completely due to the sim2real domain shift. In this
work, we propose a method that leverages the advantages of the augmentation
process and adversarial training to synthesize realistic data for the
pedestrian recognition task. Our approach utilizes an attention mechanism
driven by an adversarial loss to learn domain discrepancies and improve
sim2real adaptation. Our experiments confirm that the proposed adaptation
method is robust to such discrepancies and reveals both visual realism and
semantic consistency. Furthermore, we evaluate our data generation pipeline on
the task of pedestrian recognition and demonstrate that generated data resemble
properties of the real domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Different Learning Styles for Improved Knowledge Distillation
  in Biomedical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02931v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02931v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usma Niyaz, Abhishek Singh Sambyal, Deepti R. Bathula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning style refers to a type of training mechanism adopted by an
individual to gain new knowledge. As suggested by the VARK model, humans have
different learning preferences, like Visual (V), Auditory (A), Read/Write (R),
and Kinesthetic (K), for acquiring and effectively processing information. Our
work endeavors to leverage this concept of knowledge diversification to improve
the performance of model compression techniques like Knowledge Distillation
(KD) and Mutual Learning (ML). Consequently, we use a single-teacher and
two-student network in a unified framework that not only allows for the
transfer of knowledge from teacher to students (KD) but also encourages
collaborative learning between students (ML). Unlike the conventional approach,
where the teacher shares the same knowledge in the form of predictions or
feature representations with the student network, our proposed approach employs
a more diversified strategy by training one student with predictions and the
other with feature maps from the teacher. We further extend this knowledge
diversification by facilitating the exchange of predictions and feature maps
between the two student networks, enriching their learning experiences. We have
conducted comprehensive experiments with three benchmark datasets for both
classification and segmentation tasks using two different network architecture
combinations. These experimental results demonstrate that knowledge
diversification in a combined KD and ML framework outperforms conventional KD
or ML techniques (with similar network configuration) that only use predictions
with an average improvement of 2%. Furthermore, consistent improvement in
performance across different tasks, with various network architectures, and
over state-of-the-art techniques establishes the robustness and
generalizability of the proposed model
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Computers in Biology and Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confident Naturalness Explanation (CNE): A Framework to Explain and
  Assess Patterns Forming Naturalness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Emam, Mohamed Farag, Ribana Roscher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protected natural areas are regions that have been minimally affected by
human activities such as urbanization, agriculture, and other human
interventions. To better understand and map the naturalness of these areas,
machine learning models can be used to analyze satellite imagery. Specifically,
explainable machine learning methods show promise in uncovering patterns that
contribute to the concept of naturalness within these protected environments.
Additionally, addressing the uncertainty inherent in machine learning models is
crucial for a comprehensive understanding of this concept. However, existing
approaches have limitations. They either fail to provide explanations that are
both valid and objective or struggle to offer a quantitative metric that
accurately measures the contribution of specific patterns to naturalness, along
with the associated confidence. In this paper, we propose a novel framework
called the Confident Naturalness Explanation (CNE) framework. This framework
combines explainable machine learning and uncertainty quantification to assess
and explain naturalness. We introduce a new quantitative metric that describes
the confident contribution of patterns to the concept of naturalness.
Furthermore, we generate an uncertainty-aware segmentation mask for each input
sample, highlighting areas where the model lacks knowledge. To demonstrate the
effectiveness of our framework, we apply it to a study site in Fennoscandia
using two open-source satellite datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BEVTrack: A Simple and Strong Baseline for 3D Single Object Tracking in
  Bird's-Eye View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02185v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02185v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Yang, Yingqi Deng, Jing Zhang, Jiahao Nie, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Single Object Tracking (SOT) is a fundamental task of computer vision,
proving essential for applications like autonomous driving. It remains
challenging to localize the target from surroundings due to appearance
variations, distractors, and the high sparsity of point clouds. The spatial
information indicating objects' spatial adjacency across consecutive frames is
crucial for effective object tracking. However, existing trackers typically
employ point-wise representation with irregular formats, leading to
insufficient use of this important spatial knowledge. As a result, these
trackers usually require elaborate designs and solving multiple subtasks. In
this paper, we propose BEVTrack, a simple yet effective baseline that performs
tracking in Bird's-Eye View (BEV). This representation greatly retains spatial
information owing to its ordered structure and inherently encodes the implicit
motion relations of the target as well as distractors. To achieve accurate
regression for targets with diverse attributes (\textit{e.g.}, sizes and motion
patterns), BEVTrack constructs the likelihood function with the learned
underlying distributions adapted to different targets, rather than making a
fixed Laplace or Gaussian assumption as in previous works. This provides
valuable priors for tracking and thus further boosts performance. While only
using a single regression loss with a plain convolutional architecture,
BEVTrack achieves state-of-the-art performance on three large-scale datasets,
KITTI, NuScenes, and Waymo Open Dataset while maintaining a high inference
speed of about 200 FPS. The code will be released at
https://github.com/xmm-prio/BEVTrack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code will be released at https://github.com/xmm-prio/BEVTrack</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete approximations of Gaussian smoothing and Gaussian derivatives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Lindeberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops an in-depth treatment concerning the problem of
approximating the Gaussian smoothing and Gaussian derivative computations in
scale-space theory for application on discrete data. With close connections to
previous axiomatic treatments of continuous and discrete scale-space theory, we
consider three main ways discretizing these scale-space operations in terms of
explicit discrete convolutions, based on either (i) sampling the Gaussian
kernels and the Gaussian derivative kernels, (ii) locally integrating the
Gaussian kernels and the Gaussian derivative kernels over each pixel support
region and (iii) basing the scale-space analysis on the discrete analogue of
the Gaussian kernel, and then computing derivative approximations by applying
small-support central difference operators to the spatially smoothed image
data.
  We study the properties of these three main discretization methods both
theoretically and experimentally, and characterize their performance by
quantitative measures, including the results they give rise to with respect to
the task of scale selection, investigated for four different use cases, and
with emphasis on the behaviour at fine scales. The results show that the
sampled Gaussian kernels and derivatives as well as the integrated Gaussian
kernels and derivatives perform very poorly at very fine scales. At very fine
scales, the discrete analogue of the Gaussian kernel with its corresponding
discrete derivative approximations performs substantially better. The sampled
Gaussian kernel and the sampled Gaussian derivatives do, on the other hand,
lead to numerically very good approximations of the corresponding continuous
results, when the scale parameter is sufficiently large, in the experiments
presented in the paper, when the scale parameter is greater than a value of
about 1, in units of the grid spacing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 34 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Vision <span class="highlight-title">Transformer</span> for Human Pose Estimation via Patch
  Selection <span class="chip">BMVC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaleab A. Kinfu, Rene Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Convolutional Neural Networks (CNNs) have been widely successful in 2D
human pose estimation, Vision Transformers (ViTs) have emerged as a promising
alternative to CNNs, boosting state-of-the-art performance. However, the
quadratic computational complexity of ViTs has limited their applicability for
processing high-resolution images. In this paper, we propose three methods for
reducing ViT's computational complexity, which are based on selecting and
processing a small number of most informative patches while disregarding
others. The first two methods leverage a lightweight pose estimation network to
guide the patch selection process, while the third method utilizes a set of
learnable joint tokens to ensure that the selected patches contain the most
important information about body joints. Experiments across six benchmarks show
that our proposed methods achieve a significant reduction in computational
complexity, ranging from 30% to 44%, with only a minimal drop in accuracy
between 0% and 3.5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BMVC 2023 Oral Paper: https://proceedings.bmvc2023.org/167/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Looking at the posterior: accuracy and uncertainty of neural-network
  predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14605v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14605v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Linander, O. Balabanov, H. Yang, B. Mehlig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian inference can quantify uncertainty in the predictions of neural
networks using posterior distributions for model parameters and network output.
By looking at these posterior distributions, one can separate the origin of
uncertainty into aleatoric and epistemic contributions. One goal of uncertainty
quantification is to inform on prediction accuracy. Here we show that
prediction accuracy depends on both epistemic and aleatoric uncertainty in an
intricate fashion that cannot be understood in terms of marginalized
uncertainty distributions alone. How the accuracy relates to epistemic and
aleatoric uncertainties depends not only on the model architecture, but also on
the properties of the dataset. We discuss the significance of these results for
active learning and introduce a novel acquisition function that outperforms
common uncertainty-based methods. To arrive at our results, we approximated the
posteriors using deep ensembles, for fully-connected, convolutional and
attention-based neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 10 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Site-specific Styles for Multi-institutional Unsupervised
  Cross-modality Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12437v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12437v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Liu, Yubo Fan, Zhoubing Xu, Benoit M. Dawant, Ipek Oguz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised cross-modality domain adaptation is a challenging task in
medical image analysis, and it becomes more challenging when source and target
domain data are collected from multiple institutions. In this paper, we present
our solution to tackle the multi-institutional unsupervised domain adaptation
for the crossMoDA 2023 challenge. First, we perform unpaired image translation
to translate the source domain images to the target domain, where we design a
dynamic network to generate synthetic target domain images with controllable,
site-specific styles. Afterwards, we train a segmentation model using the
synthetic images and further reduce the domain gap by self-training. Our
solution achieved the 1st place during both the validation and testing phases
of the challenge. The code repository is publicly available at
https://github.com/MedICL-VU/crossmoda2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>crossMoDA 2023 challenge 1st place solution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Rank-Consistent Pyramid Model for Enhanced Crowd Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.04819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.04819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Gao, Zhizhong Huang, Yiming Lei, Hongming Shan, James Z. Wang, Fei-Yue Wang, Junping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most conventional crowd counting methods utilize a fully-supervised learning
framework to establish a mapping between scene images and crowd density maps.
They usually rely on a large quantity of costly and time-intensive pixel-level
annotations for training supervision. One way to mitigate the intensive
labeling effort and improve counting accuracy is to leverage large amounts of
unlabeled images. This is attributed to the inherent self-structural
information and rank consistency within a single image, offering additional
qualitative relation supervision during training. Contrary to earlier methods
that utilized the rank relations at the original image level, we explore such
rank-consistency relation within the latent feature spaces. This approach
enables the incorporation of numerous pyramid partial orders, strengthening the
model representation capability. A notable advantage is that it can also
increase the utilization ratio of unlabeled samples. Specifically, we propose a
Deep Rank-consistEnt pyrAmid Model (DREAM), which makes full use of rank
consistency across coarse-to-fine pyramid features in latent spaces for
enhanced crowd counting with massive unlabeled images. In addition, we have
collected a new unlabeled crowd counting dataset, FUDAN-UCC, comprising 4,000
images for training purposes. Extensive experiments on four benchmark datasets,
namely UCF-QNRF, ShanghaiTech PartA and PartB, and UCF-CC-50, show the
effectiveness of our method compared with previous semi-supervised methods. The
codes are available at https://github.com/bridgeqiqi/DREAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Neural Networks and Learning Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time
  Adaptation <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manogna Sreenivas, Goirik Chakrabarty, Soma Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test Time Adaptation (TTA) is a pivotal concept in machine learning, enabling
models to perform well in real-world scenarios, where test data distribution
differs from training. In this work, we propose a novel approach called pseudo
Source guided Target Clustering (pSTarC) addressing the relatively unexplored
area of TTA under real-world domain shifts. This method draws inspiration from
target clustering techniques and exploits the source classifier for generating
pseudo-source samples. The test samples are strategically aligned with these
pseudo-source samples, facilitating their clustering and thereby enhancing TTA
performance. pSTarC operates solely within the fully test-time adaptation
protocol, removing the need for actual source data. Experimental validation on
a variety of domain shift datasets, namely VisDA, Office-Home, DomainNet-126,
CIFAR-100C verifies pSTarC's effectiveness. This method exhibits significant
improvements in prediction accuracy along with efficient computational
requirements. Furthermore, we also demonstrate the universality of the pSTarC
framework by showing its effectiveness for the continuous TTA framework. The
source code for our method is available at https://manogna-s.github.io/pstarc
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ USL-Net: Uncertainty Self-Learning Network for Unsupervised Skin Lesion
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofan Li, Bo Peng, Jie Hu, Changyou Ma, Daipeng Yang, Zhuyang Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised skin lesion segmentation offers several benefits, including
conserving expert human resources, reducing discrepancies due to subjective
human labeling, and adapting to novel environments. However, segmenting
dermoscopic images without manual labeling guidance presents significant
challenges due to dermoscopic image artifacts such as hair noise, blister
noise, and subtle edge differences. To address these challenges, we introduce
an innovative Uncertainty Self-Learning Network (USL-Net) designed for skin
lesion segmentation. The USL-Net can effectively segment a range of lesions,
eliminating the need for manual labeling guidance. Initially, features are
extracted using contrastive learning, followed by the generation of Class
Activation Maps (CAMs) as saliency maps using these features. The different CAM
locations correspond to the importance of the lesion region based on their
saliency. High-saliency regions in the map serve as pseudo-labels for lesion
regions while low-saliency regions represent the background. However,
intermediate regions can be hard to classify, often due to their proximity to
lesion edges or interference from hair or blisters. Rather than risk potential
pseudo-labeling errors or learning confusion by forcefully classifying these
regions, we consider them as uncertainty regions, exempting them from
pseudo-labeling and allowing the network to self-learn. Further, we employ
connectivity detection and centrality detection to refine foreground
pseudo-labels and reduce noise-induced errors. The application of cycle
refining enhances performance further. Our method underwent thorough
experimental validation on the ISIC-2017, ISIC-2018, and PH2 datasets,
demonstrating that its performance is on par with weakly supervised and
supervised methods, and exceeds that of other existing unsupervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 71 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PsyMo: A <span class="highlight-title">Dataset</span> for Estimating Self-Reported Psychological Traits from
  Gait <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10631v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10631v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Cosma, Emilian Radoi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Psychological trait estimation from external factors such as movement and
appearance is a challenging and long-standing problem in psychology, and is
principally based on the psychological theory of embodiment. To date, attempts
to tackle this problem have utilized private small-scale datasets with
intrusive body-attached sensors. Potential applications of an automated system
for psychological trait estimation include estimation of occupational fatigue
and psychology, and marketing and advertisement. In this work, we propose PsyMo
(Psychological traits from Motion), a novel, multi-purpose and multi-modal
dataset for exploring psychological cues manifested in walking patterns. We
gathered walking sequences from 312 subjects in 7 different walking variations
and 6 camera angles. In conjunction with walking sequences, participants filled
in 6 psychological questionnaires, totalling 17 psychometric attributes related
to personality, self-esteem, fatigue, aggressiveness and mental health. We
propose two evaluation protocols for psychological trait estimation. Alongside
the estimation of self-reported psychological traits from gait, the dataset can
be used as a drop-in replacement to benchmark methods for gait recognition. We
anonymize all cues related to the identity of the subjects and publicly release
only silhouettes, 2D / 3D human skeletons and 3D SMPL human meshes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hand-Eye Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radu Horaud, Fadi Dornaika
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whenever a sensor is mounted on a robot hand it is important to know the
relationship between the sensor and the hand. The problem of determining this
relationship is referred to as hand-eye calibration, which is important in at
least two types of tasks: (i) map sensor centered measurements into the robot
workspace and (ii) allow the robot to precisely move the sensor. In the past
some solutions were proposed in the particular case of a camera. With almost no
exception, all existing solutions attempt to solve the homogeneous matrix
equation AX=XB. First we show that there are two possible formulations of the
hand-eye calibration problem. One formulation is the classical one that we just
mentioned. A second formulation takes the form of the following homogeneous
matrix equation: MY=M'YB. The advantage of the latter is that the extrinsic and
intrinsic camera parameters need not be made explicit. Indeed, this formulation
directly uses the 3 by 4 perspective matrices (M and M') associated with two
positions of the camera. Moreover, this formulation together with the classical
one cover a wider range of camera-based sensors to be calibrated with respect
to the robot hand. Second, we develop a common mathematical framework to solve
for the hand-eye calibration problem using either of the two formulations. We
present two methods, (i) a rotation then translation and (ii) a non-linear
solver for rotation and translation. Third, we perform a stability analysis
both for our two methods and for the classical linear method of Tsai and Lenz
(1989). In the light of this comparison, the non-linear optimization method,
that solves for rotation and translation simultaneously, seems to be the most
robust one with respect to noise and to measurement errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symphonize 3D Semantic Scene Completion with Contextual Instance Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyi Jiang, Tianheng Cheng, Naiyu Gao, Haoyang Zhang, Tianwei Lin, Wenyu Liu, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  `3D Semantic Scene Completion (SSC) has emerged as a nascent and pivotal
undertaking in autonomous driving, aiming to predict voxel occupancy within
volumetric scenes. However, prevailing methodologies primarily focus on
voxel-wise feature aggregation, while neglecting instance semantics and scene
context. In this paper, we present a novel paradigm termed Symphonies
(Scene-from-Insts), that delves into the integration of instance queries to
orchestrate 2D-to-3D reconstruction and 3D scene modeling. Leveraging our
proposed Serial Instance-Propagated Attentions, Symphonies dynamically encodes
instance-centric semantics, facilitating intricate interactions between
image-based and volumetric domains. Simultaneously, Symphonies enables holistic
scene comprehension by capturing context through the efficient fusion of
instance queries, alleviating geometric ambiguity such as occlusion and
perspective errors through contextual scene reasoning. Experimental results
demonstrate that Symphonies achieves state-of-the-art performance on
challenging benchmarks SemanticKITTI and SSCBench-KITTI-360, yielding
remarkable mIoU scores of 15.04 and 18.58, respectively. These results showcase
the paradigm's promising advancements. The code is available at
https://github.com/hustvl/Symphonies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Code and models at:
  https://github.com/hustvl/Symphonies</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP Guided Image-perceptive <span class="highlight-title">Prompt</span> Learning for Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwen Chen, Qiuhong Ke, Zinuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image enhancement is a significant research area in the fields of computer
vision and image processing. In recent years, many learning-based methods for
image enhancement have been developed, where the Look-up-table (LUT) has proven
to be an effective tool. In this paper, we delve into the potential of
Contrastive Language-Image Pre-Training (CLIP) Guided Prompt Learning,
proposing a simple structure called CLIP-LUT for image enhancement. We found
that the prior knowledge of CLIP can effectively discern the quality of
degraded images, which can provide reliable guidance. To be specific, We
initially learn image-perceptive prompts to distinguish between original and
target images using CLIP model, in the meanwhile, we introduce a very simple
network by incorporating a simple baseline to predict the weights of three
different LUT as enhancement network. The obtained prompts are used to steer
the enhancement network like a loss function and improve the performance of
model. We demonstrate that by simply combining a straightforward method with
CLIP, we can obtain satisfactory results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A trial work to the image enhancement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with
  Variational Score Distillation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score distillation sampling (SDS) has shown great promise in text-to-3D
generation by distilling pretrained large-scale text-to-image diffusion models,
but suffers from over-saturation, over-smoothing, and low-diversity problems.
In this work, we propose to model the 3D parameter as a random variable instead
of a constant as in SDS and present variational score distillation (VSD), a
principled particle-based variational framework to explain and address the
aforementioned issues in text-to-3D generation. We show that SDS is a special
case of VSD and leads to poor samples with both small and large CFG weights. In
comparison, VSD works well with various CFG weights as ancestral sampling from
diffusion models and simultaneously improves the diversity and sample quality
with a common CFG weight (i.e., $7.5$). We further present various improvements
in the design space for text-to-3D such as distillation time schedule and
density initialization, which are orthogonal to the distillation algorithm yet
not well explored. Our overall approach, dubbed ProlificDreamer, can generate
high rendering resolution (i.e., $512\times512$) and high-fidelity NeRF with
rich structure and complex effects (e.g., smoke and drops). Further,
initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and
photo-realistic. Project page and codes:
https://ml.cs.tsinghua.edu.cn/prolificdreamer/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PhysGaussian, a new method that seamlessly integrates physically
grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel
motion synthesis. Employing a custom Material Point Method (MPM), our approach
enriches 3D Gaussian kernels with physically meaningful kinematic deformation
and mechanical stress attributes, all evolved in line with continuum mechanics
principles. A defining characteristic of our method is the seamless integration
between physical simulation and visual rendering: both components utilize the
same 3D Gaussian kernels as their discrete representations. This negates the
necessity for triangle/tetrahedron meshing, marching cubes, "cage meshes," or
any other geometry embedding, highlighting the principle of "what you see is
what you simulate (WS$^2$)." Our method demonstrates exceptional versatility
across a wide variety of materials--including elastic entities, metals,
non-Newtonian fluids, and granular materials--showcasing its strong
capabilities in creating diverse visual content with novel viewpoints and
movements. Our project page is at: https://xpandora.github.io/PhysGaussian/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLAD: Global-Local View Alignment and Background Debiasing for
  Unsupervised Video Domain Adaptation with Large Domain Gap <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyogun Lee, Kyungho Bae, Seong Jong Ha, Yumin Ko, Gyeong-Moon Park, Jinwoo Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the challenging problem of unsupervised video domain
adaptation (UVDA) for action recognition. We specifically focus on scenarios
with a substantial domain gap, in contrast to existing works primarily deal
with small domain gaps between labeled source domains and unlabeled target
domains. To establish a more realistic setting, we introduce a novel UVDA
scenario, denoted as Kinetics->BABEL, with a more considerable domain gap in
terms of both temporal dynamics and background shifts. To tackle the temporal
shift, i.e., action duration difference between the source and target domains,
we propose a global-local view alignment approach. To mitigate the background
shift, we propose to learn temporal order sensitive representations by temporal
order learning and background invariant representations by background
augmentation. We empirically validate that the proposed method shows
significant improvement over the existing methods on the Kinetics->BABEL
dataset with a large domain gap. The code is available at
https://github.com/KHUVLL/GLAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an accepted WACV 2024 paper. Our code is available at
  https://github.com/KHUVLL/GLAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene
  Graphs with Weak Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07647v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07647v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiani Huang, Ziyang Li, Mayur Naik, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose LASER, a neuro-symbolic approach to learn semantic video
representations that capture rich spatial and temporal properties in video data
by leveraging high-level logic specifications. In particular, we formulate the
problem in terms of alignment between raw videos and spatio-temporal logic
specifications. The alignment algorithm leverages a differentiable symbolic
reasoner and a combination of contrastive, temporal, and semantics losses. It
effectively and efficiently trains low-level perception models to extract
fine-grained video representation in the form of a spatio-temporal scene graph
that conforms to the desired high-level specification. In doing so, we explore
a novel methodology that weakly supervises the learning of video semantic
representations through logic specifications. We evaluate our method on two
datasets with rich spatial and temporal specifications:
20BN-Something-Something and MUGEN. We demonstrate that our method learns
better fine-grained video semantics than existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self supervised convolutional kernel based handcrafted feature
  harmonization: Enhanced left ventricle hypertension disease phenotyping on
  echocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08897v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08897v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jina Lee, Youngtaek Hong, Dawun Jeong, Yeonggul Jang, Jaeik Jeon, Sihyeon Jeong, Taekgeun Jung, Yeonyee E. Yoon, Inki Moon, Seung-Ah Lee, Hyuk-Jae Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiomics, a medical imaging technique, extracts quantitative handcrafted
features from images to predict diseases. Harmonization in those features
ensures consistent feature extraction across various imaging devices and
protocols. Methods for harmonization include standardized imaging protocols,
statistical adjustments, and evaluating feature robustness. Myocardial diseases
such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD)
are diagnosed via echocardiography, but variable imaging settings pose
challenges. Harmonization techniques are crucial for applying handcrafted
features in disease diagnosis in such scenario. Self-supervised learning (SSL)
enhances data understanding within limited datasets and adapts to diverse data
settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying
superior performance in various tasks. This study focuses on convolutional
filters within SSL, using them as preprocessing to convert images into feature
maps for handcrafted feature harmonization. Our proposed method excelled in
harmonization evaluation and exhibited superior LVH classification performance
compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Better Data Exploitation in <span class="highlight-title">Self-Supervised</span> Monocular Depth
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinfeng Liu, Lingtong Kong, Jie Yang, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation plays an important role in the robotic perception system.
Self-supervised monocular paradigm has gained significant attention since it
can free training from the reliance on depth annotations. Despite recent
advancements, existing self-supervised methods still underutilize the available
training data, limiting their generalization ability. In this paper, we take
two data augmentation techniques, namely Resizing-Cropping and
Splitting-Permuting, to fully exploit the potential of training datasets.
Specifically, the original image and the generated two augmented images are fed
into the training pipeline simultaneously and we leverage them to conduct
self-distillation. Additionally, we introduce the detail-enhanced DepthNet with
an extra full-scale branch in the encoder and a grid decoder to enhance the
restoration of fine details in depth maps. Experimental results demonstrate our
method can achieve state-of-the-art performance on the KITTI benchmark, with
both raw ground truth and improved ground truth. Moreover, our models also show
superior generalization performance when transferring to Make3D and NYUv2
datasets. Our codes are available at https://github.com/Sauf4896/BDEdepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, accepted by IEEE Robotics and Automation Letters
  (RA-L, 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual
  Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, Zhuowen Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs), which extend Large Language Models (LLM) by
incorporating visual understanding capability, have demonstrated significant
advancements in addressing open-ended visual question-answering (VQA) tasks.
However, these models cannot accurately interpret images infused with text, a
common occurrence in real-world scenarios. Standard procedures for extracting
information from images often involve learning a fixed set of query embeddings.
These embeddings are designed to encapsulate image contexts and are later used
as soft prompt inputs in LLMs. Yet, this process is limited to the token count,
potentially curtailing the recognition of scenes with text-rich context. To
improve upon them, the present study introduces BLIVA: an augmented version of
InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings
from InstructBLIP and also directly projects encoded patch embeddings into the
LLM, a technique inspired by LLaVA. This approach assists the model to capture
intricate details potentially missed during the query decoding process.
Empirical evidence demonstrates that our model, BLIVA, significantly enhances
performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA
benchmark) and in undertaking general (not particularly text-rich) VQA
benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), comparing to our
baseline InstructBLIP. BLIVA demonstrates significant capability in decoding
real-world images, irrespective of text presence. To demonstrate the broad
industry applications enabled by BLIVA, we evaluate the model using a new
dataset comprising YouTube thumbnails paired with question-answer sets across
11 diverse categories. For researchers interested in further exploration, our
code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Novel Object Detection via Cooperative Foundational Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Bharadwaj, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the challenging and emergent problem of novel object
detection (NOD), focusing on the accurate detection of both known and novel
object categories during inference. Traditional object detection algorithms are
inherently closed-set, limiting their capability to handle NOD. We present a
novel approach to transform existing closed-set detectors into open-set
detectors. This transformation is achieved by leveraging the complementary
strengths of pre-trained foundational models, specifically CLIP and SAM,
through our cooperative mechanism. Furthermore, by integrating this mechanism
with state-of-the-art open-set detectors such as GDINO, we establish new
benchmarks in object detection performance. Our method achieves 17.42 mAP in
novel object detection and 42.08 mAP for known objects on the challenging LVIS
dataset. Adapting our approach to the COCO OVD split, we surpass the current
state-of-the-art by a margin of 7.2 $ \text{AP}_{50} $ for novel classes. Our
code is available at
https://github.com/rohit901/cooperative-foundational-models .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/rohit901/cooperative-foundational-models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Disentangling of Facial Representations with 3D-aware
  Latent Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruian He, Zhen Xing, Weimin Tan, Bo Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised learning of facial representations has gained increasing
attention for face understanding ability without heavily relying on large-scale
annotated datasets. However, it remains unsolved due to the coupling of facial
identities, expressions, and external factors like pose and light. Prior
methods primarily focus on 2D factors and pixel-level consistency, leading to
incomplete disentangling and suboptimal performance in downstream tasks. In
this paper, we propose LatentFace, a novel unsupervised disentangling framework
for facial expression and identity representation. We suggest the disentangling
problem should be performed in latent space and propose the solution using a
3D-aware latent diffusion model. First, we introduce a 3D-aware autoencoder to
encode face images into 3D latent embeddings. Second, we propose a novel
representation diffusion model (RDM) to disentangle 3D latent into facial
identity and expression. Consequently, our method achieves state-of-the-art
performance in facial expression recognition and face verification among
unsupervised facial representation learning models. Codes are available at
\url{https://github.com/ryanhe312/LatentFace}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChemScraper: Graphics Extraction, Molecular Diagram Parsing, and
  Annotated Data Generation for PDF Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Kumar Shah, Bryan Manrique Amador, Abhisek Dey, Ming Creekmore, Blake Ocampo, Scott Denmark, Richard Zanibbi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing visual parsers for molecule diagrams translate pixel-based raster
images such as PNGs to chemical structure representations (e.g., SMILES).
However, PDFs created by word processors including LaTeX and Word provide
explicit locations and shapes for characters, lines, and polygons. We extract
symbols from born-digital PDF molecule images and then apply simple graph
transformations to capture both visual and chemical structure in editable
ChemDraw files (CDXML). Our fast ( PDF $\rightarrow$ visual graph $\rightarrow$
chemical graph ) pipeline does not require GPUs, Optical Character Recognition
(OCR) or vectorization. We evaluate on standard benchmarks using SMILES
strings, along with a novel evaluation that provides graph-based metrics and
error compilation using LgEval. The geometric information in born-digital PDFs
produces a highly accurate parser, motivating generating training data for
visual parsers that recognize from raster images, with extracted graphics,
visual structure, and chemical structure as annotations. To do this we render
SMILES strings in Indigo, parse molecule structure, and then validate
recognized structure to select correct files.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages without references, 10 figures, 3 Tables, submitted to
  International Journal on Document Analysis and Recognition (IJDAR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D
  Shape Detailization and Differentiable Rendering <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qimin Chen, Zhiqin Chen, Hang Zhou, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ShaDDR, an example-based deep generative neural network which
produces a high-resolution textured 3D shape through geometry detailization and
conditional texture generation applied to an input coarse voxel shape. Trained
on a small set of detailed and textured exemplar shapes, our method learns to
detailize the geometry via multi-resolution voxel upsampling and generate
textures on voxel surfaces via differentiable rendering against exemplar
texture images from a few views. The generation is interactive, taking less
than 1 second to produce a 3D model with voxel resolutions up to 512^3. The
generated shape preserves the overall structure of the input coarse voxel
model, while the style of the generated geometric details and textures can be
manipulated through learned latent codes. In the experiments, we show that our
method can generate higher-resolution shapes with plausible and improved
geometric details and clean textures compared to prior works. Furthermore, we
showcase the ability of our method to learn geometric details and textures from
shapes reconstructed from real-world photos. In addition, we have developed an
interactive modeling application to demonstrate the generalizability of our
method to various user inputs and the controllability it offers, allowing users
to interactively sculpt a coarse voxel shape to define the overall structure of
the detailized 3D shape. Code and data are available at
https://github.com/qiminchen/ShaDDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGGRAPH Asia 2023 conference track. Code:
  https://github.com/qiminchen/ShaDDR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Early Detection of Late Blight Tomato Disease using Histogram Oriented
  Gradient based Support Vector Machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Ishaq, M. Waqas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tomato is one of the most important fruits on earth. It plays an
important and useful role in the agricultural production of any country. This
research propose a novel smart technique for early detection of late blight
diseases in tomatoes. This work improve the dataset with an increase in images
from the field (the Plant Village dataset) and proposed a hybrid algorithm
composed of support vector machines (SVM) and histogram-oriented gradients
(HOG) for real-time detection of late blight tomato disease. To propose a
HOG-based SVM model for early detection of late blight tomato leaf disease. To
check the performance of the proposed model in terms of MSE, accuracy,
precision, and recall as compared to Decision Tree and KNN. The integration of
advanced technology in agriculture has the potential to revolutionize the
industry, making it more efficient, sustainable, and profitable. This research
work on the early detection of tomato diseases contributes to the growing
importance of smart farming, the need for climate-smart agriculture, the rising
need to more efficiently utilize natural resources, and the demand for higher
crop yields. The proposed hybrid algorithm of SVM and HOG has significant
potential for the early detection of late blight disease in tomato plants. The
performance of the proposed model against decision tree and KNN algorithms and
the results may assist in selecting the best algorithm for future applications.
The research work can help farmers make data-driven decisions to optimize crop
yield and quality while also reducing the environmental impact of farming
practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article titled "Early Detection of Late Blight Tomato Disease
  using Histogram Oriented Gradient based Support Vector Machine" need to be
  withdrawn there are other contributors in the improvement of this article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applications of Large Scale Foundation Models for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Huang, Yue Chen, Zhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,
autonomous driving has been the most active field of AI applications. Recently
powered by large language models (LLMs), chat systems, such as chatGPT and
PaLM, emerge and rapidly become a promising direction to achieve artificial
general intelligence (AGI) in natural language processing (NLP). There comes a
natural thinking that we could employ these abilities to reformulate autonomous
driving. By combining LLM with foundation models, it is possible to utilize the
human knowledge, commonsense and reasoning to rebuild autonomous driving
systems from the current long-tailed AI dilemma. In this paper, we investigate
the techniques of foundation models and LLMs applied for autonomous driving,
categorized as simulation, world model, data annotation and planning or E2E
solutions etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages. arXiv admin note: text overlap with arXiv:2304.03589,
  arXiv:2111.05849, arXiv:2306.03000, arXiv:2301.02691, arXiv:2309.16292,
  arXiv:2309.17080, arXiv:2309.10228, arXiv:2310.01415 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pose-Graph Attentional Graph Neural Network for Lidar Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milad Ramezani, Liang Wang, Joshua Knights, Zhibin Li, Pauline Pounds, Peyman Moghadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a pose-graph attentional graph neural network, called
P-GAT, which compares (key)nodes between sequential and non-sequential
sub-graphs for place recognition tasks as opposed to a common frame-to-frame
retrieval problem formulation currently implemented in SOTA place recognition
methods. P-GAT uses the maximum spatial and temporal information between
neighbour cloud descriptors -- generated by an existing encoder -- utilising
the concept of pose-graph SLAM. Leveraging intra- and inter-attention and graph
neural network, P-GAT relates point clouds captured in nearby locations in
Euclidean space and their embeddings in feature space. Experimental results on
the large-scale publically available datasets demonstrate the effectiveness of
our approach in scenes lacking distinct features and when training and testing
environments have different distributions (domain adaptation). Further, an
exhaustive comparison with the state-of-the-art shows improvements in
performance gains. Code is available at
https://github.com/csiro-robotics/P-GAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surgical Temporal Action-aware Network with Sequence Regularization for
  Phase Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Chen, Yuhao Zhai, Jun Zhang, Jinqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To assist surgeons in the operating theatre, surgical phase recognition is
critical for developing computer-assisted surgical systems, which requires
comprehensive understanding of surgical videos. Although existing studies made
great progress, there are still two significant limitations worthy of
improvement. First, due to the compromise of resource consumption, frame-wise
visual features are extracted by 2D networks and disregard spatial and temporal
knowledge of surgical actions, which hinders subsequent inter-frame modeling
for phase prediction. Second, these works simply utilize ordinary
classification loss with one-hot phase labels to optimize the phase
predictions, and cannot fully explore surgical videos under inadequate
supervision. To overcome these two limitations, we propose a Surgical Temporal
Action-aware Network with sequence Regularization, named STAR-Net, to recognize
surgical phases more accurately from input videos. Specifically, we propose an
efficient multi-scale surgical temporal action (MS-STA) module, which
integrates visual features with spatial and temporal knowledge of surgical
actions at the cost of 2D networks. Moreover, we devise the dual-classifier
sequence regularization (DSR) to facilitate the training of STAR-Net by the
sequence guidance of an auxiliary classifier with a smaller capacity. Our
STAR-Net with MS-STA and DSR can exploit visual features of surgical actions
with effective regularization, thereby leading to the superior performance of
surgical phase recognition. Extensive experiments on a large-scale gastrectomy
surgery dataset and the public Cholec80 benchmark prove that our STAR-Net
significantly outperforms state-of-the-arts of surgical phase recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2023 IEEE International Conference on Bioinformatics and
  Biomedicine (BIBM 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mapping EEG Signals to Visual Stimuli: A Deep Learning Approach to Match
  vs. Mismatch Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqian Yang, Zhengqiao Zhao, Qian Wang, Yan Yang, Jingdong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches to modeling associations between visual stimuli and brain
responses are facing difficulties in handling between-subject variance and
model generalization. Inspired by the recent progress in modeling speech-brain
response, we propose in this work a "match-vs-mismatch" deep learning model to
classify whether a video clip induces excitatory responses in recorded EEG
signals and learn associations between the visual content and corresponding
neural recordings. Using an exclusive experimental dataset, we demonstrate that
the proposed model is able to achieve the highest accuracy on unseen subjects
as compared to other baseline models. Furthermore, we analyze the inter-subject
noise using a subject-level silhouette score in the embedding space and show
that the developed model is able to mitigate inter-subject noise and
significantly reduce the silhouette score. Moreover, we examine the Grad-CAM
activation score and show that the brain regions associated with language
processing contribute most to the model predictions, followed by regions
associated with visual processing. These results have the potential to
facilitate the development of neural recording-based video reconstruction and
its related applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Drilling Down into the Discourse Structure with LLMs for Long Document
  Question Answering <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inderjeet Nair, Shwetha Somasundaram, Apoorv Saxena, Koustava Goswami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the task of evidence retrieval for long document question
answering, which involves locating relevant paragraphs within a document to
answer a question. We aim to assess the applicability of large language models
(LLMs) in the task of zero-shot long document evidence retrieval, owing to
their unprecedented performance across various NLP tasks. However, currently
the LLMs can consume limited context lengths as input, thus providing document
chunks as inputs might overlook the global context while missing out on
capturing the inter-segment dependencies. Moreover, directly feeding the large
input sets can incur significant computational costs, particularly when
processing the entire document (and potentially incurring monetary expenses
with enterprise APIs like OpenAI's GPT variants). To address these challenges,
we propose a suite of techniques that exploit the discourse structure commonly
found in documents. By utilizing this structure, we create a condensed
representation of the document, enabling a more comprehensive understanding and
analysis of relationships between different parts. We retain $99.6\%$ of the
best zero-shot approach's performance, while processing only $26\%$ of the
total tokens used by the best approach in the information seeking evidence
retrieval setup. We also show how our approach can be combined with
\textit{self-ask} reasoning agent to achieve best zero-shot performance in
complex multi-hop question answering, just $\approx 4\%$ short of zero-shot
performance using gold evidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Findings of EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LM-Cocktail: Resilient Tuning of Language Models via Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shitao Xiao, Zheng Liu, Peitian Zhang, Xingrun Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pre-trained language models are continually fine-tuned to better support
downstream applications. However, this operation may result in significant
performance degeneration on general tasks beyond the targeted domain. To
overcome this problem, we propose a novel method which enables the fine-tuned
model to stay resilient in general perspectives. Our method is conducted in the
form of model merging (namely LM-Cocktail), where the fine-tuned language model
is merged with the pre-trained base model or the peer models from other domains
through weighted average. Despite simplicity, LM-Cocktail is surprisingly
effective: the resulted model is able to achieve a strong empirical performance
in the whole scope of general tasks while preserving a superior capacity in its
targeted domain. We conduct comprehensive experiments with LLama and BGE model
on popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the
efficacy of our proposed method. The code and checkpoints are available at
https://github.com/FlagOpen/FlagEmbedding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Analysis of Supportive Navigation on Movie Recommenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Sualeh Ali, Muhammed Maaz Tariq, Alina Ahmed, Abdul Razaque Soomro, Danysh Syed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This literature review covers the research and thought process that went into
making a solution for the infinite scrolling problem faced in streaming
services such as Netflix. Using the data collected, we have come to the
conclusion that an alternate layout can somewhat alleviate the problems it
takes in navigating a list of movies. We also found out by a comparative
analysis that some layouts, the circular one in particular, is advantageous in
certain settings making it an ideal candidate for a movie recommender system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This was an extensive survey and prototyping we did to purpose and
  alternative user interface for movie recommender systems like Netflix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact-based Court Judgment Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Kumar Nigam, Aniket Deroy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This extended abstract extends the research presented in "ILDC for CJPE:
Indian Legal Documents Corpus for Court Judgment Prediction and Explanation"
\cite{malik-etal-2021-ildc}, focusing on fact-based judgment prediction within
the context of Indian legal documents. We introduce two distinct problem
variations: one based solely on facts, and another combining facts with rulings
from lower courts (RLC). Our research aims to enhance early-phase case outcome
prediction, offering significant benefits to legal professionals and the
general public. The results, however, indicated a performance decline compared
to the original ILDC for CJPE study, even after implementing various weightage
schemes in our DELSumm algorithm. Additionally, using only facts for legal
judgment prediction with different transformer models yielded results inferior
to the state-of-the-art outcomes reported in the "ILDC for CJPE" study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Matrix Factorization for Interpretable Collaborative
  Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Sugahara, Kazushi Okamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matrix factorization (MF) is a simple collaborative filtering technique that
achieves superior recommendation accuracy by decomposing the user-item rating
matrix into user and item latent matrices. This approach relies on learning
from user-item interactions, which may not effectively capture the underlying
shared dependencies between users or items. Therefore, there is scope to
explicitly capture shared dependencies to further improve recommendation
accuracy and the interpretability of learning results by summarizing user-item
interactions. Based on these insights, we propose "Hierarchical Matrix
Factorization" (HMF), which incorporates clustering concepts to capture the
hierarchy, where leaf nodes and other nodes correspond to users/items and
clusters, respectively. Central to our approach, called hierarchical
embeddings, is the additional decomposition of the user and item latent
matrices (embeddings) into probabilistic connection matrices, which link the
hierarchy, and a root cluster latent matrix. Thus, each node is represented by
the weighted average of the embeddings of its parent clusters. The embeddings
are differentiable, allowing simultaneous learning of interactions and
clustering using a single gradient descent method. Furthermore, the obtained
cluster-specific interactions naturally summarize user-item interactions and
provide interpretability. Experimental results on rating and ranking
predictions demonstrated the competitiveness of HMF over vanilla and
hierarchical MF methods, especially its robustness in sparse interactions.
Additionally, it was confirmed that the clustering integration of HMF has the
potential for faster learning convergence and mitigation of overfitting
compared to MF, and also provides interpretability through a cluster-centered
case study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GENET: Unleashing the Power of Side Information for Recommendation via
  Hypergraph <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Li, Qi'ao Zhao, Chen Lin, Zhenjie Zhang, Xiaomin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation with side information has drawn significant research interest
due to its potential to mitigate user feedback sparsity. However, existing
models struggle with generalization across diverse domains and types of side
information. In particular, three challenges have not been addressed, and they
are (1) the diverse formats of side information, including text sequences. (2)
The diverse semantics of side information that describes items and users from
multi-level in a context different from recommendation systems. (3) The diverse
correlations in side information to measure similarity over multiple objects
beyond pairwise relations. In this paper, we introduce GENET (Generalized
hypErgraph pretraiNing on sidE informaTion), which pre-trains user and item
representations on feedback-irrelevant side information and fine-tunes the
representations on user feedback data. GENET leverages pre-training as a means
to prevent side information from overshadowing critical ID features and
feedback signals. It employs a hypergraph framework to accommodate various
types of diverse side information. During pre-training, GENET integrates tasks
for hyperlink prediction and self-supervised contrast to capture fine-grained
semantics at both local and global levels. Additionally, it introduces a unique
strategy to enhance pre-training robustness by perturbing positive samples
while maintaining high-order relations. Extensive experiments demonstrate that
GENET exhibits strong generalization capabilities, outperforming the SOTA
method by up to 38% in TOP-N recommendation and Sequential recommendation tasks
on various datasets with different side information.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">132</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual In-Context <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Chunyuan Li, Jianwei Yang, Lei Zhang, Jianfeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context prompting in large language models (LLMs) has become a prevalent
approach to improve zero-shot capabilities, but this idea is less explored in
the vision domain. Existing visual prompting methods focus on referring
segmentation to segment the most relevant object, falling short of addressing
many generic vision tasks like open-set segmentation and detection. In this
paper, we introduce a universal visual in-context prompting framework for both
tasks. In particular, we build on top of an encoder-decoder architecture, and
develop a versatile prompt encoder to support a variety of prompts like
strokes, boxes, and points. We further enhance it to take an arbitrary number
of reference image segments as the context. Our extensive explorations show
that the proposed visual in-context prompting elicits extraordinary referring
and generic segmentation capabilities to refer and detect, yielding competitive
performance to close-set in-domain datasets and showing promising results on
many open-set segmentation datasets. By joint training on COCO and SA-1B, our
model achieves $57.7$ PQ on COCO and $23.2$ PQ on ADE20K. Code will be
available at https://github.com/UX-Decoder/DINOv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, Varun Jampani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods for finetuning generative models for concept-driven personalization
generally achieve strong results for subject-driven or style-driven generation.
Recently, low-rank adaptations (LoRA) have been proposed as a
parameter-efficient way of achieving concept-driven personalization. While
recent work explores the combination of separate LoRAs to achieve joint
generation of learned styles and subjects, existing techniques do not reliably
address the problem; they often compromise either subject fidelity or style
fidelity. We propose ZipLoRA, a method to cheaply and effectively merge
independently trained style and subject LoRAs in order to achieve generation of
any user-provided subject in any user-provided style. Experiments on a wide
range of subject and style combinations show that ZipLoRA can generate
compelling results with meaningful improvements over baselines in subject and
style fidelity while preserving the ability to recontextualize. Project page:
https://ziplora.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ziplora.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Covariance alignment: from maximum likelihood estimation to
  Gromov-Wasserstein 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Han, Philippe Rigollet, George Stepaniants
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature alignment methods are used in many scientific disciplines for data
pooling, annotation, and comparison. As an instance of a permutation learning
problem, feature alignment presents significant statistical and computational
challenges. In this work, we propose the covariance alignment model to study
and compare various alignment methods and establish a minimax lower bound for
covariance alignment that has a non-standard dimension scaling because of the
presence of a nuisance parameter. This lower bound is in fact minimax optimal
and is achieved by a natural quasi MLE. However, this estimator involves a
search over all permutations which is computationally infeasible even when the
problem has moderate size. To overcome this limitation, we show that the
celebrated Gromov-Wasserstein algorithm from optimal transport which is more
amenable to fast implementation even on large-scale problems is also minimax
optimal. These results give the first statistical justification for the
deployment of the Gromov-Wasserstein algorithm in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Labeling Neural Representations with Inverse Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Bykov, Laura Kopf, Shinichi Nakajima, Marius Kloft, Marina M. -C. Höhne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) demonstrated remarkable capabilities in learning
complex hierarchical data representations, but the nature of these
representations remains largely unknown. Existing global explainability
methods, such as Network Dissection, face limitations such as reliance on
segmentation masks, lack of statistical significance testing, and high
computational demands. We propose Inverse Recognition (INVERT), a scalable
approach for connecting learned representations with human-understandable
concepts by leveraging their capacity to discriminate between these concepts.
In contrast to prior work, INVERT is capable of handling diverse types of
neurons, exhibits less computational complexity, and does not rely on the
availability of segmentation masks. Moreover, INVERT provides an interpretable
metric assessing the alignment between the representation and its corresponding
explanation and delivering a measure of statistical significance, emphasizing
its utility and credibility. We demonstrate the applicability of INVERT in
various scenarios, including the identification of representations affected by
spurious correlations, and the interpretation of the hierarchical structure of
decision-making within the models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk-sensitive Markov Decision Process and Learning under General
  Utility Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengqi Wu, Renyuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has gained substantial attention across diverse
application domains and theoretical investigations. Existing literature on RL
theory largely focuses on risk-neutral settings where the decision-maker learns
to maximize the expected cumulative reward. However, in practical scenarios
such as portfolio management and e-commerce recommendations, decision-makers
often persist in heterogeneous risk preferences subject to outcome
uncertainties, which can not be well-captured by the risk-neural framework.
Incorporating these preferences can be approached through utility theory, yet
the development of risk-sensitive RL under general utility functions remains an
open question for theoretical exploration.
  In this paper, we consider a scenario where the decision-maker seeks to
optimize a general utility function of the cumulative reward in the framework
of a Markov decision process (MDP). To facilitate the Dynamic Programming
Principle and Bellman equation, we enlarge the state space with an additional
dimension that accounts for the cumulative reward. We propose a discretized
approximation scheme to the MDP under enlarged state space, which is tractable
and key for algorithmic design. We then propose a modified value iteration
algorithm that employs an epsilon-covering over the space of cumulative reward.
When a simulator is accessible, our algorithm efficiently learns a near-optimal
policy with guaranteed sample complexity. In the absence of a simulator, our
algorithm, designed with an upper-confidence-bound exploration approach,
identifies a near-optimal policy while ensuring a guaranteed regret bound. For
both algorithms, we match the theoretical lower bounds for the risk-neutral
setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Serverless Machine Learning Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Kojs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in Generative AI, Computer Vision, and Natural Language
Processing have led to an increased integration of AI models into various
products. This widespread adoption of AI requires significant efforts in
deploying these models in production environments. When hosting machine
learning models for real-time predictions, it is important to meet defined
Service Level Objectives (SLOs), ensuring reliability, minimal downtime, and
optimizing operational costs of the underlying infrastructure. Large machine
learning models often demand GPU resources for efficient inference to meet
SLOs. In the context of these trends, there is growing interest in hosting AI
models in a serverless architecture while still providing GPU access for
inference tasks. This survey aims to summarize and categorize the emerging
challenges and optimization opportunities for large-scale deep learning serving
systems. By providing a novel taxonomy and summarizing recent trends, we hope
that this survey could shed light on new optimization perspectives and motivate
novel works in large-scale deep learning serving systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On diffusion-based generative models and their error bounds: The
  log-concave case with full convergence estimates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Bruno, Ying Zhang, Dong-Young Lim, Ömer Deniz Akyildiz, Sotirios Sabanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide full theoretical guarantees for the convergence behaviour of
diffusion-based generative models under the assumption of strongly logconcave
data distributions while our approximating class of functions used for score
estimation is made of Lipschitz continuous functions. We demonstrate via a
motivating example, sampling from a Gaussian distribution with unknown mean,
the powerfulness of our approach. In this case, explicit estimates are provided
for the associated optimization problem, i.e. score approximation, while these
are combined with the corresponding sampling estimates. As a result, we obtain
the best known upper bound estimates in terms of key quantities of interest,
such as the dimension and rates of convergence, for the Wasserstein-2 distance
between the data distribution (Gaussian with unknown mean) and our sampling
algorithm.
  Beyond the motivating example and in order to allow for the use of a diverse
range of stochastic optimizers, we present our results using an $L^2$-accurate
score estimation assumption, which crucially is formed under an expectation
with respect to the stochastic optimizer and our novel auxiliary process that
uses only known information. This approach yields the best known convergence
rate for our sampling algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Sampling for Deep Learning via Efficient Nonparametric Proxies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shabnam Daghaghi, Benjamin Coleman, Benito Geordie, Anshumali Shrivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data sampling is an effective method to improve the training speed of neural
networks, with recent results demonstrating that it can even break the neural
scaling laws. These results critically rely on high-quality scores to estimate
the importance of an input to the network. We observe that there are two
dominant strategies: static sampling, where the scores are determined before
training, and dynamic sampling, where the scores can depend on the model
weights. Static algorithms are computationally inexpensive but less effective
than their dynamic counterparts, which can cause end-to-end slowdown due to
their need to explicitly compute losses. To address this problem, we propose a
novel sampling distribution based on nonparametric kernel regression that
learns an effective importance score as the neural network trains. However,
nonparametric regression models are too computationally expensive to accelerate
end-to-end training. Therefore, we develop an efficient sketch-based
approximation to the Nadaraya-Watson estimator. Using recent techniques from
high-dimensional statistics and randomized algorithms, we prove that our
Nadaraya-Watson sketch approximates the estimator with exponential convergence
guarantees. Our sampling algorithm outperforms the baseline in terms of
wall-clock time and accuracy on four datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $σ$-PCA: a unified neural model for linear and nonlinear principal
  component analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fahdi Kanavati, Lucy Katsnith, Masayuki Tsuneki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear principal component analysis (PCA), nonlinear PCA, and linear
independent component analysis (ICA) -- those are three methods with
single-layer autoencoder formulations for learning linear transformations from
data. Linear PCA learns orthogonal transformations (rotations) that orient axes
to maximise variance, but it suffers from a subspace rotational indeterminacy:
it fails to find a unique rotation for axes that share the same variance. Both
nonlinear PCA and linear ICA reduce the subspace indeterminacy from rotational
to permutational by maximising statistical independence under the assumption of
unit variance. The main difference between them is that nonlinear PCA only
learns rotations while linear ICA learns not just rotations but any linear
transformation with unit variance. The relationship between all three can be
understood by the singular value decomposition of the linear ICA transformation
into a sequence of rotation, scale, rotation. Linear PCA learns the first
rotation; nonlinear PCA learns the second. The scale is simply the inverse of
the standard deviations. The problem is that, in contrast to linear PCA,
conventional nonlinear PCA cannot be used directly on the data to learn the
first rotation, the first being special as it reduces dimensionality and orders
by variances. In this paper, we have identified the cause, and as a solution we
propose $\sigma$-PCA: a unified neural model for linear and nonlinear PCA as
single-layer autoencoders. One of its key ingredients: modelling not just the
rotation but also the scale -- the variances. This model bridges the disparity
between linear and nonlinear PCA. And so, like linear PCA, it can learn a
semi-orthogonal transformation that reduces dimensionality and orders by
variances, but, unlike linear PCA, it does not suffer from rotational
indeterminacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Framework for Trace-induced Quantum Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beng Yee Gan, Daniel Leykam, Supanut Thanasilp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum kernel methods are promising candidates for achieving a practical
quantum advantage for certain machine learning tasks. Similar to classical
machine learning, an exact form of a quantum kernel is expected to have a great
impact on the model performance. In this work we combine all trace-induced
quantum kernels, including the commonly-used global fidelity and local
projected quantum kernels, into a common framework. We show how generalized
trace-induced quantum kernels can be constructed as combinations of the
fundamental building blocks we coin "Lego" kernels, which impose an inductive
bias on the resulting quantum models. We relate the expressive power and
generalization ability to the number of non-zero weight Lego kernels and
propose a systematic approach to increase the complexity of a quantum kernel
model, leading to a new form of the local projected kernels that require fewer
quantum resources in terms of the number of quantum gates and measurement
shots. We show numerically that models based on local projected kernels can
achieve comparable performance to the global fidelity quantum kernel. Our work
unifies existing quantum kernels and provides a systematic framework to compare
their properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 + 15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Numerical Integration in Reproducing Kernel Hil<span class="highlight-title">bert</span> Spaces via
  Leverage Scores Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Chatalic, Nicolas Schreuder, Ernesto De Vito, Lorenzo Rosasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we consider the problem of numerical integration, i.e.,
approximating integrals with respect to a target probability measure using only
pointwise evaluations of the integrand. We focus on the setting in which the
target distribution is only accessible through a set of $n$ i.i.d.
observations, and the integrand belongs to a reproducing kernel Hilbert space.
We propose an efficient procedure which exploits a small i.i.d. random subset
of $m<n$ samples drawn either uniformly or using approximate leverage scores
from the initial observations. Our main result is an upper bound on the
approximation error of this procedure for both sampling strategies. It yields
sufficient conditions on the subsample size to recover the standard (optimal)
$n^{-1/2}$ rate while reducing drastically the number of functions evaluations,
and thus the overall computational cost. Moreover, we obtain rates with respect
to the number $m$ of evaluations of the integrand which adapt to its
smoothness, and match known optimal rates for instance for Sobolev spaces. We
illustrate our theoretical findings with numerical experiments on real
datasets, which highlight the attractive efficiency-accuracy tradeoff of our
method compared to existing randomized and greedy quadrature methods. We note
that, the problem of numerical integration in RKHS amounts to designing a
discrete approximation of the kernel mean embedding of the target distribution.
As a consequence, direct applications of our results also include the efficient
computation of maximum mean discrepancies between distributions and the design
of efficient kernel-based tests.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 5 figures. Submitted to JMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear Log-Normal Attention with Unbiased Concentration <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yury Nahshan, Joseph Kampeas, Emir Haleva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models have achieved remarkable results in a wide range of
applications. However, their scalability is hampered by the quadratic time and
memory complexity of the self-attention mechanism concerning the sequence
length. This limitation poses a substantial obstacle when dealing with long
documents or high-resolution images. In this work, we study the self-attention
mechanism by analyzing the distribution of the attention matrix and its
concentration ability. Furthermore, we propose instruments to measure these
quantities and introduce a novel self-attention mechanism, Linear Log-Normal
Attention, designed to emulate the distribution and concentration behavior of
the original self-attention. Our experimental results on popular natural
language benchmarks reveal that our proposed Linear Log-Normal Attention
outperforms other linearized attention alternatives, offering a promising
avenue for enhancing the scalability of transformer models. Our code is
available in supplementary materials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 20 figures, 5 tables, submitted to ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learned Nonlinear Predictor for Critically Sampled 3D Point Cloud
  Attribute Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tam Thuc Do, Philip A. Chou, Gene Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study 3D point cloud attribute compression via a volumetric approach:
assuming point cloud geometry is known at both encoder and decoder, parameters
$\theta$ of a continuous attribute function $f: \mathbb{R}^3 \mapsto
\mathbb{R}$ are quantized to $\hat{\theta}$ and encoded, so that discrete
samples $f_{\hat{\theta}}(\mathbf{x}_i)$ can be recovered at known 3D points
$\mathbf{x}_i \in \mathbb{R}^3$ at the decoder. Specifically, we consider a
nested sequences of function subspaces $\mathcal{F}^{(p)}_{l_0} \subseteq
\cdots \subseteq \mathcal{F}^{(p)}_L$, where $\mathcal{F}_l^{(p)}$ is a family
of functions spanned by B-spline basis functions of order $p$, $f_l^*$ is the
projection of $f$ on $\mathcal{F}_l^{(p)}$ and encoded as low-pass coefficients
$F_l^*$, and $g_l^*$ is the residual function in orthogonal subspace
$\mathcal{G}_l^{(p)}$ (where $\mathcal{G}_l^{(p)} \oplus \mathcal{F}_l^{(p)} =
\mathcal{F}_{l+1}^{(p)}$) and encoded as high-pass coefficients $G_l^*$. In
this paper, to improve coding performance over [1], we study predicting
$f_{l+1}^*$ at level $l+1$ given $f_l^*$ at level $l$ and encoding of $G_l^*$
for the $p=1$ case (RAHT($1$)). For the prediction, we formalize RAHT(1) linear
prediction in MPEG-PCC in a theoretical framework, and propose a new nonlinear
predictor using a polynomial of bilateral filter. We derive equations to
efficiently compute the critically sampled high-pass coefficients $G_l^*$
amenable to encoding. We optimize parameters in our resulting feed-forward
network on a large training set of point clouds by minimizing a rate-distortion
Lagrangian. Experimental results show that our improved framework outperformed
the MPEG G-PCC predictor by $11$ to $12\%$ in bit rate reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speak Like a Native: <span class="highlight-title">Prompt</span>ing Large Language Models in a Native Style 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Yang, Yiwei Wang, Yinya Huang, Jing Xiong, Xiaodan Liang, Jing Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing work has found that the prompt engineering heavily influences the
performance of large language models (LLMs). Chain-of-thought (CoT), as a
popular prompt engineering technique, prompted LLMs using in-context examples
with reasoning steps. In current studies, the few-shot examples of CoT are
generally handcrafted by humans. However, how the text style of in-context
examples influence the outputs of LLMs still remains under-explored. This paper
presents a novel and effective approach, named \textbf{AlignCoT}, to improve
the reasoning capability of LLMs by aligning the in-context examples with the
native style of LLMs. ``Native'' refers to the inherent characteristic style of
LLMs which can be probed by original zero-shot scenarios. AlignCoT is
orthogonal to other prompt engineering methods, making it easy to combine with
state-of-the-art techniques to further improve the LLMs' performance. We
conduct extensive and comprehensive experiments on several benchmarks. The
empirical results demonstrate that our AlignCoTsignificantly improves
performance over the carefully handcrafted in-context examples. For instance,
with GPT-3.5-turbo, we observed a +2.5\% improvement on GSM8K. Furthermore, our
AlignCoT consistently improve the performance when combined with other
state-of-the-art prompt engineering methods. The source code and dataset will
be available at
\href{https://github.com/yangzhch6/AlignCoT}{https://github.com/yangzhch6/AlignCoT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging CNNs and Ensemble Learning for Automated Disaster Image
  Classification <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archit Rathod, Veer Pariawala, Mokshit Surana, Kumkum Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural disasters act as a serious threat globally, requiring effective and
efficient disaster management and recovery. This paper focuses on classifying
natural disaster images using Convolutional Neural Networks (CNNs). Multiple
CNN architectures were built and trained on a dataset containing images of
earthquakes, floods, wildfires, and volcanoes. A stacked CNN ensemble approach
proved to be the most effective, achieving 95% accuracy and an F1 score going
up to 0.96 for individual classes. Tuning hyperparameters of individual models
for optimization was critical to maximize the models' performance. The stacking
of CNNs with XGBoost acting as the meta-model utilizes the strengths of the CNN
and ResNet models to improve the overall accuracy of the classification.
Results obtained from the models illustrated the potency of CNN-based models
for automated disaster image classification. This lays the foundation for
expanding these techniques to build robust systems for disaster response,
damage assessment, and recovery management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures, 4 tables, ICSISCET 2023 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Naturalness of Attention: Revisiting Attention in Code Language Models <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mootez Saad, Tushar Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models for code such as CodeBERT offer the capability to learn
advanced source code representation, but their opacity poses barriers to
understanding of captured properties. Recent attention analysis studies provide
initial interpretability insights by focusing solely on attention weights
rather than considering the wider context modeling of Transformers. This study
aims to shed some light on the previously ignored factors of the attention
mechanism beyond the attention weights. We conduct an initial empirical study
analyzing both attention distributions and transformed representations in
CodeBERT. Across two programming languages, Java and Python, we find that the
scaled transformation norms of the input better capture syntactic structure
compared to attention weights alone. Our analysis reveals characterization of
how CodeBERT embeds syntactic code properties. The findings demonstrate the
importance of incorporating factors beyond just attention weights for
rigorously understanding neural code models. This lays the groundwork for
developing more interpretable models and effective uses of attention mechanisms
in program analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICSE-NIER (2024) track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying Dimensionality Reduction as Precursor to LSTM-CNN Models for
  Classifying Imagery and Motor Signals in ECoG-Based BCIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soham Bafana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motor impairments, frequently caused by neurological incidents like strokes
or traumatic brain injuries, present substantial obstacles in rehabilitation
therapy. This research aims to elevate the field by optimizing motor imagery
classification algorithms within Brain-Computer Interfaces (BCIs). By improving
the efficiency of BCIs, we offer a novel approach that holds significant
promise for enhancing motor rehabilitation outcomes. Utilizing unsupervised
techniques for dimensionality reduction, namely Uniform Manifold Approximation
and Projection (UMAP) coupled with K-Nearest Neighbors (KNN), we evaluate the
necessity of employing supervised methods such as Long Short-Term Memory (LSTM)
and Convolutional Neural Networks (CNNs) for classification tasks. Importantly,
participants who exhibited high KNN scores following UMAP dimensionality
reduction also achieved high accuracy in supervised deep learning (DL) models.
Due to individualized model requirements and massive neural training data,
dimensionality reduction becomes an effective preprocessing step that minimizes
the need for extensive data labeling and supervised deep learning techniques.
This approach has significant implications not only for targeted therapies in
motor dysfunction but also for addressing regulatory, safety, and reliability
concerns in the rapidly evolving BCI field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, 12 Figures. The dataset used in this paper can be found
  here: https://osf.io/ksqv8/download, from the Miller 2010 paper. All code
  used in this research can be found at
  https://github.com/bafanaS/dim-reduction-with-cnn-lstm.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bitformer: An efficient <span class="highlight-title">Transformer</span> with bitwise operation-based
  attention for Big Data Analytics at low-cost low-precision devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaoxiang Duan, Junkai Zhang, Xiaoying Zheng, Yongxin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current landscape of large models, the Transformer stands as a
cornerstone, playing a pivotal role in shaping the trajectory of modern models.
However, its application encounters challenges attributed to the substantial
computational intricacies intrinsic to its attention mechanism. Moreover, its
reliance on high-precision floating-point operations presents specific hurdles,
particularly evident in computation-intensive scenarios such as edge computing
environments. These environments, characterized by resource-constrained devices
and a preference for lower precision, necessitate innovative solutions.
  To tackle the exacting data processing demands posed by edge devices, we
introduce the Bitformer model, an inventive extension of the Transformer
paradigm. Central to this innovation is a novel attention mechanism that
adeptly replaces conventional floating-point matrix multiplication with bitwise
operations. This strategic substitution yields dual advantages. Not only does
it maintain the attention mechanism's prowess in capturing intricate long-range
information dependencies, but it also orchestrates a profound reduction in the
computational complexity inherent in the attention operation. The transition
from an $O(n^2d)$ complexity, typical of floating-point operations, to an
$O(n^2T)$ complexity characterizing bitwise operations, substantiates this
advantage. Notably, in this context, the parameter $T$ remains markedly smaller
than the conventional dimensionality parameter $d$.
  The Bitformer model in essence endeavors to reconcile the indomitable
requirements of modern computing landscapes with the constraints posed by edge
computing scenarios. By forging this innovative path, we bridge the gap between
high-performing models and resource-scarce environments, thus unveiling a
promising trajectory for further advancements in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Current Topological and Machine Learning Applications for Bias Detection
  in Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colleen Farrelly, Yashbir Singh, Quincy A. Hathaway, Gunnar Carlsson, Ashok Choudhary, Rahul Paul, Gianfranco Doretto, Yassine Himeur, Shadi Atalls, Wathiq Mansoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Institutional bias can impact patient outcomes, educational attainment, and
legal system navigation. Written records often reflect bias, and once bias is
identified; it is possible to refer individuals for training to reduce bias.
Many machine learning tools exist to explore text data and create predictive
models that can search written records to identify real-time bias. However, few
previous studies investigate large language model embeddings and geometric
models of biased text data to understand geometry's impact on bias modeling
accuracy. To overcome this issue, this study utilizes the RedditBias database
to analyze textual biases. Four transformer models, including BERT and RoBERTa
variants, were explored. Post-embedding, t-SNE allowed two-dimensional
visualization of data. KNN classifiers differentiated bias types, with lower
k-values proving more effective. Findings suggest BERT, particularly mini BERT,
excels in bias classification, while multilingual models lag. The
recommendation emphasizes refining monolingual models and exploring
domain-specific biases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grad-Shafranov equilibria via data-free physics informed neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byoungchan Jang, Alan A. Kaptanoglu, Rahul Gaur, Shaw Pan, Matt Landreman, William Dorland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A large number of magnetohydrodynamic (MHD) equilibrium calculations are
often required for uncertainty quantification, optimization, and real-time
diagnostic information, making MHD equilibrium codes vital to the field of
plasma physics. In this paper, we explore a method for solving the
Grad-Shafranov equation by using Physics-Informed Neural Networks (PINNs). For
PINNs, we optimize neural networks by directly minimizing the residual of the
PDE as a loss function. We show that PINNs can accurately and effectively solve
the Grad-Shafranov equation with several different boundary conditions. We also
explore the parameter space by varying the size of the model, the learning
rate, and boundary conditions to map various trade-offs such as between
reconstruction error and computational speed. Additionally, we introduce a
parameterized PINN framework, expanding the input space to include variables
such as pressure, aspect ratio, elongation, and triangularity in order to
handle a broader range of plasma scenarios within a single network.
Parametrized PINNs could be used in future work to solve inverse problems such
as shape optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Toxic Molecule Classification using Graph Neural Networks
  and Few Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhavya Mehta, Kush Kothari, Reshmika Nambiar, Seema Shrawne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional methods like Graph Convolutional Networks (GCNs) face challenges
with limited data and class imbalance, leading to suboptimal performance in
graph classification tasks during toxicity prediction of molecules as a whole.
To address these issues, we harness the power of Graph Isomorphic Networks,
Multi Headed Attention and Free Large-scale Adversarial Augmentation separately
on Graphs for precisely capturing the structural data of molecules and their
toxicological properties. Additionally, we incorporate Few-Shot Learning to
improve the model's generalization with limited annotated samples. Extensive
experiments on a diverse toxicology dataset demonstrate that our method
achieves an impressive state-of-art AUC-ROC value of 0.816, surpassing the
baseline GCN model by 11.4%. This highlights the significance of our proposed
methodology and Few Shot Learning in advancing Toxic Molecular Classification,
with the potential to enhance drug discovery and environmental risk assessment
processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep-learning-based acceleration of MRI for radiotherapy planning of
  pediatric patients with brain tumors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahinur Alam, Jinsoo Uh, Alexander Dresner, Chia-ho Hua, Khaled Khairy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic Resonance Imaging (MRI) is a non-invasive diagnostic and
radiotherapy (RT) planning tool, offering detailed insights into the anatomy of
the human body. The extensive scan time is stressful for patients, who must
remain motionless in a prolonged imaging procedure that prioritizes reduction
of imaging artifacts. This is challenging for pediatric patients who may
require measures for managing voluntary motions such as anesthesia. Several
computational approaches reduce scan time (fast MRI), by recording fewer
measurements and digitally recovering full information via post-acquisition
reconstruction. However, most fast MRI approaches were developed for diagnostic
imaging, without addressing reconstruction challenges specific to RT planning.
In this work, we developed a deep learning-based method (DeepMRIRec) for MRI
reconstruction from undersampled data acquired with RT-specific receiver coil
arrangements. We evaluated our method against fully sampled data of T1-weighted
MR images acquired from 73 children with brain tumors/surgical beds using loop
and posterior coils (12 channels), with and without applying virtual
compression of coil elements. DeepMRIRec reduced scanning time by a factor of
four producing a structural similarity score surpassing the evaluated
state-of-the-art method (0.960 vs 0.896), thereby demonstrating its potential
for accelerating MRI scanning for RT planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Translation to Control Formality Features in the Target Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshita Tyagi, Prashasta Jung, Hyowon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formality plays a significant role in language communication, especially in
low-resource languages such as Hindi, Japanese and Korean. These languages
utilise formal and informal expressions to convey messages based on social
contexts and relationships. When a language translation technique is used to
translate from a source language that does not pertain the formality (e.g.
English) to a target language that does, there is a missing information on
formality that could be a challenge in producing an accurate outcome. This
research explores how this issue should be resolved when machine learning
methods are used to translate from English to languages with formality, using
Hindi as the example data. This was done by training a bilingual model in a
formality-controlled setting and comparing its performance with a pre-trained
multilingual model in a similar setting. Since there are not a lot of training
data with ground truth, automated annotation techniques were employed to
increase the data size. The primary modeling approach involved leveraging
transformer models, which have demonstrated effectiveness in various natural
language processing tasks. We evaluate the official formality accuracy(ACC) by
comparing the predicted masked tokens with the ground truth. This metric
provides a quantitative measure of how well the translations align with the
desired outputs. Our study showcases a versatile translation strategy that
considers the nuances of formality in the target language, catering to diverse
language communication needs and scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, based on DCU MCM Practicum 2022/2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis of Linear Regression, Gaussian Elimination, and LU
  Decomposition for CT Real Estate Purchase Decisions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xilin Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive evaluation of three distinct
computational algorithms applied to the decision-making process of real estate
purchases. Specifically, we analyze the efficacy of Linear Regression from
Scikit-learn library, Gaussian Elimination with partial pivoting, and LU
Decomposition in predicting the advisability of buying a house in the State of
Connecticut based on a set of financial and market-related parameters. The
algorithms' performances were compared using a dataset encompassing
town-specific details, yearly data, interest rates, and median sale ratios. Our
results demonstrate significant differences in predictive accuracy, with Linear
Regression and LU Decomposition providing the most reliable recommendations and
Gaussian Elimination showing limitations in stability and performance. The
study's findings emphasize the importance of algorithm selection in predictive
analytic and offer insights into the practical applications of computational
methods in real estate investment strategies. By evaluating model efficacy
through metrics such as R-squared scores and Mean Squared Error, we provide a
nuanced understanding of each method's strengths and weaknesses, contributing
valuable knowledge to the fields of real estate analysis and predictive
modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Span-Based Optimal Sample Complexity for Average Reward MDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Zurek, Yudong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the sample complexity of learning an $\varepsilon$-optimal policy in
an average-reward Markov decision process (MDP) under a generative model. We
establish the complexity bound $\widetilde{O}\left(SA\frac{H}{\varepsilon^2}
\right)$, where $H$ is the span of the bias function of the optimal policy and
$SA$ is the cardinality of the state-action space. Our result is the first that
is minimax optimal (up to log factors) in all parameters $S,A,H$ and
$\varepsilon$, improving on existing work that either assumes uniformly bounded
mixing times for all policies or has suboptimal dependence on the parameters.
  Our result is based on reducing the average-reward MDP to a discounted MDP.
To establish the optimality of this reduction, we develop improved bounds for
$\gamma$-discounted MDPs, showing that
$\widetilde{O}\left(SA\frac{H}{(1-\gamma)^2\varepsilon^2} \right)$ samples
suffice to learn a $\varepsilon$-optimal policy in weakly communicating MDPs
under the regime that $\gamma \geq 1 - \frac{1}{H}$, circumventing the
well-known lower bound of
$\widetilde{\Omega}\left(SA\frac{1}{(1-\gamma)^3\varepsilon^2} \right)$ for
general $\gamma$-discounted MDPs. Our analysis develops upper bounds on certain
instance-dependent variance parameters in terms of the span parameter. These
bounds are tighter than those based on the mixing time or diameter of the MDP
and may be of broader use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Inference in Molecular Diffusion Models with Latent
  Representations of Protein Structure <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Dunn, David Ryan Koes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion generative models have emerged as a powerful framework for
addressing problems in structural biology and structure-based drug design.
These models operate directly on 3D molecular structures. Due to the
unfavorable scaling of graph neural networks (GNNs) with graph size as well as
the relatively slow inference speeds inherent to diffusion models, many
existing molecular diffusion models rely on coarse-grained representations of
protein structure to make training and inference feasible. However, such
coarse-grained representations discard essential information for modeling
molecular interactions and impair the quality of generated structures. In this
work, we present a novel GNN-based architecture for learning latent
representations of molecular structure. When trained end-to-end with a
diffusion model for de novo ligand design, our model achieves comparable
performance to one with an all-atom protein representation while exhibiting a
3-fold reduction in inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper appeared as a spotlight paper at the NeurIPS 2023
  Generative AI and Biology Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Bayesian Optimization with Active Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryota Ozaki, Kazuki Ishikawa, Youhei Kanzaki, Shinya Suzuki, Shion Takeno, Ichiro Takeuchi, Masayuki Karasuyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are a lot of real-world black-box optimization problems that need to
optimize multiple criteria simultaneously. However, in a multi-objective
optimization (MOO) problem, identifying the whole Pareto front requires the
prohibitive search cost, while in many practical scenarios, the decision maker
(DM) only needs a specific solution among the set of the Pareto optimal
solutions. We propose a Bayesian optimization (BO) approach to identifying the
most preferred solution in the MOO with expensive objective functions, in which
a Bayesian preference model of the DM is adaptively estimated by an interactive
manner based on the two types of supervisions called the pairwise preference
and improvement request. To explore the most preferred solution, we define an
acquisition function in which the uncertainty both in the objective functions
and the DM preference is incorporated. Further, to minimize the interaction
cost with the DM, we also propose an active learning strategy for the
preference estimation. We empirically demonstrate the effectiveness of our
proposed method through the benchmark function optimization and the
hyper-parameter optimization problems for machine learning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Tempered Hil<span class="highlight-title">bert</span> Simplex Distance and Its Application To Non-linear
  Embeddings of TEMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Amid, Frank Nielsen, Richard Nock, Manfred K. Warmuth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tempered Exponential Measures (TEMs) are a parametric generalization of the
exponential family of distributions maximizing the tempered entropy function
among positive measures subject to a probability normalization of their power
densities. Calculus on TEMs relies on a deformed algebra of arithmetic
operators induced by the deformed logarithms used to define the tempered
entropy. In this work, we introduce three different parameterizations of finite
discrete TEMs via Legendre functions of the negative tempered entropy function.
In particular, we establish an isometry between such parameterizations in terms
of a generalization of the Hilbert log cross-ratio simplex distance to a
tempered Hilbert co-simplex distance. Similar to the Hilbert geometry, the
tempered Hilbert distance is characterized as a $t$-symmetrization of the
oriented tempered Funk distance. We motivate our construction by introducing
the notion of $t$-lengths of smooth curves in a tautological Finsler manifold.
We then demonstrate the properties of our generalized structure in different
settings and numerically examine the quality of its differentiable
approximations for optimization in machine learning settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining high-dimensional text classifiers <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Odelia Melamed, Rich Caruana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability has become a valuable tool in the last few years, helping
humans better understand AI-guided decisions. However, the classic
explainability tools are sometimes quite limited when considering
high-dimensional inputs and neural network classifiers. We present a new
explainability method using theoretically proven high-dimensional properties in
neural network classifiers. We present two usages of it: 1) On the classical
sentiment analysis task for the IMDB reviews dataset, and 2) our
Malware-Detection task for our PowerShell scripts dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to "XAI in Action" workshop @ NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Non-Convex Optimization under the KL Condition
  with Optimal Rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Menart, Enayat Ullah, Raman Arora, Raef Bassily, Cristóbal Guzmán
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study private empirical risk minimization (ERM) problem for losses
satisfying the $(\gamma,\kappa)$-Kurdyka-{\L}ojasiewicz (KL) condition. The
Polyak-{\L}ojasiewicz (PL) condition is a special case of this condition when
$\kappa=2$. Specifically, we study this problem under the constraint of $\rho$
zero-concentrated differential privacy (zCDP). When $\kappa\in[1,2]$ and the
loss function is Lipschitz and smooth over a sufficiently large region, we
provide a new algorithm based on variance reduced gradient descent that
achieves the rate
$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$ on the
excess empirical risk, where $n$ is the dataset size and $d$ is the dimension.
We further show that this rate is nearly optimal. When $\kappa \geq 2$ and the
loss is instead Lipschitz and weakly convex, we show it is possible to achieve
the rate $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$
with a private implementation of the proximal point method. When the KL
parameters are unknown, we provide a novel modification and analysis of the
noisy gradient descent algorithm and show that this algorithm achieves a rate
of
$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^{\frac{2\kappa}{4-\kappa}}\big)$
adaptively, which is nearly optimal when $\kappa = 2$. We further show that,
without assuming the KL condition, the same gradient descent algorithm can
achieve fast convergence to a stationary point when the gradient stays
sufficiently large during the run of the algorithm. Specifically, we show that
this algorithm can approximate stationary points of Lipschitz, smooth (and
possibly nonconvex) objectives with rate as fast as
$\tilde{O}\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)$ and never worse than
$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^{1/2}\big)$. The latter
rate matches the best known rate for methods that do not rely on variance
reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Attacks and Defenses for Large Language Models on Coding Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Zhang, Zifan Wang, Ravi Mangal, Matt Fredrikson, Limin Jia, Corina Pasareanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large language models (LLMs), such as ChatGPT, have demonstrated
impressive capabilities for coding tasks including writing and reasoning about
code. They improve upon previous neural network models of code, such as
code2seq or seq2seq, that already demonstrated competitive results when
performing tasks such as code summarization and identifying code
vulnerabilities. However, these previous code models were shown vulnerable to
adversarial examples, i.e. small syntactic perturbations that do not change the
program's semantics, such as the inclusion of "dead code" through false
conditions or the addition of inconsequential print statements, designed to
"fool" the models. LLMs can also be vulnerable to the same adversarial
perturbations but a detailed study on this concern has been lacking so far. In
this paper we aim to investigate the effect of adversarial perturbations on
coding tasks with LLMs. In particular, we study the transferability of
adversarial examples, generated through white-box attacks on smaller code
models, to LLMs. Furthermore, to make the LLMs more robust against such
adversaries without incurring the cost of retraining, we propose prompt-based
defenses that involve modifying the prompt to include additional information
such as examples of adversarially perturbed code and explicit instructions for
reversing adversarial perturbations. Our experiments show that adversarial
examples obtained with a smaller code model are indeed transferable, weakening
the LLMs' performance. The proposed defenses show promise in improving the
model's resilience, paving the way to more robust defensive solutions for LLMs
in code-related applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Flows for Generative Modeling and Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinqing Zheng, Matt Le, Neta Shaul, Yaron Lipman, Aditya Grover, Ricky T. Q. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifier-free guidance is a key component for improving the performance of
conditional generative models for many downstream tasks. It drastically
improves the quality of samples produced, but has so far only been used for
diffusion models. Flow Matching (FM), an alternative simulation-free approach,
trains Continuous Normalizing Flows (CNFs) based on regressing vector fields.
It remains an open question whether classifier-free guidance can be performed
for Flow Matching models, and to what extent does it improve performance. In
this paper, we explore the usage of Guided Flows for a variety of downstream
applications involving conditional image generation, speech synthesis, and
reinforcement learning. In particular, we are the first to apply flow models to
the offline reinforcement learning setting. We also show that Guided Flows
significantly improves the sample quality in image generation and zero-shot
text-to-speech synthesis, and can make use of drastically low amounts of
computation without affecting the agent's overall performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recurrent neural networks and transfer learning for elasto-plasticity in
  woven composites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Ghane, Martin Fagerström, Mohsen Mirkhalaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a surrogate for computationally intensive meso-scale simulation of woven
composites, this article presents Recurrent Neural Network (RNN) models.
Leveraging the power of transfer learning, the initialization challenges and
sparse data issues inherent in cyclic shear strain loads are addressed in the
RNN models. A mean-field model generates a comprehensive data set representing
elasto-plastic behavior. In simulations, arbitrary six-dimensional strain
histories are used to predict stresses under random walking as the source task
and cyclic loading conditions as the target task. Incorporating sub-scale
properties enhances RNN versatility. In order to achieve accurate predictions,
the model uses a grid search method to tune network architecture and
hyper-parameter configurations. The results of this study demonstrate that
transfer learning can be used to effectively adapt the RNN to varying strain
conditions, which establishes its potential as a useful tool for modeling
path-dependent responses in woven composites.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There are 25 pages and 13 EPS images. The paper includes links to
  supporting materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting individual variable information for their decoupling, direct
  mutual information and multi-feature Granger causality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jarek Duda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Working with multiple variables they usually contain difficult to control
complex dependencies. This article proposes extraction of their individual
information, e.g. $\overline{X|Y}$ as random variable containing information
from $X$, but with removed information about $Y$, by using $(x,y)
\leftrightarrow (\bar{x}=\textrm{CDF}_{X|Y=y}(x),y)$ reversible normalization.
One application can be decoupling of individual information of variables:
reversibly transform $(X_1,\ldots,X_n)\leftrightarrow(\tilde{X}_1,\ldots
\tilde{X}_n)$ together containing the same information, but being independent:
$\forall_{i\neq j} \tilde{X}_i\perp \tilde{X}_j, \tilde{X}_i\perp X_j$. It
requires detailed models of complex conditional probability distributions - it
is generally a difficult task, but here can be done through multiple dependency
reducing iterations, using imperfect methods (here HCR: Hierarchical
Correlation Reconstruction). It could be also used for direct mutual
information - evaluating direct information transfer: without use of
intermediate variables. For causality direction there is discussed
multi-feature Granger causality, e.g. to trace various types of individual
information transfers between such decoupled variables, including propagation
time (delay).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Images to Connections: Can DQN with GNNs learn the Strategic Game
  of Hex? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannik Keller, Jannis Blüml, Gopika Sudhakaran, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The gameplay of strategic board games such as chess, Go and Hex is often
characterized by combinatorial, relational structures -- capturing distinct
interactions and non-local patterns -- and not just images. Nonetheless, most
common self-play reinforcement learning (RL) approaches simply approximate
policy and value functions using convolutional neural networks (CNN). A key
feature of CNNs is their relational inductive bias towards locality and
translational invariance. In contrast, graph neural networks (GNN) can encode
more complicated and distinct relational structures. Hence, we investigate the
crucial question: Can GNNs, with their ability to encode complex connections,
replace CNNs in self-play reinforcement learning? To this end, we do a
comparison with Hex -- an abstract yet strategically rich board game -- serving
as our experimental platform. Our findings reveal that GNNs excel at dealing
with long range dependency situations in game states and are less prone to
overfitting, but also showing a reduced proficiency in discerning local
patterns. This suggests a potential paradigm shift, signaling the use of
game-specific structures to reshape self-play reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian inference of a new Mallows model for characterising symptom
  sequences applied in primary progressive aphasia <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatrice Taylor, Cameron Shand, Chris J. D. Hardy, Neil Oxtoby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models offer the potential to understand diverse datasets in
a data-driven way, powering insights into individual disease experiences and
ensuring equitable healthcare. In this study, we explore Bayesian inference for
characterising symptom sequences, and the associated modelling challenges. We
adapted the Mallows model to account for partial rankings and right-censored
data, employing custom MCMC fitting. Our evaluation, encompassing synthetic
data and a primary progressive aphasia dataset, highlights the model's efficacy
in revealing mean orderings and estimating ranking variance. This holds the
potential to enhance clinical comprehension of symptom occurrence. However, our
work encounters limitations concerning model scalability and small dataset
sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended Abstract presented at Machine Learning for Health (ML4H)
  symposium 2023, December 10th, 2023, New Orleans, United States, 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidant: Customizing <span class="highlight-title">Transformer</span>-based LLMs via Collaborative Edge
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Chen, Yuxuan Yan, Qianqian Yang, Yuanchao Shu, Shibo He, Jiming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large language models (LLMs) have demonstrated impressive
capabilities in a variety of natural language processing (NLP) tasks.
Nonetheless, it is challenging to deploy and fine-tune LLMs on mobile edge
devices with limited computing, memory, and energy budgets. In this paper, we
propose Confidant, a multi-backend collaborative training framework for
customizing state-of-the-art LLMs on commodity mobile devices like smartphones.
Confidant partitions an LLM into several sub-models so that each fits into a
mobile device's memory. A pipeline parallel training mechanism is further
developed to ensure fast and efficient distributed training. In addition, we
propose a novel backend scheduler to allocate different attention heads to
heterogeneous compute hardware, including mobile CPU and GPUs, to maximize the
compute resource utilization on each edge device. Our preliminary experimental
results show that Confidant achieves at most 45.3% memory reduction and 8.03x
inference speedup in practical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures; Submitted to HotMobile 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of Uncertainty Estimation Techniques for Detecting
  Drift in Data Streams <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Winter, Nicolas Jourdan, Tristan Wirth, Volker Knauthe, Arjan Kuijper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In safety-critical domains such as autonomous driving and medical diagnosis,
the reliability of machine learning models is crucial. One significant
challenge to reliability is concept drift, which can cause model deterioration
over time. Traditionally, drift detectors rely on true labels, which are often
scarce and costly. This study conducts a comprehensive empirical evaluation of
using uncertainty values as substitutes for error rates in detecting drifts,
aiming to alleviate the reliance on labeled post-deployment data. We examine
five uncertainty estimation methods in conjunction with the ADWIN detector
across seven real-world datasets. Our results reveal that while the SWAG method
exhibits superior calibration, the overall accuracy in detecting drifts is not
notably impacted by the choice of uncertainty estimation method, with even the
most basic method demonstrating competitive performance. These findings offer
valuable insights into the practical applicability of uncertainty-based drift
detection in real-world, safety-critical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023: Workshop on Distribution Shifts</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Classification and Rejection: A One-versus-All Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifying patterns of known classes and rejecting ambiguous and novel (also
called as out-of-distribution (OOD)) inputs are involved in open world pattern
recognition. Deep neural network models usually excel in closed-set
classification while performing poorly in rejecting OOD. To tackle this
problem, numerous methods have been designed to perform open set recognition
(OSR) or OOD rejection/detection tasks. Previous methods mostly take
post-training score transformation or hybrid models to ensure low scores on OOD
inputs while separating known classes. In this paper, we attempt to build a
unified framework for building open set classifiers for both classification and
OOD rejection. We formulate the open set recognition of $ K $-known-class as a
$ (K + 1) $-class classification problem with model trained on known-class
samples only. By decomposing the $ K $-class problem into $ K $ one-versus-all
(OVA) binary classification tasks and binding some parameters, we show that
combining the scores of OVA classifiers can give $ (K + 1) $-class posterior
probabilities, which enables classification and OOD rejection in a unified
framework. To maintain the closed-set classification accuracy of the OVA
trained classifier, we propose a hybrid training strategy combining OVA loss
and multi-class cross-entropy loss. We implement the OVA framework and hybrid
training strategy on the recently proposed convolutional prototype network.
Experiments on popular OSR and OOD detection datasets demonstrate that the
proposed framework, using a single multi-class classifier, yields competitive
performance in closed-set classification, OOD detection, and misclassification
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact-based Court Judgment Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Kumar Nigam, Aniket Deroy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This extended abstract extends the research presented in "ILDC for CJPE:
Indian Legal Documents Corpus for Court Judgment Prediction and Explanation"
\cite{malik-etal-2021-ildc}, focusing on fact-based judgment prediction within
the context of Indian legal documents. We introduce two distinct problem
variations: one based solely on facts, and another combining facts with rulings
from lower courts (RLC). Our research aims to enhance early-phase case outcome
prediction, offering significant benefits to legal professionals and the
general public. The results, however, indicated a performance decline compared
to the original ILDC for CJPE study, even after implementing various weightage
schemes in our DELSumm algorithm. Additionally, using only facts for legal
judgment prediction with different transformer models yielded results inferior
to the state-of-the-art outcomes reported in the "ILDC for CJPE" study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Corti, Balz Maag, Joachim Schauer, Ulrich Pferschy, Olga Saukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep models deployed on edge devices frequently encounter resource
variability, which arises from fluctuating energy levels, timing constraints,
or prioritization of other critical tasks within the system. State-of-the-art
machine learning pipelines generate resource-agnostic models, not capable to
adapt at runtime. In this work we introduce Resource-Efficient Deep Subnetworks
(REDS) to tackle model adaptation to variable resources. In contrast to the
state-of-the-art, REDS use structured sparsity constructively by exploiting
permutation invariance of neurons, which allows for hardware-specific
optimizations. Specifically, REDS achieve computational efficiency by (1)
skipping sequential computational blocks identified by a novel iterative
knapsack optimizer, and (2) leveraging simple math to re-arrange the order of
operations in REDS computational graph to take advantage of the data cache.
REDS support conventional deep networks frequently deployed on the edge and
provide computational benefits even for small and simple networks. We evaluate
REDS on six benchmark architectures trained on the Google Speech Commands,
FMNIST and CIFAR10 datasets, and test on four off-the-shelf mobile and embedded
hardware platforms. We provide a theoretical result and empirical evidence for
REDS outstanding performance in terms of submodels' test set accuracy, and
demonstrate an adaptation time in response to dynamic resource constraints of
under 40$\mu$s, utilizing a 2-layer fully-connected network on Arduino Nano 33
BLE Sense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MergeSFL: Split Federated Learning with Feature Merging and Batch Size
  Regulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunming Liao, Yang Xu, Hongli Xu, Lun Wang, Zhiwei Yao, Chunming Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, federated learning (FL) has emerged as a popular technique for edge
AI to mine valuable knowledge in edge computing (EC) systems. To mitigate the
computing/communication burden on resource-constrained workers and protect
model privacy, split federated learning (SFL) has been released by integrating
both data and model parallelism. Despite resource limitations, SFL still faces
two other critical challenges in EC, i.e., statistical heterogeneity and system
heterogeneity. To address these challenges, we propose a novel SFL framework,
termed MergeSFL, by incorporating feature merging and batch size regulation in
SFL. Concretely, feature merging aims to merge the features from workers into a
mixed feature sequence, which is approximately equivalent to the features
derived from IID data and is employed to promote model accuracy. While batch
size regulation aims to assign diverse and suitable batch sizes for
heterogeneous workers to improve training efficiency. Moreover, MergeSFL
explores to jointly optimize these two strategies upon their coupled
relationship to better enhance the performance of SFL. Extensive experiments
are conducted on a physical platform with 80 NVIDIA Jetson edge devices, and
the experimental results show that MergeSFL can improve the final model
accuracy by 5.82% to 26.22%, with a speedup by about 1.74x to 4.14x, compared
to the baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning principle and mathematical realization of the learning
  mechanism in the brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taisuke Katayose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning has achieved remarkable success, there is no clear
explanation about why it works so well. In order to discuss this question
quantitatively, we need a mathematical framework that explains what learning is
in the first place. After several considerations, we succeeded in constructing
a mathematical framework that can provide a unified understanding of all types
of learning, including deep learning and learning in the brain. We call it
learning principle, and it follows that all learning is equivalent to
estimating the probability of input data. We not only derived this principle,
but also mentioned its application to actual machine learning models. For
example, we found that conventional supervised learning is equivalent to
estimating conditional probabilities, and succeeded in making supervised
learning more effective and generalized. We also proposed a new method of
defining the values of estimated probability using differentiation, and showed
that unsupervised learning can be performed on arbitrary dataset without any
prior knowledge. Namely, this method is a general-purpose machine learning in
the true sense. Moreover, we succeeded in describing the learning mechanism in
the brain by considering the time evolution of a fully or partially connected
model and applying this new method. The learning principle provides solutions
to many unsolved problems in deep learning and cognitive neuroscience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curriculum Learning and Imitation Learning for Model-free Control on
  Financial Time-series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Woosung Koh, Insu Choi, Yuntae Jang, Gimin Kang, Woo Chang Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curriculum learning and imitation learning have been leveraged extensively in
the robotics domain. However, minimal research has been done on leveraging
these ideas on control tasks over highly stochastic time-series data. Here, we
theoretically and empirically explore these approaches in a representative
control task over complex time-series data. We implement the fundamental ideas
of curriculum learning via data augmentation, while imitation learning is
implemented via policy distillation from an oracle. Our findings reveal that
curriculum learning should be considered a novel direction in improving
control-task performance over complex time-series. Our ample random-seed
out-sample empirics and ablation studies are highly encouraging for curriculum
learning for time-series control. These findings are especially encouraging as
we tune all overlapping hyperparameters on the baseline -- giving an advantage
to the baseline. On the other hand, we find that imitation learning should be
used with caution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Supervision for Continual Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Marczak, Sebastian Cygert, Tomasz Trzciński, Bartłomiej Twardowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of continual learning, models are designed to learn tasks one
after the other. While most research has centered on supervised continual
learning, recent studies have highlighted the strengths of self-supervised
continual representation learning. The improved transferability of
representations built with self-supervised methods is often associated with the
role played by the multi-layer perceptron projector. In this work, we depart
from this observation and reexamine the role of supervision in continual
representation learning. We reckon that additional information, such as human
annotations, should not deteriorate the quality of representations. Our
findings show that supervised models when enhanced with a multi-layer
perceptron head, can outperform self-supervised models in continual
representation learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Vascular Segmentation and Applications in Phase
  Contrast Tomography Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekin Yagis, Shahab Aslani, Yashvardhan Jain, Yang Zhou, Shahrokh Rahmani, Joseph Brunet, Alexandre Bellier, Christopher Werlein, Maximilian Ackermann, Danny Jonigk, Paul Tafforeau, Peter D Lee, Claire Walsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated blood vessel segmentation is vital for biomedical imaging, as
vessel changes indicate many pathologies. Still, precise segmentation is
difficult due to the complexity of vascular structures, anatomical variations
across patients, the scarcity of annotated public datasets, and the quality of
images. We present a thorough literature review, highlighting the state of
machine learning techniques across diverse organs. Our goal is to provide a
foundation on the topic and identify a robust baseline model for application to
vascular segmentation in a new imaging modality, Hierarchical Phase Contrast
Tomography (HiP CT). Introduced in 2020 at the European Synchrotron Radiation
Facility, HiP CT enables 3D imaging of complete organs at an unprecedented
resolution of ca. 20mm per voxel, with the capability for localized zooms in
selected regions down to 1mm per voxel without sectioning. We have created a
training dataset with double annotator validated vascular data from three
kidneys imaged with HiP CT in the context of the Human Organ Atlas Project.
Finally, utilising the nnU Net model, we conduct experiments to assess the
models performance on both familiar and unseen samples, employing vessel
specific metrics. Our results show that while segmentations yielded reasonably
high scores such as clDice values ranging from 0.82 to 0.88, certain errors
persisted. Large vessels that collapsed due to the lack of hydrostatic pressure
(HiP CT is an ex vivo technique) were segmented poorly. Moreover, decreased
connectivity in finer vessels and higher segmentation errors at vessel
boundaries were observed. Such errors obstruct the understanding of the
structures by interrupting vascular tree connectivity. Through our review and
outputs, we aim to set a benchmark for subsequent model evaluations using
various modalities, especially with the HiP CT imaging database.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Inference in Reinforcement Learning Done Right <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Tarbouriech, Tor Lattimore, Brendan O'Donoghue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A popular perspective in Reinforcement learning (RL) casts the problem as
probabilistic inference on a graphical model of the Markov decision process
(MDP). The core object of study is the probability of each state-action pair
being visited under the optimal policy. Previous approaches to approximate this
quantity can be arbitrarily poor, leading to algorithms that do not implement
genuine statistical inference and consequently do not perform well in
challenging problems. In this work, we undertake a rigorous Bayesian treatment
of the posterior probability of state-action optimality and clarify how it
flows through the MDP. We first reveal that this quantity can indeed be used to
generate a policy that explores efficiently, as measured by regret.
Unfortunately, computing it is intractable, so we derive a new variational
Bayesian approximation yielding a tractable convex optimization problem and
establish that the resulting policy also explores efficiently. We call our
approach VAPOR and show that it has strong connections to Thompson sampling,
K-learning, and maximum entropy exploration. We conclude with some experiments
demonstrating the performance advantage of a deep RL version of VAPOR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Influence of Neural Networks on Hydropower Plant Management in
  Agriculture: Addressing Challenges and Exploring Untapped Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. Coelho, M. Fernanda P. Costa, L. L. Ferrás
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hydropower plants are crucial for stable renewable energy and serve as vital
water sources for sustainable agriculture. However, it is essential to assess
the current water management practices associated with hydropower plant
management software. A key concern is the potential conflict between
electricity generation and agricultural water needs. Prioritising water for
electricity generation can reduce irrigation availability in agriculture during
crucial periods like droughts, impacting crop yields and regional food
security. Coordination between electricity and agricultural water allocation is
necessary to ensure optimal and environmentally sound practices. Neural
networks have become valuable tools for hydropower plant management, but their
black-box nature raises concerns about transparency in decision making.
Additionally, current approaches often do not take advantage of their potential
to create a system that effectively balances water allocation.
  This work is a call for attention and highlights the potential risks of
deploying neural network-based hydropower plant management software without
proper scrutiny and control. To address these concerns, we propose the adoption
of the Agriculture Conscious Hydropower Plant Management framework, aiming to
maximise electricity production while prioritising stable irrigation for
agriculture. We also advocate reevaluating government-imposed minimum water
guidelines for irrigation to ensure flexibility and effective water allocation.
Additionally, we suggest a set of regulatory measures to promote model
transparency and robustness, certifying software that makes conscious and
intelligent water allocation decisions, ultimately safeguarding agriculture
from undue strain during droughts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving performance of heart rate time series classification by
  grouping subjects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Beekhuizen, Arman Naseri, David Tax, Ivo van der Bilt, Marcel Reinders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike the more commonly analyzed ECG or PPG data for activity
classification, heart rate time series data is less detailed, often noisier and
can contain missing data points. Using the BigIdeasLab_STEP dataset, which
includes heart rate time series annotated with specific tasks performed by
individuals, we sought to determine if general classification was achievable.
Our analyses showed that the accuracy is sensitive to the choice of
window/stride size. Moreover, we found variable classification performances
between subjects due to differences in the physical structure of their hearts.
Various techniques were used to minimize this variability. First of all,
normalization proved to be a crucial step and significantly improved the
performance. Secondly, grouping subjects and performing classification inside a
group helped to improve performance and decrease inter-subject variability.
Finally, we show that including handcrafted features as input to a deep
learning (DL) network improves the classification performance further.
Together, these findings indicate that heart rate time series can be utilized
for classification tasks like predicting activity. However, normalization or
grouping techniques need to be chosen carefully to minimize the issue of
subject variability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Evaluation of GNN Training Systems: A Data Management
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yuan, Yajiong Liu, Yanfeng Zhang, Xin Ai, Qiange Wang, Chaoyi Chen, Yu Gu, Ge Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many Graph Neural Network (GNN) training systems have emerged recently to
support efficient GNN training. Since GNNs embody complex data dependencies
between training samples, the training of GNNs should address distinct
challenges different from DNN training in data management, such as data
partitioning, batch preparation for mini-batch training, and data transferring
between CPUs and GPUs. These factors, which take up a large proportion of
training time, make data management in GNN training more significant. This
paper reviews GNN training from a data management perspective and provides a
comprehensive analysis and evaluation of the representative approaches. We
conduct extensive experiments on various benchmark datasets and show many
interesting and valuable results. We also provide some practical tips learned
from these experiments, which are helpful for designing GNN training systems in
the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedFN: Feature Normalization for Alleviating Data Heterogeneity Problem
  in Federated Learning <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongyoon Kim, Gihun Lee, Jaehoon Oh, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a collaborative method for training models while
preserving data privacy in decentralized settings. However, FL encounters
challenges related to data heterogeneity, which can result in performance
degradation. In our study, we observe that as data heterogeneity increases,
feature representation in the FedAVG model deteriorates more significantly
compared to classifier weight. Additionally, we observe that as data
heterogeneity increases, the gap between higher feature norms for observed
classes, obtained from local models, and feature norms of unobserved classes
widens, in contrast to the behavior of classifier weight norms. This widening
gap extends to encompass the feature norm disparities between local and the
global models. To address these issues, we introduce Federated Averaging with
Feature Normalization Update (FedFN), a straightforward learning method. We
demonstrate the superior performance of FedFN through extensive experiments,
even when applied to pretrained ResNet18. Subsequently, we confirm the
applicability of FedFN to foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS Workshop: "Federated Learning in the Age of Foundation
  Models" 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved identification accuracy in equation learning via comprehensive
  $\boldsymbol{R^2}$-elimination and Bayesian model selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Nickelsen, Bubacarr Bah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of equation learning, exhaustively considering all possible
equations derived from a basis function dictionary is infeasible. Sparse
regression and greedy algorithms have emerged as popular approaches to tackle
this challenge. However, the presence of multicollinearity poses difficulties
for sparse regression techniques, and greedy steps may inadvertently exclude
terms of the true equation, leading to reduced identification accuracy. In this
article, we present an approach that strikes a balance between
comprehensiveness and efficiency in equation learning. Inspired by stepwise
regression, our approach combines the coefficient of determination, $R^2$, and
the Bayesian model evidence, $p(\boldsymbol y|\mathcal M)$, in a novel way. Our
procedure is characterized by a comprehensive search with just a minor
reduction of the model space at each iteration step. With two flavors of our
approach and the adoption of $p(\boldsymbol y|\mathcal M)$ for bi-directional
stepwise regression, we present a total of three new avenues for equation
learning. Through three extensive numerical experiments involving random
polynomials and dynamical systems, we compare our approach against four
state-of-the-art methods and two standard approaches. The results demonstrate
that our comprehensive search approach surpasses all other methods in terms of
identification accuracy. In particular, the second flavor of our approach
establishes an efficient overfitting penalty solely based on $R^2$, which
achieves highest rates of exact equation recovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages main text and 11 pages appendix, accepted in Transactions on
  Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Immunohistochemistry guided segmentation of benign epithelial cells, in
  situ lesions, and invasive epithelial cells in breast cancer slides 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maren Høibø, André Pedersen, Vibeke Grotnes Dale, Sissel Marie Berget, Borgny Ytterhus, Cecilia Lindskog, Elisabeth Wik, Lars A. Akslen, Ingerid Reinertsen, Erik Smistad, Marit Valla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital pathology enables automatic analysis of histopathological sections
using artificial intelligence (AI). Automatic evaluation could improve
diagnostic efficiency and help find associations between morphological features
and clinical outcome. For development of such prediction models, identifying
invasive epithelial cells, and separating these from benign epithelial cells
and in situ lesions would be the first step. In this study, we aimed to develop
an AI model for segmentation of epithelial cells in sections from breast
cancer. We generated epithelial ground truth masks by restaining hematoxylin
and eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists'
annotations. HE/CK image pairs were used to train a convolutional neural
network, and data augmentation was used to make the model more robust. Tissue
microarrays (TMAs) from 839 patients, and whole slide images from two patients
were used for training and evaluation of the models. The sections were derived
from four cohorts of breast cancer patients. TMAs from 21 patients from a fifth
cohort was used as a second test set. In quantitative evaluation, a mean Dice
score of 0.70, 0.79, and 0.75 for invasive epithelial cells, benign epithelial
cells, and in situ lesions, respectively, were achieved. In qualitative scoring
(0-5) by pathologists, results were best for all epithelium and invasive
epithelium, with scores of 4.7 and 4.4. Scores for benign epithelium and in
situ lesions were 3.7 and 2.0. The proposed model segmented epithelial cells in
HE stained breast cancer slides well, but further work is needed for accurate
division between the classes. Immunohistochemistry, together with pathologists'
annotations, enabled the creation of accurate ground truths. The model is made
freely available in FastPathology and the code is available at
https://github.com/AICAN-Research/breast-epithelium-segmentation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures. Submitted to a scientific journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided
  Code-Vision Representation <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyi Chen, Xingyao Wang, Manling Li, Derek Hoiem, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art vision-language models (VLMs) still have limited performance
in structural knowledge extraction, such as relations between objects. In this
work, we present ViStruct, a training framework to learn VLMs for effective
visual structural knowledge extraction. Two novel designs are incorporated.
First, we propose to leverage the inherent structure of programming language to
depict visual structural information. This approach enables explicit and
consistent representation of visual structural information of multiple
granularities, such as concepts, relations, and events, in a well-organized
structured format. Second, we introduce curriculum-based learning for VLMs to
progressively comprehend visual structures, from fundamental visual concepts to
intricate event structures. Our intuition is that lower-level knowledge may
contribute to complex visual structure understanding. Furthermore, we compile
and release a collection of datasets tailored for visual structural knowledge
extraction. We adopt a weakly-supervised approach to directly generate visual
event structures from captions for ViStruct training, capitalizing on abundant
image-caption pairs from the web. In experiments, we evaluate ViStruct on
visual structure prediction tasks, demonstrating its effectiveness in improving
the understanding of visual structures. The code is public at
\url{https://github.com/Yangyi-Chen/vi-struct}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Hetero-Client Federated Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Lu, Suizhi Huang, Yuwen Yang, Shalayiding Sirejiding, Yue Ding, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables joint training across distributed clients
using their local data privately. Federated Multi-Task Learning (FMTL) builds
on FL to handle multiple tasks, assuming model congruity that identical model
architecture is deployed in each client. To relax this assumption and thus
extend real-world applicability, we introduce a novel problem setting,
Hetero-Client Federated Multi-Task Learning (HC-FMTL), to accommodate diverse
task setups. The main challenge of HC-FMTL is the model incongruity issue that
invalidates conventional aggregation methods. It also escalates the
difficulties in accurate model aggregation to deal with data and task
heterogeneity inherent in FMTL. To address these challenges, we propose the
FedHCA$^2$ framework, which allows for federated training of personalized
models by modeling relationships among heterogeneous clients. Drawing on our
theoretical insights into the difference between multi-task and federated
optimization, we propose the Hyper Conflict-Averse Aggregation scheme to
mitigate conflicts during encoder updates. Additionally, inspired by task
interaction in MTL, the Hyper Cross Attention Aggregation scheme uses
layer-wise cross attention to enhance decoder interactions while alleviating
model incongruity. Moreover, we employ learnable Hyper Aggregation Weights for
each client to customize personalized parameter updates. Extensive experiments
demonstrate the superior performance of FedHCA$^2$ in various HC-FMTL scenarios
compared to representative methods. Our code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hard Label Black Box Node Injection Attack on Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhou, Zihao Dong, Guofeng Zhang, Jingchen Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While graph neural networks have achieved state-of-the-art performances in
many real-world tasks including graph classification and node classification,
recent works have demonstrated they are also extremely vulnerable to
adversarial attacks. Most previous works have focused on attacking node
classification networks under impractical white-box scenarios. In this work, we
will propose a non-targeted Hard Label Black Box Node Injection Attack on Graph
Neural Networks, which to the best of our knowledge, is the first of its kind.
Under this setting, more real world tasks can be studied because our attack
assumes no prior knowledge about (1): the model architecture of the GNN we are
attacking; (2): the model's gradients; (3): the output logits of the target GNN
model. Our attack is based on an existing edge perturbation attack, from which
we restrict the optimization process to formulate a node injection attack. In
the work, we will evaluate the performance of the attack using three datasets,
COIL-DEL, IMDB-BINARY, and NCI1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Human Feedback to Fine-tune Diffusion Models without Any Reward
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using reinforcement learning with human feedback (RLHF) has shown significant
promise in fine-tuning diffusion models. Previous methods start by training a
reward model that aligns with human preferences, then leverage RL techniques to
fine-tune the underlying models. However, crafting an efficient reward model
demands extensive datasets, optimal architecture, and manual hyperparameter
tuning, making the process both time and cost-intensive. The direct preference
optimization (DPO) method, effective in fine-tuning large language models,
eliminates the necessity for a reward model. However, the extensive GPU memory
requirement of the diffusion model's denoising process hinders the direct
application of the DPO method. To address this issue, we introduce the Direct
Preference for Denoising Diffusion Policy Optimization (D3PO) method to
directly fine-tune diffusion models. The theoretical analysis demonstrates that
although D3PO omits training a reward model, it effectively functions as the
optimal reward model trained using human feedback data to guide the learning
process. This approach requires no training of a reward model, proving to be
more direct, cost-effective, and minimizing computational overhead. In
experiments, our method uses the relative scale of objectives as a proxy for
human preference, delivering comparable results to methods using ground-truth
rewards. Moreover, D3PO demonstrates the ability to reduce image distortion
rates and generate safer images, overcoming challenges lacking robust reward
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeutronOrch: Rethinking Sample-based GNN Training under CPU-GPU
  Heterogeneous Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Ai, Qiange Wang, Chunyu Cao, Yanfeng Zhang, Chaoyi Chen, Hao Yuan, Yu Gu, Ge Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have demonstrated outstanding performance in
various applications. Existing frameworks utilize CPU-GPU heterogeneous
environments to train GNN models and integrate mini-batch and sampling
techniques to overcome the GPU memory limitation. In CPU-GPU heterogeneous
environments, we can divide sample-based GNN training into three steps: sample,
gather, and train. Existing GNN systems use different task orchestrating
methods to employ each step on CPU or GPU. After extensive experiments and
analysis, we find that existing task orchestrating methods fail to fully
utilize the heterogeneous resources, limited by inefficient CPU processing or
GPU resource contention. In this paper, we propose NeutronOrch, a system for
sample-based GNN training that incorporates a layer-based task orchestrating
method and ensures balanced utilization of the CPU and GPU. NeutronOrch
decouples the training process by layer and pushes down the training task of
the bottom layer to the CPU. This significantly reduces the computational load
and memory footprint of GPU training. To avoid inefficient CPU processing,
NeutronOrch only offloads the training of frequently accessed vertices to the
CPU and lets GPU reuse their embeddings with bounded staleness. Furthermore,
NeutronOrch provides a fine-grained pipeline design for the layer-based task
orchestrating method, fully overlapping different tasks on heterogeneous
resources while strictly guaranteeing bounded staleness. The experimental
results show that compared with the state-of-the-art GNN systems, NeutronOrch
can achieve up to 4.61x performance speedup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cracking the Code of Negative Transfer: A Cooperative Game Theoretic
  Approach for Cross-Domain Sequential Recommendation <span class="chip">CIKM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chung Park, Taesan Kim, Taekyoon Choi, Junui Hong, Yelim Yu, Mincheol Cho, Kyunam Lee, Sungil Ryu, Hyungjun Yoon, Minsung Choi, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates Cross-Domain Sequential Recommendation (CDSR), a
promising method that uses information from multiple domains (more than three)
to generate accurate and diverse recommendations, and takes into account the
sequential nature of user interactions. The effectiveness of these systems
often depends on the complex interplay among the multiple domains. In this
dynamic landscape, the problem of negative transfer arises, where heterogeneous
knowledge between dissimilar domains leads to performance degradation due to
differences in user preferences across these domains. As a remedy, we propose a
new CDSR framework that addresses the problem of negative transfer by assessing
the extent of negative transfer from one domain to another and adaptively
assigning low weight values to the corresponding prediction losses. To this
end, the amount of negative transfer is estimated by measuring the marginal
contribution of each domain to model performance based on a cooperative game
theory. In addition, a hierarchical contrastive learning approach that
incorporates information from the sequence of coarse-level categories into that
of fine-level categories (e.g., item level) when implementing contrastive
learning was developed to mitigate negative transfer. Despite the potentially
low relevance between domains at the fine-level, there may be higher relevance
at the category level due to its generalised and broader preferences. We show
that our model is superior to prior works in terms of model performance on two
real-world datasets across ten different domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 32nd ACM International Conference on Information and
  Knowledge Management (CIKM 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AS-LLM: When Algorithm Selection Meets Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Wu, Yan Zhong, Jibin Wu, Kay Chen Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithm selection aims to identify the most suitable algorithm for solving
a specific problem before execution, which has become a critical process of the
AutoML. Current mainstream algorithm selection techniques rely heavily on
feature representations of various problems and employ the performance of each
algorithm as supervised information. However, there is a significant research
gap concerning the consideration of algorithm features. This gap is primarily
attributed to the inherent complexity of algorithms, making it particularly
challenging to find a universally effective feature extraction method that is
applicable across a diverse range of algorithms. Unfortunately, neglecting this
aspect undoubtedly impacts the accuracy of algorithm selection and indirectly
necessitates an increased volume of problem data for training purposes. This
paper takes a significant stride towards addressing this gap by proposing an
approach that integrates algorithm representation into the algorithm selection
process. Specifically, our proposed model employs distinct modules to extract
representations of both problems and algorithms, where the algorithm
representation leverages the capabilities of pre-trained LLMs in the realm of
code comprehension. Following the extraction of embedding vectors for both
algorithms and problems, the most suitable algorithm is determined through
calculations of matching degrees. Our experiments not only validate the
effectiveness of the proposed model but also showcase the performance of
different embedded pre-trained LLMs, which suggests that the proposed algorithm
selection framework holds the potential to serve as a baseline task for
evaluating the code representation capabilities of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Efficient High-Dimensional Bandit Learning with Batched
  Feedbacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqing Fan, Zhaoran Wang, Zhuoran Yang, Chenlu Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study high-dimensional multi-armed contextual bandits with batched
feedback where the $T$ steps of online interactions are divided into $L$
batches. In specific, each batch collects data according to a policy that
depends on previous batches and the rewards are revealed only at the end of the
batch. Such a feedback structure is popular in applications such as
personalized medicine and online advertisement, where the online data often do
not arrive in a fully serial manner. We consider high-dimensional and linear
settings where the reward function of the bandit model admits either a sparse
or low-rank structure and ask how small a number of batches are needed for a
comparable performance with fully dynamic data in which $L = T$. For these
settings, we design a provably sample-efficient algorithm which achieves a $
\mathcal{\tilde O}(s_0^2 \log^2 T)$ regret in the sparse case and $
\mathcal{\tilde O} ( r ^2 \log^2 T)$ regret in the low-rank case, using only $L
= \mathcal{O}( \log T)$ batches. Here $s_0$ and $r$ are the sparsity and rank
of the reward parameter in sparse and low-rank cases, respectively, and $
\mathcal{\tilde O}(\cdot)$ omits logarithmic factors involving the feature
dimensions. In other words, our algorithm achieves regret bounds comparable to
those in fully sequential setting with only $\mathcal{O}( \log T)$ batches. Our
algorithm features a novel batch allocation method that adjusts the batch sizes
according to the estimation accuracy within each batch and cumulative regret.
Furthermore, we also conduct experiments with synthetic and real-world data to
validate our theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SecureCut: Federated Gradient Boosting Decision Trees with Efficient
  Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Zhang, Bowen Li Jie Li, Chentao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to legislation mandating companies to honor the \textit{right to
be forgotten} by erasing user data, it has become imperative to enable data
removal in Vertical Federated Learning (VFL) where multiple parties provide
private features for model training. In VFL, data removal, i.e.,
\textit{machine unlearning}, often requires removing specific features across
all samples under privacy guarentee in federated learning. To address this
challenge, we propose \methname, a novel Gradient Boosting Decision Tree (GBDT)
framework that effectively enables both \textit{instance unlearning} and
\textit{feature unlearning} without the need for retraining from scratch.
Leveraging a robust GBDT structure, we enable effective data deletion while
reducing degradation of model performance. Extensive experimental results on
popular datasets demonstrate that our method achieves superior model utility
and forgetfulness compared to \textit{state-of-the-art} methods. To our best
knowledge, this is the first work that investigates machine unlearning in VFL
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComPEFT: Compression for Communicating Parameter Efficient Updates via
  Sparsification and Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Yadav, Leshem Choshen, Colin Raffel, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) techniques make it possible to
efficiently adapt a language model to create "expert" models that specialize to
new tasks or domains. Recent techniques in model merging and compositional
generalization leverage these expert models by dynamically composing modules to
improve zero/few-shot generalization. Despite the efficiency of PEFT methods,
the size of expert models can make it onerous to retrieve expert models per
query over high-latency networks like the Internet or serve multiple experts on
a single GPU. To address these issues, we present ComPEFT, a novel method for
compressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT
employs sparsification and ternary quantization to reduce the size of the PEFT
module without performing any additional retraining while preserving or
enhancing model performance. In extensive evaluation across T5, T0, and
LLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression
ratios of 8x - 50x. In particular, we show that ComPEFT improves with scale -
stronger models exhibit higher compressibility and better performance. For
example, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on
MMLU with a storage size reduction of up to 26x. In addition, we show that the
compressed experts produced by ComPEFT maintain few-shot compositional
generalization capabilities, facilitate efficient communication and
computation, and exhibit enhanced performance when merged. Lastly, we provide
an analysis of different method components, compare it with other PEFT methods,
and test ComPEFT's efficacy for compressing the residual of full-finetuning.
Our code is available at https://github.com/prateeky2806/compeft.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 Pages, 6 Figures, 16 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SiGeo: Sub-One-Shot NAS via Information Theory and Geometry of Loss
  Landscape 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hua Zheng, Kuang-Hung Liu, Igor Fedorov, Xin Zhang, Wen-Yen Chen, Wei Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) has become a widely used tool for automating
neural network design. While one-shot NAS methods have successfully reduced
computational requirements, they often require extensive training. On the other
hand, zero-shot NAS utilizes training-free proxies to evaluate a candidate
architecture's test performance but has two limitations: (1) inability to use
the information gained as a network improves with training and (2) unreliable
performance, particularly in complex domains like RecSys, due to the
multi-modal data inputs and complex architecture configurations. To synthesize
the benefits of both methods, we introduce a "sub-one-shot" paradigm that
serves as a bridge between zero-shot and one-shot NAS. In sub-one-shot NAS, the
supernet is trained using only a small subset of the training data, a phase we
refer to as "warm-up." Within this framework, we present SiGeo, a proxy founded
on a novel theoretical framework that connects the supernet warm-up with the
efficacy of the proxy. Extensive experiments have shown that SiGeo, with the
benefit of warm-up, consistently outperforms state-of-the-art NAS proxies on
various established NAS benchmarks. When a supernet is warmed up, it can
achieve comparable performance to weight-sharing one-shot NAS methods, but with
a significant reduction ($\sim 60$\%) in computational costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaptiveFL: Adaptive Heterogeneous Federated Learning for
  Resource-Constrained AIoT Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chentao Jia, Ming Hu, Zekai Chen, Yanxin Yang, Xiaofei Xie, Yang Liu, Mingsong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Federated Learning (FL) is promising to enable collaborative
learning among Artificial Intelligence of Things (AIoT) devices, it suffers
from the problem of low classification performance due to various heterogeneity
factors (e.g., computing capacity, memory size) of devices and uncertain
operating environments. To address these issues, this paper introduces an
effective FL approach named AdaptiveFL based on a novel fine-grained width-wise
model pruning strategy, which can generate various heterogeneous local models
for heterogeneous AIoT devices. By using our proposed reinforcement
learning-based device selection mechanism, AdaptiveFL can adaptively dispatch
suitable heterogeneous models to corresponding AIoT devices on the fly based on
their available resources for local training. Experimental results show that,
compared to state-of-the-art methods, AdaptiveFL can achieve up to 16.83%
inference improvements for both IID and non-IID scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Have Your Cake and Eat It Too: Toward Efficient and Accurate Split
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengke Yan, Ming Hu, Zeke Xia, Yanxin Yang, Jun Xia, Xiaofei Xie, Mingsong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to its advantages in resource constraint scenarios, Split Federated
Learning (SFL) is promising in AIoT systems. However, due to data heterogeneity
and stragglers, SFL suffers from the challenges of low inference accuracy and
low efficiency. To address these issues, this paper presents a novel SFL
approach, named Sliding Split Federated Learning (S$^2$FL), which adopts an
adaptive sliding model split strategy and a data balance-based training
mechanism. By dynamically dispatching different model portions to AIoT devices
according to their computing capability, S$^2$FL can alleviate the low training
efficiency caused by stragglers. By combining features uploaded by devices with
different data distributions to generate multiple larger batches with a uniform
distribution for back-propagation, S$^2$FL can alleviate the performance
degradation caused by data heterogeneity. Experimental results demonstrate
that, compared to conventional SFL, S$^2$FL can achieve up to 16.5\% inference
accuracy improvement and 3.54X training acceleration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Optimization via Wasserstein-Fisher-Rao Gradient Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinuo Ren, Tesi Xiao, Tanmay Gangwani, Anshuka Rangi, Holakou Rahmanian, Lexing Ying, Subhajit Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective optimization (MOO) aims to optimize multiple, possibly
conflicting objectives with widespread applications. We introduce a novel
interacting particle method for MOO inspired by molecular dynamics simulations.
Our approach combines overdamped Langevin and birth-death dynamics,
incorporating a "dominance potential" to steer particles toward global Pareto
optimality. In contrast to previous methods, our method is able to relocate
dominated particles, making it particularly adept at managing Pareto fronts of
complicated geometries. Our method is also theoretically grounded as a
Wasserstein-Fisher-Rao gradient flow with convergence guarantees. Extensive
experiments confirm that our approach outperforms state-of-the-art methods on
challenging synthetic and real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing Closeness of Multivariate Distributions via Ramsey Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Diakonikolas, Daniel M. Kane, Sihan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the statistical task of closeness (or equivalence) testing for
multidimensional distributions. Specifically, given sample access to two
unknown distributions $\mathbf p, \mathbf q$ on $\mathbb R^d$, we want to
distinguish between the case that $\mathbf p=\mathbf q$ versus $\|\mathbf
p-\mathbf q\|_{A_k} > \epsilon$, where $\|\mathbf p-\mathbf q\|_{A_k}$ denotes
the generalized ${A}_k$ distance between $\mathbf p$ and $\mathbf q$ --
measuring the maximum discrepancy between the distributions over any collection
of $k$ disjoint, axis-aligned rectangles. Our main result is the first
closeness tester for this problem with {\em sub-learning} sample complexity in
any fixed dimension and a nearly-matching sample complexity lower bound.
  In more detail, we provide a computationally efficient closeness tester with
sample complexity $O\left((k^{6/7}/ \mathrm{poly}_d(\epsilon))
\log^d(k)\right)$. On the lower bound side, we establish a qualitatively
matching sample complexity lower bound of
$\Omega(k^{6/7}/\mathrm{poly}(\epsilon))$, even for $d=2$. These sample
complexity bounds are surprising because the sample complexity of the problem
in the univariate setting is $\Theta(k^{4/5}/\mathrm{poly}(\epsilon))$. This
has the interesting consequence that the jump from one to two dimensions leads
to a substantial increase in sample complexity, while increases beyond that do
not.
  As a corollary of our general $A_k$ tester, we obtain $d_{\mathrm
TV}$-closeness testers for pairs of $k$-histograms on $\mathbb R^d$ over a
common unknown partition, and pairs of uniform distributions supported on the
union of $k$ unknown disjoint axis-aligned rectangles.
  Both our algorithm and our lower bound make essential use of tools from
Ramsey theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Transport with Cyclic Symmetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoichiro Takeda, Yasunori Akagi, Naoki Marumo, Kenta Niwa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose novel fast algorithms for optimal transport (OT) utilizing a
cyclic symmetry structure of input data. Such OT with cyclic symmetry appears
universally in various real-world examples: image processing, urban planning,
and graph processing. Our main idea is to reduce OT to a small optimization
problem that has significantly fewer variables by utilizing cyclic symmetry and
various optimization techniques. On the basis of this reduction, our algorithms
solve the small optimization problem instead of the original OT. As a result,
our algorithms obtain the optimal solution and the objective function value of
the original OT faster than solving the original OT directly. In this paper,
our focus is on two crucial OT formulations: the linear programming OT (LOT)
and the strongly convex-regularized OT, which includes the well-known
entropy-regularized OT (EROT). Experiments show the effectiveness of our
algorithms for LOT and EROT in synthetic/real-world data that has a
strict/approximate cyclic symmetry structure. Through theoretical and
experimental results, this paper successfully introduces the concept of
symmetry into the OT research field for the first time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditi Jha, Sam Havens, Jeremey Dohmann, Alex Trott, Jacob Portes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models are traditionally finetuned on large instruction
datasets. However recent studies suggest that small, high-quality datasets can
suffice for general purpose instruction following. This lack of consensus
surrounding finetuning best practices is in part due to rapidly diverging
approaches to LLM evaluation. In this study, we ask whether a small amount of
diverse finetuning samples can improve performance on both traditional
perplexity-based NLP benchmarks, and on open-ended, model-based evaluation. We
finetune open-source MPT-7B and MPT-30B models on instruction finetuning
datasets of various sizes ranging from 1k to 60k samples. We find that subsets
of 1k-6k instruction finetuning samples are sufficient to achieve good
performance on both (1) traditional NLP benchmarks and (2) model-based
evaluation. Finally, we show that mixing textbook-style and open-ended QA
finetuning datasets optimizes performance on both evaluation paradigms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 12 figures, NeurIPS 2023 Workshop on Instruction Tuning and
  Instruction Following</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combatting Human Trafficking in the Cyberspace: A Natural Language
  Processing-Based Methodology to Analyze the Language in Online Advertisements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Rodriguez Perez, Pablo Rivas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project tackles the pressing issue of human trafficking in online C2C
marketplaces through advanced Natural Language Processing (NLP) techniques. We
introduce a novel methodology for generating pseudo-labeled datasets with
minimal supervision, serving as a rich resource for training state-of-the-art
NLP models. Focusing on tasks like Human Trafficking Risk Prediction (HTRP) and
Organized Activity Detection (OAD), we employ cutting-edge Transformer models
for analysis. A key contribution is the implementation of an interpretability
framework using Integrated Gradients, providing explainable insights crucial
for law enforcement. This work not only fills a critical gap in the literature
but also offers a scalable, machine learning-driven approach to combat human
exploitation online. It serves as a foundation for future research and
practical applications, emphasizing the role of machine learning in addressing
complex social issues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ White-Box <span class="highlight-title">Transformer</span>s via Sparse Rate Reduction: Compression Is All
  There Is? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Hao Bai, Yuexiang Zhai, Benjamin D. Haeffele, Yi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we contend that a natural objective of representation learning
is to compress and transform the distribution of the data, say sets of tokens,
towards a low-dimensional Gaussian mixture supported on incoherent subspaces.
The goodness of such a representation can be evaluated by a principled measure,
called sparse rate reduction, that simultaneously maximizes the intrinsic
information gain and extrinsic sparsity of the learned representation. From
this perspective, popular deep network architectures, including transformers,
can be viewed as realizing iterative schemes to optimize this measure.
Particularly, we derive a transformer block from alternating optimization on
parts of this objective: the multi-head self-attention operator compresses the
representation by implementing an approximate gradient descent step on the
coding rate of the features, and the subsequent multi-layer perceptron
sparsifies the features. This leads to a family of white-box transformer-like
deep network architectures, named CRATE, which are mathematically fully
interpretable. We show, by way of a novel connection between denoising and
compression, that the inverse to the aforementioned compressive encoding can be
realized by the same class of CRATE architectures. Thus, the so-derived
white-box architectures are universal to both encoders and decoders.
Experiments show that these networks, despite their simplicity, indeed learn to
compress and sparsify representations of large-scale real-world image and text
datasets, and achieve performance very close to highly engineered
transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the
proposed computational framework demonstrates great potential in bridging the
gap between theory and practice of deep learning, from a unified perspective of
data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper integrates the works arXiv:2306.01129 and
  arXiv:2308.16271, as well as this under-review work:
  https://openreview.net/forum?id=PvyOYleymy into a complete story. In this
  paper, we improve the writing and organization, and also add conceptual,
  empirical, and theoretical improvements over the previous work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting out-of-distribution text using topological features of
  <span class="highlight-title">transformer</span>-based language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andres Pollano, Anupam Chaudhuri, Anj Simmons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We attempt to detect out-of-distribution (OOD) text samples though applying
Topological Data Analysis (TDA) to attention maps in transformer-based language
models. We evaluate our proposed TDA-based approach for out-of-distribution
detection on BERT, a transformer-based language model, and compare the to a
more traditional OOD approach based on BERT CLS embeddings. We found that our
TDA approach outperforms the CLS embedding approach at distinguishing
in-distribution data (politics and entertainment news articles from HuffPost)
from far out-of-domain samples (IMDB reviews), but its effectiveness
deteriorates with near out-of-domain (CNN/Dailymail) or same-domain (business
news articles from HuffPost) datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Feng, Yintong Shang, Xuan Li, Tianjia Shao, Chenfanfu Jiang, Yin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that physics-based simulations can be seamlessly integrated with NeRF
to generate high-quality elastodynamics of real-world objects. Unlike existing
methods, we discretize nonlinear hyperelasticity in a meshless way, obviating
the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh
or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed
to capture nonlinear dynamics and large deformation on the implicit model. Such
meshless integration enables versatile simulations of complex and codimensional
shapes. We adaptively place the least-square kernels according to the NeRF
density field to significantly reduce the complexity of the nonlinear
simulation. As a result, physically realistic animations can be conveniently
synthesized using our method for a wide range of hyperelastic materials at an
interactive rate. For more information, please visit our project page at
https://fytalon.github.io/pienerf/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Newton-CG methods for nonconvex unconstrained optimization with Hölder
  continuous Hessian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan He, Zhaosong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we consider a nonconvex unconstrained optimization problem
minimizing a twice differentiable objective function with H\"older continuous
Hessian. Specifically, we first propose a Newton-conjugate gradient (Newton-CG)
method for finding an approximate first-order stationary point (FOSP) of this
problem, assuming the associated the H\"older parameters are explicitly known.
Then we develop a parameter-free Newton-CG method without requiring any prior
knowledge of these parameters. To the best of our knowledge, this method is the
first parameter-free second-order method achieving the best-known iteration and
operation complexity for finding an approximate FOSP of this problem.
Furthermore, we propose a Newton-CG method for finding an approximate
second-order stationary point (SOSP) of the considered problem with high
probability and establish its iteration and operation complexity. Finally, we
present preliminary numerical results to demonstrate the superior practical
performance of our parameter-free Newton-CG method over a well-known
regularized Newton method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2301.03139</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Unlearnable Example: Enhancing the Robustness of Unlearnable
  Examples via Stable Error-Minimizing Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Liu, Kaidi Xu, Xun Chen, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The open source of large amounts of image data promotes the development of
deep learning techniques. Along with this comes the privacy risk of these
open-source image datasets being exploited by unauthorized third parties to
train deep learning models for commercial or illegal purposes. To avoid the
abuse of public data, a poisoning-based technique, the unlearnable example, is
proposed to significantly degrade the generalization performance of models by
adding a kind of imperceptible noise to the data. To further enhance its
robustness against adversarial training, existing works leverage iterative
adversarial training on both the defensive noise and the surrogate model.
However, it still remains unknown whether the robustness of unlearnable
examples primarily comes from the effect of enhancement in the surrogate model
or the defensive noise. Observing that simply removing the adversarial noise on
the training process of the defensive noise can improve the performance of
robust unlearnable examples, we identify that solely the surrogate model's
robustness contributes to the performance. Furthermore, we found a negative
correlation exists between the robustness of defensive noise and the protection
performance, indicating defensive noise's instability issue. Motivated by this,
to further boost the robust unlearnable example, we introduce stable
error-minimizing noise (SEM), which trains the defensive noise against random
perturbation instead of the time-consuming adversarial perturbation to improve
the stability of defensive noise. Through extensive experiments, we demonstrate
that SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100,
and ImageNet Subset in terms of both effectiveness and efficiency. The code is
available at https://github.com/liuyixin-louis/Stable-Unlearnable-Example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predict-Then-Optimize by Proxy: Learning Joint Models of Prediction and
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Kotary, Vincenzo Di Vito, Jacob Christopher, Pascal Van Hentenryck, Ferdinando Fioretto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world decision processes are modeled by optimization problems whose
defining parameters are unknown and must be inferred from observable data. The
Predict-Then-Optimize framework uses machine learning models to predict unknown
parameters of an optimization problem from features before solving. Recent
works show that decision quality can be improved in this setting by solving and
differentiating the optimization problem in the training loop, enabling
end-to-end training with loss functions defined directly on the resulting
decisions. However, this approach can be inefficient and requires handcrafted,
problem-specific rules for backpropagation through the optimization step. This
paper proposes an alternative method, in which optimal solutions are learned
directly from the observable features by predictive models. The approach is
generic, and based on an adaptation of the Learning-to-Optimize paradigm, from
which a rich variety of existing techniques can be employed. Experimental
evaluations show the ability of several Learning-to-Optimize methods to provide
efficient, accurate, and flexible solutions to an array of challenging
Predict-Then-Optimize problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Fly in Seconds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Eschmann, Dario Albani, Giuseppe Loianno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based methods, particularly Reinforcement Learning (RL), hold great
promise for streamlining deployment, enhancing performance, and achieving
generalization in the control of autonomous multirotor aerial vehicles. Deep RL
has been able to control complex systems with impressive fidelity and agility
in simulation but the simulation-to-reality transfer often brings a
hard-to-bridge reality gap. Moreover, RL is commonly plagued by prohibitively
long training times. In this work, we propose a novel asymmetric
actor-critic-based architecture coupled with a highly reliable RL-based
training paradigm for end-to-end quadrotor control. We show how curriculum
learning and a highly optimized simulator enhance sample complexity and lead to
fast training times. To precisely discuss the challenges related to
low-level/end-to-end multirotor control, we also introduce a taxonomy that
classifies the existing levels of control abstractions as well as
non-linearities and domain parameters. Our framework enables
Simulation-to-Reality (Sim2Real) transfer for direct RPM control after only 18
seconds of training on a consumer-grade laptop as well as its deployment on
microcontrollers to control a multirotor under real-time guarantees. Finally,
our solution exhibits competitive performance in trajectory tracking, as
demonstrated through various experimental comparisons with existing
state-of-the-art control solutions using a real Crazyflie nano quadrotor. We
open source the code including a very fast multirotor dynamics simulator that
can simulate about 5 months of flight per second on a laptop GPU. The fast
training times and deployment to a cheap, off-the-shelf quadrotor lower the
barriers to entry and help democratize the research and development of these
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FusionFrames: Efficient Architectural Aspects for Text-to-Video
  Generation Pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Arkhipkin, Zein Shaheen, Viacheslav Vasilev, Elizaveta Dakhova, Andrey Kuznetsov, Denis Dimitrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia generation approaches occupy a prominent place in artificial
intelligence research. Text-to-image models achieved high-quality results over
the last few years. However, video synthesis methods recently started to
develop. This paper presents a new two-stage latent diffusion text-to-video
generation architecture based on the text-to-image diffusion model. The first
stage concerns keyframes synthesis to figure the storyline of a video, while
the second one is devoted to interpolation frames generation to make movements
of the scene and objects smooth. We compare several temporal conditioning
approaches for keyframes generation. The results show the advantage of using
separate temporal blocks over temporal layers in terms of metrics reflecting
video generation quality aspects and human preference. The design of our
interpolation model significantly reduces computational costs compared to other
masked frame interpolation approaches. Furthermore, we evaluate different
configurations of MoVQ-based video decoding scheme to improve consistency and
achieve higher PSNR, SSIM, MSE, and LPIPS scores. Finally, we compare our
pipeline with existing solutions and achieve top-2 scores overall and top-1
among open-source solutions: CLIPSIM = 0.2976 and FVD = 433.054. Project page:
https://ai-forever.github.io/kandinsky-video/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ai-forever.github.io/kandinsky-video/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grad-Shafranov equilibria via data-free physics informed neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byoungchan Jang, Alan A. Kaptanoglu, Rahul Gaur, Shaowu Pan, Matt Landreman, William Dorland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A large number of magnetohydrodynamic (MHD) equilibrium calculations are
often required for uncertainty quantification, optimization, and real-time
diagnostic information, making MHD equilibrium codes vital to the field of
plasma physics. In this paper, we explore a method for solving the
Grad-Shafranov equation by using Physics-Informed Neural Networks (PINNs). For
PINNs, we optimize neural networks by directly minimizing the residual of the
PDE as a loss function. We show that PINNs can accurately and effectively solve
the Grad-Shafranov equation with several different boundary conditions. We also
explore the parameter space by varying the size of the model, the learning
rate, and boundary conditions to map various trade-offs such as between
reconstruction error and computational speed. Additionally, we introduce a
parameterized PINN framework, expanding the input space to include variables
such as pressure, aspect ratio, elongation, and triangularity in order to
handle a broader range of plasma scenarios within a single network.
Parametrized PINNs could be used in future work to solve inverse problems such
as shape optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction of Effective Elastic Moduli of Rocks using Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehong Chung, Rasool Ahmad, WaiChing Sun, Wei Cai, Tapan Mukerji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a Graph Neural Networks (GNNs)-based approach for
predicting the effective elastic moduli of rocks from their digital CT-scan
images. We use the Mapper algorithm to transform 3D digital rock images into
graph datasets, encapsulating essential geometrical information. These graphs,
after training, prove effective in predicting elastic moduli. Our GNN model
shows robust predictive capabilities across various graph sizes derived from
various subcube dimensions. Not only does it perform well on the test dataset,
but it also maintains high prediction accuracy for unseen rocks and unexplored
subcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs)
reveals the superior performance of GNNs in predicting unseen rock properties.
Moreover, the graph representation of microstructures significantly reduces GPU
memory requirements (compared to the grid representation for CNNs), enabling
greater flexibility in the batch size selection. This work demonstrates the
potential of GNN models in enhancing the prediction accuracy of rock properties
and boosting the efficiency of digital rock analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Edge2Node: Reducing Edge Prediction to Node Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02921v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02921v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahed Rahmati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of graph neural network models in node classification,
edge prediction (the task of predicting missing or potential links between
nodes in a graph) remains a challenging problem for these models. A common
approach for edge prediction is to first obtain the embeddings of two nodes,
and then a predefined scoring function is used to predict the existence of an
edge between the two nodes. Here, we introduce a preliminary idea called
Edge2Node which suggests to directly obtain an embedding for each edge, without
the need for a scoring function. This idea wants to create a new graph H based
on the graph G given for the edge prediction task, and then suggests reducing
the edge prediction task on G to a node classification task on H. We anticipate
that this introductory method could stimulate further investigations for edge
prediction task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Good Feature Extractor Is All You Need for Weakly Supervised Learning
  in Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Wölflein, Dyke Ferber, Asier Rabasco Meneghetti, Omar S. M. El Nahhas, Daniel Truhn, Zunamys I. Carrero, David J. Harrison, Ognjen Arandjelović, Jakob N. Kather
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is revolutionising pathology, offering novel opportunities in
disease prognosis and personalised treatment. Historically, stain normalisation
has been a crucial preprocessing step in computational pathology pipelines, and
persists into the deep learning era. Yet, with the emergence of feature
extractors trained using self-supervised learning (SSL) on diverse pathology
datasets, we call this practice into question. In an empirical evaluation of
publicly available feature extractors, we find that omitting stain
normalisation and image augmentations does not compromise downstream
performance, while incurring substantial savings in memory and compute.
Further, we show that the top-performing feature extractors are remarkably
robust to variations in stain and augmentations like rotation in their latent
space. Contrary to previous patch-level benchmarking studies, our approach
emphasises clinical relevance by focusing on slide-level prediction tasks in a
weakly supervised setting with external validation cohorts. This work
represents the most comprehensive robustness evaluation of public pathology SSL
feature extractors to date, involving more than 6,000 training runs across nine
tasks, five datasets, three downstream architectures, and various preprocessing
setups. Our findings stand to streamline digital pathology workflows by
minimising preprocessing needs and informing the selection of feature
extractors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphCFC: A Directed Graph Based Cross-Modal Feature Complementation
  Approach for Multimodal Conversational Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12261v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12261v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Li, Xiaoping Wang, Guoqing Lv, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation (ERC) plays a significant part in
Human-Computer Interaction (HCI) systems since it can provide empathetic
services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.
Recently, Graph Neural Networks (GNNs) have been widely used in a variety of
fields due to their superior performance in relation modeling. In multimodal
ERC, GNNs are capable of extracting both long-distance contextual information
and inter-modal interactive information. Unfortunately, since existing methods
such as MMGCN directly fuse multiple modalities, redundant information may be
generated and diverse information may be lost. In this work, we present a
directed Graph based Cross-modal Feature Complementation (GraphCFC) module that
can efficiently model contextual and interactive information. GraphCFC
alleviates the problem of heterogeneity gap in multimodal fusion by utilizing
multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)
strategy. We extract various types of edges from the constructed graph for
encoding, thus enabling GNNs to extract crucial contextual and interactive
information more accurately when performing message passing. Furthermore, we
design a GNN structure called GAT-MLP, which can provide a new unified network
framework for multimodal learning. The experimental results on two benchmark
datasets show that our GraphCFC outperforms the state-of-the-art (SOTA)
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Multimedia (TMM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensor Train for Global Optimization Problems in Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05077v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05077v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhan Shetty, Teguh Lembono, Tobias Loew, Sylvain Calinon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The convergence of many numerical optimization techniques is highly dependent
on the initial guess given to the solver. To address this issue, we propose a
novel approach that utilizes tensor methods to initialize existing optimization
solvers near global optima. Our method does not require access to a database of
good solutions. We first transform the cost function, which depends on both
task parameters and optimization variables, into a probability density
function. Unlike existing approaches, the joint probability distribution of the
task parameters and optimization variables is approximated using the Tensor
Train model, which enables efficient conditioning and sampling. We treat the
task parameters as random variables, and for a given task, we generate samples
for decision variables from the conditional distribution to initialize the
optimization solver. Our method can produce multiple solutions (when they
exist) faster than existing methods. We first evaluate the approach on
benchmark functions for numerical optimization that are hard to solve using
gradient-based optimization solvers with a naive initialization. The results
show that the proposed method can generate samples close to global optima and
from multiple modes. We then demonstrate the generality and relevance of our
framework to robotics by applying it to inverse kinematics with obstacles and
motion planning problems with a 7-DoF manipulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating <span class="highlight-title">Pre-train</span>ed Language Model into Neural Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19680v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19680v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soon-Jae Hwang, Chang-Sung Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Machine Translation (NMT) has become a significant technology in
natural language processing through extensive research and development.
However, the deficiency of high-quality bilingual language pair data still
poses a major challenge to improving NMT performance. Recent studies have been
exploring the use of contextual information from pre-trained language model
(PLM) to address this problem. Yet, the issue of incompatibility between PLM
and NMT model remains unresolved. This study proposes PLM-integrated NMT
(PiNMT) model to overcome the identified problems. PiNMT model consists of
three critical components, PLM Multi Layer Converter, Embedding Fusion, and
Cosine Alignment, each playing a vital role in providing effective PLM
information to NMT. Furthermore, two training strategies, Separate Learning
Rates and Dual Step Training, are also introduced in this paper. By
implementing the proposed PiNMT model and training strategy, we achieve
state-of-the-art performance on the IWSLT'14 En$\leftrightarrow$De dataset.
This study's outcomes are noteworthy as they demonstrate a novel approach for
efficiently integrating PLM with NMT to overcome incompatibility and enhance
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DNA-TEQ: An Adaptive Exponential Quantization of Tensors for DNN
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bahareh Khabbazan, Marc Riera, Antonio González
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization is commonly used in Deep Neural Networks (DNNs) to reduce the
storage and computational complexity by decreasing the arithmetical precision
of activations and weights, a.k.a. tensors. Efficient hardware architectures
employ linear quantization to enable the deployment of recent DNNs onto
embedded systems and mobile devices. However, linear uniform quantization
cannot usually reduce the numerical precision to less than 8 bits without
sacrificing high performance in terms of model accuracy. The performance loss
is due to the fact that tensors do not follow uniform distributions. In this
paper, we show that a significant amount of tensors fit into an exponential
distribution. Then, we propose DNA-TEQ to exponentially quantize DNN tensors
with an adaptive scheme that achieves the best trade-off between numerical
precision and accuracy loss. The experimental results show that DNA-TEQ
provides a much lower quantization bit-width compared to previous proposals,
resulting in an average compression ratio of 40% over the linear INT8 baseline,
with negligible accuracy loss and without retraining the DNNs. Besides, DNA-TEQ
leads the way in performing dot-product operations in the exponential domain,
which saves 66% of energy consumption on average for a set of widely used DNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A principled deep learning approach for geological facies generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ferdinand Bhavsar, Nicolas Desassis, Fabien Ors, Thomas Romary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The simulation of geological facies in an unobservable volume is essential in
various geoscience applications. Given the complexity of the problem, deep
generative learning is a promising approach to overcome the limitations of
traditional geostatistical simulation models, in particular their lack of
physical realism. This research aims to investigate the application of
generative adversarial networks and deep variational inference for
conditionally simulating meandering channels in underground volumes. In this
paper, we review the generative deep learning approaches, in particular the
adversarial ones and the stabilization techniques that aim to facilitate their
training. The proposed approach is tested on 2D and 3D simulations generated by
the stochastic process-based model Flumy. Morphological metrics are utilized to
compare our proposed method with earlier iterations of generative adversarial
networks. The results indicate that by utilizing recent stabilization
techniques, generative adversarial networks can efficiently sample from target
data distributions. Moreover, we demonstrate the ability to simulate
conditioned simulations through the latent variable model property of the
proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Different Learning Styles for Improved Knowledge Distillation
  in Biomedical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02931v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02931v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usma Niyaz, Abhishek Singh Sambyal, Deepti R. Bathula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning style refers to a type of training mechanism adopted by an
individual to gain new knowledge. As suggested by the VARK model, humans have
different learning preferences, like Visual (V), Auditory (A), Read/Write (R),
and Kinesthetic (K), for acquiring and effectively processing information. Our
work endeavors to leverage this concept of knowledge diversification to improve
the performance of model compression techniques like Knowledge Distillation
(KD) and Mutual Learning (ML). Consequently, we use a single-teacher and
two-student network in a unified framework that not only allows for the
transfer of knowledge from teacher to students (KD) but also encourages
collaborative learning between students (ML). Unlike the conventional approach,
where the teacher shares the same knowledge in the form of predictions or
feature representations with the student network, our proposed approach employs
a more diversified strategy by training one student with predictions and the
other with feature maps from the teacher. We further extend this knowledge
diversification by facilitating the exchange of predictions and feature maps
between the two student networks, enriching their learning experiences. We have
conducted comprehensive experiments with three benchmark datasets for both
classification and segmentation tasks using two different network architecture
combinations. These experimental results demonstrate that knowledge
diversification in a combined KD and ML framework outperforms conventional KD
or ML techniques (with similar network configuration) that only use predictions
with an average improvement of 2%. Furthermore, consistent improvement in
performance across different tasks, with various network architectures, and
over state-of-the-art techniques establishes the robustness and
generalizability of the proposed model
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Computers in Biology and Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Principle to Practice: Vertical Data Minimization for Machine
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Staab, Nikola Jovanović, Mislav Balunović, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming to train and deploy predictive models, organizations collect large
amounts of detailed client data, risking the exposure of private information in
the event of a breach. To mitigate this, policymakers increasingly demand
compliance with the data minimization (DM) principle, restricting data
collection to only that data which is relevant and necessary for the task.
Despite regulatory pressure, the problem of deploying machine learning models
that obey DM has so far received little attention. In this work, we address
this challenge in a comprehensive manner. We propose a novel vertical DM (vDM)
workflow based on data generalization, which by design ensures that no
full-resolution client data is collected during training and deployment of
models, benefiting client privacy by reducing the attack surface in case of a
breach. We formalize and study the corresponding problem of finding
generalizations that both maximize data utility and minimize empirical privacy
risk, which we quantify by introducing a diverse set of policy-aligned
adversarial scenarios. Finally, we propose a range of baseline vDM algorithms,
as well as Privacy-aware Tree (PAT), an especially effective vDM algorithm that
outperforms all baselines across several settings. We plan to release our code
as a publicly available library, helping advance the standardization of DM for
machine learning. Overall, we believe our work can help lay the foundation for
further exploration and adoption of DM principles in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE S&P 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confident Naturalness Explanation (CNE): A Framework to Explain and
  Assess Patterns Forming Naturalness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Emam, Mohamed Farag, Ribana Roscher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protected natural areas are regions that have been minimally affected by
human activities such as urbanization, agriculture, and other human
interventions. To better understand and map the naturalness of these areas,
machine learning models can be used to analyze satellite imagery. Specifically,
explainable machine learning methods show promise in uncovering patterns that
contribute to the concept of naturalness within these protected environments.
Additionally, addressing the uncertainty inherent in machine learning models is
crucial for a comprehensive understanding of this concept. However, existing
approaches have limitations. They either fail to provide explanations that are
both valid and objective or struggle to offer a quantitative metric that
accurately measures the contribution of specific patterns to naturalness, along
with the associated confidence. In this paper, we propose a novel framework
called the Confident Naturalness Explanation (CNE) framework. This framework
combines explainable machine learning and uncertainty quantification to assess
and explain naturalness. We introduce a new quantitative metric that describes
the confident contribution of patterns to the concept of naturalness.
Furthermore, we generate an uncertainty-aware segmentation mask for each input
sample, highlighting areas where the model lacks knowledge. To demonstrate the
effectiveness of our framework, we apply it to a study site in Fennoscandia
using two open-source satellite datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The effect of speech pathology on automatic speaker verification -- a
  large-scale study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06450v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06450v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroosh Tayebi Arasteh, Tobias Weise, Maria Schuster, Elmar Noeth, Andreas Maier, Seung Hee Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating the challenges of data-driven speech processing, one of the
primary hurdles is accessing reliable pathological speech data. While public
datasets appear to offer solutions, they come with inherent risks of potential
unintended exposure of patient health information via re-identification
attacks. Using a comprehensive real-world pathological speech corpus, with over
n=3,800 test subjects spanning various age groups and speech disorders, we
employed a deep-learning-driven automatic speaker verification (ASV) approach.
This resulted in a notable mean equal error rate (EER) of 0.89% with a standard
deviation of 0.06%, outstripping traditional benchmarks. Our comprehensive
assessments demonstrate that pathological speech overall faces heightened
privacy breach risks compared to healthy speech. Specifically, adults with
dysphonia are at heightened re-identification risks, whereas conditions like
dysarthria yield results comparable to those of healthy speakers. Crucially,
speech intelligibility does not influence the ASV system's performance metrics.
In pediatric cases, particularly those with cleft lip and palate, the recording
environment plays a decisive role in re-identification. Merging data across
pathological types led to a marked EER decrease, suggesting the potential
benefits of pathological diversity in ASV, accompanied by a logarithmic boost
in ASV effectiveness. In essence, this research sheds light on the dynamics
between pathological speech and speaker verification, emphasizing its crucial
role in safeguarding patient confidentiality in our increasingly digitized
healthcare era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Scientific Reports</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hinge-Wasserstein: Mitigating Overconfidence in Regression by
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziliang Xiong, Arvi Jonnarth, Abdelrahman Eldesokey, Joakim Johnander, Bastian Wandt, Per-Erik Forssen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer vision systems that are deployed in safety-critical applications
need to quantify their output uncertainty. We study regression from images to
parameter values and here it is common to detect uncertainty by predicting
probability distributions. In this context, we investigate the
regression-by-classification paradigm which can represent multimodal
distributions, without a prior assumption on the number of modes. Through
experiments on a specifically designed synthetic dataset, we demonstrate that
traditional loss functions lead to poor probability distribution estimates and
severe overconfidence, in the absence of full ground truth distributions. In
order to alleviate these issues, we propose hinge-Wasserstein -- a simple
improvement of the Wasserstein loss that reduces the penalty for weak secondary
modes during training. This enables prediction of complex distributions with
multiple modes, and allows training on datasets where full ground truth
distributions are not available. In extensive experiments, we show that the
proposed loss leads to substantially better uncertainty estimation on two
challenging computer vision tasks: horizon line detection and stereo disparity
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Droplets of Good Representations: Grokking as a First Order Phase
  Transition in Two Layer Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noa Rubin, Inbar Seroussi, Zohar Ringel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key property of deep neural networks (DNNs) is their ability to learn new
features during training. This intriguing aspect of deep learning stands out
most clearly in recently reported Grokking phenomena. While mainly reflected as
a sudden increase in test accuracy, Grokking is also believed to be a beyond
lazy-learning/Gaussian Process (GP) phenomenon involving feature learning. Here
we apply a recent development in the theory of feature learning, the adaptive
kernel approach, to two teacher-student models with cubic-polynomial and
modular addition teachers. We provide analytical predictions on feature
learning and Grokking properties of these models and demonstrate a mapping
between Grokking and the theory of phase transitions. We show that after
Grokking, the state of the DNN is analogous to the mixed phase following a
first-order phase transition. In this mixed phase, the DNN generates useful
internal representations of the teacher that are sharply distinct from those
before the transition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Vision <span class="highlight-title">Transformer</span> for Human Pose Estimation via Patch
  Selection <span class="chip">BMVC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaleab A. Kinfu, Rene Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Convolutional Neural Networks (CNNs) have been widely successful in 2D
human pose estimation, Vision Transformers (ViTs) have emerged as a promising
alternative to CNNs, boosting state-of-the-art performance. However, the
quadratic computational complexity of ViTs has limited their applicability for
processing high-resolution images. In this paper, we propose three methods for
reducing ViT's computational complexity, which are based on selecting and
processing a small number of most informative patches while disregarding
others. The first two methods leverage a lightweight pose estimation network to
guide the patch selection process, while the third method utilizes a set of
learnable joint tokens to ensure that the selected patches contain the most
important information about body joints. Experiments across six benchmarks show
that our proposed methods achieve a significant reduction in computational
complexity, ranging from 30% to 44%, with only a minimal drop in accuracy
between 0% and 3.5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BMVC 2023 Oral Paper: https://proceedings.bmvc2023.org/167/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predict, Refine, Synthesize: Self-Guiding Diffusion Models for
  Probabilistic Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao Wang, Yuyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved state-of-the-art performance in generative
modeling tasks across various domains. Prior works on time series diffusion
models have primarily focused on developing conditional models tailored to
specific forecasting or imputation tasks. In this work, we explore the
potential of task-agnostic, unconditional diffusion models for several time
series applications. We propose TSDiff, an unconditionally-trained diffusion
model for time series. Our proposed self-guidance mechanism enables
conditioning TSDiff for downstream tasks during inference, without requiring
auxiliary networks or altering the training procedure. We demonstrate the
effectiveness of our method on three different time series tasks: forecasting,
refinement, and synthetic data generation. First, we show that TSDiff is
competitive with several task-specific conditional forecasting methods
(predict). Second, we leverage the learned implicit probability density of
TSDiff to iteratively refine the predictions of base forecasters with reduced
computational overhead over reverse diffusion (refine). Notably, the generative
performance of the model remains intact -- downstream forecasters trained on
synthetic samples from TSDiff outperform forecasters that are trained on
samples from other state-of-the-art generative time series models, occasionally
even outperforming models trained on real data (synthesize).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at
  https://github.com/amazon-science/unconditional-time-series-diffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Looking at the posterior: accuracy and uncertainty of neural-network
  predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14605v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14605v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Linander, O. Balabanov, H. Yang, B. Mehlig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian inference can quantify uncertainty in the predictions of neural
networks using posterior distributions for model parameters and network output.
By looking at these posterior distributions, one can separate the origin of
uncertainty into aleatoric and epistemic contributions. One goal of uncertainty
quantification is to inform on prediction accuracy. Here we show that
prediction accuracy depends on both epistemic and aleatoric uncertainty in an
intricate fashion that cannot be understood in terms of marginalized
uncertainty distributions alone. How the accuracy relates to epistemic and
aleatoric uncertainties depends not only on the model architecture, but also on
the properties of the dataset. We discuss the significance of these results for
active learning and introduce a novel acquisition function that outperforms
common uncertainty-based methods. To arrive at our results, we approximated the
posteriors using deep ensembles, for fully-connected, convolutional and
attention-based neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 10 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Explanation for Regression via Disentanglement in Latent
  Space <span class="chip">ICDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Zhao, Klaus Broelemann, Gjergji Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual Explanations (CEs) help address the question: How can the
factors that influence the prediction of a predictive model be changed to
achieve a more favorable outcome from a user's perspective? Thus, they bear the
potential to guide the user's interaction with AI systems since they represent
easy-to-understand explanations. To be applicable, CEs need to be realistic and
actionable. In the literature, various methods have been proposed to generate
CEs. However, the majority of research on CEs focuses on classification
problems where questions like "What should I do to get my rejected loan
approved?" are raised. In practice, answering questions like "What should I do
to increase my salary?" are of a more regressive nature. In this paper, we
introduce a novel method to generate CEs for a pre-trained regressor by first
disentangling the label-relevant from the label-irrelevant dimensions in the
latent space. CEs are then generated by combining the label-irrelevant
dimensions and the predefined output. The intuition behind this approach is
that the ideal counterfactual search should focus on the label-irrelevant
characteristics of the input and suggest changes toward target-relevant
characteristics. Searching in the latent space could help achieve this goal. We
show that our method maintains the characteristics of the query sample during
the counterfactual search. In various experiments, we demonstrate that the
proposed method is competitive based on different quality measures on image and
tabular datasets in regression problem settings. It efficiently returns results
closer to the original data manifold compared to three state-of-the-art
methods, which is essential for realistic high-dimensional machine learning
applications. Our code will be made available as an open-source package upon
the publication of this work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CXAI workshop @ ICDM 2023. arXiv admin note: text overlap with
  arXiv:2307.13390</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time
  Adaptation <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manogna Sreenivas, Goirik Chakrabarty, Soma Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test Time Adaptation (TTA) is a pivotal concept in machine learning, enabling
models to perform well in real-world scenarios, where test data distribution
differs from training. In this work, we propose a novel approach called pseudo
Source guided Target Clustering (pSTarC) addressing the relatively unexplored
area of TTA under real-world domain shifts. This method draws inspiration from
target clustering techniques and exploits the source classifier for generating
pseudo-source samples. The test samples are strategically aligned with these
pseudo-source samples, facilitating their clustering and thereby enhancing TTA
performance. pSTarC operates solely within the fully test-time adaptation
protocol, removing the need for actual source data. Experimental validation on
a variety of domain shift datasets, namely VisDA, Office-Home, DomainNet-126,
CIFAR-100C verifies pSTarC's effectiveness. This method exhibits significant
improvements in prediction accuracy along with efficient computational
requirements. Furthermore, we also demonstrate the universality of the pSTarC
framework by showing its effectiveness for the continuous TTA framework. The
source code for our method is available at https://manogna-s.github.io/pstarc
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Goal Space Abstraction in Hierarchical Reinforcement Learning via
  Set-Based Reachability Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Zadem, Sergio Mover, Sao Mai Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-ended learning benefits immensely from the use of symbolic methods for
goal representation as they offer ways to structure knowledge for efficient and
transferable learning. However, the existing Hierarchical Reinforcement
Learning (HRL) approaches relying on symbolic reasoning are often limited as
they require a manual goal representation. The challenge in autonomously
discovering a symbolic goal representation is that it must preserve critical
information, such as the environment dynamics. In this paper, we propose a
developmental mechanism for goal discovery via an emergent representation that
abstracts (i.e., groups together) sets of environment states that have similar
roles in the task. We introduce a Feudal HRL algorithm that concurrently learns
both the goal representation and a hierarchical policy. The algorithm uses
symbolic reachability analysis for neural networks to approximate the
transition relation among sets of states and to refine the goal representation.
We evaluate our approach on complex navigation tasks, showing the learned
representation is interpretable, transferrable and results in data efficient
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Practitioner Perspectives On Training Data Attribution
  Explanations <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisa Nguyen, Evgenii Kortukov, Jean Y. Song, Seong Joon Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable AI (XAI) aims to provide insight into opaque model reasoning to
humans and as such is an interdisciplinary field by nature. In this paper, we
interviewed 10 practitioners to understand the possible usability of training
data attribution (TDA) explanations and to explore the design space of such an
approach. We confirmed that training data quality is often the most important
factor for high model performance in practice and model developers mainly rely
on their own experience to curate data. End-users expect explanations to
enhance their interaction with the model and do not necessarily prioritise but
are open to training data as a means of explanation. Within our participants,
we found that TDA explanations are not well-known and therefore not used. We
urge the community to focus on the utility of TDA techniques from the
human-machine collaboration perspective and broaden the TDA evaluation to
reflect common use cases in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS XAI in Action workshop 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable Anomaly Detection using Masked Latent Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daesoo Lee, Sara Malacarne, Erlend Aune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel time series anomaly detection method that achieves
excellent detection accuracy while offering a superior level of explainability.
Our proposed method, TimeVQVAE-AD, leverages masked generative modeling adapted
from the cutting-edge time series generation method known as TimeVQVAE. The
prior model is trained on the discrete latent space of a time-frequency domain.
Notably, the dimensional semantics of the time-frequency domain are preserved
in the latent space, enabling us to compute anomaly scores across different
frequency bands, which provides a better insight into the detected anomalies.
Additionally, the generative nature of the prior model allows for sampling
likely normal states for detected anomalies, enhancing the explainability of
the detected anomalies through counterfactuals. Our experimental evaluation on
the UCR Time Series Anomaly archive demonstrates that TimeVQVAE-AD
significantly surpasses the existing methods in terms of detection accuracy and
explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Network Pruning by Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12526v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12526v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhang Zhang, Ruyi Tao, Jiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in the parameters of deep learning models has led to
significant costs, challenging computational efficiency and model
interpretability. In this paper, we introduce a novel and straightforward
neural network pruning framework that incorporates the Gumbel-Softmax
technique. This framework enables the simultaneous optimization of a network's
weights and topology in an end-to-end process using stochastic gradient
descent. Empirical results demonstrate its exceptional compression capability,
maintaining high accuracy on the MNIST dataset with only 0.15\% of the original
network parameters. Moreover, our framework enhances neural network
interpretability, not only by allowing easy extraction of feature importance
directly from the pruned network but also by enabling visualization of feature
symmetry and the pathways of information propagation from features to outcomes.
Although the pruning strategy is learned through deep learning, it is
surprisingly intuitive and understandable, focusing on selecting key
representative features and exploiting data patterns to achieve extreme sparse
pruning. We believe our method opens a promising new avenue for deep learning
pruning and the creation of interpretable machine learning systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-Robust Learning on Heterogeneous <span class="highlight-title">Dataset</span>s via Bucketing <span class="chip">ICLR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.09365v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.09365v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Praneeth Karimireddy, Lie He, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Byzantine robust distributed or federated learning, a central server wants
to train a machine learning model over data distributed across multiple
workers. However, a fraction of these workers may deviate from the prescribed
algorithm and send arbitrary messages. While this problem has received
significant attention recently, most current defenses assume that the workers
have identical data. For realistic cases when the data across workers are
heterogeneous (non-iid), we design new attacks which circumvent current
defenses, leading to significant loss of performance. We then propose a simple
bucketing scheme that adapts existing robust algorithms to heterogeneous
datasets at a negligible computational cost. We also theoretically and
experimentally validate our approach, showing that combining bucketing with
existing robust algorithms is effective against challenging attacks. Our work
is the first to establish guaranteed convergence for the non-iid Byzantine
robust problem under realistic assumptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v5 is the camera-ready version of this paper on ICLR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On sampling determinantal and Pfaffian point processes on a quantum
  computer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15851v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15851v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rémi Bardenet, Michaël Fanuel, Alexandre Feller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DPPs were introduced by Macchi as a model in quantum optics the 1970s. Since
then, they have been widely used as models and subsampling tools in statistics
and computer science. Most applications require sampling from a DPP, and given
their quantum origin, it is natural to wonder whether sampling a DPP on a
quantum computer is easier than on a classical one. We focus here on DPPs over
a finite state space, which are distributions over the subsets of
$\{1,\dots,N\}$ parametrized by an $N\times N$ Hermitian kernel matrix. Vanilla
sampling consists in two steps, of respective costs $\mathcal{O}(N^3)$ and
$\mathcal{O}(Nr^2)$ operations on a classical computer, where $r$ is the rank
of the kernel matrix. A large first part of the current paper consists in
explaining why the state-of-the-art in quantum simulation of fermionic systems
already yields quantum DPP sampling algorithms. We then modify existing quantum
circuits, and discuss their insertion in a full DPP sampling pipeline that
starts from practical kernel specifications. The bottom line is that, with $P$
(classical) parallel processors, we can divide the preprocessing cost by $P$
and build a quantum circuit with $\mathcal{O}(Nr)$ gates that sample a given
DPP, with depth varying from $\mathcal{O}(N)$ to $\mathcal{O}(r\log N)$
depending on qubit-communication constraints on the target machine. We also
connect existing work on the simulation of superconductors to Pfaffian point
processes, which generalize DPPs and would be a natural addition to the machine
learner's toolbox. In particular, we describe "projective" Pfaffian point
processes, the cardinality of which has constant parity, almost surely.
Finally, the circuits are empirically validated on a classical simulator and on
5-qubit IBM machines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 9 figures. Additional results about parity of cardinality
  of PfPP samples. Minor corrections in Section 5 and slight generalization of
  Lemma 5.4. Extra example and derivations in appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSN: A General Federated Learning Framework over LEO Satellite
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Learning Functions with Varying Number of Minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Oniani, Yanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have proven effective at In-Context Learning
(ICL), an ability that allows them to create predictors from labeled examples.
Few studies have explored the interplay between ICL and specific properties of
functions it attempts to approximate. In our study, we use a formal framework
to explore ICL and propose a new task of approximating functions with varying
number of minima. We implement a method that allows for producing functions
with given inputs as minima. We find that increasing the number of minima
degrades ICL performance. At the same time, our evaluation shows that ICL
outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster
than 2NN in all settings. We validate the findings through a set of few-shot
experiments across various hyperparameter configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Degree-Preserving Randomized Response for Graph Neural Networks under
  Local Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.10209v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.10209v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seira Hidano, Takao Murakami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially private GNNs (Graph Neural Networks) have been recently
studied to provide high accuracy in various tasks on graph data while strongly
protecting user privacy. In particular, a recent study proposes an algorithm to
protect each user's feature vector in an attributed graph with LDP (Local
Differential Privacy), a strong privacy notion without a trusted third party.
However, this algorithm does not protect edges (friendships) in a social graph,
hence cannot protect user privacy in unattributed graphs. How to provide strong
privacy with high accuracy in unattributed graphs remains open.
  In this paper, we propose a novel LDP algorithm called the DPRR
(Degree-Preserving Randomized Response) to provide LDP for edges in GNNs. Our
DPRR preserves each user's degree hence a graph structure while providing edge
LDP. Technically, our DPRR uses Warner's RR (Randomized Response) and strategic
edge sampling, where each user's sampling probability is automatically tuned
using the Laplacian mechanism to preserve the degree information under edge
LDP. We also propose a privacy budget allocation method to make the noise in
both Warner's RR and the Laplacian mechanism small. We focus on graph
classification as a task of GNNs and evaluate the DPRR using three social graph
datasets. Our experimental results show that the DPRR significantly outperforms
three baselines and provides accuracy close to a non-private algorithm in all
datasets with a reasonable privacy budget, e.g., epsilon=1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with
  Variational Score Distillation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score distillation sampling (SDS) has shown great promise in text-to-3D
generation by distilling pretrained large-scale text-to-image diffusion models,
but suffers from over-saturation, over-smoothing, and low-diversity problems.
In this work, we propose to model the 3D parameter as a random variable instead
of a constant as in SDS and present variational score distillation (VSD), a
principled particle-based variational framework to explain and address the
aforementioned issues in text-to-3D generation. We show that SDS is a special
case of VSD and leads to poor samples with both small and large CFG weights. In
comparison, VSD works well with various CFG weights as ancestral sampling from
diffusion models and simultaneously improves the diversity and sample quality
with a common CFG weight (i.e., $7.5$). We further present various improvements
in the design space for text-to-3D such as distillation time schedule and
density initialization, which are orthogonal to the distillation algorithm yet
not well explored. Our overall approach, dubbed ProlificDreamer, can generate
high rendering resolution (i.e., $512\times512$) and high-fidelity NeRF with
rich structure and complex effects (e.g., smoke and drops). Further,
initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and
photo-realistic. Project page and codes:
https://ml.cs.tsinghua.edu.cn/prolificdreamer/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Patch-Mix Contrastive Learning with Audio Spectrogram <span class="highlight-title">Transformer</span> on
  Respiratory Sound Classification <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14032v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14032v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangmin Bae, June-Woo Kim, Won-Yang Cho, Hyerim Baek, Soyoun Son, Byungjo Lee, Changwan Ha, Kyongpil Tae, Sungnyun Kim, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Respiratory sound contains crucial information for the early diagnosis of
fatal lung diseases. Since the COVID-19 pandemic, there has been a growing
interest in contact-free medical care based on electronic stethoscopes. To this
end, cutting-edge deep learning models have been developed to diagnose lung
diseases; however, it is still challenging due to the scarcity of medical data.
In this study, we demonstrate that the pretrained model on large-scale visual
and audio datasets can be generalized to the respiratory sound classification
task. In addition, we introduce a straightforward Patch-Mix augmentation, which
randomly mixes patches between different samples, with Audio Spectrogram
Transformer (AST). We further propose a novel and effective Patch-Mix
Contrastive Learning to distinguish the mixed representations in the latent
space. Our method achieves state-of-the-art performance on the ICBHI dataset,
outperforming the prior leading score by an improvement of 4.08%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INTERSPEECH 2023, Code URL:
  https://github.com/raymin0223/patch-mix_contrastive_learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PhysGaussian, a new method that seamlessly integrates physically
grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel
motion synthesis. Employing a custom Material Point Method (MPM), our approach
enriches 3D Gaussian kernels with physically meaningful kinematic deformation
and mechanical stress attributes, all evolved in line with continuum mechanics
principles. A defining characteristic of our method is the seamless integration
between physical simulation and visual rendering: both components utilize the
same 3D Gaussian kernels as their discrete representations. This negates the
necessity for triangle/tetrahedron meshing, marching cubes, "cage meshes," or
any other geometry embedding, highlighting the principle of "what you see is
what you simulate (WS$^2$)." Our method demonstrates exceptional versatility
across a wide variety of materials--including elastic entities, metals,
non-Newtonian fluids, and granular materials--showcasing its strong
capabilities in creating diverse visual content with novel viewpoints and
movements. Our project page is at: https://xpandora.github.io/PhysGaussian/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EdgeFM: Leveraging Foundation Model for Open-set Learning on the Edge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bufang Yang, Lixing He, Neiwen Ling, Zhenyu Yan, Guoliang Xing, Xian Shuai, Xiaozhe Ren, Xin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) models have been widely deployed on IoT devices with the
help of advancements in DL algorithms and chips. However, the limited resources
of edge devices make these on-device DL models hard to be generalizable to
diverse environments and tasks. Although the recently emerged foundation models
(FMs) show impressive generalization power, how to effectively leverage the
rich knowledge of FMs on resource-limited edge devices is still not explored.
In this paper, we propose EdgeFM, a novel edge-cloud cooperative system with
open-set recognition capability. EdgeFM selectively uploads unlabeled data to
query the FM on the cloud and customizes the specific knowledge and
architectures for edge models. Meanwhile, EdgeFM conducts dynamic model
switching at run-time taking into account both data uncertainty and dynamic
network variations, which ensures the accuracy always close to the original FM.
We implement EdgeFM using two FMs on two edge platforms. We evaluate EdgeFM on
three public datasets and two self-collected datasets. Results show that EdgeFM
can reduce the end-to-end latency up to 3.2x and achieve 34.3% accuracy
increase compared with the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 21th ACM Conference on Embedded Networked Sensor
  Systems (SenSys 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sensor Fault Detection and Isolation in Autonomous Nonlinear Systems
  Using Neural Network-Based Observers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Cao, Muhammad Umar B. Niazi, Matthieu Barreau, Karl Henrik Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel observer-based approach to detect and isolate
faulty sensors in nonlinear systems. The proposed sensor fault detection and
isolation (s-FDI) method applies to a general class of nonlinear systems. Our
focus is on s-FDI for two types of faults: complete failure and sensor
degradation. The key aspect of this approach lies in the utilization of a
neural network-based Kazantzis-Kravaris/Luenberger (KKL) observer. The neural
network is trained to learn the dynamics of the observer, enabling accurate
output predictions of the system. Sensor faults are detected by comparing the
actual output measurements with the predicted values. If the difference
surpasses a theoretical threshold, a sensor fault is detected. To identify and
isolate which sensor is faulty, we compare the numerical difference of each
sensor meassurement with an empirically derived threshold. We derive both
theoretical and empirical thresholds for detection and isolation, respectively.
Notably, the proposed approach is robust to measurement noise and system
uncertainties. Its effectiveness is demonstrated through numerical simulations
of sensor faults in a network of Kuramoto oscillators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene
  Graphs with Weak Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07647v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07647v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiani Huang, Ziyang Li, Mayur Naik, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose LASER, a neuro-symbolic approach to learn semantic video
representations that capture rich spatial and temporal properties in video data
by leveraging high-level logic specifications. In particular, we formulate the
problem in terms of alignment between raw videos and spatio-temporal logic
specifications. The alignment algorithm leverages a differentiable symbolic
reasoner and a combination of contrastive, temporal, and semantics losses. It
effectively and efficiently trains low-level perception models to extract
fine-grained video representation in the form of a spatio-temporal scene graph
that conforms to the desired high-level specification. In doing so, we explore
a novel methodology that weakly supervises the learning of video semantic
representations through logic specifications. We evaluate our method on two
datasets with rich spatial and temporal specifications:
20BN-Something-Something and MUGEN. We demonstrate that our method learns
better fine-grained video semantics than existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self supervised convolutional kernel based handcrafted feature
  harmonization: Enhanced left ventricle hypertension disease phenotyping on
  echocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08897v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08897v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jina Lee, Youngtaek Hong, Dawun Jeong, Yeonggul Jang, Jaeik Jeon, Sihyeon Jeong, Taekgeun Jung, Yeonyee E. Yoon, Inki Moon, Seung-Ah Lee, Hyuk-Jae Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiomics, a medical imaging technique, extracts quantitative handcrafted
features from images to predict diseases. Harmonization in those features
ensures consistent feature extraction across various imaging devices and
protocols. Methods for harmonization include standardized imaging protocols,
statistical adjustments, and evaluating feature robustness. Myocardial diseases
such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD)
are diagnosed via echocardiography, but variable imaging settings pose
challenges. Harmonization techniques are crucial for applying handcrafted
features in disease diagnosis in such scenario. Self-supervised learning (SSL)
enhances data understanding within limited datasets and adapts to diverse data
settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying
superior performance in various tasks. This study focuses on convolutional
filters within SSL, using them as preprocessing to convert images into feature
maps for handcrafted feature harmonization. Our proposed method excelled in
harmonization evaluation and exhibited superior LVH classification performance
compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning continuous models for continuous physics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.08494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.08494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditi S. Krishnapriyan, Alejandro F. Queiruga, N. Benjamin Erichson, Michael W. Mahoney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamical systems that evolve continuously over time are ubiquitous
throughout science and engineering. Machine learning (ML) provides data-driven
approaches to model and predict the dynamics of such systems. A core issue with
this approach is that ML models are typically trained on discrete data, using
ML methodologies that are not aware of underlying continuity properties. This
results in models that often do not capture any underlying continuous dynamics
-- either of the system of interest, or indeed of any related system. To
address this challenge, we develop a convergence test based on numerical
analysis theory. Our test verifies whether a model has learned a function that
accurately approximates an underlying continuous dynamics. Models that fail
this test fail to capture relevant dynamics, rendering them of limited utility
for many scientific prediction tasks; while models that pass this test enable
both better interpolation and better extrapolation in multiple ways. Our
results illustrate how principled numerical analysis methods can be coupled
with existing ML training/testing methodologies to validate models for science
and engineering applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Tradeoff between Privacy Preservation and Byzantine-Robustness in
  Decentralized Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxiang Ye, Heng Zhu, Qing Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper jointly considers privacy preservation and Byzantine-robustness in
decentralized learning. In a decentralized network, honest-but-curious agents
faithfully follow the prescribed algorithm, but expect to infer their
neighbors' private data from messages received during the learning process,
while dishonest-and-Byzantine agents disobey the prescribed algorithm, and
deliberately disseminate wrong messages to their neighbors so as to bias the
learning process. For this novel setting, we investigate a generic
privacy-preserving and Byzantine-robust decentralized stochastic gradient
descent (SGD) framework, in which Gaussian noise is injected to preserve
privacy and robust aggregation rules are adopted to counteract Byzantine
attacks. We analyze its learning error and privacy guarantee, discovering an
essential tradeoff between privacy preservation and Byzantine-robustness in
decentralized learning -- the learning error caused by defending against
Byzantine attacks is exacerbated by the Gaussian noise added to preserve
privacy. For a class of state-of-the-art robust aggregation rules, we give
unified analysis of the "mixing abilities". Building upon this analysis, we
reveal how the "mixing abilities" affect the tradeoff between privacy
preservation and Byzantine-robustness. The theoretical results provide
guidelines for achieving a favorable tradeoff with proper design of robust
aggregation rules. Numerical experiments are conducted and corroborate our
theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual
  Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, Zhuowen Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs), which extend Large Language Models (LLM) by
incorporating visual understanding capability, have demonstrated significant
advancements in addressing open-ended visual question-answering (VQA) tasks.
However, these models cannot accurately interpret images infused with text, a
common occurrence in real-world scenarios. Standard procedures for extracting
information from images often involve learning a fixed set of query embeddings.
These embeddings are designed to encapsulate image contexts and are later used
as soft prompt inputs in LLMs. Yet, this process is limited to the token count,
potentially curtailing the recognition of scenes with text-rich context. To
improve upon them, the present study introduces BLIVA: an augmented version of
InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings
from InstructBLIP and also directly projects encoded patch embeddings into the
LLM, a technique inspired by LLaVA. This approach assists the model to capture
intricate details potentially missed during the query decoding process.
Empirical evidence demonstrates that our model, BLIVA, significantly enhances
performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA
benchmark) and in undertaking general (not particularly text-rich) VQA
benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), comparing to our
baseline InstructBLIP. BLIVA demonstrates significant capability in decoding
real-world images, irrespective of text presence. To demonstrate the broad
industry applications enabled by BLIVA, we evaluate the model using a new
dataset comprising YouTube thumbnails paired with question-answer sets across
11 diverse categories. For researchers interested in further exploration, our
code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Novel Object Detection via Cooperative Foundational Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Bharadwaj, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the challenging and emergent problem of novel object
detection (NOD), focusing on the accurate detection of both known and novel
object categories during inference. Traditional object detection algorithms are
inherently closed-set, limiting their capability to handle NOD. We present a
novel approach to transform existing closed-set detectors into open-set
detectors. This transformation is achieved by leveraging the complementary
strengths of pre-trained foundational models, specifically CLIP and SAM,
through our cooperative mechanism. Furthermore, by integrating this mechanism
with state-of-the-art open-set detectors such as GDINO, we establish new
benchmarks in object detection performance. Our method achieves 17.42 mAP in
novel object detection and 42.08 mAP for known objects on the challenging LVIS
dataset. Adapting our approach to the COCO OVD split, we surpass the current
state-of-the-art by a margin of 7.2 $ \text{AP}_{50} $ for novel classes. Our
code is available at
https://github.com/rohit901/cooperative-foundational-models .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/rohit901/cooperative-foundational-models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Connectionist Temporal Classification for Order-Preserving
  Sequence Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Nan, Ting Dang, Vidhyasaharan Sethu, Beena Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connectionist temporal classification (CTC) is commonly adopted for sequence
modeling tasks like speech recognition, where it is necessary to preserve order
between the input and target sequences. However, CTC is only applied to
deterministic sequence models, where the latent space is discontinuous and
sparse, which in turn makes them less capable of handling data variability when
compared to variational models. In this paper, we integrate CTC with a
variational model and derive loss functions that can be used to train more
generalizable sequence models that preserve order. Specifically, we derive two
versions of the novel variational CTC based on two reasonable assumptions, the
first being that the variational latent variables at each time step are
conditionally independent; and the second being that these latent variables are
Markovian. We show that both loss functions allow direct optimization of the
variational lower bound for the model log-likelihood, and present
computationally tractable forms for implementing them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering stochastic dynamical equations from biological time series
  data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.02645v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.02645v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arshed Nabeel, Ashwin Karichannavar, Shuaib Palathingal, Jitesh Jhawar, David B. Brückner, Danny Raj M., Vishwesha Guttal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic differential equations (SDEs) are an important framework to model
dynamics with randomness, as is common in most biological systems. The inverse
problem of integrating these models with empirical data remains a major
challenge. Here, we present a software package, PyDaDDy (Python Library for
Data Driven Dynamics) that takes time series data as an input and outputs an
interpretable SDE. We achieve this by combining traditional approaches from
stochastic calculus literature with state-of-the-art equation discovery
techniques. We validate our approach on synthetic datasets, and demonstrate the
generality and applicability of the method on two real-world datasets of vastly
different spatiotemporal scales: (i) collective movement of fish school where
stochasticity plays a crucial role, and (ii) confined migration of a single
cell, primarily following a relaxed oscillation. We make the method available
as an easy-to-use, open-source Python package, PyDaddy (Python Library for Data
Driven Dynamics).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages (+ 9 page appendix), 6 figures (+ 8 appendix figures).
  Updates: v3: Significantly reorganized the paper and added a section analysis
  of a cell migration dataset. v4: Update arXiv title to match the updated
  title of the manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Wireless Federated Learning Using Orthogonal
  Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xizixiang Wei, Tianhao Wang, Ruiquan Huang, Cong Shen, Jing Yang, H. Vincent Poor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a privacy-preserving uplink over-the-air computation (AirComp)
method, termed FLORAS, for single-input single-output (SISO) wireless federated
learning (FL) systems. From the perspective of communication designs, FLORAS
eliminates the requirement of channel state information at the transmitters
(CSIT) by leveraging the properties of orthogonal sequences. From the privacy
perspective, we prove that FLORAS offers both item-level and client-level
differential privacy (DP) guarantees. Moreover, by properly adjusting the
system parameters, FLORAS can flexibly achieve different DP levels at no
additional cost. A new FL convergence bound is derived which, combined with the
privacy guarantees, allows for a smooth tradeoff between the achieved
convergence rate and differential privacy levels. Experimental results
demonstrate the advantages of FLORAS compared with the baseline AirComp method,
and validate that the analytical results can guide the design of
privacy-preserving FL with different tradeoff requirements on the model
convergence and privacy levels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D
  Shape Detailization and Differentiable Rendering <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qimin Chen, Zhiqin Chen, Hang Zhou, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ShaDDR, an example-based deep generative neural network which
produces a high-resolution textured 3D shape through geometry detailization and
conditional texture generation applied to an input coarse voxel shape. Trained
on a small set of detailed and textured exemplar shapes, our method learns to
detailize the geometry via multi-resolution voxel upsampling and generate
textures on voxel surfaces via differentiable rendering against exemplar
texture images from a few views. The generation is interactive, taking less
than 1 second to produce a 3D model with voxel resolutions up to 512^3. The
generated shape preserves the overall structure of the input coarse voxel
model, while the style of the generated geometric details and textures can be
manipulated through learned latent codes. In the experiments, we show that our
method can generate higher-resolution shapes with plausible and improved
geometric details and clean textures compared to prior works. Furthermore, we
showcase the ability of our method to learn geometric details and textures from
shapes reconstructed from real-world photos. In addition, we have developed an
interactive modeling application to demonstrate the generalizability of our
method to various user inputs and the controllability it offers, allowing users
to interactively sculpt a coarse voxel shape to define the overall structure of
the detailized 3D shape. Code and data are available at
https://github.com/qiminchen/ShaDDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGGRAPH Asia 2023 conference track. Code:
  https://github.com/qiminchen/ShaDDR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verified Compositional Neuro-Symbolic Control for Stochastic Systems
  with Temporal Logic Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10863v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10863v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Wang, Haojun Chen, Zihe Sun, Yiannis Kantaros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several methods have been proposed recently to learn neural network (NN)
controllers for autonomous agents, with unknown and stochastic dynamics, tasked
with complex missions captured by Linear Temporal Logic (LTL). Due to the
sample-inefficiency of the majority of these works, compositional learning
methods have been proposed decomposing the LTL specification into smaller
sub-tasks. Then, separate controllers are learned and composed to satisfy the
original task. A key challenge within these approaches is that they often lack
safety guarantees or the provided guarantees are impractical. This paper aims
to address this challenge. Particularly, we consider autonomous systems with
unknown and stochastic dynamics and LTL-encoded tasks. We assume that the
system is equipped with a finite set of base skills modeled by trained NN
feedback controllers. Our goal is to check if there exists a temporal
composition of the trained NN controllers - and if so, to compute it - that
will yield a composite system behavior that satisfies the assigned LTL task
with probability one. We propose a new approach that relies on a novel
integration of automata theory and data-driven reachability analysis tools for
NN-controlled stochastic systems. The resulting neuro-symbolic controller
allows the agent to generate safe behaviors for unseen complex temporal logic
tasks in a zero-shot fashion by leveraging its base skills. We show correctness
of the proposed method and we provide conditions under which it is complete. To
the best of our knowledge, this is the first work that designs verified
temporal compositions of NN controllers for unknown and stochastic systems.
Finally, we provide extensive numerical simulations and hardware experiments on
robot navigation tasks to demonstrate the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.06130</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Gap Between Offline and Online Reinforcement Learning
  Evaluation Methodologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivakanth Sujit, Pedro H. M. Braga, Jorg Bornschein, Samira Ebrahimi Kahou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has shown great promise with algorithms learning
in environments with large state and action spaces purely from scalar reward
signals. A crucial challenge for current deep RL algorithms is that they
require a tremendous amount of environment interactions for learning. This can
be infeasible in situations where such interactions are expensive; such as in
robotics. Offline RL algorithms try to address this issue by bootstrapping the
learning process from existing logged data without needing to interact with the
environment from the very beginning. While online RL algorithms are typically
evaluated as a function of the number of environment interactions, there exists
no single established protocol for evaluating offline RL methods.In this paper,
we propose a sequential approach to evaluate offline RL algorithms as a
function of the training set size and thus by their data efficiency. Sequential
evaluation provides valuable insights into the data efficiency of the learning
process and the robustness of algorithms to distribution changes in the dataset
while also harmonizing the visualization of the offline and online learning
phases. Our approach is generally applicable and easy to implement. We compare
several existing offline RL algorithms using this approach and present insights
from a variety of tasks and offline datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Channel and Gradient-Importance Aware Device Scheduling for Over-the-Air
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16854v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16854v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchang Sun, Zehong lin, Yuyi Mao, Shi Jin, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a popular privacy-preserving distributed training
scheme, where multiple devices collaborate to train machine learning models by
uploading local model updates. To improve communication efficiency,
over-the-air computation (AirComp) has been applied to FL, which leverages
analog modulation to harness the superposition property of radio waves such
that numerous devices can upload their model updates concurrently for
aggregation. However, the uplink channel noise incurs considerable model
aggregation distortion, which is critically determined by the device scheduling
and compromises the learned model performance. In this paper, we propose a
probabilistic device scheduling framework for over-the-air FL, named PO-FL, to
mitigate the negative impact of channel noise, where each device is scheduled
according to a certain probability and its model update is reweighted using
this probability in aggregation. We prove the unbiasedness of this aggregation
scheme and demonstrate the convergence of PO-FL on both convex and non-convex
loss functions. Our convergence bounds unveil that the device scheduling
affects the learning performance through the communication distortion and
global update variance. Based on the convergence analysis, we further develop a
channel and gradient-importance aware algorithm to optimize the device
scheduling probabilities in PO-FL. Extensive simulation results show that the
proposed PO-FL framework with channel and gradient-importance awareness
achieves faster convergence and produces better models than baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creating Temporally Correlated High-Resolution Power Injection Profiles
  Using Physics-Aware GAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hritik Gopal Shah, Behrouz Azimian, Anamitra Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional smart meter measurements lack the granularity needed for
real-time decision-making. To address this practical problem, we create a
generative adversarial networks (GAN) model that enforces temporal consistency
on its high-resolution outputs via hard inequality constraints using a convex
optimization layer. A unique feature of our GAN model is that it is trained
solely on slow timescale aggregated power information obtained from historical
smart meter data. The results demonstrate that the model can successfully
create minutely interval temporally-correlated instantaneous power injection
profiles from 15-minute average power consumption information. This innovative
approach, emphasizing inter-neuron constraints, offers a promising avenue for
improved high-speed state estimation in distribution systems and enhances the
applicability of data-driven solutions for monitoring such systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Representational Capacity of Recurrent Neural Language Models <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12942v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12942v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franz Nowak, Anej Svete, Li Du, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates the computational expressivity of language models
(LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992)
famously showed that RNNs with rational weights and hidden states and unbounded
computation time are Turing complete. However, LMs define weightings over
strings in addition to just (unweighted) language membership and the analysis
of the computational power of RNN LMs (RLMs) should reflect this. We extend the
Turing completeness result to the probabilistic case, showing how a rationally
weighted RLM with unbounded computation time can simulate any deterministic
probabilistic Turing machine (PTM) with rationally weighted transitions. Since,
in practice, RLMs work in real-time, processing a symbol at every time step, we
treat the above result as an upper bound on the expressivity of RLMs. We also
provide a lower bound by showing that under the restriction to real-time
computation, such models can simulate deterministic real-time rational PTMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published at EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gates Are Not What You Need in RNNs <span class="chip">SC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.00527v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.00527v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronalds Zakovskis, Andis Draguns, Eliza Gaile, Emils Ozolins, Karlis Freivalds
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks have flourished in many areas. Consequently, we can
see new RNN cells being developed continuously, usually by creating or using
gates in a new, original way. But what if we told you that gates in RNNs are
redundant? In this paper, we propose a new recurrent cell called Residual
Recurrent Unit (RRU) which beats traditional cells and does not employ a single
gate. It is based on the residual shortcut connection, linear transformations,
ReLU, and normalization. To evaluate our cell's effectiveness, we compare its
performance against the widely-used GRU and LSTM cells and the recently
proposed Mogrifier LSTM on several tasks including, polyphonic music modeling,
language modeling, and sentiment analysis. Our experiments show that RRU
outperforms the traditional gated units on most of these tasks. Also, it has
better robustness to parameter selection, allowing immediate application in new
tasks without much tuning. We have implemented the RRU in TensorFlow, and the
code is made available at https://github.com/LUMII-Syslab/RRU .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Artificial Intelligence and Soft Computing. ICAISC 2023.
  Lecture Notes in Computer Science(), vol 14125. Springer, Cham., and is
  available online at https://doi.org/10.1007/978-3-031-42505-9_27</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics <span class="chip">NeurIPS23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06202v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06202v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anwar Said, Roza G. Bayrak, Tyler Derr, Mudassir Shabbir, Daniel Moyer, Catie Chang, Xenofon Koutsoukos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning provides a valuable tool for analyzing high-dimensional
functional neuroimaging data, and is proving effective in predicting various
neurological conditions, psychiatric disorders, and cognitive patterns. In
functional magnetic resonance imaging (MRI) research, interactions between
brain regions are commonly modeled using graph-based representations. The
potency of graph machine learning methods has been established across myriad
domains, marking a transformative step in data interpretation and predictive
modeling. Yet, despite their promise, the transposition of these techniques to
the neuroimaging domain has been challenging due to the expansive number of
potential preprocessing pipelines and the large parameter search space for
graph-based dataset construction. In this paper, we introduce NeuroGraph, a
collection of graph-based neuroimaging datasets, and demonstrated its utility
for predicting multiple categories of behavioral and cognitive traits. We delve
deeply into the dataset generation search space by crafting 35 datasets that
encompass static and dynamic brain connectivity, running in excess of 15
baseline methods for benchmarking. Additionally, we provide generic frameworks
for learning on both static and dynamic graphs. Our extensive experiments lead
to several key observations. Notably, using correlation vectors as node
features, incorporating larger number of regions of interest, and employing
sparser graphs lead to improved performance. To foster further advancements in
graph-based data driven neuroimaging analysis, we offer a comprehensive
open-source Python package that includes the benchmark datasets, baseline
implementations, model training, and standard evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BOIS: Bayesian Optimization of Interconnected Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo D. González, Victor M. Zavala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimization (BO) has proven to be an effective paradigm for the
global optimization of expensive-to-sample systems. One of the main advantages
of BO is its use of Gaussian processes (GPs) to characterize model uncertainty
which can be leveraged to guide the learning and search process. However, BO
typically treats systems as black-boxes and this limits the ability to exploit
structural knowledge (e.g., physics and sparse interconnections). Composite
functions of the form $f(x, y(x))$, wherein GP modeling is shifted from the
performance function $f$ to an intermediate function $y$, offer an avenue for
exploiting structural knowledge. However, the use of composite functions in a
BO framework is complicated by the need to generate a probability density for
$f$ from the Gaussian density of $y$ calculated by the GP (e.g., when $f$ is
nonlinear it is not possible to obtain a closed-form expression). Previous work
has handled this issue using sampling techniques; these are easy to implement
and flexible but are computationally intensive. In this work, we introduce a
new paradigm which allows for the efficient use of composite functions in BO;
this uses adaptive linearizations of $f$ to obtain closed-form expressions for
the statistical moments of the composite function. We show that this simple
approach (which we call BOIS) enables the exploitation of structural knowledge,
such as that arising in interconnected systems as well as systems that embed
multiple GP models and combinations of physics and GP models. Using a chemical
process optimization case study, we benchmark the effectiveness of BOIS against
standard BO and sampling approaches. Our results indicate that BOIS achieves
performance gains and accurately captures the statistics of composite
functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Theoretical Paradigm to Understand Learning from Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, Rémi Munos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevalent deployment of learning from human preferences through
reinforcement learning (RLHF) relies on two important approximations: the first
assumes that pairwise preferences can be substituted with pointwise rewards.
The second assumes that a reward model trained on these pointwise rewards can
generalize from collected data to out-of-distribution data sampled by the
policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an
approach that bypasses the second approximation and learn directly a policy
from collected data without the reward modelling stage. However, this method
still heavily relies on the first approximation.
  In this paper we try to gain a deeper theoretical understanding of these
practical algorithms. In particular we derive a new general objective called
$\Psi$PO for learning from human preferences that is expressed in terms of
pairwise preferences and therefore bypasses both approximations. This new
general objective allows us to perform an in-depth analysis of the behavior of
RLHF and DPO (as special cases of $\Psi$PO) and to identify their potential
pitfalls. We then consider another special case for $\Psi$PO by setting $\Psi$
simply to Identity, for which we can derive an efficient optimisation
procedure, prove performance guarantees and demonstrate its empirical
superiority to DPO on some illustrative examples.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CompenHR: Efficient Full Compensation for High-resolution Projector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Wang, Haibin Ling, Bingyao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Full projector compensation is a practical task of projector-camera systems.
It aims to find a projector input image, named compensation image, such that
when projected it cancels the geometric and photometric distortions due to the
physical environment and hardware. State-of-the-art methods use deep learning
to address this problem and show promising performance for low-resolution
setups. However, directly applying deep learning to high-resolution setups is
impractical due to the long training time and high memory cost. To address this
issue, this paper proposes a practical full compensation solution. Firstly, we
design an attention-based grid refinement network to improve geometric
correction quality. Secondly, we integrate a novel sampling scheme into an
end-to-end compensation network to alleviate computation and introduce
attention blocks to preserve key features. Finally, we construct a benchmark
dataset for high-resolution projector full compensation. In experiments, our
method demonstrates clear advantages in both efficiency and quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Radiology Report Generation via Causal Reasoning and
  Counterfactual Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Song, Jiafan Liu, Yun Li, Wenbin Lei, Ruxin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology Report Generation (RRG) draws attention as an interaction between
vision and language fields. Previous works inherited the ideology of
vision-to-language generation tasks,aiming to generate paragraphs with high
consistency as reports. However, one unique characteristic of RRG, the
independence between diseases, was neglected, leading to the injection of the
spurious confounder, i.e., the disease co-occurrence. Unfortunately, this
confounder confuses the process of report generation worse because of the
biased RRG data distribution. In this paper, to rethink this issue thoroughly,
we reason about its causes and effects from a novel perspective of statistics
and causality, where the Joint Vision Coupling and the Conditional Sentence
Coherence Coupling are two aspects prone to implicitly decrease the accuracy of
reports. Then, a counterfactual augmentation strategy that contains the
Counterfactual Sample Synthesis and the Counterfactual Report Reconstruction
sub-methods is proposed to break these two aspects of spurious effects.
Experimental results and further analyses on two widely used datasets justify
our reasoning and proposed methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FusionFrames: Efficient Architectural Aspects for Text-to-Video
  Generation Pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Arkhipkin, Zein Shaheen, Viacheslav Vasilev, Elizaveta Dakhova, Andrey Kuznetsov, Denis Dimitrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia generation approaches occupy a prominent place in artificial
intelligence research. Text-to-image models achieved high-quality results over
the last few years. However, video synthesis methods recently started to
develop. This paper presents a new two-stage latent diffusion text-to-video
generation architecture based on the text-to-image diffusion model. The first
stage concerns keyframes synthesis to figure the storyline of a video, while
the second one is devoted to interpolation frames generation to make movements
of the scene and objects smooth. We compare several temporal conditioning
approaches for keyframes generation. The results show the advantage of using
separate temporal blocks over temporal layers in terms of metrics reflecting
video generation quality aspects and human preference. The design of our
interpolation model significantly reduces computational costs compared to other
masked frame interpolation approaches. Furthermore, we evaluate different
configurations of MoVQ-based video decoding scheme to improve consistency and
achieve higher PSNR, SSIM, MSE, and LPIPS scores. Finally, we compare our
pipeline with existing solutions and achieve top-2 scores overall and top-1
among open-source solutions: CLIPSIM = 0.2976 and FVD = 433.054. Project page:
https://ai-forever.github.io/kandinsky-video/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ai-forever.github.io/kandinsky-video/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GA2MIF: Graph and Attention Based Two-Stage Multi-Source Information
  Fusion for Conversational Emotion Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11900v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11900v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Li, Xiaoping Wang, Guoqing Lv, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Emotion Recognition in Conversation (ERC) plays an influential
role in the field of human-computer interaction and conversational robotics
since it can motivate machines to provide empathetic services. Multimodal data
modeling is an up-and-coming research area in recent years, which is inspired
by human capability to integrate multiple senses. Several graph-based
approaches claim to capture interactive information between modalities, but the
heterogeneity of multimodal data makes these methods prohibit optimal
solutions. In this work, we introduce a multimodal fusion approach named Graph
and Attention based Two-stage Multi-source Information Fusion (GA2MIF) for
emotion detection in conversation. Our proposed method circumvents the problem
of taking heterogeneous graph as input to the model while eliminating complex
redundant connections in the construction of graph. GA2MIF focuses on
contextual modeling and cross-modal modeling through leveraging Multi-head
Directed Graph ATtention networks (MDGATs) and Multi-head Pairwise Cross-modal
ATtention networks (MPCATs), respectively. Extensive experiments on two public
datasets (i.e., IEMOCAP and MELD) demonstrate that the proposed GA2MIF has the
capacity to validly capture intra-modal long-range contextual information and
inter-modal complementary information, as well as outperforms the prevalent
State-Of-The-Art (SOTA) models by a remarkable margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Affective Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphCFC: A Directed Graph Based Cross-Modal Feature Complementation
  Approach for Multimodal Conversational Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12261v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12261v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Li, Xiaoping Wang, Guoqing Lv, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation (ERC) plays a significant part in
Human-Computer Interaction (HCI) systems since it can provide empathetic
services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.
Recently, Graph Neural Networks (GNNs) have been widely used in a variety of
fields due to their superior performance in relation modeling. In multimodal
ERC, GNNs are capable of extracting both long-distance contextual information
and inter-modal interactive information. Unfortunately, since existing methods
such as MMGCN directly fuse multiple modalities, redundant information may be
generated and diverse information may be lost. In this work, we present a
directed Graph based Cross-modal Feature Complementation (GraphCFC) module that
can efficiently model contextual and interactive information. GraphCFC
alleviates the problem of heterogeneity gap in multimodal fusion by utilizing
multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)
strategy. We extract various types of edges from the constructed graph for
encoding, thus enabling GNNs to extract crucial contextual and interactive
information more accurately when performing message passing. Furthermore, we
design a GNN structure called GAT-MLP, which can provide a new unified network
framework for multimodal learning. The experimental results on two benchmark
datasets show that our GraphCFC outperforms the state-of-the-art (SOTA)
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Multimedia (TMM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval
  Score Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, Yingcong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in text-to-3D generation mark a significant milestone
in generative models, unlocking new possibilities for creating imaginative 3D
assets across various real-world scenarios. While recent advancements in
text-to-3D generation have shown promise, they often fall short in rendering
detailed and high-quality 3D models. This problem is especially prevalent as
many methods base themselves on Score Distillation Sampling (SDS). This paper
identifies a notable deficiency in SDS, that it brings inconsistent and
low-quality updating direction for the 3D model, causing the over-smoothing
effect. To address this, we propose a novel approach called Interval Score
Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes
interval-based score matching to counteract over-smoothing. Furthermore, we
incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline.
Extensive experiments show that our model largely outperforms the
state-of-the-art in quality and training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. Our code will
  be available at: https://github.com/EnVision-Research/LucidDreamer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphMFT: A Graph Network based Multimodal Fusion Technique for Emotion
  Recognition in Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.00339v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.00339v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Li, Xiaoping Wang, Guoqing Lv, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal machine learning is an emerging area of research, which has
received a great deal of scholarly attention in recent years. Up to now, there
are few studies on multimodal Emotion Recognition in Conversation (ERC). Since
Graph Neural Networks (GNNs) possess the powerful capacity of relational
modeling, they have an inherent advantage in the field of multimodal learning.
GNNs leverage the graph constructed from multimodal data to perform intra- and
inter-modal information interaction, which effectively facilitates the
integration and complementation of multimodal data. In this work, we propose a
novel Graph network based Multimodal Fusion Technique (GraphMFT) for emotion
recognition in conversation. Multimodal data can be modeled as a graph, where
each data object is regarded as a node, and both intra- and inter-modal
dependencies existing between data objects can be regarded as edges. GraphMFT
utilizes multiple improved graph attention networks to capture intra-modal
contextual information and inter-modal complementary information. In addition,
the proposed GraphMFT attempts to address the challenges of existing
graph-based multimodal conversational emotion recognition models such as MMGCN.
Empirical results on two public multimodal datasets reveal that our model
outperforms the State-Of-The-Art (SOTA) approaches with the accuracy of 67.90%
and 61.30%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neurocomputing</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-11-21T00:00:00Z">2023-11-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">63</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LowResource at BLP-2023 Task 2: Leveraging Bangla<span class="highlight-title">Bert</span> for Low Resource
  Sentiment Analysis of Bangla Language <span class="chip">EMNLP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aunabil Chakma, Masum Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the system of the LowResource Team for Task 2 of
BLP-2023, which involves conducting sentiment analysis on a dataset composed of
public posts and comments from diverse social media platforms. Our primary aim
is to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus,
using various strategies including fine-tuning, dropping random tokens, and
using several external datasets. Our final model is an ensemble of the three
best BanglaBert variations. Our system has achieved overall 3rd in the Test Set
among 30 participating teams with a score of 0.718. Additionally, we discuss
the promising systems that didn't perform well namely task-adaptive pertaining
and paraphrasing using BanglaT5. Training codes and external datasets which are
used for our system are publicly available at
https://github.com/Aunabil4602/bnlp-workshop-task2-2023
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BLP Workshop @EMNLP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Random Sampling: A Theoretical and Empirical Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Cui, Ashish Mittal, Songtao Lu, Wei Zhang, George Saon, Brian Kingsbury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft random sampling (SRS) is a simple yet effective approach for efficient
training of large-scale deep neural networks when dealing with massive data.
SRS selects a subset uniformly at random with replacement from the full data
set in each epoch. In this paper, we conduct a theoretical and empirical
analysis of SRS. First, we analyze its sampling dynamics including data
coverage and occupancy. Next, we investigate its convergence with non-convex
objective functions and give the convergence rate. Finally, we provide its
generalization performance. We empirically evaluate SRS for image recognition
on CIFAR10 and automatic speech recognition on Librispeech and an in-house
payload dataset to demonstrate its effectiveness. Compared to existing
coreset-based data selection methods, SRS offers a better accuracy-efficiency
trade-off. Especially on real-world industrial scale data sets, it is shown to
be a powerful training strategy with significant speedup and competitive
performance with almost no additional computing cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keeping Users Engaged During Repeated Administration of the Same
  Questionnaire: Using Large Language Models to Reliably Diversify Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hye Sun Yun, Mehdi Arjmand, Phillip Raymond Sherlock, Michael Paasche-Orlow, James W. Griffith, Timothy Bickmore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standardized, validated questionnaires are vital tools in HCI research and
healthcare, offering dependable self-report data. However, their repeated use
in longitudinal or pre-post studies can induce respondent fatigue, impacting
data quality via response biases and decreased response rates. We propose
utilizing large language models (LLMs) to generate diverse questionnaire
versions while retaining good psychometric properties. In a longitudinal study,
participants engaged with our agent system and responded daily for two weeks to
either a standardized depression questionnaire or one of two LLM-generated
questionnaire variants, alongside a validated depression questionnaire.
Psychometric testing revealed consistent covariation between the external
criterion and the focal measure administered across the three conditions,
demonstrating the reliability and validity of the LLM-generated variants.
Participants found the repeated administration of the standardized
questionnaire significantly more repetitive compared to the variants. Our
findings highlight the potential of LLM-generated variants to invigorate
questionnaires, fostering engagement and interest without compromising
validity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Understand Content and Propagation for
  Misinformation Detection: An Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyang Chen, Lingwei Wei, Han Cao, Wei Zhou, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have garnered significant attention for their
powerful ability in natural language understanding and reasoning. In this
paper, we present a comprehensive empirical study to explore the performance of
LLMs on misinformation detection tasks. This study stands as the pioneering
investigation into the understanding capabilities of multiple LLMs regarding
both content and propagation across social media platforms. Our empirical
studies on five misinformation detection datasets show that LLMs with diverse
prompts achieve comparable performance in text-based misinformation detection
but exhibit notably constrained capabilities in comprehending propagation
structure compared to existing models in propagation-based misinformation
detection. Besides, we further design four instruction-tuned strategies to
enhance LLMs for both content and propagation-based misinformation detection.
These strategies boost LLMs to actively learn effective features from multiple
instances or hard instances, and eliminate irrelevant propagation structures,
thereby achieving better detection performance. Extensive experiments further
demonstrate LLMs would play a better capacity in content and propagation
structure under these proposed strategies and achieve promising detection
performance. These findings highlight the potential ability of LLMs to detect
misinformation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Text Classification with Wasserstein Independence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibaud Leteno, Antoine Gourru, Charlotte Laclau, Rémi Emonet, Christophe Gravier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group fairness is a central research topic in text classification, where
reaching fair treatment between sensitive groups (e.g. women vs. men) remains
an open challenge. This paper presents a novel method for mitigating biases in
neural text classification, agnostic to the model architecture. Considering the
difficulty to distinguish fair from unfair information in a text encoder, we
take inspiration from adversarial training to induce Wasserstein independence
between representations learned to predict our target label and the ones
learned to predict some sensitive attribute. Our approach provides two
significant advantages. Firstly, it does not require annotations of sensitive
attributes in both testing and training data. This is more suitable for
real-life scenarios compared to existing methods that require annotations of
sensitive attributes at train time. Second, our approach exhibits a comparable
or better fairness-accuracy trade-off compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The DURel Annotation Tool: Human and Computational Measurement of
  Semantic Proximity, Sense Clusters and Semantic Change 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Schlechtweg, Shafqat Mumtaz Virk, Pauline Sander, Emma Sköldberg, Lukas Theuer Linke, Tuo Zhang, Nina Tahmasebi, Jonas Kuhn, Sabine Schulte im Walde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the DURel tool that implements the annotation of semantic
proximity between uses of words into an online, open source interface. The tool
supports standardized human annotation as well as computational annotation,
building on recent advances with Word-in-Context models. Annotator judgments
are clustered with automatic graph clustering techniques and visualized for
analysis. This allows to measure word senses with simple and intuitive
micro-task judgments between use pairs, requiring minimal preparation efforts.
The tool offers additional functionalities to compare the agreement between
annotators to guarantee the inter-subjectivity of the obtained judgments and to
calculate summary statistics giving insights into sense frequency
distributions, semantic variation or changes of senses over time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathGloss: Building mathematical glossaries from text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucy Horowitz, Valeria de Paiva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MathGloss is a project to create a knowledge graph (KG) for undergraduate
mathematics from text, automatically, using modern natural language processing
(NLP) tools and resources already available on the web. MathGloss is a linked
database of undergraduate concepts in mathematics. So far, it combines five
resources: (i) Wikidata, a collaboratively edited, multilingual knowledge graph
hosted by the Wikimedia Foundation, (ii) terms covered in mathematics courses
at the University of Chicago, (iii) the syllabus of the French undergraduate
mathematics curriculum which includes hyperlinks to the automated theorem
prover Lean 4, (iv) MuLiMa, a multilingual dictionary of mathematics curated by
mathematicians, and (v) the nLab, a wiki for category theory also curated by
mathematicians. MathGloss's goal is to bring together resources for learning
mathematics and to allow every mathematician to tailor their learning to their
own preferences. Moreover, by organizing different resources for learning
undergraduate mathematics alongside those for learning formal mathematics, we
hope to make it easier for mathematicians and formal tools (theorem provers,
computer algebra systems, etc) experts to "understand" each other and break
down some of the barriers to formal math.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IMGTB: A Framework for Machine-Generated Text Detection Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Spiegel, Dominik Macko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of large language models generating high quality texts, it is a
necessity to develop methods for detection of machine-generated text to avoid
harmful use or simply due to annotation purposes. It is, however, also
important to properly evaluate and compare such developed methods. Recently, a
few benchmarks have been proposed for this purpose; however, integration of
newest detection methods is rather challenging, since new methods appear each
month and provide slightly different evaluation pipelines. In this paper, we
present the IMGTB framework, which simplifies the benchmarking of
machine-generated text detection methods by easy integration of custom (new)
methods and evaluation datasets. Its configurability and flexibility makes
research and development of new detection methods easier, especially their
comparison to the existing state-of-the-art detectors. The default set of
analyses, metrics and visualizations offered by the tool follows the
established practices of machine-generated text detection benchmarking found in
state-of-the-art literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Learning Functions with Varying Number of Minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Oniani, Yanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have proven effective at In-Context Learning
(ICL), an ability that allows them to create predictors from labeled examples.
Few studies have explored the interplay between ICL and specific properties of
functions it attempts to approximate. In our study, we use a formal framework
to explore ICL and propose a new task of approximating functions with varying
number of minima. We implement a method that allows for producing functions
with given inputs as minima. We find that increasing the number of minima
degrades ICL performance. At the same time, our evaluation shows that ICL
outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster
than 2NN in all settings. We validate the findings through a set of few-shot
experiments across various hyperparameter configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Oasis: Data Curation and Assessment System for <span class="highlight-title">Pretrain</span>ing of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Zhou, Yubo Chen, Pengfei Cao, Kang Liu, Jun Zhao, Shengping Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is one of the most critical elements in building a large language model.
However, existing systems either fail to customize a corpus curation pipeline
or neglect to leverage comprehensive corpus assessment for iterative
optimization of the curation. To this end, we present a pretraining corpus
curation and assessment platform called Oasis -- a one-stop system for data
quality improvement and quantification with user-friendly interactive
interfaces. Specifically, the interactive modular rule filter module can devise
customized rules according to explicit feedback. The debiased neural filter
module builds the quality classification dataset in a negative-centric manner
to remove the undesired bias. The adaptive document deduplication module could
execute large-scale deduplication with limited memory resources. These three
parts constitute the customized data curation module. And in the holistic data
assessment module, a corpus can be assessed in local and global views, with
three evaluation means including human, GPT-4, and heuristic metrics. We
exhibit a complete process to use Oasis for the curation and assessment of
pretraining data. In addition, an 800GB bilingual corpus curated by Oasis is
publicly released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation Metrics of Language Generation Models for Synthetic Traffic
  Generation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Filice, Jason Ingyu Choi, Giuseppe Castellucci, Eugene Agichtein, Oleg Rokhlenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many Natural Language Generation (NLG) tasks aim to generate a single output
text given an input prompt. Other settings require the generation of multiple
texts, e.g., for Synthetic Traffic Generation (STG). This generation task is
crucial for training and evaluating QA systems as well as conversational
agents, where the goal is to generate multiple questions or utterances
resembling the linguistic variability of real users. In this paper, we show
that common NLG metrics, like BLEU, are not suitable for evaluating STG. We
propose and evaluate several metrics designed to compare the generated traffic
to the distribution of real user texts. We validate our metrics with an
automatic procedure to verify whether they capture different types of quality
issues of generated data; we also run human annotations to verify the
correlation with human judgements. Experiments on three tasks, i.e., Shopping
Utterance Generation, Product Question Generation and Query Auto Completion,
demonstrate that our metrics are effective for evaluating STG tasks, and
improve the agreement with human judgement up to 20% with respect to common NLG
metrics. We believe these findings can pave the way towards better solutions
for estimating the representativeness of synthetic text data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Word Embeddings for Low-Resource Languages using Anchors
  and a Chain of Related Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Hangya, Silvia Severini, Radoslav Ralev, Alexander Fraser, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Very low-resource languages, having only a few million tokens worth of data,
are not well-supported by multilingual NLP approaches due to poor quality
cross-lingual word representations. Recent work showed that good cross-lingual
performance can be achieved if a source language is related to the low-resource
target language. However, not all language pairs are related. In this paper, we
propose to build multilingual word embeddings (MWEs) via a novel language
chain-based approach, that incorporates intermediate related languages to
bridge the gap between the distant source and target. We build MWEs one
language at a time by starting from the resource rich source and sequentially
adding each language in the chain till we reach the target. We extend a
semi-joint bilingual approach to multiple languages in order to eliminate the
main weakness of previous works, i.e., independently trained monolingual
embeddings, by anchoring the target language around the multilingual space. We
evaluate our method on bilingual lexicon induction for 4 language families,
involving 4 very low-resource (<5M tokens) and 4 moderately low-resource (<50M)
target languages, showing improved performance in both categories.
Additionally, our analysis reveals the importance of good quality embeddings
for intermediate languages as well as the importance of leveraging anchor
points from all languages in the multilingual space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the MRL 2023 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speaker-Adapted End-to-End Visual Speech Recognition for Continuous
  Spanish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different studies have shown the importance of visual cues throughout the
speech perception process. In fact, the development of audiovisual approaches
has led to advances in the field of speech technologies. However, although
noticeable results have recently been achieved, visual speech recognition
remains an open research problem. It is a task in which, by dispensing with the
auditory sense, challenges such as visual ambiguities and the complexity of
modeling silence must be faced. Nonetheless, some of these challenges can be
alleviated when the problem is approached from a speaker-dependent perspective.
Thus, this paper studies, using the Spanish LIP-RTVE database, how the
estimation of specialized end-to-end systems for a specific person could affect
the quality of speech recognition. First, different adaptation strategies based
on the fine-tuning technique were proposed. Then, a pre-trained CTC/Attention
architecture was used as a baseline throughout our experiments. Our findings
showed that a two-step fine-tuning process, where the VSR system is first
adapted to the task domain, provided significant improvements when the speaker
adaptation was addressed. Furthermore, results comparable to the current state
of the art were reached even when only a limited amount of data was available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Proceedings of IberSpeech 2022 (
  https://www.isca-speech.org/archive/iberspeech_2022/gimenogomez22_iberspeech.html
  )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhayaThai<span class="highlight-title">BERT</span>: Enhancing a <span class="highlight-title">Pretrain</span>ed Thai Language Model with
  Unassimilated Loanwords 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panyut Sriwirote, Jalinee Thapiang, Vasan Timtong, Attapol T. Rutherford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While WangchanBERTa has become the de facto standard in transformer-based
Thai language modeling, it still has shortcomings in regard to the
understanding of foreign words, most notably English words, which are often
borrowed without orthographic assimilation into Thai in many contexts. We
identify the lack of foreign vocabulary in WangchanBERTa's tokenizer as the
main source of these shortcomings. We then expand WangchanBERTa's vocabulary
via vocabulary transfer from XLM-R's pretrained tokenizer and pretrain a new
model using the expanded tokenizer, starting from WangchanBERTa's checkpoint,
on a new dataset that is larger than the one used to train WangchanBERTa. Our
results show that our new pretrained model, PhayaThaiBERT, outperforms
WangchanBERTa in many downstream tasks and datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSMeD: Bridging the <span class="highlight-title">Dataset</span> Gap in Automated Citation Screening for
  Systematic Literature <span class="highlight-title">Review</span>s <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wojciech Kusa, Oscar E. Mendoza, Matthias Samwald, Petr Knoth, Allan Hanbury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systematic literature reviews (SLRs) play an essential role in summarising,
synthesising and validating scientific evidence. In recent years, there has
been a growing interest in using machine learning techniques to automate the
identification of relevant studies for SLRs. However, the lack of standardised
evaluation datasets makes comparing the performance of such automated
literature screening systems difficult. In this paper, we analyse the citation
screening evaluation datasets, revealing that many of the available datasets
are either too small, suffer from data leakage or have limited applicability to
systems treating automated literature screening as a classification task, as
opposed to, for example, a retrieval or question-answering task. To address
these challenges, we introduce CSMeD, a meta-dataset consolidating nine
publicly released collections, providing unified access to 325 SLRs from the
fields of medicine and computer science. CSMeD serves as a comprehensive
resource for training and evaluating the performance of automated citation
screening models. Additionally, we introduce CSMeD-FT, a new dataset designed
explicitly for evaluating the full text publication screening task. To
demonstrate the utility of CSMeD, we conduct experiments and establish
baselines on new datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Visual Features for Continuous Lipreading in Spanish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During a conversation, our brain is responsible for combining information
obtained from multiple senses in order to improve our ability to understand the
message we are perceiving. Different studies have shown the importance of
presenting visual information in these situations. Nevertheless, lipreading is
a complex task whose objective is to interpret speech when audio is not
available. By dispensing with a sense as crucial as hearing, it will be
necessary to be aware of the challenge that this lack presents. In this paper,
we propose an analysis of different speech visual features with the intention
of identifying which of them is the best approach to capture the nature of lip
movements for natural Spanish and, in this way, dealing with the automatic
visual speech recognition task. In order to estimate our system, we present an
audiovisual corpus compiled from a subset of the RTVE database, which has been
used in the Albayz\'in evaluations. We employ a traditional system based on
Hidden Markov Models with Gaussian Mixture Models. Results show that, although
the task is difficult, in restricted conditions we obtain recognition results
which determine that using eigenlips in combination with deep features is the
best visual approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Proceedings of IberSpeech 2020 (
  https://www.isca-speech.org/archive/iberspeech_2021/gimenogomez21_iberspeech.html
  )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild <span class="chip">LREC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech is considered as a multi-modal process where hearing and vision are
two fundamentals pillars. In fact, several studies have demonstrated that the
robustness of Automatic Speech Recognition systems can be improved when audio
and visual cues are combined to represent the nature of speech. In addition,
Visual Speech Recognition, an open research problem whose purpose is to
interpret speech by reading the lips of the speaker, has been a focus of
interest in the last decades. Nevertheless, in order to estimate these systems
in the currently Deep Learning era, large-scale databases are required. On the
other hand, while most of these databases are dedicated to English, other
languages lack sufficient resources. Thus, this paper presents a
semi-automatically annotated audiovisual database to deal with unconstrained
natural Spanish, providing 13 hours of data extracted from Spanish television.
Furthermore, baseline results for both speaker-dependent and
speaker-independent scenarios are reported using Hidden Markov Models, a
traditional paradigm that has been widely used in the field of Speech
Technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Proceedings of LREC 2022 (
  https://aclanthology.org/2022.lrec-1.294 )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Far Have We Gone in Vulnerability Detection Using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Gao, Hao Wang, Yuchen Zhou, Wenyu Zhu, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As software becomes increasingly complex and prone to vulnerabilities,
automated vulnerability detection is critically important, yet challenging.
Given the significant successes of Large Language Models (LLMs) in various
tasks, there is growing anticipation of their efficacy in vulnerability
detection. However, a quantitative understanding of their potential in
vulnerability detection is still missing. To bridge this gap, we introduce a
comprehensive vulnerability benchmark VulBench. This benchmark aggregates
high-quality data from a wide range of CTF (Capture-the-Flag) challenges and
real-world applications, with annotations for each vulnerable function
detailing the vulnerability type and its root cause. Through our experiments
encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models
and static analyzers, we find that several LLMs outperform traditional deep
learning approaches in vulnerability detection, revealing an untapped potential
in LLMs. This work contributes to the understanding and utilization of LLMs for
enhanced software security.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Analytics for Generative <span class="highlight-title">Transformer</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raymond Li, Ruixin Yang, Wen Xiao, Ahmed AbuRaed, Gabriel Murray, Giuseppe Carenini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While transformer-based models have achieved state-of-the-art results in a
variety of classification and generation tasks, their black-box nature makes
them challenging for interpretability. In this work, we present a novel visual
analytical framework to support the analysis of transformer-based generative
networks. In contrast to previous work, which has mainly focused on
encoder-based models, our framework is one of the first dedicated to supporting
the analysis of transformer-based encoder-decoder models and decoder-only
models for generative and classification tasks. Hence, we offer an intuitive
overview that allows the user to explore different facets of the model through
interactive visualization. To demonstrate the feasibility and usefulness of our
framework, we present three detailed case studies based on real-world NLP
research problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages (reference excluded), 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ nach0: Multimodal Natural and Chemical Languages Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Micha Livne, Zulfat Miftahutdinov, Elena Tutubalina, Maksim Kuznetsov, Daniil Polykovskiy, Annika Brundyn, Aastha Jhunjhunwala, Anthony Costa, Alex Aliper, Alex Zhavoronkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have substantially driven scientific progress in
various domains, and many papers have demonstrated their ability to tackle
complex problems with creative solutions. Our paper introduces a new foundation
model, nach0, capable of solving various chemical and biological tasks:
biomedical question answering, named entity recognition, molecular generation,
molecular synthesis, attributes prediction, and others. nach0 is a multi-domain
and multi-task encoder-decoder LLM pre-trained on unlabeled text from
scientific literature, patents, and molecule strings to incorporate a range of
chemical and linguistic knowledge. We employed instruction tuning, where
specific task-related instructions are utilized to fine-tune nach0 for the
final set of tasks. To train nach0 effectively, we leverage the NeMo framework,
enabling efficient parallel optimization of both base and large model versions.
Extensive experiments demonstrate that our model outperforms state-of-the-art
baselines on single-domain and cross-domain tasks. Furthermore, it can generate
high-quality outputs in molecular and textual formats, showcasing its
effectiveness in multi-domain setups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Nature Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IndoRobusta: Towards Robustness Against Diverse Code-Mixed Indonesian
  Local Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Genta Indra Winata, Pascale Fung, Ayu Purwarianti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant progress has been made on Indonesian NLP. Nevertheless,
exploration of the code-mixing phenomenon in Indonesian is limited, despite
many languages being frequently mixed with Indonesian in daily conversation. In
this work, we explore code-mixing in Indonesian with four embedded languages,
i.e., English, Sundanese, Javanese, and Malay; and introduce IndoRobusta, a
framework to evaluate and improve the code-mixing robustness. Our analysis
shows that the pre-training corpus bias affects the model's ability to better
handle Indonesian-English code-mixing when compared to other local languages,
despite having higher language diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inter<span class="highlight-title">Prompt</span>: Interpretable <span class="highlight-title">Prompt</span>ing for Interrelated Interpersonal Risk
  Factors in Reddit Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MSVPJ Sathvik, Surjodeep Sarkar, Chandni Saxena, Sunghwan Sohn, Muskan Garg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health professionals and clinicians have observed the upsurge of
mental disorders due to Interpersonal Risk Factors (IRFs). To simulate the
human-in-the-loop triaging scenario for early detection of mental health
disorders, we recognized textual indications to ascertain these IRFs : Thwarted
Belongingness (TBe) and Perceived Burdensomeness (PBu) within personal
narratives. In light of this, we use N-shot learning with GPT-3 model on the
IRF dataset, and underscored the importance of fine-tuning GPT-3 model to
incorporate the context-specific sensitivity and the interconnectedness of
textual cues that represent both IRFs.
  In this paper, we introduce an Interpretable Prompting (InterPrompt)} method
to boost the attention mechanism by fine-tuning the GPT-3 model. This allows a
more sophisticated level of language modification by adjusting the pre-trained
weights. Our model learns to detect usual patterns and underlying connections
across both the IRFs, which leads to better system-level explainability and
trustworthiness. The results of our research demonstrate that all four variants
of GPT-3 model, when fine-tuned with InterPrompt, perform considerably better
as compared to the baseline methods, both in terms of classification and
explanation generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Graph Meets Large Language Model: Progress and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph plays a significant role in representing and analyzing complex
relationships in real-world applications such as citation networks, social
networks, and biological data. Recently, Large Language Models (LLMs), which
have achieved tremendous success in various domains, have also been leveraged
in graph-related tasks to surpass traditional Graph Neural Networks (GNNs)
based methods and yield state-of-the-art performance. In this survey, we first
present a comprehensive review and analysis of existing methods that integrate
LLMs with graphs. First of all, we propose a new taxonomy, which organizes
existing methods into three categories based on the role (i.e., enhancer,
predictor, and alignment component) played by LLMs in graph-related tasks. Then
we systematically survey the representative methods along the three categories
of the taxonomy. Finally, we discuss the remaining limitations of existing
studies and highlight promising avenues for future research. The relevant
papers are summarized and will be consistently updated at:
https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress; 13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Problems of Non-equivalent Words in Technical Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ibrahim Qani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating words which do not have equivalent in target language is not easy
and finding proper equivalent of those words are very important to render
correctly and understandably, the article defines some thoughts and ideas of
scientists on the common problems of non-equivalent words from English to
Russian language and includes English and Russian examples and ideas of certain
scientist. The English language is worldwide spoken and there are 1.35 billion
English speakers and over 258 million Russian speakers according to the 2021s
statistics. Inevitably, these billions of speakers around the world have
connection and they may have deal in different criteria. In order to understand
one another they need to have a pure and fully-understood language. These pure
languages understanding directly relates to translation knowledge where
linguists and translators need to work and research to eradicate
misunderstanding. Misunderstandings mostly appear in non-equivalent words
because there are different local and internal words like food, garment,
cultural and traditional words and others in every notion. Truly, most of these
words do not have equivalent in the target language and these words need to be
worked and find their equivalent in the target language to fully understand the
both languages. However, some of these non-equivalent words are already
professionally rendered to the target language but still there many other words
to be rendered. Hence, this research paper includes different ways and rules of
rendering non-equivalent words from source language to the target language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Obscure Limitation of Modular Multilingual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Ayu Purwarianti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We expose the limitation of modular multilingual language models (MLMs) in
multilingual inference scenarios with unknown languages. Existing evaluations
of modular MLMs exclude the involvement of language identification (LID)
modules, which obscures the performance of real-case multilingual scenarios of
modular MLMs. In this work, we showcase the effect of adding LID on the
multilingual evaluation of modular MLMs and provide discussions for closing the
performance gap of caused by the pipelined approach of LID and modular MLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Turing: A Comparative Analysis of Approaches for Detecting
  Machine-Generated Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Farid Adilazuarda, Nikolaos Nektarios Arkoulis, Oleksii Chumakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant progress has been made on text generation by pre-trained language
models (PLMs), yet distinguishing between human and machine-generated text
poses an escalating challenge. This paper offers an in-depth evaluation of
three distinct methods used to address this task: traditional shallow learning,
Language Model (LM) fine-tuning, and Multilingual Model fine-tuning. These
approaches are rigorously tested on a wide range of machine-generated texts,
providing a benchmark of their competence in distinguishing between
human-authored and machine-authored linguistic constructs. The results reveal
considerable differences in performance across methods, thus emphasizing the
continued need for advancement in this crucial area of NLP. This study offers
valuable insights and paves the way for future research aimed at creating
robust and highly discriminative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Language Models for Tour Itinerary Recommendation <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ngai Lam Ho, Kwan Hui Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tour itinerary recommendation involves planning a sequence of relevant
Point-of-Interest (POIs), which combines challenges from the fields of both
Operations Research (OR) and Recommendation Systems (RS). As an OR problem,
there is the need to maximize a certain utility (e.g., popularity of POIs in
the tour) while adhering to some constraints (e.g., maximum time for the tour).
As a RS problem, it is heavily related to problem or filtering or ranking a
subset of POIs that are relevant to a user and recommending it as part of an
itinerary. In this paper, we explore the use of language models for the task of
tour itinerary recommendation and planning. This task has the unique
requirement of recommending personalized POIs relevant to users and planning
these POIs as an itinerary that satisfies various constraints. We discuss some
approaches in this area, such as using word embedding techniques like Word2Vec
and GloVe for learning POI embeddings and transformer-based techniques like
BERT for generating
  itineraries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PMAI23 @IJCAI 2023 2nd International Workshop on Process Management
  in the AI era</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing <span class="highlight-title">Transformer</span> Architecture in Long-Context Large Language
  Models: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, Xiaoxing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the bomb ignited by ChatGPT, Transformer-based Large Language Models
(LLMs) have paved a revolutionary path toward Artificial General Intelligence
(AGI) and have been applied in diverse areas as knowledge bases, human
interfaces, and dynamic agents. However, a prevailing limitation exists: many
current LLMs, constrained by resources, are primarily pre-trained on shorter
texts, rendering them less effective for longer-context prompts, commonly
encountered in real-world settings. In this paper, we present a comprehensive
survey focusing on the advancement of model architecture in Transformer-based
LLMs to optimize long-context capabilities across all stages from pre-training
to inference. We firstly delineate and analyze the problems of handling
long-context input and output with the current Transformer-based models. Then,
we mainly offer a holistic taxonomy to navigate the landscape of Transformer
upgrades on architecture to solve these problems. Afterward, we provide the
investigation on wildly used evaluation necessities tailored for long-context
LLMs, including datasets, metrics, and baseline models, as well as some amazing
optimization toolkits like libraries, systems, and compilers to augment LLMs'
efficiency and efficacy across different stages. Finally, we further discuss
the predominant challenges and potential avenues for future research in this
domain. Additionally, we have established a repository where we curate relevant
literature with real-time updates at
https://github.com/Strivin0311/long-llms-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Smaller Language Models Answer Contextualised Questions Through
  Memorisation Or Generalisation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Hartill, Joshua Bensemann, Michael Witbrock, Patricia J. Riddle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A distinction is often drawn between a model's ability to predict a label for
an evaluation sample that is directly memorised from highly similar training
samples versus an ability to predict the label via some method of
generalisation. In the context of using Language Models for question-answering,
discussion continues to occur as to the extent to which questions are answered
through memorisation. We consider this issue for questions that would ideally
be answered through reasoning over an associated context. We propose a method
of identifying evaluation samples for which it is very unlikely our model would
have memorised the answers. Our method is based on semantic similarity of input
tokens and label tokens between training and evaluation samples. We show that
our method offers advantages upon some prior approaches in that it is able to
surface evaluation-train pairs that have overlap in either contiguous or
discontiguous sequences of tokens. We use this method to identify unmemorisable
subsets of our evaluation datasets. We train two Language Models in a multitask
fashion whereby the second model differs from the first only in that it has two
additional datasets added to the training regime that are designed to impart
simple numerical reasoning strategies of a sort known to improve performance on
some of our evaluation datasets but not on others. We then show that there is
performance improvement between the two models on the unmemorisable subsets of
the evaluation datasets that were expected to benefit from the additional
training datasets. Specifically, performance on unmemorisable subsets of two of
our evaluation datasets, DROP and ROPES significantly improves by 9.0%, and
25.7% respectively while other evaluation datasets have no significant change
in performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Political Orientation of Social Media Posts: An Extended
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadia Kamal, Brenner Little, Jade Gullic, Trevor Harms, Kristin Olofsson, Arunkumar Bagavathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing machine learning models to characterize political polarization on
online social media presents significant challenges. These challenges mainly
stem from various factors such as the lack of annotated data, presence of noise
in social media datasets, and the sheer volume of data. The common research
practice typically examines the biased structure of online user communities for
a given topic or qualitatively measuring the impacts of polarized topics on
social media. However, there is limited work focusing on analyzing polarization
at the ground-level, specifically in the social media posts themselves. Such
existing analysis heavily relies on annotated data, which often requires
laborious human labeling, offers labels only to specific problems, and lacks
the ability to determine the near-future bias state of a social media
conversations. Understanding the degree of political orientation conveyed in
social media posts is crucial for quantifying the bias of online user
communities and investigating the spread of polarized content. In this work, we
first introduce two heuristic methods that leverage on news media bias and post
content to label social media posts. Next, we compare the efficacy and quality
of heuristically labeled dataset with a randomly sampled human-annotated
dataset. Additionally, we demonstrate that current machine learning models can
exhibit improved performance in predicting political orientation of social
media posts, employing both traditional supervised learning and few-shot
learning setups. We conduct experiments using the proposed heuristic methods
and machine learning approaches to predict the political orientation of posts
collected from two social media forums with diverse political ideologies: Gab
and Twitter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Academic<span class="highlight-title">GPT</span>: Empowering Academic Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shufa Wei, Xiaolong Xu, Xianbiao Qi, Xi Yin, Jun Xia, Jingyi Ren, Peijun Tang, Yuxiang Zhong, Yihao Chen, Xiaoqin Ren, Yuxin Liang, Liankai Huang, Kai Xie, Weikang Gui, Wei Tan, Shuanglong Sun, Yongquan Hu, Qinxian Liu, Nanjin Li, Chihao Dai, Lihua Wang, Xiaohui Liu, Lei Zhang, Yutao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated exceptional capabilities
across various natural language processing tasks. Yet, many of these advanced
LLMs are tailored for broad, general-purpose applications. In this technical
report, we introduce AcademicGPT, designed specifically to empower academic
research. AcademicGPT is a continual training model derived from LLaMA2-70B.
Our training corpus mainly consists of academic papers, thesis, content from
some academic domain, high-quality Chinese data and others. While it may not be
extensive in data scale, AcademicGPT marks our initial venture into a
domain-specific GPT tailored for research area. We evaluate AcademicGPT on
several established public benchmarks such as MMLU and CEval, as well as on
some specialized academic benchmarks like PubMedQA, SCIEval, and our
newly-created ComputerScienceQA, to demonstrate its ability from general
knowledge ability, to Chinese ability, and to academic ability. Building upon
AcademicGPT's foundation model, we also developed several applications catered
to the academic area, including General Academic Question Answering,
AI-assisted Paper Reading, Paper Review, and AI-assisted Title and Abstract
Generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. arXiv admin note: text overlap with
  arXiv:2310.12081, arXiv:2310.10053 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise in Relation Classification <span class="highlight-title">Dataset</span> TACRED: Characterization and
  Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Parekh, Ashish Anand, Amit Awekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The overarching objective of this paper is two-fold. First, to explore
model-based approaches to characterize the primary cause of the noise. in the
RE dataset TACRED Second, to identify the potentially noisy instances. Towards
the first objective, we analyze predictions and performance of state-of-the-art
(SOTA) models to identify the root cause of noise in the dataset. Our analysis
of TACRED shows that the majority of the noise in the dataset originates from
the instances labeled as no-relation which are negative examples. For the
second objective, we explore two nearest-neighbor-based strategies to
automatically identify potentially noisy examples for elimination and
reannotation. Our first strategy, referred to as Intrinsic Strategy (IS), is
based on the assumption that positive examples are clean. Thus, we have used
false-negative predictions to identify noisy negative examples. Whereas, our
second approach, referred to as Extrinsic Strategy, is based on using a clean
subset of the dataset to identify potentially noisy negative examples. Finally,
we retrained the SOTA models on the eliminated and reannotated dataset. Our
empirical results based on two SOTA models trained on TACRED-E following the IS
show an average 4% F1-score improvement, whereas reannotation (TACRED-R) does
not improve the original results. However, following ES, SOTA models show the
average F1-score improvement of 3.8% and 4.4% when trained on respective
eliminated (TACRED-EN) and reannotated (TACRED-RN) datasets respectively. We
further extended the ES for cleaning positive examples as well, which resulted
in an average performance improvement of 5.8% and 5.6% for the eliminated
(TACRED-ENP) and reannotated (TACRED-RNP) datasets respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for
  Interdisciplinary Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Munikoti, Anurag Acharya, Sridevi Wagle, Sameera Horawalavithana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models record impressive performance on many natural language
processing tasks. However, their knowledge capacity is limited to the
pretraining corpus. Retrieval augmentation offers an effective solution by
retrieving context from external knowledge sources to complement the language
model. However, existing retrieval augmentation techniques ignore the
structural relationships between these documents. Furthermore, retrieval models
are not explored much in scientific tasks, especially in regard to the
faithfulness of retrieved documents. In this paper, we propose a novel
structure-aware retrieval augmented language model that accommodates document
structure during retrieval augmentation. We create a heterogeneous document
graph capturing multiple types of relationships (e.g., citation, co-authorship,
etc.) that connect documents from more than 15 scientific disciplines (e.g.,
Physics, Medicine, Chemistry, etc.). We train a graph neural network on the
curated document graph to act as a structural encoder for the corresponding
passages retrieved during the model pretraining. Particularly, along with text
embeddings of the retrieved passages, we obtain structural embeddings of the
documents (passages) and fuse them together before feeding them to the language
model. We evaluate our model extensively on various scientific benchmarks that
include science question-answering and scientific document classification
tasks. Experimental results demonstrate that structure-aware retrieval improves
retrieving more coherent, faithful and contextually relevant passages, while
showing a comparable performance in the overall accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling On-Device Large Language Model Personalization with
  <span class="highlight-title">Self-Supervised</span> Data Selection and Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyang Qin, Jun Xia, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Peipei Zhou, Jingtong Hu, Yiyu Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  After a large language model (LLM) is deployed on edge devices, it is
desirable for these devices to learn from user-generated conversation data to
generate user-specific and personalized responses in real-time. However,
user-generated data usually contains sensitive and private information, and
uploading such data to the cloud for annotation is not preferred if not
prohibited. While it is possible to obtain annotation locally by directly
asking users to provide preferred responses, such annotations have to be sparse
to not affect user experience. In addition, the storage of edge devices is
usually too limited to enable large-scale fine-tuning with full user-generated
data. It remains an open question how to enable on-device LLM personalization,
considering sparse annotation and limited on-device storage. In this paper, we
propose a novel framework to select and store the most representative data
online in a self-supervised way. Such data has a small memory footprint and
allows infrequent requests of user annotations for further fine-tuning. To
enhance fine-tuning quality, multiple semantically similar pairs of question
texts and expected responses are generated using the LLM. Our experiments show
that the proposed framework achieves the best user-specific content-generating
capability (accuracy) and fine-tuning speed (performance) compared with vanilla
baselines. To the best of our knowledge, this is the very first on-device LLM
personalization framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attribution and Alignment: Effects of Local Context Repetition on
  Utterance Production and Comprehension in Dialogue <span class="chip">CoNLL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aron Molnar, Jaap Jumelet, Mario Giulianelli, Arabella Sinclair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are often used as the backbone of modern dialogue systems.
These models are pre-trained on large amounts of written fluent language.
Repetition is typically penalised when evaluating language model generations.
However, it is a key component of dialogue. Humans use local and partner
specific repetitions; these are preferred by human users and lead to more
successful communication in dialogue. In this study, we evaluate (a) whether
language models produce human-like levels of repetition in dialogue, and (b)
what are the processing mechanisms related to lexical re-use they use during
comprehension. We believe that such joint analysis of model production and
comprehension behaviour can inform the development of cognitively inspired
dialogue generation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoNLL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Text: Unveiling Multimodal Proficiency of Large Language Models
  with MultiAPI Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Jianfeng Lin, Jiawei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of Large Language Models like ChatGPT has significantly
advanced language understanding and generation, impacting a broad spectrum of
applications. However, these models predominantly excel in text-based tasks,
overlooking the complexity of real-world multimodal information. This study
introduces MultiAPI, a pioneering comprehensive large-scale API benchmark
dataset aimed at expanding LLMs' proficiency in multimodal contexts. Developed
collaboratively through ChatGPT, MultiAPI consists of 235 diverse API calls and
2,038 contextual prompts, offering a unique platform evaluation of
tool-augmented LLMs handling multimodal tasks. Through comprehensive
experiments, our findings reveal that while LLMs demonstrate proficiency in API
call decision-making, they face challenges in domain identification, function
selection, and argument generation. What's more, we surprisingly notice that
auxiliary context can actually impair the performance. An in-depth error
analysis paves the way for a new paradigm to address these challenges,
suggesting a potential direction for future LLM research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic word meta-sense extension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The meaning of polysemous words often varies in a highly productive yet
predictable way. Generalizing the regularity between conventional senses to
derive novel word meaning is crucial for automated processing of non-literal
language uses such as figurative expressions. We introduce a novel task called
systematic word meta-sense extension (SWORME) to test and improve language
models' ability to extend word meaning to denote new semantic domains (also
called meta-senses) that bear regular semantic relations with existing senses.
We found that language models prefer incremental lexical semantic change toward
conceptually similar meta-senses such as logical metonymy, and are much worse
at predicting highly non-literal meaning extensions such as metaphors. We
propose a novel analogy-based method of word meaning extension, and show that
it effectively improves language model systematicity in making both gradual and
radical types of meta-sense extension. We further demonstrate that learning
systematic meta-sense extensions benefits language models on multiple
benchmarks of figurative language understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Graph Attention Autoencoder for Attributed Networks using
  K-means Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelfateh Bekkaira, Slimane Bellaouar, Slimane Oulad-Naoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Sentiment Analysis (MSA) has recently become a centric research
direction for many real-world applications. This proliferation is due to the
fact that opinions are central to almost all human activities and are key
influencers of our behaviors. In addition, the recent deployment of Deep
Learning-based (DL) models has proven their high efficiency for a wide range of
Western languages. In contrast, Arabic DL-based multimodal sentiment analysis
(MSA) is still in its infantile stage due, mainly, to the lack of standard
datasets. % The contribution In this paper, our investigation is twofold.
First, we design a pipeline that helps building our Arabic Multimodal dataset
leveraging both state-of-the-art transformers and feature extraction tools
within word alignment techniques. Thereafter, we validate our dataset using
state-of-the-art transformer-based model dealing with multimodality. Despite
the small size of the outcome dataset, experiments show that Arabic
multimodality is very promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> GAIA: a benchmark for General AI Assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, <span class="highlight-author">Yann LeCun</span>, Thomas Scialom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GAIA, a benchmark for General AI Assistants that, if solved,
would represent a milestone in AI research. GAIA proposes real-world questions
that require a set of fundamental abilities such as reasoning, multi-modality
handling, web browsing, and generally tool-use proficiency. GAIA questions are
conceptually simple for humans yet challenging for most advanced AIs: we show
that human respondents obtain 92\% vs. 15\% for GPT-4 equipped with plugins.
This notable performance disparity contrasts with the recent trend of LLMs
outperforming humans on tasks requiring professional skills in e.g. law or
chemistry. GAIA's philosophy departs from the current trend in AI benchmarks
suggesting to target tasks that are ever more difficult for humans. We posit
that the advent of Artificial General Intelligence (AGI) hinges on a system's
capability to exhibit similar robustness as the average human does on such
questions. Using GAIA's methodology, we devise 466 questions and their answer.
We release our questions while retaining answers to 300 of them to power a
leader-board available at https://huggingface.co/gaia-benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Banach-Tarski Embeddings and <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Maher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new construction of embeddings of arbitrary recursive data
structures into high dimensional vectors. These embeddings provide an
interpretable model for the latent state vectors of transformers. We
demonstrate that these embeddings can be decoded to the original data structure
when the embedding dimension is sufficiently large. This decoding algorithm has
a natural implementation as a transformer. We also show that these embedding
vectors can be manipulated directly to perform computations on the underlying
data without decoding. As an example we present an algorithm that constructs
the embedded parse tree of an embedded token sequence using only vector
operations in embedding space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 7 figures. v2: Fixed order of matrix multiplication in
  section 2.4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Editing Personality for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative task focused on editing the personality
traits of Large Language Models (LLMs). This task seeks to adjust the models'
responses to opinion-related questions on specified topics since an
individual's personality often manifests in the form of their expressed
opinions, thereby showcasing different personality traits. Specifically, we
construct a new benchmark dataset PersonalityEdit to address this task. Drawing
on the theory in Social Psychology, we isolate three representative traits,
namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our
benchmark. We then gather data using GPT-4, generating responses that not only
align with a specified topic but also embody the targeted personality trait. We
conduct comprehensive experiments involving various baselines and discuss the
representation of personality behavior in LLMs. Our intriguing findings uncover
potential challenges of the proposed task, illustrating several remaining
issues. We anticipate that our work can provide the NLP community with
insights. Code and datasets will be released at
https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress, add more experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code is available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress, add more experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relphormer: Relational Graph <span class="highlight-title">Transformer</span> for Knowledge Graph
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10852v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10852v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Bi, Siyuan Cheng, Jing Chen, Xiaozhuan Liang, Feiyu Xiong, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have achieved remarkable performance in widespread fields,
including natural language processing, computer vision and graph mining.
However, vanilla Transformer architectures have not yielded promising
improvements in the Knowledge Graph (KG) representations, where the
translational distance paradigm dominates this area. Note that vanilla
Transformer architectures struggle to capture the intrinsically heterogeneous
structural and semantic information of knowledge graphs. To this end, we
propose a new variant of Transformer for knowledge graph representations dubbed
Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample
contextualized sub-graph sequences as the input to alleviate the heterogeneity
issue. We propose a novel structure-enhanced self-attention mechanism to encode
the relational information and keep the semantic information within entities
and relations. Moreover, we utilize masked knowledge modeling for general
knowledge graph representation learning, which can be applied to various
KG-based tasks including knowledge graph completion, question answering, and
recommendation. Experimental results on six datasets show that Relphormer can
obtain better performance compared with baselines. Code is available in
https://github.com/zjunlp/Relphormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurocomputing 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by
  Whispering to Chat<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17103v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17103v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi LI, Ge Zhang, Si Liu, Roger Dannenberg, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenhu Chen, Wei Xue, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic
lyrics transcription method achieving state-of-the-art performance on various
lyrics transcription datasets, even in challenging genres such as rock and
metal. Our novel, training-free approach utilizes Whisper, a weakly supervised
robust speech recognition model, and GPT-4, today's most performant chat-based
large language model. In the proposed method, Whisper functions as the "ear" by
transcribing the audio, while GPT-4 serves as the "brain," acting as an
annotator with a strong performance for contextualized output selection and
correction. Our experiments show that LyricWhiz significantly reduces Word
Error Rate compared to existing methods in English and can effectively
transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to
create the first publicly available, large-scale, multilingual lyrics
transcription dataset with a CC-BY-NC-SA copyright license, based on
MTG-Jamendo, and offer a human-annotated subset for noise level estimation and
evaluation. We anticipate that our proposed method and dataset will advance the
development of multilingual lyrics transcription, a challenging and emerging
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 5 tables, accepted by ISMIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Influencer Videos: Unboxing the Mystique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.12311v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.12311v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Rajaram, Puneet Manchanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influencer marketing has become a very popular tool to reach customers.
Despite the rapid growth in influencer videos, there has been little research
on the effectiveness of their constituent features in explaining video
engagement. We study YouTube influencers and analyze their unstructured video
data across text, audio and images using an "interpretable deep learning"
framework that accomplishes both goals of prediction and interpretation. Our
prediction-based approach analyzes unstructured data and finds that "what is
said" in words (text) is more influential than "how it is said" in imagery
(images) or acoustics (audio). Our novel interpretation-based approach is
implemented after completion of model prediction by analyzing the same source
of unstructured data to measure importance attributed to the video features. We
eliminate several spurious relationships in two steps, identifying a subset of
relationships which are confirmed using theory. We uncover novel findings that
establish distinct associations for measures of shallow and deep engagement
based on the dual-system framework of human thinking. Our approach is validated
using simulated data, and we discuss the learnings from our findings for
influencers and brands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, Online Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open Sesame! Universal Black Box Jailbreaking of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01446v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01446v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raz Lapid, Ron Langberg, Moshe Sipper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), designed to provide helpful and safe responses,
often rely on alignment techniques to align with user intent and social
guidelines. Unfortunately, this alignment can be exploited by malicious actors
seeking to manipulate an LLM's outputs for unintended purposes. In this paper
we introduce a novel approach that employs a genetic algorithm (GA) to
manipulate LLMs when model architecture and parameters are inaccessible. The GA
attack works by optimizing a universal adversarial prompt that -- when combined
with a user's query -- disrupts the attacked model's alignment, resulting in
unintended and potentially harmful outputs. Our novel approach systematically
reveals a model's limitations and vulnerabilities by uncovering instances where
its responses deviate from expected behavior. Through extensive experiments we
demonstrate the efficacy of our technique, thus contributing to the ongoing
discussion on responsible AI development by providing a diagnostic tool for
evaluating and enhancing alignment of LLMs with human intent. To our knowledge
this is the first automated universal black box jailbreak attack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing the Power of Large Language Models for Empathetic Response
  Generation: Empirical Investigations and Improvements <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushan Qian, Wei-Nan Zhang, Ting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathetic dialogue is an indispensable part of building harmonious social
relationships and contributes to the development of a helpful AI. Previous
approaches are mainly based on fine small-scale language models. With the
advent of ChatGPT, the application effect of large language models (LLMs) in
this field has attracted great attention. This work empirically investigates
the performance of LLMs in generating empathetic responses and proposes three
improvement methods of semantically similar in-context learning, two-stage
interactive generation, and combination with the knowledge base. Extensive
experiments show that LLMs can significantly benefit from our proposed methods
and is able to achieve state-of-the-art performance in both automatic and human
evaluations. Additionally, we explore the possibility of GPT-4 simulating human
evaluators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>the Findings of EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigTranslate: Augmenting Large Language Models with Multilingual
  Translation Capability over 100 Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Yang, Chong Li, Jiajun Zhang, Chengqing Zong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate promising translation performance
among various natural languages. However, many LLMs especially the open-sourced
ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of
natural languages, making the potential of LLMs on language translation less
explored. In this work, we present BigTranslate which adapts LLaMA that covers
only 20 languages and enhances it with multilingual translation capability on
more than 100 languages. BigTranslate is built upon LLaMA-13B and it is
optimized in three steps. First, we continue training LLaMA with massive
Chinese monolingual data. Second, we continue training the model with a
large-scale parallel dataset that covers 102 natural languages. Third, we
instruct-tune the foundation model with multilingual translation instructions,
leading to our BigTranslate model. The preliminary experiments on multilingual
translation show that BigTranslate performs comparably with ChatGPT and Google
Translate in many languages and even outperforms ChatGPT in 8 language pairs.
We release the BigTranslate model and hope it can advance the research
progress.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures. Our model is available at
  https://github.com/ZNLP/BigTranslate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Segment-to-Segment Framework for Simultaneous Sequence
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17940v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17940v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaolei Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous sequence generation is a pivotal task for real-time scenarios,
such as streaming speech recognition, simultaneous machine translation and
simultaneous speech translation, where the target sequence is generated while
receiving the source sequence. The crux of achieving high-quality generation
with low latency lies in identifying the optimal moments for generating,
accomplished by learning a mapping between the source and target sequences.
However, existing methods often rely on task-specific heuristics for different
sequence types, limiting the model's capacity to adaptively learn the
source-target mapping and hindering the exploration of multi-task learning for
various simultaneous tasks. In this paper, we propose a unified
segment-to-segment framework (Seg2Seg) for simultaneous sequence generation,
which learns the mapping in an adaptive and unified manner. During the process
of simultaneous generation, the model alternates between waiting for a source
segment and generating a target segment, making the segment serve as the
natural bridge between the source and target. To accomplish this, Seg2Seg
introduces a latent segment as the pivot between source to target and explores
all potential source-target mappings via the proposed expectation training,
thereby learning the optimal moments for generating. Experiments on multiple
simultaneous generation tasks demonstrate that Seg2Seg achieves
state-of-the-art performance and exhibits better generality across various
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Grammatical errors prevent the article from being indexed. This is
  not a problem that can be solved by replacing a new version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personas as a Way to Model Truthfulness in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18168v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18168v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung Kim, He He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are trained on vast amounts of text from the
internet, which contains both factual and misleading information about the
world. Can language models discern truth from falsehood in this contradicting
data? Expanding on the view that LLMs can model different communicative agents,
we present the persona hypothesis: LLMs can cluster agents into personas using
common features of their generations. For instance, a truthful persona is a
group of agents that are likely to produce truthful text and that share similar
features like formal writing styles and scientific references. By modeling this
persona, LLMs can generalize truthfulness beyond the specific contexts in which
each agent generated the training text. For example, the model can infer that
the agent ``Wikipedia'' will behave truthfully on topics that were only
generated by ``Science'' because they both belong to the truthful persona. We
show evidence for the persona hypothesis via two observations: (1) we can probe
whether a model's answer will be truthful before it is generated; (2)
finetuning a model on a set of facts improves its truthfulness on unseen
topics. Next, using arithmetics as a synthetic environment, we show that
language models can separate true and false statements, and generalize
truthfulness across agents; but only if agents in the training data share a
truthful generative process that enables the creation of a truthful persona.
Overall, our findings suggest that models can exploit hierarchical structures
in the data to learn abstract concepts like truthfulness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Konan, Ojas Bhargave, Shikhar Agnihotri, Shuo Han, Yunyang Zeng, Ankit Shah, Bhiksha Raj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the ambit of VoIP (Voice over Internet Protocol) telecommunications,
the complexities introduced by acoustic transformations merit rigorous
analysis. This research, rooted in the exploration of proprietary sender-side
denoising effects, meticulously evaluates platforms such as Google Meets and
Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset,
ensuring a structured examination tailored to various denoising settings and
receiver interfaces. A methodological novelty is introduced via the Oaxaca
decomposition, traditionally an econometric tool, repurposed herein to analyze
acoustic-phonetic perturbations within VoIP systems. To further ground the
implications of these transformations, psychoacoustic metrics, specifically
PESQ and STOI, were harnessed to furnish a comprehensive understanding of
speech alterations. Cumulatively, the insights garnered underscore the
intricate landscape of VoIP-influenced acoustic dynamics. In addition to the
primary findings, a multitude of metrics are reported, extending the research
purview. Moreover, out-of-domain benchmarking for both time and time-frequency
domain speech enhancement models is included, thereby enhancing the depth and
applicability of this inquiry. Repository:
github.com/deepology/VoIP-DNS-Challenge
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic <span class="highlight-title">Review</span> of Aspect-based Sentiment Analysis (ABSA): Domains,
  Methods, and Trends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Cathy Hua, Paul Denny, Katerina Taskova, Jörg Wicker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based Sentiment Analysis (ABSA) is a type of fine-grained sentiment
analysis (SA) that identifies aspects and the associated opinions from a given
text. In the digital era, ABSA gained increasing popularity and applications in
mining opinionated text data to obtain insights and support decisions. ABSA
research employs linguistic, statistical, and machine-learning approaches and
utilises resources such as labelled datasets, aspect and sentiment lexicons and
ontology. By its nature, ABSA is domain-dependent and can be sensitive to the
impact of misalignment between the resource and application domains. However,
to our knowledge, this topic has not been explored by the existing ABSA
literature reviews. In this paper, we present a Systematic Literature Review
(SLR) of ABSA studies with a focus on the research application domain, dataset
domain, and the research methods to examine their relationships and identify
trends over time. Our results suggest a number of potential systemic issues in
the ABSA research literature, including the predominance of the
``product/service review'' dataset domain among the majority of studies that
did not have a specific research application domain, coupled with the
prevalence of dataset-reliant methods such as supervised machine learning. This
review makes a number of unique contributions to the ABSA research field: 1) To
our knowledge, it is the first SLR that links the research domain, dataset
domain, and research method through a systematic perspective; 2) it is one of
the largest scoped SLR on ABSA, with 519 eligible studies filtered from 4191
search results without time constraint; and 3) our review methodology adopted
an innovative automatic filtering process based on PDF-mining, which enhanced
screening quality and reliability. Suggestions and our review limitations are
also discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exponentially Faster Language Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Belcak, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models only really need to use an exponential fraction of their
neurons for individual inferences. As proof, we present UltraFastBERT, a BERT
variant that uses 0.3% of its neurons during inference while performing on par
with similar BERT models. UltraFastBERT selectively engages just 12 out of 4095
neurons for each layer inference. This is achieved by replacing feedforward
networks with fast feedforward networks (FFFs). While no truly efficient
implementation currently exists to unlock the full acceleration potential of
conditional neural execution, we provide high-level CPU code achieving 78x
speedup over the optimized baseline feedforward implementation, and a PyTorch
implementation delivering 40x speedup over the equivalent batched feedforward
inference. We publish our training code, benchmarking setup, and model weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Streaming Language Models with Attention Sinks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying Large Language Models (LLMs) in streaming applications such as
multi-round dialogue, where long interactions are expected, is urgently needed
but poses two major challenges. Firstly, during the decoding stage, caching
previous tokens' Key and Value states (KV) consumes extensive memory. Secondly,
popular LLMs cannot generalize to longer texts than the training sequence
length. Window attention, where only the most recent KVs are cached, is a
natural approach -- but we show that it fails when the text length surpasses
the cache size. We observe an interesting phenomenon, namely attention sink,
that keeping the KV of initial tokens will largely recover the performance of
window attention. In this paper, we first demonstrate that the emergence of
attention sink is due to the strong attention scores towards initial tokens as
a ``sink'' even if they are not semantically important. Based on the above
analysis, we introduce StreamingLLM, an efficient framework that enables LLMs
trained with a finite length attention window to generalize to infinite
sequence lengths without any fine-tuning. We show that StreamingLLM can enable
Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language
modeling with up to 4 million tokens and more. In addition, we discover that
adding a placeholder token as a dedicated attention sink during pre-training
can further improve streaming deployment. In streaming settings, StreamingLLM
outperforms the sliding window recomputation baseline by up to 22.2x speedup.
Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Short Text Matching Model Enhanced with Knowledge via Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqiang Liu, Mengmeng Cui, Hanjie Mai, Qiang Zhang, Shaohua Xu, Xiangzheng Liu, Yanlong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, short Text Matching tasks have been widely applied in the
fields ofadvertising search and recommendation. The difficulty lies in the lack
of semantic information and word ambiguity caused by the short length of the
text. Previous works have introduced complement sentences or knowledge bases to
provide additional feature information. However, these methods have not fully
interacted between the original sentence and the complement sentence, and have
not considered the noise issue that may arise from the introduction of external
knowledge bases. Therefore, this paper proposes a short Text Matching model
that combines contrastive learning and external knowledge. The model uses a
generative model to generate corresponding complement sentences and uses the
contrastive learning method to guide the model to obtain more semantically
meaningful encoding of the original sentence. In addition, to avoid noise, we
use keywords as the main semantics of the original sentence to retrieve
corresponding knowledge words in the knowledge base, and construct a knowledge
graph. The graph encoding model is used to integrate the knowledge base
information into the model. Our designed model achieves state-of-the-art
performance on two publicly available Chinese Text Matching datasets,
demonstrating the effectiveness of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages,2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extraction and Summarization of Explicit Video Content using Multi-Modal
  Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaunak Joshi, Raghav Gaggar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increase in video-sharing platforms across the internet, it is
difficult for humans to moderate the data for explicit content. Hence, an
automated pipeline to scan through video data for explicit content has become
the need of the hour. We propose a novel pipeline that uses multi-modal deep
learning to first extract the explicit segments of input videos and then
summarize their content using text to determine its age appropriateness and age
rating. We also evaluate our pipeline's effectiveness in the end using standard
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DoReMi: Optimizing Data Mixtures Speeds Up Language Model <span class="highlight-title">Pretrain</span>ing <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10429v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10429v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V. Le, Tengyu Ma, Adams Wei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mixture proportions of pretraining data domains (e.g., Wikipedia, books,
web text) greatly affect language model (LM) performance. In this paper, we
propose Domain Reweighting with Minimax Optimization (DoReMi), which first
trains a small proxy model using group distributionally robust optimization
(Group DRO) over domains to produce domain weights (mixture proportions)
without knowledge of downstream tasks. We then resample a dataset with these
domain weights and train a larger, full-sized model. In our experiments, we use
DoReMi on a 280M-parameter proxy model to set the domain weights for training
an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi
improves perplexity across all domains, even when it downweights a domain.
DoReMi improves average few-shot downstream accuracy by 6.5% points over a
baseline model trained using The Pile's default domain weights and reaches the
baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi,
which has no knowledge of downstream tasks, even matches the performance of
using domain weights tuned on downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling
  Approaches <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08371v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08371v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Fried, Nicholas Tomlin, Jennifer Hu, Roma Patel, Aida Nematzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People rely heavily on context to enrich meaning beyond what is literally
said, enabling concise but effective communication. To interact successfully
and naturally with people, user-facing artificial intelligence systems will
require similar skills in pragmatics: relying on various types of context --
from shared linguistic goals and conventions, to the visual and embodied world
-- to use language effectively. We survey existing grounded settings and
pragmatic modeling approaches and analyze how the task goals, environmental
contexts, and communicative affordances in each work enrich linguistic meaning.
We present recommendations for future grounded task design to naturally elicit
pragmatic phenomena, and suggest directions that focus on a broader range of
communicative contexts and affordances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Projection Invariance Mitigates Representation Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11603v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11603v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasia Razdaibiedina, Ashish Khetan, Zohar Karnin, Daniel Khashabi, Vishaal Kapoor, Vivek Madan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning contextualized representations learned by pre-trained language
models remains a prevalent practice in NLP. However, fine-tuning can lead to
representation degradation (also known as representation collapse), which may
result in instability, sub-optimal performance, and weak generalization.
  In this paper, we propose Representation Projection Invariance (REPINA), a
novel regularization method to maintain the information content of
representation and reduce representation collapse during fine-tuning by
discouraging undesirable changes in the representations. We study the empirical
behavior of the proposed regularization in comparison to 5 comparable baselines
across 13 language understanding tasks (GLUE benchmark and six additional
datasets). When evaluating in-domain performance, REPINA consistently
outperforms other baselines on most tasks (10 out of 13). We also demonstrate
its effectiveness in few-shot settings and robustness to label perturbation. As
a by-product, we extend previous studies of representation collapse and propose
several metrics to quantify it. Our empirical findings show that our approach
is significantly more effective at mitigating representation collapse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ing Language Models for Comparative Reasoning <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14457v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14457v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengxia Yu, Zhihan Zhang, Wenhao Yu, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comparative reasoning is a process of comparing objects, concepts, or
entities to draw conclusions, which constitutes a fundamental cognitive
ability. In this paper, we propose a novel framework to pre-train language
models for enhancing their abilities of comparative reasoning over texts. While
there have been approaches for NLP tasks that require comparative reasoning,
they suffer from costly manual data labeling and limited generalizability to
different tasks. Our approach introduces a novel method of collecting scalable
data for text-based entity comparison, which leverages both structured and
unstructured data. Moreover, we present a framework of pre-training language
models via three novel objectives on comparative reasoning. Evaluation on
downstream tasks including comparative question answering, question generation,
and summarization shows that our pre-training framework significantly improves
the comparative reasoning abilities of language models, especially under
low-resource conditions. This work also releases the first integrated benchmark
for comparative reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 - Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Merging Experts into One: Improving Computational Efficiency of Mixture
  of Experts <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09832v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09832v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shwai He, Run-Ze Fan, Liang Ding, Li Shen, Tianyi Zhou, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling the size of language models usually leads to remarkable advancements
in NLP tasks. But it often comes with a price of growing computational cost.
Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a
small subset of parameters (e.g., one expert) for each input, its computation
escalates significantly if increasing the number of activated experts, limiting
its practical utility. Can we retain the advantages of adding more experts
without substantially increasing the computational costs? In this paper, we
first demonstrate the superiority of selecting multiple experts and then
propose a computation-efficient approach called \textbf{\texttt{Merging Experts
into One}} (MEO), which reduces the computation cost to that of a single
expert. Extensive experiments show that MEO significantly improves
computational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G
(MEO). Moreover, we propose a token-level attention block that further enhances
the efficiency and performance of token-level MEO, e.g., 83.3\% (MEO) vs.
82.6\% (vanilla MoE) average score on the GLUE benchmark. Our code will be
released upon acceptance. Code will be released at:
\url{https://github.com/Shwai-He/MEO}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Main Conference (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Persian Typographical Error Type Detection Using Deep Neural Networks on
  Algorithmically-Generated Misspellings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Dehghani, Heshaam Faili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spelling correction is a remarkable challenge in the field of natural
language processing. The objective of spelling correction tasks is to recognize
and rectify spelling errors automatically. The development of applications that
can effectually diagnose and correct Persian spelling and grammatical errors
has become more important in order to improve the quality of Persian text. The
Typographical Error Type Detection in Persian is a relatively understudied
area. Therefore, this paper presents a compelling approach for detecting
typographical errors in Persian texts. Our work includes the presentation of a
publicly available dataset called FarsTypo, which comprises 3.4 million words
arranged in chronological order and tagged with their corresponding
part-of-speech. These words cover a wide range of topics and linguistic styles.
We develop an algorithm designed to apply Persian-specific errors to a scalable
portion of these words, resulting in a parallel dataset of correct and
incorrect words. By leveraging FarsTypo, we establish a strong foundation and
conduct a thorough comparison of various methodologies employing different
architectures. Additionally, we introduce a groundbreaking Deep Sequential
Neural Network that utilizes both word and character embeddings, along with
bidirectional LSTM layers, for token classification aimed at detecting
typographical errors across 51 distinct classes. Our approach is contrasted
with highly advanced industrial systems that, unlike this study, have been
developed using a diverse range of resources. The outcomes of our final method
proved to be highly competitive, achieving an accuracy of 97.62%, precision of
98.83%, recall of 98.61%, and surpassing others in terms of speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Language Agent for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10813v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10813v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-level driving is an ultimate goal of autonomous driving. Conventional
approaches formulate autonomous driving as a perception-prediction-planning
framework, yet their systems do not capitalize on the inherent reasoning
ability and experiential knowledge of humans. In this paper, we propose a
fundamental paradigm shift from current pipelines, exploiting Large Language
Models (LLMs) as a cognitive agent to integrate human-like intelligence into
autonomous driving systems. Our approach, termed Agent-Driver, transforms the
traditional autonomous driving pipeline by introducing a versatile tool library
accessible via function calls, a cognitive memory of common sense and
experiential knowledge for decision-making, and a reasoning engine capable of
chain-of-thought reasoning, task planning, motion planning, and
self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive
common sense and robust reasoning capabilities, thus enabling a more nuanced,
human-like approach to autonomous driving. We evaluate our approach on the
large-scale nuScenes benchmark, and extensive experiments substantiate that our
Agent-Driver significantly outperforms the state-of-the-art driving methods by
a large margin. Our approach also demonstrates superior interpretability and
few-shot learning ability to these methods. Code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">145</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-guided Shape-from-Template: Monocular Video Perception through
  Neural Surrogate Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Stotko, Nils Wandel, Reinhard Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction of dynamic scenes is a long-standing problem in computer
graphics and increasingly difficult the less information is available.
Shape-from-Template (SfT) methods aim to reconstruct a template-based geometry
from RGB images or video sequences, often leveraging just a single monocular
camera without depth information, such as regular smartphone recordings.
Unfortunately, existing reconstruction methods are either unphysical and noisy
or slow in optimization. To solve this problem, we propose a novel SfT
reconstruction algorithm for cloth using a pre-trained neural surrogate model
that is fast to evaluate, stable, and produces smooth reconstructions due to a
regularizing physics simulation. Differentiable rendering of the simulated mesh
enables pixel-wise comparisons between the reconstruction and a target video
sequence that can be used for a gradient-based optimization procedure to
extract not only shape information but also physical parameters such as
stretching, shearing, or bending stiffness of the cloth. This allows to retain
a precise, stable, and smooth reconstructed geometry while reducing the runtime
by a factor of 400-500 compared to $\phi$-SfT, a state-of-the-art physics-based
SfT approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Share<span class="highlight-title">GPT</span>4V: Improving Large Multi-Modal Models with Better Captions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of large multi-modal models (LMMs), efficient modality alignment
is crucial yet often constrained by the scarcity of high-quality image-text
data. To address this bottleneck, we introduce the ShareGPT4V dataset, a
pioneering large-scale resource featuring 1.2 million highly descriptive
captions, which surpasses existing datasets in diversity and information
content, covering world knowledge, object properties, spatial relationships,
and aesthetic evaluations. Specifically, ShareGPT4V originates from a curated
100K high-quality captions collected from advanced GPT4-Vision and has been
expanded to 1.2M with a superb caption model trained on this subset. ShareGPT4V
first demonstrates its effectiveness for the Supervised Fine-Tuning (SFT)
phase, by substituting an equivalent quantity of detailed captions in existing
SFT datasets with a subset of our high-quality captions, significantly
enhancing the LMMs like LLaVA-7B, LLaVA-1.5-13B, and Qwen-VL-Chat-7B on the MME
and MMBench benchmarks, with respective gains of 222.8/22.0/22.3 and
2.7/1.3/1.5. We further incorporate ShareGPT4V data into both the pre-training
and SFT phases, obtaining ShareGPT4V-7B, a superior LMM based on a simple
architecture that has remarkable performance across a majority of the
multi-modal benchmarks. This project is available at
https://ShareGPT4V.github.io to serve as a pivotal resource for advancing the
LMMs community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intrinsic Image Decomposition via Ordinal Shading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Careaga, Yağız Aksoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intrinsic decomposition is a fundamental mid-level vision problem that plays
a crucial role in various inverse rendering and computational photography
pipelines. Generating highly accurate intrinsic decompositions is an inherently
under-constrained task that requires precisely estimating continuous-valued
shading and albedo. In this work, we achieve high-resolution intrinsic
decomposition by breaking the problem into two parts. First, we present a dense
ordinal shading formulation using a shift- and scale-invariant loss in order to
estimate ordinal shading cues without restricting the predictions to obey the
intrinsic model. We then combine low- and high-resolution ordinal estimations
using a second network to generate a shading estimate with both global
coherency and local details. We encourage the model to learn an accurate
decomposition by computing losses on the estimated shading as well as the
albedo implied by the intrinsic model. We develop a straightforward method for
generating dense pseudo ground truth using our model's predictions and
multi-illumination data, enabling generalization to in-the-wild imagery. We
present an exhaustive qualitative and quantitative analysis of our predicted
intrinsic components against state-of-the-art methods. Finally, we demonstrate
the real-world applicability of our estimations by performing otherwise
difficult editing tasks such as recoloring and relighting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 23 figures, Accepted to ACM Transactions on Graphics
  (2023). Project page: https://yaksoy.github.io/intrinsic/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh
  Reconstruction and High-Quality Mesh Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Guédon, Vincent Lepetit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to allow precise and extremely fast mesh extraction from
3D Gaussian Splatting. Gaussian Splatting has recently become very popular as
it yields realistic rendering while being significantly faster to train than
NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D
gaussians as these gaussians tend to be unorganized after optimization and no
method has been proposed so far. Our first key contribution is a regularization
term that encourages the gaussians to align well with the surface of the scene.
We then introduce a method that exploits this alignment to extract a mesh from
the Gaussians using Poisson reconstruction, which is fast, scalable, and
preserves details, in contrast to the Marching Cubes algorithm usually applied
to extract meshes from Neural SDFs. Finally, we introduce an optional
refinement strategy that binds gaussians to the surface of the mesh, and
jointly optimizes these Gaussians and the mesh through Gaussian splatting
rendering. This enables easy editing, sculpting, rigging, animating,
compositing and relighting of the Gaussians using traditional softwares by
manipulating the mesh instead of the gaussians themselves. Retrieving such an
editable mesh for realistic rendering is done within minutes with our method,
compared to hours with the state-of-the-art methods on neural SDFs, while
providing a better rendering quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://imagine.enpc.fr/~guedona/sugar/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iris Presentation Attack: Assessing the Impact of Combining Vanadium
  Dioxide Films with Artificial Eyes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darshika Jauhari, Renu Sharma, Cunjian Chen, Nelson Sepulveda, Arun Ross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Iris recognition systems, operating in the near infrared spectrum (NIR), have
demonstrated vulnerability to presentation attacks, where an adversary uses
artifacts such as cosmetic contact lenses, artificial eyes or printed iris
images in order to circumvent the system. At the same time, a number of
effective presentation attack detection (PAD) methods have been developed.
These methods have demonstrated success in detecting artificial eyes (e.g.,
fake Van Dyke eyes) as presentation attacks. In this work, we seek to alter the
optical characteristics of artificial eyes by affixing Vanadium Dioxide (VO2)
films on their surface in various spatial configurations. VO2 films can be used
to selectively transmit NIR light and can, therefore, be used to regulate the
amount of NIR light from the object that is captured by the iris sensor. We
study the impact of such images produced by the sensor on two state-of-the-art
iris PA detection methods. We observe that the addition of VO2 films on the
surface of artificial eyes can cause the PA detection methods to misclassify
them as bonafide eyes in some cases. This represents a vulnerability that must
be systematically analyzed and effectively addressed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Swift Parameter-free Attention Network for Efficient Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Wan, Hongyuan Yu, Zhiqi Li, Yihang Chen, Yajun Zou, Yuqing Liu, Xuanwu Yin, Kunlong Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single Image Super-Resolution (SISR) is a crucial task in low-level computer
vision, aiming to reconstruct high-resolution images from low-resolution
counterparts. Conventional attention mechanisms have significantly improved
SISR performance but often result in complex network structures and large
number of parameters, leading to slow inference speed and large model size. To
address this issue, we propose the Swift Parameter-free Attention Network
(SPAN), a highly efficient SISR model that balances parameter count, inference
speed, and image quality. SPAN employs a novel parameter-free attention
mechanism, which leverages symmetric activation functions and residual
connections to enhance high-contribution information and suppress redundant
information. Our theoretical analysis demonstrates the effectiveness of this
design in achieving the attention mechanism's purpose. We evaluate SPAN on
multiple benchmarks, showing that it outperforms existing efficient
super-resolution models in terms of both image quality and inference speed,
achieving a significant quality-speed trade-off. This makes SPAN highly
suitable for real-world applications, particularly in resource-constrained
scenarios. Notably, our model attains the best PSNR of 27.09 dB, and the test
runtime of our team is reduced by 7.08ms in the NTIRE 2023 efficient
super-resolution challenge. Our code and models are made publicly available at
\url{https://github.com/hongyuanyu/SPAN}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Weight-Perturbed Deep Neural Networks With Application in
  Iris Presentation Attack Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renu Sharma, Redwan Sony, Arun Ross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) exhibit superior performance in various machine
learning tasks, e.g., image classification, speech recognition, biometric
recognition, object detection, etc. However, it is essential to analyze their
sensitivity to parameter perturbations before deploying them in real-world
applications. In this work, we assess the sensitivity of DNNs against
perturbations to their weight and bias parameters. The sensitivity analysis
involves three DNN architectures (VGG, ResNet, and DenseNet), three types of
parameter perturbations (Gaussian noise, weight zeroing, and weight scaling),
and two settings (entire network and layer-wise). We perform experiments in the
context of iris presentation attack detection and evaluate on two publicly
available datasets: LivDet-Iris-2017 and LivDet-Iris-2020. Based on the
sensitivity analysis, we propose improved models simply by perturbing
parameters of the network without undergoing training. We further combine these
perturbed models at the score-level and at the parameter-level to improve the
performance over the original model. The ensemble at the parameter-level shows
an average improvement of 43.58% on the LivDet-Iris-2017 dataset and 9.25% on
the LivDet-Iris-2020 dataset. The source code is available at
\href{https://github.com/redwankarimsony/WeightPerturbation-MSU}{https://github.com/redwankarimsony/WeightPerturbation-MSU}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-resolution Image-based Malware Classification using Multiple
  Instance Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Peters, Hikmat Farhat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel method of classifying malware into families using
high-resolution greyscale images and multiple instance learning to overcome
adversarial binary enlargement. Current methods of visualisation-based malware
classification largely rely on lossy transformations of inputs such as resizing
to handle the large, variable-sized images. Through empirical analysis and
experimentation, it is shown that these approaches cause crucial information
loss that can be exploited. The proposed solution divides the images into
patches and uses embedding-based multiple instance learning with a
convolutional neural network and an attention aggregation function for
classification. The implementation is evaluated on the Microsoft Malware
Classification dataset and achieves accuracies of up to $96.6\%$ on
adversarially enlarged samples compared to the baseline of $22.8\%$. The Python
code is available online at https://github.com/timppeters/MIL-Malware-Images .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 13 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfOcc: <span class="highlight-title">Self-Supervised</span> Vision-Based 3D Occupancy Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhui Huang, Wenzhao Zheng, Borui Zhang, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D occupancy prediction is an important task for the robustness of
vision-centric autonomous driving, which aims to predict whether each point is
occupied in the surrounding 3D space. Existing methods usually require 3D
occupancy labels to produce meaningful results. However, it is very laborious
to annotate the occupancy status of each voxel. In this paper, we propose
SelfOcc to explore a self-supervised way to learn 3D occupancy using only video
sequences. We first transform the images into the 3D space (e.g., bird's eye
view) to obtain 3D representation of the scene. We directly impose constraints
on the 3D representations by treating them as signed distance fields. We can
then render 2D images of previous and future frames as self-supervision signals
to learn the 3D representations. We propose an MVS-embedded strategy to
directly optimize the SDF-induced weights with multiple depth proposals. Our
SelfOcc outperforms the previous best method SceneRF by 58.7% using a single
frame as input on SemanticKITTI and is the first self-supervised work that
produces reasonable 3D occupancy for surround cameras on Occ3D. SelfOcc
produces high-quality depth and achieves state-of-the-art results on novel
depth synthesis, monocular depth estimation, and surround-view depth estimation
on the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code:
https://github.com/huang-yh/SelfOcc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/huang-yh/SelfOcc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with
  Spatially Relation Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Chu, Zhedong Zheng, Wei Ji, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drone navigation through natural language commands remains a significant
challenge due to the lack of publicly available multi-modal datasets and the
intricate demands of fine-grained visual-text alignment. In response to this
pressing need, we present a new human-computer interaction annotation benchmark
called GeoText-1652, meticulously curated through a robust Large Language Model
(LLM)-based data generation framework and the expertise of pre-trained vision
models. This new dataset seamlessly extends the existing image dataset, \ie,
University-1652, with spatial-aware text annotations, encompassing intricate
image-text-bounding box associations. Besides, we introduce a new optimization
objective to leverage fine-grained spatial associations, called blending
spatial matching, for region-level spatial relation matching. Extensive
experiments reveal that our approach maintains an exceptional recall rate under
varying description complexities. This underscores the promising potential of
our approach in elevating drone control and navigation through the seamless
integration of natural language commands in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attacking Motion Planners Using Adversarial Perception Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Sadeghi, Nicholas A. Lord, John Redford, Romain Mueller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving (AD) systems are often built and tested in a modular
fashion, where the performance of different modules is measured using
task-specific metrics. These metrics should be chosen so as to capture the
downstream impact of each module and the performance of the system as a whole.
For example, high perception quality should enable prediction and planning to
be performed safely. Even though this is true in general, we show here that it
is possible to construct planner inputs that score very highly on various
perception quality metrics but still lead to planning failures. In an analogy
to adversarial attacks on image classifiers, we call such inputs
\textbf{adversarial perception errors} and show they can be systematically
constructed using a simple boundary-attack algorithm. We demonstrate the
effectiveness of this algorithm by finding attacks for two different black-box
planners in several urban and highway driving scenarios using the CARLA
simulator. Finally, we analyse the properties of these attacks and show that
they are isolated in the input space of the planner, and discuss their
implications for AD system deployment and testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cascade Learning Localises Discriminant Features in Visual Scene
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwen Wang, Katayoun Farrahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lack of interpretability of deep convolutional neural networks (DCNN) is a
well-known problem particularly in the medical domain as clinicians want
trustworthy automated decisions. One way to improve trust is to demonstrate the
localisation of feature representations with respect to expert labeled regions
of interest. In this work, we investigate the localisation of features learned
via two varied learning paradigms and demonstrate the superiority of one
learning approach with respect to localisation. Our analysis on medical and
natural datasets show that the traditional end-to-end (E2E) learning strategy
has a limited ability to localise discriminative features across multiple
network layers. We show that a layer-wise learning strategy, namely cascade
learning (CL), results in more localised features. Considering localisation
accuracy, we not only show that CL outperforms E2E but that it is a promising
method of predicting regions. On the YOLO object detection framework, our best
result shows that CL outperforms the E2E scheme by $2\%$ in mAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transferring to Real-World Layouts: A Depth-aware Framework for Scene
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mu Chen, Zhedong Zheng, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene segmentation via unsupervised domain adaptation (UDA) enables the
transfer of knowledge acquired from source synthetic data to real-world target
data, which largely reduces the need for manual pixel-level annotations in the
target domain. To facilitate domain-invariant feature learning, existing
methods typically mix data from both the source domain and target domain by
simply copying and pasting the pixels. Such vanilla methods are usually
sub-optimal since they do not take into account how well the mixed layouts
correspond to real-world scenarios. Real-world scenarios are with an inherent
layout. We observe that semantic categories, such as sidewalks, buildings, and
sky, display relatively consistent depth distributions, and could be clearly
distinguished in a depth map. Based on such observation, we propose a
depth-aware framework to explicitly leverage depth estimation to mix the
categories and facilitate the two complementary tasks, i.e., segmentation and
depth learning in an end-to-end manner. In particular, the framework contains a
Depth-guided Contextual Filter (DCF) forndata augmentation and a cross-task
encoder for contextual learning. DCF simulates the real-world layouts, while
the cross-task encoder further adaptively fuses the complementing features
between two tasks. Besides, it is worth noting that several public datasets do
not provide depth annotation. Therefore, we leverage the off-the-shelf depth
estimation network to generate the pseudo depth. Extensive experiments show
that our proposed methods, even with pseudo depth, achieve competitive
performance on two widely-used bench-marks, i.e. 77.7 mIoU on GTA to Cityscapes
and 69.3 mIoU on Synthia to Cityscapes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse
  Multiview Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Albanis, Nikolaos Zioulis, Kostas Kolomvatsos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing smooth motions from videos using markerless techniques typically
involves complex processes such as temporal constraints, multiple stages with
data-driven regression and optimization, and bundle solving over temporal
windows. These processes can be inefficient and require tuning multiple
objectives across stages. In contrast, BundleMoCap introduces a novel and
efficient approach to this problem. It solves the motion capture task in a
single stage, eliminating the need for temporal smoothness objectives while
still delivering smooth motions. BundleMoCap outperforms the state-of-the-art
without increasing complexity. The key concept behind BundleMoCap is manifold
interpolation between latent keyframes. By relying on a local manifold
smoothness assumption, we can efficiently solve a bundle of frames using a
single code. Additionally, the method can be implemented as a sliding window
optimization and requires only the first frame to be properly initialized,
reducing the overall computational burden. BundleMoCap's strength lies in its
ability to achieve high-quality motion capture results with simplicity and
efficiency. More details can be found at https://moverseai.github.io/bundle/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in European Conference on Visual Media Production (CVMP
  '23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Similar Document Template Matching Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshitha Yenigalla, Bommareddy Revanth Srinivasa Reddy, Batta Venkata Rahul, Nannapuraju Hemanth Raju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study outlines a comprehensive methodology for verifying medical
documents, integrating advanced techniques in template extraction, comparison,
and fraud detection. It begins with template extraction using sophisticated
region-of-interest (ROI) methods, incorporating contour analysis and edge
identification. Pre-processing steps ensure template clarity through
morphological operations and adaptive thresholding. The template comparison
algorithm utilizes advanced feature matching with key points and descriptors,
enhancing robustness through histogram-based analysis for accounting
variations. Fraud detection involves the SSIM computation and OCR for textual
information extraction. The SSIM quantifies structural similarity, aiding in
potential match identification. OCR focuses on critical areas like patient
details, provider information, and billing amounts. Extracted information is
compared with a reference dataset, and confidence thresholding ensures reliable
fraud detection. Adaptive parameters enhance system flexibility for dynamic
adjustments to varying document layouts. This methodology provides a robust
approach to medical document verification, addressing complexities in template
extraction, comparison, fraud detection, and adaptability to diverse document
structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visually Guided Object Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radu Horaud, Fadi Dornaika, Bernard Espiau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a visual servoing approach to the problem of object
grasping and more generally, to the problem of aligning an end-effector with an
object. First we extend the method proposed by Espiau et al. [1] to the case of
a camera which is not mounted onto the robot being controlled and we stress the
importance of the real-time estimation of the image Jacobian. Second, we show
how to represent a grasp or more generally, an alignment between two solids in
3-D projective space using an uncalibrated stereo rig. Such a 3-D projective
representation is view-invariant in the sense that it can be easily mapped into
an image set-point without any knowledge about the camera parameters. Third, we
perform an analysis of the performances of the visual servoing algorithm and of
the grasping precision that can be expected from this type of approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hand-Eye Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radu Horaud, Fadi Dornaika
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whenever a sensor is mounted on a robot hand it is important to know the
relationship between the sensor and the hand. The problem of determining this
relationship is referred to as hand-eye calibration, which is important in at
least two types of tasks: (i) map sensor centered measurements into the robot
workspace and (ii) allow the robot to precisely move the sensor. In the past
some solutions were proposed in the particular case of a camera. With almost no
exception, all existing solutions attempt to solve the homogeneous matrix
equation AX=XB. First we show that there are two possible formulations of the
hand-eye calibration problem. One formulation is the classical one that we just
mentioned. A second formulation takes the form of the following homogeneous
matrix equation: MY=M'YB. The advantage of the latter is that the extrinsic and
intrinsic camera parameters need not be made explicit. Indeed, this formulation
directly uses the 3 by 4 perspective matrices (M and M') associated with two
positions of the camera. Moreover, this formulation together with the classical
one cover a wider range of camera-based sensors to be calibrated with respect
to the robot hand. Second, we develop a common mathematical framework to solve
for the hand-eye calibration problem using either of the two formulations. We
present two methods, (i) a rotation then translation and (ii) a non-linear
solver for rotation and translation. Third, we perform a stability analysis
both for our two methods and for the classical linear method developed. In the
light of this comparison, the non-linear optimization method, that solves for
rotation and translation simultaneously, seems to be the most robust one with
respect to noise and to measurement errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for
  Mobile Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youqi Liao, Shuhao Kang, Jianping Li, Yang Liu, Yun Liu, Zhen Dong, Bisheng Yang, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise and rapid delineation of sharp boundaries and robust semantics is
essential for numerous downstream robotic tasks, such as robot grasping and
manipulation, real-time semantic mapping, and online sensor calibration
performed on edge computing units. Although boundary detection and semantic
segmentation are complementary tasks, most studies focus on lightweight models
for semantic segmentation but overlook the critical role of boundary detection.
In this work, we introduce Mobile-Seed, a lightweight, dual-task framework
tailored for simultaneous semantic segmentation and boundary detection. Our
framework features a two-stream encoder, an active fusion decoder (AFD) and a
dual-task regularization approach. The encoder is divided into two pathways:
one captures category-aware semantic information, while the other discerns
boundaries from multi-scale features. The AFD module dynamically adapts the
fusion of semantic and boundary information by learning channel-wise
relationships, allowing for precise weight assignment of each channel.
Furthermore, we introduce a regularization loss to mitigate the conflicts in
dual-task learning and deep diversity supervision. Compared to existing
methods, the proposed Mobile-Seed offers a lightweight framework to
simultaneously improve semantic segmentation performance and accurately locate
object boundaries. Experiments on the Cityscapes dataset have shown that
Mobile-Seed achieves notable improvement over the state-of-the-art (SOTA)
baseline by 2.2 percentage points (pp) in mIoU and 4.2 pp in mF-score, while
maintaining an online inference speed of 23.9 frames-per-second (FPS) with
1024x2048 resolution input on an RTX 2080 Ti GPU. Additional experiments on
CamVid and PASCAL Context datasets confirm our method's generalizability. Code
and additional results are publicly available at
\url{https://martin-liao.github.io/Mobile-Seed/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, IEEE conference/letter underreview. Code and additional
  results are available at: \url{https://martin-liao.github.io/Mobile-Seed/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Polyhedral Object Recognition by Indexing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radu Horaud, Humberto Sossa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision, the indexing problem is the problem of recognizing a few
objects in a large database of objects while avoiding the help of the classical
image-feature-to-object-feature matching paradigm. In this paper we address the
problem of recognizing 3-D polyhedral objects from 2-D images by indexing. Both
the objects to be recognized and the images are represented by weighted graphs.
The indexing problem is therefore the problem of determining whether a graph
extracted from the image is present or absent in a database of model graphs. We
introduce a novel method for performing this graph indexing process which is
based both on polynomial characterization of binary and weighted graphs and on
hashing. We describe in detail this polynomial characterization and then we
show how it can be used in the context of polyhedral object recognition. Next
we describe a practical recognition-by-indexing system that includes the
organization of the database, the representation of polyhedral objects in terms
of 2-D characteristic views, the representation of this views in terms of
weighted graphs, and the associated image processing. Finally, some
experimental results allow the evaluation of the system performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KNVQA: A Benchmark for evaluation knowledge-based VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sirui Cheng, Siyu Zhang, Jiayi Wu, Muchen Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the multimodal field, large vision-language models (LVLMs) have made
significant progress due to their strong perception and reasoning capabilities
in the visual and language systems. However, LVLMs are still plagued by the two
critical issues of object hallucination and factual accuracy, which limit the
practicality of LVLMs in different scenarios. Furthermore, previous evaluation
methods focus more on the comprehension and reasoning of language content but
lack a comprehensive evaluation of multimodal interactions, thereby resulting
in potential limitations. To this end, we propose a novel KNVQA-Eval, which is
devoted to knowledge-based VQA task evaluation to reflect the factuality of
multimodal LVLMs. To ensure the robustness and scalability of the evaluation,
we develop a new KNVQA dataset by incorporating human judgment and perception,
aiming to evaluate the accuracy of standard answers relative to AI-generated
answers in knowledge-based VQA. This work not only comprehensively evaluates
the contextual information of LVLMs using reliable human annotations, but also
further analyzes the fine-grained capabilities of current methods to reveal
potential avenues for subsequent optimization of LVLMs-based estimators. Our
proposed VQA-Eval and corresponding dataset KNVQA will facilitate the
development of automatic evaluation tools with the advantages of low cost,
privacy protection, and reproducibility. Our code will be released upon
publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>4Motion: Scripting Physical Motions in Text-to-Video Generation via
  Blender-Oriented <span class="highlight-title">GPT</span> Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxi Lv, Yi Huang, Mingfu Yan, Jiancheng Huang, Jianzhuang Liu, Yifan Liu, Yafei Wen, Xiaoxin Chen, Shifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-video generation have harnessed the power of
diffusion models to create visually compelling content conditioned on text
prompts. However, they usually encounter high computational costs and often
struggle to produce videos with coherent physical motions. To tackle these
issues, we propose GPT4Motion, a training-free framework that leverages the
planning capability of large language models such as GPT, the physical
simulation strength of Blender, and the excellent image generation ability of
text-to-image diffusion models to enhance the quality of video synthesis.
Specifically, GPT4Motion employs GPT-4 to generate a Blender script based on a
user textual prompt, which commands Blender's built-in physics engine to craft
fundamental scene components that encapsulate coherent physical motions across
frames. Then these components are inputted into Stable Diffusion to generate a
video aligned with the textual prompt. Experimental results on three basic
physical motion scenarios, including rigid object drop and collision, cloth
draping and swinging, and liquid flow, demonstrate that GPT4Motion can generate
high-quality videos efficiently in maintaining motion coherency and entity
consistency. GPT4Motion offers new insights in text-to-video research,
enhancing its quality and broadening its horizon for future explorations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Generalization Gaps in High Content Imaging Through Online
  <span class="highlight-title">Self-Supervised</span> Domain Adaptation <span class="chip">WACV
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Fredin Haslum, Christos Matsoukas, Karl-Johan Leuchowius, Kevin Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Content Imaging (HCI) plays a vital role in modern drug discovery and
development pipelines, facilitating various stages from hit identification to
candidate drug characterization. Applying machine learning models to these
datasets can prove challenging as they typically consist of multiple batches,
affected by experimental variation, especially if different imaging equipment
have been used. Moreover, as new data arrive, it is preferable that they are
analyzed in an online fashion. To overcome this, we propose CODA, an online
self-supervised domain adaptation approach. CODA divides the classifier's role
into a generic feature extractor and a task-specific model. We adapt the
feature extractor's weights to the new domain using cross-batch
self-supervision while keeping the task-specific model unchanged. Our results
demonstrate that this strategy significantly reduces the generalization gap,
achieving up to a 300% improvement when applied to data from different labs
utilizing different microscopes. CODA can be applied to new, unlabeled
out-of-domain data sources of different sizes, from a single plate to multiple
experimental batches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crowd management, crime detection, work monitoring using aiml 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        P. R. Adithya, Dheepak. S, B. Akash, Harshini. V, Sai Lakshana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research endeavors to harness the potential of existing Closed-Circuit
Television (CCTV) networks for a comprehensive approach to crowd management,
crime prevention, and workplace monitoring through the integration of
Artificial Intelligence (AI) and Machine Learning (ML) technologies. The
primary objective is to develop and implement advanced algorithms capable of
real-time analysis of video feeds, enabling the identification and assessment
of crowd dynamics, early detection of potential criminal activities, and
continuous monitoring of workplace environments. By leveraging AI/ML, the
project aims to optimize surveillance capabilities, thereby enhancing public
safety measures and improving organizational productivity. This initiative
underscores the transformative impact that intelligent video analytics can have
on existing infrastructure, mitigating the need for extensive system overhauls
while significantly advancing security and operational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Unlabeled Data for 3D Medical Image Segmentation through
  <span class="highlight-title">Self-Supervised</span> Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanaz Karimijafarbigloo, Reza Azad, Yury Velichko, Ulas Bagci, Dorit Merhof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current 3D semi-supervised segmentation methods face significant challenges
such as limited consideration of contextual information and the inability to
generate reliable pseudo-labels for effective unsupervised data use. To address
these challenges, we introduce two distinct subnetworks designed to explore and
exploit the discrepancies between them, ultimately correcting the erroneous
prediction results. More specifically, we identify regions of inconsistent
predictions and initiate a targeted verification training process. This
procedure strategically fine-tunes and harmonizes the predictions of the
subnetworks, leading to enhanced utilization of contextual information.
Furthermore, to adaptively fine-tune the network's representational capacity
and reduce prediction uncertainty, we employ a self-supervised contrastive
learning paradigm. For this, we use the network's confidence to distinguish
between reliable and unreliable predictions. The model is then trained to
effectively minimize unreliable predictions. Our experimental results for organ
segmentation, obtained from clinical MRI and CT scans, demonstrate the
effectiveness of our approach when compared to state-of-the-art methods. The
codebase is accessible on
\href{https://github.com/xmindflow/SSL-contrastive}{GitHub}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChessVision -- A <span class="highlight-title">Dataset</span> for Logically Coherent Multi-label
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumadeep Saha, Utpal Garain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Starting with early successes in computer vision tasks, deep learning based
techniques have since overtaken state of the art approaches in a multitude of
domains. However, it has been demonstrated time and again that these techniques
fail to capture semantic context and logical constraints, instead often relying
on spurious correlations to arrive at the answer. Since application of deep
learning techniques to critical scenarios are dependent on adherence to domain
specific constraints, several attempts have been made to address this issue.
One limitation holding back a thorough exploration of this area, is a lack of
suitable datasets which feature a rich set of rules. In order to address this,
we present the ChessVision Dataset, consisting of 200,000+ images of annotated
chess games in progress, requiring recreation of the game state from its
corresponding image. This is accompanied by a curated set of rules which
constrains the set of predictions to "reasonable" game states, and are designed
to probe key semantic abilities like localization and enumeration. Alongside
standard metrics, additional metrics to measure performance with regards to
logical consistency is presented. We analyze several popular and state of the
art vision models on this task, and show that, although their performance on
standard metrics are laudable, they produce a plethora of incoherent results,
indicating that this dataset presents a significant challenge for future works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Dense Pseudo Label Selection for Semi-supervised Oriented
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Zhao, Qiang Fang, Shuohao Shi, Xin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, dense pseudo-label, which directly selects pseudo labels from the
original output of the teacher model without any complicated post-processing
steps, has received considerable attention in semi-supervised object detection
(SSOD). However, for the multi-oriented and dense objects that are common in
aerial scenes, existing dense pseudo-label selection methods are inefficient
and impede the performance in semi-supervised oriented object detection.
Therefore, we propose Adaptive Dense Pseudo Label Selection (ADPLS) for
semi-supervised oriented object detection. In ADPLS, we design a simple but
effective adaptive mechanism to guide the selection of dense pseudo labels.
Specifically, we propose the mean Feature-Richness Score (mFRS) to estimate the
density of potential objects and use this score to adjust the number of dense
pseudo labels. On the DOTA-v1.5 benchmark, the proposed method outperforms
previous methods especially when labeled data are scarce. For example, it
achieves 49.78 mAP given only 5% of annotated data, which surpasses previous
state-of-the-art method given 10% of annotated data by 1.15 mAP. Our codes will
be available soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surgical Temporal Action-aware Network with Sequence Regularization for
  Phase Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Chen, Yuhao Zhai, Jun Zhang, Jinqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To assist surgeons in the operating theatre, surgical phase recognition is
critical for developing computer-assisted surgical systems, which requires
comprehensive understanding of surgical videos. Although existing studies made
great progress, there are still two significant limitations worthy of
improvement. First, due to the compromise of resource consumption, frame-wise
visual features are extracted by 2D networks and disregard spatial and temporal
knowledge of surgical actions, which hinders subsequent inter-frame modeling
for phase prediction. Second, these works simply utilize ordinary
classification loss with one-hot phase labels to optimize the phase
predictions, and cannot fully explore surgical videos under inadequate
supervision. To overcome these two limitations, we propose a Surgical Temporal
Action-aware Network with sequence Regularization, named STAR-Net, to recognize
surgical phases more accurately from input videos. Specifically, we propose an
efficient multi-scale surgical temporal action (MS-STA) module, which
integrates visual features with spatial and temporal knowledge of surgical
actions at the cost of 2D networks. Moreover, we devise the dual-classifier
sequence regularization (DSR) to facilitate the training of STAR-Net by the
sequence guidance of an auxiliary classifier with a smaller capacity. Our
STAR-Net with MS-STA and DSR can exploit visual features of surgical actions
with effective regularization, thereby leading to the superior performance of
surgical phase recognition. Extensive experiments on a large-scale gastrectomy
surgery dataset and the public Cholec80 benchmark prove that our STAR-Net
significantly outperforms state-of-the-arts of surgical phase recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2023 IEEE International Conference on Bioinformatics and
  Biomedicine (BIBM 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TouchSDF: A DeepSDF Approach for 3D Shape Reconstruction using
  Vision-Based Tactile Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mauro Comi, Yijiong Lin, Alex Church, Alessio Tonioni, Laurence Aitchison, Nathan F. Lepora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans rely on their visual and tactile senses to develop a comprehensive 3D
understanding of their physical environment. Recently, there has been a growing
interest in exploring and manipulating objects using data-driven approaches
that utilise high-resolution vision-based tactile sensors. However, 3D shape
reconstruction using tactile sensing has lagged behind visual shape
reconstruction because of limitations in existing techniques, including the
inability to generalise over unseen shapes, the absence of real-world testing,
and limited expressive capacity imposed by discrete representations. To address
these challenges, we propose TouchSDF, a Deep Learning approach for tactile 3D
shape reconstruction that leverages the rich information provided by a
vision-based tactile sensor and the expressivity of the implicit neural
representation DeepSDF. Our technique consists of two components: (1) a
Convolutional Neural Network that maps tactile images into local meshes
representing the surface at the touch location, and (2) an implicit neural
function that predicts a signed distance function to extract the desired 3D
shape. This combination allows TouchSDF to reconstruct smooth and continuous 3D
shapes from tactile inputs in simulation and real-world settings, opening up
research avenues for robust 3D-aware representations and improved multimodal
perception in robotics. Code and supplementary material are available at:
https://touchsdf.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning-based detection of morphological features associated with
  hypoxia in H&E breast cancer whole slide images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petru Manescu, Joseph Geradts, Delmiro Fernandez-Reyes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypoxia occurs when tumour cells outgrow their blood supply, leading to
regions of low oxygen levels within the tumour. Calculating hypoxia levels can
be an important step in understanding the biology of tumours, their clinical
progression and response to treatment. This study demonstrates a novel
application of deep learning to evaluate hypoxia in the context of breast
cancer histomorphology. More precisely, we show that Weakly Supervised Deep
Learning (WSDL) models can accurately detect hypoxia associated features in
routine Hematoxylin and Eosin (H&E) whole slide images (WSI). We trained and
evaluated a deep Multiple Instance Learning model on tiles from WSI H&E tissue
from breast cancer primary sites (n=240) obtaining on average an AUC of 0.87 on
a left-out test set. We also showed significant differences between features of
hypoxic and normoxic tissue regions as distinguished by the WSDL models. Such
DL hypoxia H&E WSI detection models could potentially be extended to other
tumour types and easily integrated into the pathology workflow without
requiring additional costly assays.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Source-Free Target Adaptation with Vision <span class="highlight-title">Transformer</span>s
  Leveraging Domain Representation Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gauransh Sawhney, Daksh Dave, Adeel Ahmed, Jiechao Gao, Khalid Saleem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Domain Adaptation (UDA) methods facilitate knowledge transfer
from a labeled source domain to an unlabeled target domain, navigating the
obstacle of domain shift. While Convolutional Neural Networks (CNNs) are a
staple in UDA, the rise of Vision Transformers (ViTs) provides new avenues for
domain generalization. This paper presents an innovative method to bolster ViT
performance in source-free target adaptation, beginning with an evaluation of
how key, query, and value elements affect ViT outcomes. Experiments indicate
that altering the key component has negligible effects on Transformer
performance. Leveraging this discovery, we introduce Domain Representation
Images (DRIs), feeding embeddings through the key element. DRIs act as
domain-specific markers, effortlessly merging with the training regimen. To
assess our method, we perform target adaptation tests on the Cross Instance DRI
source-only (SO) control. We measure the efficacy of target adaptation with and
without DRIs, against existing benchmarks like SHOT-B* and adaptations via
CDTrans. Findings demonstrate that excluding DRIs offers limited gains over
SHOT-B*, while their inclusion in the key segment boosts average precision
promoting superior domain generalization. This research underscores the vital
role of DRIs in enhancing ViT efficiency in UDA scenarios, setting a precedent
for further domain adaptation explorations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiPose: Hierarchical Binary Surface Encoding and Correspondence Pruning
  for RGB-D 6DoF Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongliang Lin, Yongzhi Su, Praveen Nathan, Sandeep Inuganti, Yan Di, Martin Sundermeyer, Fabian Manhardt, Didier Stricke, Jason Rambach, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a novel dense-correspondence method for 6DoF object
pose estimation from a single RGB-D image. While many existing data-driven
methods achieve impressive performance, they tend to be time-consuming due to
their reliance on rendering-based refinement approaches. To circumvent this
limitation, we present HiPose, which establishes 3D-3D correspondences in a
coarse-to-fine manner with a hierarchical binary surface encoding. Unlike
previous dense-correspondence methods, we estimate the correspondence surface
by employing point-to-surface matching and iteratively constricting the surface
until it becomes a correspondence point while gradually removing outliers.
Extensive experiments on public benchmarks LM-O, YCB-V, and T-Less demonstrate
that our method surpasses all refinement-free methods and is even on par with
expensive refinement-based approaches. Crucially, our approach is
computationally efficient and enables real-time critical applications with high
accuracy requirements. Code and models will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Echocardiogram Foundation Model -- Application 1: Estimating Ejection
  Fraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adil Dahlan, Cyril Zakka, Abhinav Kumar, Laura Tang, Rohan Shad, Robyn Fong, William Hiesinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiovascular diseases stand as the primary global cause of mortality. Among
the various imaging techniques available for visualising the heart and
evaluating its function, echocardiograms emerge as the preferred choice due to
their safety and low cost. Quantifying cardiac function based on
echocardiograms is very laborious, time-consuming and subject to high
interoperator variability. In this work, we introduce EchoAI, an echocardiogram
foundation model, that is trained using self-supervised learning (SSL) on 1.5
million echocardiograms. We evaluate our approach by fine-tuning EchoAI to
estimate the ejection fraction achieving a mean absolute percentage error of
9.40%. This level of accuracy aligns with the performance of expert
sonographers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Region of Interest Focused Triple UNet Architecture for Skin Lesion
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqing Liu, Yu Guo, Caiying Wu, Guoqing Chen, Barintag Saheya, Qiyu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin lesion segmentation is of great significance for skin lesion analysis
and subsequent treatment. It is still a challenging task due to the irregular
and fuzzy lesion borders, and diversity of skin lesions. In this paper, we
propose Triple-UNet to automatically segment skin lesions. It is an organic
combination of three UNet architectures with suitable modules. In order to
concatenate the first and second sub-networks more effectively, we design a
region of interest enhancement module (ROIE). The ROIE enhances the target
object region of the image by using the predicted score map of the first UNet.
The features learned by the first UNet and the enhanced image help the second
UNet obtain a better score map. Finally, the results are fine-tuned by the
third UNet. We evaluate our algorithm on a publicly available dataset of skin
lesion segmentation. Experiments show that Triple-UNet outperforms the
state-of-the-art on skin lesion segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Resolution Planar Region Extraction for Uneven Terrains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghan Sun, Linfang Zheng, Hua Chen, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of extracting planar regions in uneven
terrains from unordered point cloud measurements. Such a problem is critical in
various robotic applications such as robotic perceptive locomotion. While
existing approaches have shown promising results in effectively extracting
planar regions from the environment, they often suffer from issues such as low
computational efficiency or loss of resolution. To address these issues, we
propose a multi-resolution planar region extraction strategy in this paper that
balances the accuracy in boundaries and computational efficiency. Our method
begins with a pointwise classification preprocessing module, which categorizes
all sampled points according to their local geometric properties to facilitate
multi-resolution segmentation. Subsequently, we arrange the categorized points
using an octree, followed by an in-depth analysis of nodes to finish
multi-resolution plane segmentation. The efficiency and robustness of the
proposed approach are verified via synthetic and real-world experiments,
demonstrating our method's ability to generalize effectively across various
uneven terrains while maintaining real-time performance, achieving frame rates
exceeding 35 FPS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Neural Networks for Neuroimaging in Parkinson's Disease:
  Is Preprocessing Needed? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco J. Martinez-Murcia, Juan M. Górriz, Javier Ramírez, Andrés Ortiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial and intensity normalization are nowadays a prerequisite for
neuroimaging analysis. Influenced by voxel-wise and other univariate
comparisons, where these corrections are key, they are commonly applied to any
type of analysis and imaging modalities. Nuclear imaging modalities such as
PET-FDG or FP-CIT SPECT, a common modality used in Parkinson's Disease
diagnosis, are especially dependent on intensity normalization. However, these
steps are computationally expensive and furthermore, they may introduce
deformations in the images, altering the information contained in them.
Convolutional Neural Networks (CNNs), for their part, introduce position
invariance to pattern recognition, and have been proven to classify objects
regardless of their orientation, size, angle, etc. Therefore, a question
arises: how well can CNNs account for spatial and intensity differences when
analysing nuclear brain imaging? Are spatial and intensity normalization still
needed? To answer this question, we have trained four different CNN models
based on well-established architectures, using or not different spatial and
intensity normalization preprocessing. The results show that a sufficiently
complex model such as our three-dimensional version of the ALEXNET can
effectively account for spatial differences, achieving a diagnosis accuracy of
94.1% with an area under the ROC curve of 0.984. The visualization of the
differences via saliency maps shows that these models are correctly finding
patterns that match those found in the literature, without the need of applying
any complex spatial normalization procedure. However, the intensity
normalization -- and its type -- is revealed as very influential in the results
and accuracy of the trained model, and therefore must be well accounted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking bias: Expanding clinical AI model card to incorporate bias
  reporting of social and non-social factors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carolina A. M. Heming, Mohamed Abdalla, Monish Ahluwalia, Linglin Zhang, Hari Trivedi, MinJae Woo, Benjamin Fine, Judy Wawira Gichoya, Leo Anthony Celi, Laleh Seyyed-Kalantari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical AI model reporting cards should be expanded to incorporate a broad
bias reporting of both social and non-social factors. Non-social factors
consider the role of other factors, such as disease dependent, anatomic, or
instrument factors on AI model bias, which are essential to ensure safe
deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "HoVer-UNet": Accelerating HoVerNet with UNet-based multi-class nuclei
  segmentation via knowledge distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Tommasino, Cristiano Russo, Antonio Maria Rinaldi, Francesco Ciompi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present "HoVer-UNet", an approach to distill the knowledge of the
multi-branch HoVerNet framework for nuclei instance segmentation and
classification in histopathology. We propose a compact, streamlined single UNet
network with a Mix Vision Transformer backbone, and equip it with a custom loss
function to optimally encode the distilled knowledge of HoVerNet, reducing
computational requirements without compromising performances. We show that our
model achieved results comparable to HoVerNet on the public PanNuke and Consep
datasets with a three-fold reduction in inference time. We make the code of our
model publicly available at https://github.com/DIAGNijmegen/HoVer-UNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures, submitted to ISBI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GMISeg: General Medical Image Segmentation without Re-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep learning models have become the main method for medical image
segmentation, they often cannot be extended to unknown segmentation tasks
involving new anatomical structures, image shapes, or labels. For new
segmentation tasks, researchers often have to retrain or fine-tune the model,
which is time-consuming and poses a significant obstacle to clinical
researchers, who often lack the resources and professional knowledge to train
neural networks. Therefore, we proposed a general method that can solve unknown
medical image segmentation tasks without requiring additional training. Given
an example set of images and prompts for defining new segmentation tasks,
GMISeg applies a novel low-rank fine-tuning strategy based on the proposed
approach to the SAM (Segment Anything Model) image encoder, and works with the
prompt encoder and mask decoder to fine-tune the labeled dataset without the
need for additional training. To achieve generalization of new tasks, we used
medical image datasets with different imaging modes for different parts. We
trained and generalized GMISeg on a different set of anatomical and imaging
modes using cardiac images on other site datasets. We have demonstrated that
GMISeg outperforms the latest methods on unknown tasks and have conducted a
comprehensive analysis and summary of the important performance of the proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2304.06131 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields <span class="chip">WACV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Yi Gong, Yuan Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Neural radiance fields (NeRF) have enabled high-fidelity
scene reconstruction for novel view synthesis. However, NeRF requires hundreds
of network evaluations per pixel to approximate a volume rendering integral,
making it slow to train. Caching NeRFs into explicit data structures can
effectively enhance rendering speed but at the cost of higher memory usage. To
address these issues, we present Hyb-NeRF, a novel neural radiance field with a
multi-resolution hybrid encoding that achieves efficient neural modeling and
fast rendering, which also allows for high-quality novel view synthesis. The
key idea of Hyb-NeRF is to represent the scene using different encoding
strategies from coarse-to-fine resolution levels. Hyb-NeRF exploits
memory-efficiency learnable positional features at coarse resolutions and the
fast optimization speed and local details of hash-based feature grids at fine
resolutions. In addition, to further boost performance, we embed cone
tracing-based features in our learnable positional encoding that eliminates
encoding ambiguity and reduces aliasing artifacts. Extensive experiments on
both synthetic and real-world datasets show that Hyb-NeRF achieves faster
rendering speed with better rending quality and even a lower memory footprint
in comparison to previous state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HCA-Net: Hierarchical Context Attention Network for Intervertebral Disc
  Semantic Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afshin Bozorgpour, Bobby Azad, Reza Azad, Yury Velichko, Ulas Bagci, Dorit Merhof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and automated segmentation of intervertebral discs (IVDs) in medical
images is crucial for assessing spine-related disorders, such as osteoporosis,
vertebral fractures, or IVD herniation. We present HCA-Net, a novel contextual
attention network architecture for semantic labeling of IVDs, with a special
focus on exploiting prior geometric information. Our approach excels at
processing features across different scales and effectively consolidating them
to capture the intricate spatial relationships within the spinal cord. To
achieve this, HCA-Net models IVD labeling as a pose estimation problem, aiming
to minimize the discrepancy between each predicted IVD location and its
corresponding actual joint location. In addition, we introduce a skeletal loss
term to reinforce the model's geometric dependence on the spine. This loss
function is designed to constrain the model's predictions to a range that
matches the general structure of the human vertebral skeleton. As a result, the
network learns to reduce the occurrence of false predictions and adaptively
improves the accuracy of IVD location estimation. Through extensive
experimental evaluation on multi-center spine datasets, our approach
consistently outperforms previous state-of-the-art methods on both MRI T1w and
T2w modalities. The codebase is accessible to the public on
\href{https://github.com/xmindflow/HCA-Net}{GitHub}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speaker-Adapted End-to-End Visual Speech Recognition for Continuous
  Spanish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different studies have shown the importance of visual cues throughout the
speech perception process. In fact, the development of audiovisual approaches
has led to advances in the field of speech technologies. However, although
noticeable results have recently been achieved, visual speech recognition
remains an open research problem. It is a task in which, by dispensing with the
auditory sense, challenges such as visual ambiguities and the complexity of
modeling silence must be faced. Nonetheless, some of these challenges can be
alleviated when the problem is approached from a speaker-dependent perspective.
Thus, this paper studies, using the Spanish LIP-RTVE database, how the
estimation of specialized end-to-end systems for a specific person could affect
the quality of speech recognition. First, different adaptation strategies based
on the fine-tuning technique were proposed. Then, a pre-trained CTC/Attention
architecture was used as a baseline throughout our experiments. Our findings
showed that a two-step fine-tuning process, where the VSR system is first
adapted to the task domain, provided significant improvements when the speaker
adaptation was addressed. Furthermore, results comparable to the current state
of the art were reached even when only a limited amount of data was available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Proceedings of IberSpeech 2022 (
  https://www.isca-speech.org/archive/iberspeech_2022/gimenogomez22_iberspeech.html
  )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaskFlow: Object-Aware Motion Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aria Ahmadi, David R. Walton, Tim Atherton, Cagatay Dikici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel motion estimation method, MaskFlow, that is capable of
estimating accurate motion fields, even in very challenging cases with small
objects, large displacements and drastic appearance changes. In addition to
lower-level features, that are used in other Deep Neural Network (DNN)-based
motion estimation methods, MaskFlow draws from object-level features and
segmentations. These features and segmentations are used to approximate the
objects' translation motion field. We propose a novel and effective way of
incorporating the incomplete translation motion field into a subsequent motion
estimation network for refinement and completion. We also produced a new
challenging synthetic dataset with motion field ground truth, and also provide
extra ground truth for the object-instance matchings and corresponding
segmentation masks. We demonstrate that MaskFlow outperforms state of the art
methods when evaluated on our new challenging dataset, whilst still producing
comparable results on the popular FlyingThings3D benchmark dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Visual Features for Continuous Lipreading in Spanish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During a conversation, our brain is responsible for combining information
obtained from multiple senses in order to improve our ability to understand the
message we are perceiving. Different studies have shown the importance of
presenting visual information in these situations. Nevertheless, lipreading is
a complex task whose objective is to interpret speech when audio is not
available. By dispensing with a sense as crucial as hearing, it will be
necessary to be aware of the challenge that this lack presents. In this paper,
we propose an analysis of different speech visual features with the intention
of identifying which of them is the best approach to capture the nature of lip
movements for natural Spanish and, in this way, dealing with the automatic
visual speech recognition task. In order to estimate our system, we present an
audiovisual corpus compiled from a subset of the RTVE database, which has been
used in the Albayz\'in evaluations. We employ a traditional system based on
Hidden Markov Models with Gaussian Mixture Models. Results show that, although
the task is difficult, in restricted conditions we obtain recognition results
which determine that using eigenlips in combination with deep features is the
best visual approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Proceedings of IberSpeech 2020 (
  https://www.isca-speech.org/archive/iberspeech_2021/gimenogomez21_iberspeech.html
  )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLAD: Global-Local View Alignment and Background Debiasing for
  Unsupervised Video Domain Adaptation with Large Domain Gap <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyogun Lee, Kyungho Bae, Seongjong Ha, Yumin Ko, Gyeongmoon Park, Jinwoo Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the challenging problem of unsupervised video domain
adaptation (UVDA) for action recognition. We specifically focus on scenarios
with a substantial domain gap, in contrast to existing works primarily deal
with small domain gaps between labeled source domains and unlabeled target
domains. To establish a more realistic setting, we introduce a novel UVDA
scenario, denoted as Kinetics->BABEL, with a more considerable domain gap in
terms of both temporal dynamics and background shifts. To tackle the temporal
shift, i.e., action duration difference between the source and target domains,
we propose a global-local view alignment approach. To mitigate the background
shift, we propose to learn temporal order sensitive representations by temporal
order learning and background invariant representations by background
augmentation. We empirically validate that the proposed method shows
significant improvement over the existing methods on the Kinetics->BABEL
dataset with a large domain gap. The code is available at
https://github.com/KHUVLL/GLAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an accepted WACV 2024 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiFi-Syn: Hierarchical Granularity Discrimination for High-Fidelity
  Synthesis of MR Images with Structure Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Yu, Botao Zhao, Shengjie Zhang, Xiang Chen, Jianfeng Feng, Tingying Peng, Xiao-Yong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing medical images while preserving their structural information is
crucial in medical research. In such scenarios, the preservation of anatomical
content becomes especially important. Although recent advances have been made
by incorporating instance-level information to guide translation, these methods
overlook the spatial coherence of structural-level representation and the
anatomical invariance of content during translation. To address these issues,
we introduce hierarchical granularity discrimination, which exploits various
levels of semantic information present in medical images. Our strategy utilizes
three levels of discrimination granularity: pixel-level discrimination using a
Brain Memory Bank, structure-level discrimination on each brain structure with
a re-weighting strategy to focus on hard samples, and global-level
discrimination to ensure anatomical consistency during translation. The image
translation performance of our strategy has been evaluated on three independent
datasets (UK Biobank, IXI, and BraTS 2018), and it has outperformed
state-of-the-art algorithms. Particularly, our model excels not only in
synthesizing normal structures but also in handling abnormal (pathological)
structures, such as brain tumors, despite the variations in contrast observed
across different imaging modalities due to their pathological characteristics.
The diagnostic value of synthesized MR images containing brain tumors has been
evaluated by radiologists. This indicates that our model may offer an
alternative solution in scenarios where specific MR modalities of patients are
unavailable. Extensive experiments further demonstrate the versatility of our
method, providing unique insights into medical image translation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild <span class="chip">LREC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech is considered as a multi-modal process where hearing and vision are
two fundamentals pillars. In fact, several studies have demonstrated that the
robustness of Automatic Speech Recognition systems can be improved when audio
and visual cues are combined to represent the nature of speech. In addition,
Visual Speech Recognition, an open research problem whose purpose is to
interpret speech by reading the lips of the speaker, has been a focus of
interest in the last decades. Nevertheless, in order to estimate these systems
in the currently Deep Learning era, large-scale databases are required. On the
other hand, while most of these databases are dedicated to English, other
languages lack sufficient resources. Thus, this paper presents a
semi-automatically annotated audiovisual database to deal with unconstrained
natural Spanish, providing 13 hours of data extracted from Spanish television.
Furthermore, baseline results for both speaker-dependent and
speaker-independent scenarios are reported using Hidden Markov Models, a
traditional paradigm that has been widely used in the field of Speech
Technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Proceedings of LREC 2022 (
  https://aclanthology.org/2022.lrec-1.294 )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Site-specific Styles for Multi-institutional Unsupervised
  Cross-modality Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Liu, Yubo Fan, Zhoubing Xu, Benoit M. Dawant, Ipek Oguz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised cross-modality domain adaptation is a challenging task in
medical image analysis, and it becomes more challenging when source and target
domain data are collected from multiple institutions. In this paper, we present
our solution to tackle the multi-institutional unsupervised domain adaptation
for the crossMoDA 2023 challenge. First, we perform unpaired image translation
to translate the source domain images to the target domain, where we design a
dynamic network to generate synthetic target domain images with controllable,
site-specific styles. Afterwards, we train a segmentation model using the
synthetic images and further reduce the domain gap by self-training. Our
solution achieved the 1st place during both the validation and testing phases
of the challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>crossMoDA 2023 challenge 1st place solution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AR Visualization System for Ship Detection and Recognition Based on AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Ye, Limin Huang, Yongji Wu, Min Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmented reality technology has been widely used in industrial design
interaction, exhibition guide, information retrieval and other fields. The
combination of artificial intelligence and augmented reality technology has
also become a future development trend. This project is an AR visualization
system for ship detection and recognition based on AI, which mainly includes
three parts: artificial intelligence module, Unity development module and
Hololens2AR module. This project is based on R3Det algorithm to complete the
detection and recognition of ships in remote sensing images. The recognition
rate of model detection trained on RTX 2080Ti can reach 96%. Then, the 3D model
of the ship is obtained by ship categories and information and generated in the
virtual scene. At the same time, voice module and UI interaction module are
added. Finally, we completed the deployment of the project on Hololens2 through
MRTK. The system realizes the fusion of computer vision and augmented reality
technology, which maps the results of object detection to the AR field, and
makes a brave step toward the future technological trend and intelligent
application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages,7 figures,IEEE International Conference on Virtual Reality
  and Visualization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two Views Are Better than One: Monocular 3D Pose Estimation with
  Multiview Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Keilstrup Ingwersen, Anders Bjorholm Dahl, Janus Nørtoft Jensen, Morten Rieger Hannemose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deducing a 3D human pose from a single 2D image or 2D keypoints is inherently
challenging, given the fundamental ambiguity wherein multiple 3D poses can
correspond to the same 2D representation. The acquisition of 3D data, while
invaluable for resolving pose ambiguity, is expensive and requires an intricate
setup, often restricting its applicability to controlled lab environments. We
improve performance of monocular human pose estimation models using multiview
data for fine-tuning. We propose a novel loss function, multiview consistency,
to enable adding additional training data with only 2D supervision. This loss
enforces that the inferred 3D pose from one view aligns with the inferred 3D
pose from another view under similarity transformations. Our consistency loss
substantially improves performance for fine-tuning with no available 3D data.
Our experiments demonstrate that two views offset by 90 degrees are enough to
obtain good performance, with only marginal improvements by adding more views.
Thus, we enable the acquisition of domain-specific data by capturing activities
with off-the-shelf cameras, eliminating the need for elaborate calibration
procedures. This research introduces new possibilities for domain adaptation in
3D pose estimation, providing a practical and cost-effective solution to
customize models for specific applications. The used dataset, featuring
additional views, will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Board-to-Board: Evaluating Moonboard Grade Prediction Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Petashvili, Matthew Rodda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bouldering is a sport where athletes aim to climb up an obstacle using a set
of defined holds called a route. Typically routes are assigned a grade to
inform climbers of its difficulty and allow them to more easily track their
progression. However, the variation in individual climbers technical and
physical attributes and many nuances of an individual route make grading a
difficult and often biased task. In this work, we apply classical and
deep-learning modelling techniques to the 2016, 2017 and 2019 Moonboard
datasets, achieving state of the art grade prediction performance with 0.87 MAE
and 1.12 RMSE. We achieve this performance on a feature-set that does not
require decomposing routes into individual moves, which is a method common in
literature and introduces bias. We also demonstrate the generalization
capability of this model between editions and introduce a novel vision-based
method of grade prediction. While the generalization performance of these
techniques is below human level performance currently, we propose these methods
as a basis for future work. Such a tool could be implemented in pre-existing
mobile applications and would allow climbers to better track their progress and
assess new routes with reduced bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Part Motion of Articulated Objects Using Spatially Continuous
  Neural Implicit Representations <span class="chip">BMVC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Du, Ruihai Wu, Yan Shen, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Articulated objects (e.g., doors and drawers) exist everywhere in our life.
Different from rigid objects, articulated objects have higher degrees of
freedom and are rich in geometries, semantics, and part functions. Modeling
different kinds of parts and articulations with nerual networks plays an
essential role in articulated object understanding and manipulation, and will
further benefit 3D vision and robotics communities. To model articulated
objects, most previous works directly encode articulated objects into feature
representations, without specific designs for parts, articulations and part
motions. In this paper, we introduce a novel framework that explicitly
disentangles the part motion of articulated objects by predicting the
transformation matrix of points on the part surface, using spatially continuous
neural implicit representations to model the part motion smoothly in the space.
More importantly, while many methods could only model a certain kind of joint
motion (such as the revolution in the clockwise order), our proposed framework
is generic to different kinds of joint motions in that transformation matrix
can model diverse kinds of joint motions in the space. Quantitative and
qualitative results of experiments over diverse categories of articulated
objects demonstrate the effectiveness of our proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures. Accepted by BMVC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CASR: Refining Action Segmentation via Magrinalizing Frame-levle Causal
  Relationships 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keqing Du, Xinyu Yang, Hang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating deep learning and causal discovery has increased the
interpretability of Temporal Action Segmentation (TAS) tasks. However,
frame-level causal relationships exist many complicated noises outside the
segment-level, making it infeasible to directly express macro action semantics.
Thus, we propose \textit{\textbf{Causal Abstraction Segmentation Refiner
(CASR)}}, which can refine TAS results from various models by enhancing video
causality in marginalizing frame-level casual relationships. Specifically, we
define the equivalent frame-level casual model and segment-level causal model,
so that the causal adjacency matrix constructed from marginalized frame-level
causal relationships has the ability to represent the segmnet-level causal
relationships. CASR works out by reducing the difference in the causal
adjacency matrix between we constructed and pre-segmentation results of
backbone models. In addition, we propose a novel evaluation metric Causal Edit
Distance (CED) to evaluate the causal interpretability. Extensive experimental
results on mainstream datasets indicate that CASR significantly surpasses
existing various methods in action segmentation performance, as well as in
causal explainability and generalization. Our code will be available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RFTrans: Leveraging Refractive Flow of Transparent Objects for Surface
  Normal Estimation and Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tutian Tang, Jiyu Liu, Jieyi Zhang, Haoyuan Fu, Wenqiang Xu, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transparent objects are widely used in our daily lives, making it important
to teach robots to interact with them. However, it's not easy because the
reflective and refractive effects can make RGB-D cameras fail to give accurate
geometry measurements. To solve this problem, this paper introduces RFTrans, an
RGB-D-based method for surface normal estimation and manipulation of
transparent objects. By leveraging refractive flow as an intermediate
representation, RFTrans circumvents the drawbacks of directly predicting the
geometry (e.g. surface normal) from RGB images and helps bridge the sim-to-real
gap. RFTrans integrates the RFNet, which predicts refractive flow, object mask,
and boundaries, followed by the F2Net, which estimates surface normal from the
refractive flow. To make manipulation possible, a global optimization module
will take in the predictions, refine the raw depth, and construct the point
cloud with normal. An analytical grasp planning algorithm, ISF, is followed to
generate the grasp poses. We build a synthetic dataset with physically
plausible ray-tracing rendering techniques to train the networks. Results show
that the RFTrans trained on the synthetic dataset can consistently outperform
the baseline ClearGrasp in both synthetic and real-world benchmarks by a large
margin. Finally, a real-world robot grasping task witnesses an 83% success
rate, proving that refractive flow can help enable direct sim-to-real transfer.
The code, data, and supplementary materials are available at
https://rftrans.robotflow.ai.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rich and Poor Texture Contrast: A Simple yet Effective Approach for
  AI-generated Image Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Zhong, Yiran Xu, Zhenxing Qian, Xinpeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent generative models show impressive performance in generating
photographic images. Humans can hardly distinguish such incredibly
realistic-looking AI-generated images from real ones. AI-generated images may
lead to ubiquitous disinformation dissemination. Therefore, it is of utmost
urgency to develop a detector to identify AI-generated images. Most existing
detectors suffer from sharp performance drops over unseen generative models. In
this paper, we propose a novel AI-generated image detector capable of
identifying fake images created by a wide range of generative models. Our
approach leverages the inter-pixel correlation contrast between rich and poor
texture regions within an image. Pixels in rich texture regions exhibit more
significant fluctuations than those in poor texture regions. This discrepancy
reflects that the entropy of rich texture regions is larger than that of poor
ones. Consequently, synthesizing realistic rich texture regions proves to be
more challenging for existing generative models. Based on this principle, we
divide an image into multiple patches and reconstruct them into two images,
comprising rich-texture and poor-texture patches respectively. Subsequently, we
extract the inter-pixel correlation discrepancy feature between rich and poor
texture regions. This feature serves as a universal fingerprint used for
AI-generated image forensics across different generative models. In addition,
we build a comprehensive AI-generated image detection benchmark, which includes
16 kinds of prevalent generative models, to evaluate the effectiveness of
existing baselines and our approach. Our benchmark provides a leaderboard for
follow-up studies. Extensive experimental results show that our approach
outperforms state-of-the-art baselines by a significant margin. Our project:
https://fdmas.github.io/AIGCDetect/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project: https://fdmas.github.io/AIGCDetect/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Wrong To Right: A Recursive Approach Towards Vision-Language
  Explanation <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Ge, Sanjay Subramanian, Trevor Darrell, Boyi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenge of adapting pre-trained vision-language models for
generating insightful explanations for visual reasoning tasks with limited
annotations, we present ReVisE: a $\textbf{Re}$cursive $\textbf{Vis}$ual
$\textbf{E}$xplanation algorithm. Our method iteratively computes visual
features (conditioned on the text input), an answer, and an explanation, to
improve the explanation quality step by step until the answer converges. We
find that this multi-step approach guides the model to correct its own answers
and outperforms single-step explanation generation. Furthermore, explanations
generated by ReVisE also serve as valuable annotations for few-shot
self-training. Our approach outperforms previous methods while utilizing merely
5% of the human-annotated explanations across 10 metrics, demonstrating up to a
4.2 and 1.3 increase in BLEU-1 score on the VCR and VQA-X datasets,
underscoring the efficacy and data-efficiency of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point, Segment and Count: A Generalized Framework for Object Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huang Zhizhong, Dai Mingliang, Zhang Yi, Zhang Junping, Shan Hongming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-agnostic object counting aims to count all objects in an image with
respect to example boxes or class names, \emph{a.k.a} few-shot and zero-shot
counting. Current state-of-the-art methods highly rely on density maps to
predict object counts, which lacks model interpretability. In this paper, we
propose a generalized framework for both few-shot and zero-shot object counting
based on detection. Our framework combines the superior advantages of two
foundation models without compromising their zero-shot capability: (\textbf{i})
SAM to segment all possible objects as mask proposals, and (\textbf{ii}) CLIP
to classify proposals to obtain accurate object counts. However, this strategy
meets the obstacles of efficiency overhead and the small crowded objects that
cannot be localized and distinguished. To address these issues, our framework,
termed PseCo, follows three steps: point, segment, and count. Specifically, we
first propose a class-agnostic object localization to provide accurate but
least point prompts for SAM, which consequently not only reduces computation
costs but also avoids missing small objects. Furthermore, we propose a
generalized object classification that leverages CLIP image/text embeddings as
the classifier, following a hierarchical knowledge distillation to obtain
discriminative classifications among hierarchical mask proposals. Extensive
experimental results on FSC-147 dataset demonstrate that PseCo achieves
state-of-the-art performance in both few-shot/zero-shot object
counting/detection, with additional results on large-scale COCO and LVIS
datasets. The source code is available at
\url{https://github.com/Hzzone/PseCo}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised Medical Image Segmentation via Query Distribution
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rong Wu, Dehua Li, Cong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning is increasingly popular in medical image
segmentation due to its ability to leverage large amounts of unlabeled data to
extract additional information. However, most existing semi-supervised
segmentation methods focus only on extracting information from unlabeled data.
In this paper, we propose a novel Dual KMax UX-Net framework that leverages
labeled data to guide the extraction of information from unlabeled data. Our
approach is based on a mutual learning strategy that incorporates two modules:
3D UX-Net as our backbone meta-architecture and KMax decoder to enhance the
segmentation performance. Extensive experiments on the Atrial Segmentation
Challenge dataset have shown that our method can significantly improve
performance by merging unlabeled data. Meanwhile, our framework outperforms
state-of-the-art semi-supervised learning methods on 10\% and 20\% labeled
settings. Code located at: https://github.com/Rows21/DK-UXNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE ISBI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Post-Training Quantization with Low-precision Minifloats and Integers on
  FPGAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Aggarwal, Alessandro Pappalardo, Hans Jakob Damsgaard, Giuseppe Franco, Thomas B. Preußer, Michaela Blott, Tulika Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-Training Quantization (PTQ) is a powerful technique for model
compression, reducing the precision of neural networks without additional
training overhead. Recent works have investigated adopting 8-bit floating-point
quantization (FP8) in the context of PTQ for model inference. However, the
exploration of floating-point formats smaller than 8 bits and their comparison
with integer quantization remains relatively limited. In this work, we present
minifloats, which are reduced-precision floating-point formats capable of
further reducing the memory footprint, latency, and energy cost of a model
while approaching full-precision model accuracy. Our work presents a novel PTQ
design-space exploration, comparing minifloat and integer quantization schemes
across a range of 3 to 8 bits for both weights and activations. We examine the
applicability of various PTQ techniques to minifloats, including weight
equalization, bias correction, SmoothQuant, gradient-based learned rounding,
and the GPTQ method. Our experiments validate the effectiveness of
low-precision minifloats when compared to their integer counterparts across a
spectrum of accuracy-precision trade-offs on a set of reference deep learning
vision workloads. Finally, we evaluate our results against an FPGA-based
hardware cost model, showing that integer quantization often remains the
Pareto-optimal option, given its relatively smaller hardware resource
footprint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Diffusion For Aerial Object Detection <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Jian, Fuxun Yu, Simranjit Singh, Dimitrios Stamoulis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerial object detection is a challenging task, in which one major obstacle
lies in the limitations of large-scale data collection and the long-tail
distribution of certain classes. Synthetic data offers a promising solution,
especially with recent advances in diffusion-based methods like stable
diffusion (SD). However, the direct application of diffusion methods to aerial
domains poses unique challenges: stable diffusion's optimization for rich
ground-level semantics doesn't align with the sparse nature of aerial objects,
and the extraction of post-synthesis object coordinates remains problematic. To
address these challenges, we introduce a synthetic data augmentation framework
tailored for aerial images. It encompasses sparse-to-dense region of interest
(ROI) extraction to bridge the semantic gap, fine-tuning the diffusion model
with low-rank adaptation (LORA) to circumvent exhaustive retraining, and
finally, a Copy-Paste method to compose synthesized objects with backgrounds,
providing a nuanced approach to aerial object detection through synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023 Synthetic Data Generation with Generative AI
  workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality Mixer Exploiting Complementary Information for Multi-modal
  Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumin Lee, Sangmin Woo, Muhammad Adi Nugroho, Changick Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the distinctive characteristics of sensors, each modality exhibits
unique physical properties. For this reason, in the context of multi-modal
action recognition, it is important to consider not only the overall action
content but also the complementary nature of different modalities. In this
paper, we propose a novel network, named Modality Mixer (M-Mixer) network,
which effectively leverages and incorporates the complementary information
across modalities with the temporal context of actions for action recognition.
A key component of our proposed M-Mixer is the Multi-modal Contextualization
Unit (MCU), a simple yet effective recurrent unit. Our MCU is responsible for
temporally encoding a sequence of one modality (e.g., RGB) with action content
features of other modalities (e.g., depth and infrared modalities). This
process encourages M-Mixer network to exploit global action content and also to
supplement complementary information of other modalities. Furthermore, to
extract appropriate complementary information regarding to the given modality
settings, we introduce a new module, named Complementary Feature Extraction
Module (CFEM). CFEM incorporates sepearte learnable query embeddings for each
modality, which guide CFEM to extract complementary information and global
action content from the other modalities. As a result, our proposed method
outperforms state-of-the-art methods on NTU RGB+D 60, NTU RGB+D 120, and
NW-UCLA datasets. Moreover, through comprehensive ablation studies, we further
validate the effectiveness of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2208.11314</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoCo: Locally Constrained Training-Free Layout-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiang Zhao, Han Li, Ruiyang Jin, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-image diffusion models have reached an unprecedented level in
generating high-quality images. However, their exclusive reliance on textual
prompts often falls short in accurately conveying fine-grained spatial
compositions. In this paper, we propose LoCo, a training-free approach for
layout-to-image synthesis that excels in producing high-quality images aligned
with both textual prompts and spatial layouts. Our method introduces a
Localized Attention Constraint to refine cross-attention for individual
objects, ensuring their precise placement in designated regions. We further
propose a Padding Token Constraint to leverage the semantic information
embedded in previously neglected padding tokens, thereby preventing the
undesired fusion of synthesized objects. LoCo seamlessly integrates into
existing text-to-image and layout-to-image models, significantly amplifying
their performance and effectively addressing semantic failures observed in
prior methods. Through extensive experiments, we showcase the superiority of
our approach, surpassing existing state-of-the-art training-free
layout-to-image methods both qualitatively and quantitatively across multiple
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViLaM: A Vision-Language Model with Enhanced Visual Grounding and
  Generalization Capability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Yang, Lijian Xu, Hongsheng Li, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models have revolutionized human-computer interaction and
shown significant progress in multi-modal tasks. However, applying these models
to complex visual tasks like medical image analysis remains challenging. In
this study, we propose ViLaM, a unified Vision-Language transformer model that
integrates instruction tuning predicated on a large language model. This
approach enables us to optimally utilize the knowledge and reasoning capacities
of large pre-trained language models for an array of tasks encompassing both
language and vision. We employ frozen pre-trained encoders to encode and align
both image and text features, enabling ViLaM to handle a variety of visual
tasks following textual instructions. Besides, we've designed cycle training
for referring expressions to address the need for high-quality, paired
referring expression datasets for training large models in terms of both
quantity and quality. We evaluated ViLaM's exceptional performance on public
general datasets and further confirmed its generalizability on medical
datasets. Importantly, we've observed the model's impressive zero-shot learning
ability, indicating the potential future application of ViLaM in the medical
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overcoming Pathology Image Data Deficiency: Generating Images from
  Pathological Transformation Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Liu, Yufang He, Yu Zhao, Yunlu Feng, Guanglei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathology serves as the gold standard for medical diagnosis but faces
application limitations due to the shortage of medical resources. Leveraging
deep learning, computer-aided diagnosis has the potential to alleviate the
pathologist scarcity and provide timely clinical analysis. However, developing
a reliable model generally necessitates substantial data for training, which is
challenging in pathological field. In response, we propose an adaptive
depth-controlled bidirectional diffusion (ADBD) network for image data
generation. The domain migration approach can work with small trainset and
overcome the diffusion overfitting by source information guidance.
Specifically, we developed a hybrid attention strategy to blend global and
local attention priorities, which guides the bidirectional diffusion and
ensures the migration success. In addition, we developed the adaptive
depth-controlled strategy to simulate physiological transformations, capable of
yielding unlimited cross-domain intermediate images with corresponding soft
labels. ADBD is effective for overcoming pathological image data deficiency and
supportable for further pathology-related research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ABFL: Angular Boundary Discontinuity Free Loss for Arbitrary Oriented
  Object Detection in Aerial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifei Zhao, Shengyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arbitrary oriented object detection (AOOD) in aerial images is a widely
concerned and highly challenging task, and plays an important role in many
scenarios. The core of AOOD involves the representation, encoding, and feature
augmentation of oriented bounding-boxes (Bboxes). Existing methods lack
intuitive modeling of angle difference measurement in oriented Bbox
representations. Oriented Bboxes under different representations exhibit
rotational symmetry with varying periods due to angle periodicity. The angular
boundary discontinuity (ABD) problem at periodic boundary positions is caused
by rotational symmetry in measuring angular differences. In addition, existing
methods also use additional encoding-decoding structures for oriented Bboxes.
In this paper, we design an angular boundary free loss (ABFL) based on the von
Mises distribution. The ABFL aims to solve the ABD problem when detecting
oriented objects. Specifically, ABFL proposes to treat angles as circular data
rather than linear data when measuring angle differences, aiming to introduce
angle periodicity to alleviate the ABD problem and improve the accuracy of
angle difference measurement. In addition, ABFL provides a simple and effective
solution for various periodic boundary discontinuities caused by rotational
symmetry in AOOD tasks, as it does not require additional encoding-decoding
structures for oriented Bboxes. Extensive experiments on the DOTA and HRSC2016
datasets show that the proposed ABFL loss outperforms some state-of-the-art
methods focused on addressing the ABD problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Challenges in Video-Based Infant Action Recognition: A Critical
  Examination of the State of the Art 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elaheh Hatamimajoumerd, Pooria Daneshvar Kakhaki, Xiaofei Huang, Lingfei Luan, Somaieh Amraee, Sarah Ostadabbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated human action recognition, a burgeoning field within computer
vision, boasts diverse applications spanning surveillance, security,
human-computer interaction, tele-health, and sports analysis. Precise action
recognition in infants serves a multitude of pivotal purposes, encompassing
safety monitoring, developmental milestone tracking, early intervention for
developmental delays, fostering parent-infant bonds, advancing computer-aided
diagnostics, and contributing to the scientific comprehension of child
development. This paper delves into the intricacies of infant action
recognition, a domain that has remained relatively uncharted despite the
accomplishments in adult action recognition. In this study, we introduce a
groundbreaking dataset called ``InfActPrimitive'', encompassing five
significant infant milestone action categories, and we incorporate specialized
preprocessing for infant data. We conducted an extensive comparative analysis
employing cutting-edge skeleton-based action recognition models using this
dataset. Our findings reveal that, although the PoseC3D model achieves the
highest accuracy at approximately 71%, the remaining models struggle to
accurately capture the dynamics of infant actions. This highlights a
substantial knowledge gap between infant and adult action recognition domains
and the urgent need for data-efficient pipeline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance-aware 3D Semantic Segmentation powered by Shape Generators and
  Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Sun, Qixing Huang, Xiangru Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing 3D semantic segmentation methods rely on point-wise or voxel-wise
feature descriptors to output segmentation predictions. However, these
descriptors are often supervised at point or voxel level, leading to
segmentation models that can behave poorly at instance-level. In this paper, we
proposed a novel instance-aware approach for 3D semantic segmentation. Our
method combines several geometry processing tasks supervised at instance-level
to promote the consistency of the learned feature representation. Specifically,
our methods use shape generators and shape classifiers to perform shape
reconstruction and classification tasks for each shape instance. This enforces
the feature representation to faithfully encode both structural and local shape
information, with an awareness of shape instances. In the experiments, our
method significantly outperform existing approaches in 3D semantic segmentation
on several public benchmarks, such as Waymo Open Dataset, SemanticKITTI and
ScanNetV2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Procedural Generation of Grain Orientations using the Wave Function
  Collapse Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        G. Magny-Fokam, D. Madisetti, J. El-Awady
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistics of grain sizes and orientations in metals correlate to the
material's mechanical properties. Reproducing representative volume elements
for further analysis of deformation and failure in metals, like 316L stainless
steel, is particularly important due to their wide use in manufacturing goods
today. Two approaches, initially created for video games, were considered for
the procedural generation of representative grain microstructures. The first is
the Wave Function Collapse (WFC) algorithm, and the second is constraint
propagation and probabilistic inference through Markov Junior, a free and
open-source software. This study aimed to investigate these two algorithms'
effectiveness in using reference electron backscatter diffraction (EBSD) maps
and recreating a statistically similar one that could be used in further
research. It utilized two stainless steel EBSD maps as references to test both
algorithms. First, the WFC algorithm was too constricting and, thus, incapable
of producing images that resembled EBSDs. The second, MarkovJunior, was much
more effective in creating a Voronoi tessellation that could be used to create
an EBSD map in Python. When comparing the results between the reference and the
generated EBSD, we discovered that the orientation and volume fractions were
extremely similar. With the study, it was concluded that MarkovJunior is an
effective machine learning tool that can reproduce representative grain
microstructures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Audio-visual Zero-shot Learning with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxing Chen, Yaohui Li, Yan Hong, Zizheng Huang, Zhuoer Xu, Zhangxuan Gu, Jun Lan, Huijia Zhu, Weiqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual zero-shot learning aims to recognize unseen categories based on
paired audio-visual sequences. Recent methods mainly focus on learning aligned
and discriminative multi-modal features to boost generalization towards unseen
categories. However, these approaches ignore the obscure action concepts in
category names and may inevitably introduce complex network structures with
difficult training objectives. In this paper, we propose a simple yet effective
framework named Knowledge-aware Distribution Adaptation (KDA) to help the model
better grasp the novel action contents with an external knowledge base.
Specifically, we first propose using large language models to generate rich
descriptions from category names, which leads to a better understanding of
unseen categories. Additionally, we propose a distribution alignment loss as
well as a knowledge-aware adaptive margin loss to further improve the
generalization ability towards unseen categories. Extensive experimental
results demonstrate that our proposed KDA can outperform state-of-the-art
methods on three popular audio-visual zero-shot learning datasets. Our code
will be avaliable at \url{https://github.com/chenhaoxing/KDA}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Virtual Home Staging: Inverse Rendering and Editing an Indoor Panorama
  under Natural Illumination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanzhou Ji, Azadeh O. Sawyer, Srinivasa G. Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel inverse rendering method that enables the transformation
of existing indoor panoramas with new indoor furniture layouts under natural
illumination. To achieve this, we captured indoor HDR panoramas along with
real-time outdoor hemispherical HDR photographs. Indoor and outdoor HDR images
were linearly calibrated with measured absolute luminance values for accurate
scene relighting. Our method consists of three key components: (1) panoramic
furniture detection and removal, (2) automatic floor layout design, and (3)
global rendering with scene geometry, new furniture objects, and a real-time
outdoor photograph. We demonstrate the effectiveness of our workflow in
rendering indoor scenes under different outdoor illumination conditions.
Additionally, we contribute a new calibrated HDR (Cali-HDR) dataset that
consists of 137 calibrated indoor panoramas and their associated outdoor
photographs. The source code and dataset are available:
https://github.com/Gzhji/Cali-HDR-Dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel OCT mosaicking pipeline with Feature- and Pixel-based registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Wang, Hao Li, Dewei Hu, Yuankai K. Tao, Ipek Oguz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution Optical Coherence Tomography (OCT) images are crucial for
ophthalmology studies but are limited by their relatively narrow field of view
(FoV). Image mosaicking is a technique for aligning multiple overlapping images
to obtain a larger FoV. Current mosaicking pipelines often struggle with
substantial noise and considerable displacement between the input sub-fields.
In this paper, we propose a versatile pipeline for stitching multi-view
OCT/OCTA \textit{en face} projection images. Our method combines the strengths
of learning-based feature matching and robust pixel-based registration to align
multiple images effectively. Furthermore, we advance the application of a
trained foundational model, Segment Anything Model (SAM), to validate
mosaicking results in an unsupervised manner. The efficacy of our pipeline is
validated using an in-house dataset and a large public dataset, where our
method shows superior performance in terms of both accuracy and computational
efficiency. We also made our evaluation tool for image mosaicking and the
corresponding pipeline publicly available at
\url{https://github.com/MedICL-VU/OCT-mosaicking}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Camera-Independent Single Image Depth Estimation from Defocus Blur 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lahiru Wijayasingha, Homa Alemzadeh, John A. Stankovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation is an important step in many downstream tasks in
machine vision. We address the topic of estimating monocular depth from defocus
blur which can yield more accurate results than the semantic based depth
estimation methods. The existing monocular depth from defocus techniques are
sensitive to the particular camera that the images are taken from. We show how
several camera-related parameters affect the defocus blur using optical physics
equations and how they make the defocus blur depend on these parameters. The
simple correction procedure we propose can alleviate this problem which does
not require any retraining of the original model. We created a synthetic
dataset which can be used to test the camera independent performance of depth
from defocus blur models. We evaluate our model on both synthetic and real
datasets (DDFF12 and NYU depth V2) obtained with different cameras and show
that our methods are significantly more robust to the changes of cameras. Code:
https://github.com/sleekEagle/defocus_camind.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Multimodal Surface Registration with Geometric Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed A. Suliman, Logan Z. J. Williams, Abdulah Fawaz, Emma C. Robinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces GeoMorph, a novel geometric deep-learning framework
designed for image registration of cortical surfaces. The registration process
consists of two main steps. First, independent feature extraction is performed
on each input surface using graph convolutions, generating low-dimensional
feature representations that capture important cortical surface
characteristics. Subsequently, features are registered in a deep-discrete
manner to optimize the overlap of common structures across surfaces by learning
displacements of a set of control points. To ensure smooth and biologically
plausible deformations, we implement regularization through a deep conditional
random field implemented with a recurrent neural network. Experimental results
demonstrate that GeoMorph surpasses existing deep-learning methods by achieving
improved alignment with smoother deformations. Furthermore, GeoMorph exhibits
competitive performance compared to classical frameworks. Such versatility and
robustness suggest strong potential for various neuroscience applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention: Large Multimodal Model is Watching your Geo-privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yang, Yixian Zhang, Daoyang Li, Shuju Sun, Junhong Duan, Junzhou He, Qingyang Wu, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geographic privacy, a crucial aspect of personal security, often goes
unnoticed in daily activities. This paper addresses the underestimation of this
privacy in the context of increasing online data sharing and the advancements
in information gathering technologies. With the surge in the use of Large
Multimodal Models, such as GPT-4, for Open Source Intelligence (OSINT), the
potential risks associated with geographic privacy breaches have intensified.
This study highlights the criticality of these developments, focusing on their
implications for individual privacy. The primary objective is to demonstrate
the capabilities of advanced AI tools, specifically a GPT-4 based model named
"Dr. Watson," in identifying and potentially compromising geographic privacy
through online shared content. We developed "Dr. Watson" to analyze and extract
geographic information from publicly available data sources. The study involved
five experimental cases, each offering different perspectives on the tool's
application in extracting precise location data from partial images and social
media content. The experiments revealed that "Dr. Watson" could successfully
identify specific geographic details, thereby exposing the vulnerabilities in
current geo-privacy measures. These findings underscore the ease with which
geographic information can be unintentionally disclosed. The paper concludes
with a discussion on the broader implications of these findings for individuals
and the community at large. It emphasizes the urgency for enhanced awareness
and protective measures against geo-privacy leakage in the era of advanced AI
and widespread social media usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-Based Soil Organic Carbon Remote Sensing from Satellite Images
  with Fourier Neural Operator and Structural Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ken C. L. Wong, Levente Klein, Ademir Ferreira da Silva, Hongzhi Wang, Jitendra Singh, Tanveer Syeda-Mahmood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soil organic carbon (SOC) sequestration is the transfer and storage of
atmospheric carbon dioxide in soils, which plays an important role in climate
change mitigation. SOC concentration can be improved by proper land use, thus
it is beneficial if SOC can be estimated at a regional or global scale. As
multispectral satellite data can provide SOC-related information such as
vegetation and soil properties at a global scale, estimation of SOC through
satellite data has been explored as an alternative to manual soil sampling.
Although existing studies show promising results, they are mainly based on
pixel-based approaches with traditional machine learning methods, and
convolutional neural networks (CNNs) are uncommon. To study the use of CNNs on
SOC remote sensing, here we propose the FNO-DenseNet based on the Fourier
neural operator (FNO). By combining the advantages of the FNO and DenseNet, the
FNO-DenseNet outperformed the FNO in our experiments with hundreds of times
fewer parameters. The FNO-DenseNet also outperformed a pixel-based random
forest by 18% in the mean absolute percentage error.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by the 2023 IEEE International Geoscience and
  Remote Sensing Symposium (IGARSS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Compression Using Neural Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janis Postels, Yannick Strümpler, Klara Reichard, Luc Van Gool, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Fields (NFs) have gained momentum as a tool for compressing various
data modalities - e.g. images and videos. This work leverages previous advances
and proposes a novel NF-based compression algorithm for 3D data. We derive two
versions of our approach - one tailored to watertight shapes based on Signed
Distance Fields (SDFs) and, more generally, one for arbitrary non-watertight
shapes using Unsigned Distance Fields (UDFs). We demonstrate that our method
excels at geometry compression on 3D point clouds as well as meshes. Moreover,
we show that, due to the NF formulation, it is straightforward to extend our
compression algorithm to compress both geometry and attribute (e.g. color) of
3D data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI for Agriculture: the Comparison of Semantic Segmentation Methods for
  Crop Mapping with Sentinel-2 Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irina Korotkova, Natalia Efremova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crop mapping is one of the most common tasks in artificial intelligence for
agriculture due to higher food demands from a growing population and increased
awareness of climate change. In case of vineyards, the texture is very
important for crop segmentation: with higher resolution satellite imagery the
texture is easily detected by majority of state-of-the-art algorithms. However,
this task becomes increasingly more difficult as the resolution of satellite
imagery decreases and the information about the texture becomes unavailable. In
this paper we aim to explore the main machine learning methods that can be used
with freely available satellite imagery and discuss how and when they can be
applied for vineyard segmentation problem. We assess the effectiveness of
various widely-used machine learning techniques and offer guidance on selecting
the most suitable model for specific scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FollowMe: a Robust Person Following Framework Based on Re-Identification
  and Gestures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Rollo, Andrea Zunino, Gennaro Raiola, Fabio Amadio, Arash Ajoudani, Nikolaos Tsagarakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-robot interaction (HRI) has become a crucial enabler in houses and
industries for facilitating operational flexibility. When it comes to mobile
collaborative robots, this flexibility can be further increased due to the
autonomous mobility and navigation capacity of the robotic agents, expanding
their workspace and consequently, the personalizable assistance they can
provide to the human operators. This however requires that the robot is capable
of detecting and identifying the human counterpart in all stages of the
collaborative task, and in particular while following a human in crowded
workplaces. To respond to this need, we developed a unified perception and
navigation framework, which enables the robot to identify and follow a target
person using a combination of visual Re-Identification (Re-ID), hand gestures
detection, and collision-free navigation. The Re-ID module can autonomously
learn the features of a target person and use the acquired knowledge to
visually re-identify the target. The navigation stack is used to follow the
target avoiding obstacles and other individuals in the environment. Experiments
are conducted with few subjects in a laboratory setting where some unknown
dynamic obstacles are introduced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published in "2023 IEEE International Conference on Advanced Robotics
  and Its Social Impacts (ARSO)"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SD-NAE: Generating Natural Adversarial Examples with Stable Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqian Lin, Jingyang Zhang, Yiran Chen, Hai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robustly evaluating deep learning image classifiers is challenging due to
some limitations of standard datasets. Natural Adversarial Examples (NAEs),
arising naturally from the environment and capable of deceiving classifiers,
are instrumental in identifying vulnerabilities in trained models. Existing
works collect such NAEs by filtering from a huge set of real images, a process
that is passive and lacks control. In this work, we propose to actively
synthesize NAEs with the state-of-the-art Stable Diffusion. Specifically, our
method formulates a controlled optimization process, where we perturb the token
embedding that corresponds to a specified class to synthesize NAEs. The
generation is guided by the gradient of loss from the target classifier so that
the created image closely mimics the ground-truth class yet fools the
classifier. Named SD-NAE (Stable Diffusion for Natural Adversarial Examples),
our innovative method is effective in producing valid and useful NAEs, which is
demonstrated through a meticulously designed experiment. Our work thereby
provides a valuable method for obtaining challenging evaluation data, which in
turn can potentially advance the development of more robust deep learning
models. Code is available at https://github.com/linyueqian/SD-NAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustifying Generalizable Implicit Shape Networks with a Tunable
  Non-Parametric Model <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amine Ouasfi, Adnane Boukhayma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feedforward generalizable models for implicit shape reconstruction from
unoriented point cloud present multiple advantages, including high performance
and inference speed. However, they still suffer from generalization issues,
ranging from underfitting the input point cloud, to misrepresenting samples
outside of the training data distribution, or with toplogies unseen at
training. We propose here an efficient mechanism to remedy some of these
limitations at test time. We combine the inter-shape data prior of the network
with an intra-shape regularization prior of a Nystr\"om Kernel Ridge
Regression, that we further adapt by fitting its hyperprameters to the current
shape. The resulting shape function defined in a shape specific Reproducing
Kernel Hilbert Space benefits from desirable stability and efficiency
properties and grants a shape adaptive expressiveness-robustness trade-off. We
demonstrate the improvement obtained through our method with respect to
baselines and the state-of-the-art using synthetic and real data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Innovative Horizons in Aerial Imagery: LSKNet Meets DiffusionDet for
  Advanced Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Sharshar, Aleksandr Matsun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of aerial image analysis, object detection plays a pivotal role,
with significant implications for areas such as remote sensing, urban planning,
and disaster management. This study addresses the inherent challenges in this
domain, notably the detection of small objects, managing densely packed
elements, and accounting for diverse orientations. We present an in-depth
evaluation of an object detection model that integrates the Large Selective
Kernel Network (LSKNet)as its backbone with the DiffusionDet head, utilizing
the iSAID dataset for empirical analysis. Our approach encompasses the
introduction of novel methodologies and extensive ablation studies. These
studies critically assess various aspects such as loss functions, box
regression techniques, and classification strategies to refine the model's
precision in object detection. The paper details the experimental application
of the LSKNet backbone in synergy with the DiffusionDet heads, a combination
tailored to meet the specific challenges in aerial image object detection. The
findings of this research indicate a substantial enhancement in the model's
performance, especially in the accuracy-time tradeoff. The proposed model
achieves a mean average precision (MAP) of approximately 45.7%, which is a
significant improvement, outperforming the RCNN model by 4.7% on the same
dataset. This advancement underscores the effectiveness of the proposed
modifications and sets a new benchmark in aerial image analysis, paving the way
for more accurate and efficient object detection methodologies. The code is
publicly available at https://github.com/SashaMatsun/LSKDiffDet
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPOT! Revisiting Video-Language Models for Event Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gengyuan Zhang, Jinhe Bi, Jindong Gu, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding videos is an important research topic for multimodal learning.
Leveraging large-scale datasets of web-crawled video-text pairs as weak
supervision has become a pre-training paradigm for learning joint
representations and showcased remarkable potential in video understanding
tasks. However, videos can be multi-event and multi-grained, while these
video-text pairs usually contain only broad-level video captions. This raises a
question: with such weak supervision, can video representation in
video-language models gain the ability to distinguish even factual
discrepancies in textual description and understand fine-grained events? To
address this, we introduce SPOT Prober, to benchmark existing video-language
models's capacities of distinguishing event-level discrepancies as an indicator
of models' event understanding ability. Our approach involves extracting events
as tuples (<Subject, Predicate, Object, Attribute, Timestamps>) from videos and
generating false event tuples by manipulating tuple components systematically.
We reevaluate the existing video-language models with these positive and
negative captions and find they fail to distinguish most of the manipulated
events. Based on our findings, we propose to plug in these manipulated event
captions as hard negative samples and find them effective in enhancing models
for event understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention Deficit is Ordered! Fooling Deformable Vision <span class="highlight-title">Transformer</span>s
  with Collaborative Adversarial Patches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quazi Mishkatul Alam, Bilel Tarchoun, Ihsen Alouani, Nael Abu-Ghazaleh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The latest generation of transformer-based vision models have proven to be
superior to Convolutional Neural Network (CNN)-based models across several
vision tasks, largely attributed to their remarkable prowess in relation
modeling. Deformable vision transformers significantly reduce the quadratic
complexity of modeling attention by using sparse attention structures, enabling
them to be used in larger scale applications such as multi-view vision systems.
Recent work demonstrated adversarial attacks against transformers; we show that
these attacks do not transfer to deformable transformers due to their sparse
attention structure. Specifically, attention in deformable transformers is
modeled using pointers to the most relevant other tokens. In this work, we
contribute for the first time adversarial attacks that manipulate the attention
of deformable transformers, distracting them to focus on irrelevant parts of
the image. We also develop new collaborative attacks where a source patch
manipulates attention to point to a target patch that adversarially attacks the
system. In our experiments, we find that only 1% patched area of the input
field can lead to 0% AP. We also show that the attacks provide substantial
versatility to support different attacker scenarios because of their ability to
redirect attention under the attacker control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q-Seg: Quantum Annealing-based Unsupervised Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Supreeth Mysore Venkatesh, Antonio Macaluso, Marlon Nuske, Matthias Klusch, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we present Q-Seg, a novel unsupervised image segmentation
method based on quantum annealing, tailored for existing quantum hardware. We
formulate the pixel-wise segmentation problem, which assimilates spectral and
spatial information of the image, as a graph-cut optimization task. Our method
efficiently leverages the interconnected qubit topology of the D-Wave Advantage
device, offering superior scalability over existing quantum approaches and
outperforming state-of-the-art classical methods. Our empirical evaluations on
synthetic datasets reveal that Q-Seg offers better runtime performance against
the classical optimizer Gurobi. Furthermore, we evaluate our method on
segmentation of Earth Observation images, an area of application where the
amount of labeled data is usually very limited. In this case, Q-Seg
demonstrates near-optimal results in flood mapping detection with respect to
classical supervised state-of-the-art machine learning methods. Also, Q-Seg
provides enhanced segmentation for forest coverage compared to existing
annotated masks. Thus, Q-Seg emerges as a viable alternative for real-world
applications using available quantum hardware, particularly in scenarios where
the lack of labeled data and computational runtime are critical.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Model Alignment Using Direct Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, Nikhil Naik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are fine-tuned using human comparison data with
Reinforcement Learning from Human Feedback (RLHF) methods to make them better
aligned with users' preferences. In contrast to LLMs, human preference learning
has not been widely explored in text-to-image diffusion models; the best
existing approach is to fine-tune a pretrained model using carefully curated
high quality images and captions to improve visual appeal and text alignment.
We propose Diffusion-DPO, a method to align diffusion models to human
preferences by directly optimizing on human comparison data. Diffusion-DPO is
adapted from the recently developed Direct Preference Optimization (DPO), a
simpler alternative to RLHF which directly optimizes a policy that best
satisfies human preferences under a classification objective. We re-formulate
DPO to account for a diffusion model notion of likelihood, utilizing the
evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic
dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model
of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with
Diffusion-DPO. Our fine-tuned base model significantly outperforms both base
SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement
model in human evaluation, improving visual appeal and prompt alignment. We
also develop a variant that uses AI feedback and has comparable performance to
training on human preferences, opening the door for scaling of diffusion model
alignment methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaGIC: Multi-modality Guided Image Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11818v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11818v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongsheng Yu, Hao Wang, Tiejian Luo, Heng Fan, Libo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vanilla image completion approaches exhibit sensitivity to large missing
regions, attributed to the limited availability of reference information for
plausible generation. To mitigate this, existing methods incorporate the extra
cue as a guidance for image completion. Despite improvements, these approaches
are often restricted to employing a single modality (e.g., segmentation or
sketch maps), which lacks scalability in leveraging multi-modality for more
plausible completion. In this paper, we propose a novel, simple yet effective
method for Multi-modal Guided Image Completion, dubbed MaGIC, which not only
supports a wide range of single modality as the guidance (e.g., text, canny
edge, sketch, segmentation, depth, and pose), but also adapts to arbitrarily
customized combination of these modalities (i.e., arbitrary multi-modality) for
image completion. For building MaGIC, we first introduce a modality-specific
conditional U-Net (MCU-Net) that injects single-modal signal into a U-Net
denoiser for single-modal guided image completion. Then, we devise a consistent
modality blending (CMB) method to leverage modality signals encoded in multiple
learned MCU-Nets through gradient guidance in latent space. Our CMB is
training-free, thereby avoids the cumbersome joint re-training of different
modalities, which is the secret of MaGIC to achieve exceptional flexibility in
accommodating new modalities for completion. Experiments show the superiority
of MaGIC over state-of-the-art methods and its generalization to various
completion tasks. Our project with code and models is available at
yeates.github.io/MaGIC-Page/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEFF: Improving Any Clothes-Changing Person ReID Model using Gallery
  Enrichment with Face Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Arkushin, Bar Cohen, Shmuel Peleg, Ohad Fried
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Clothes-Changing Re-Identification (CC-ReID) problem, given a query
sample of a person, the goal is to determine the correct identity based on a
labeled gallery in which the person appears in different clothes. Several
models tackle this challenge by extracting clothes-independent features.
However, the performance of these models is still lower for the
clothes-changing setting compared to the same-clothes setting in which the
person appears with the same clothes in the labeled gallery. As
clothing-related features are often dominant features in the data, we propose a
new process we call Gallery Enrichment, to utilize these features. In this
process, we enrich the original gallery by adding to it query samples based on
their face features, using an unsupervised algorithm. Additionally, we show
that combining ReID and face feature extraction modules alongside an enriched
gallery results in a more accurate ReID model, even for query samples with new
outfits that do not include faces. Moreover, we claim that existing CC-ReID
benchmarks do not fully represent real-world scenarios, and propose a new video
CC-ReID dataset called 42Street, based on a theater play that includes crowded
scenes and numerous clothes changes. When applied to multiple ReID models, our
method (GEFF) achieves an average improvement of 33.5% and 6.7% in the Top-1
clothes-changing metric on the PRCC and LTCC benchmarks. Combined with the
latest ReID models, our method achieves new SOTA results on the PRCC, LTCC,
CCVID, LaST and VC-Clothes benchmarks and the proposed 42Street dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task Arithmetic in the Tangent Space: Improved Editing of <span class="highlight-title">Pre-Train</span>ed
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12827v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12827v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Ortiz-Jimenez, Alessandro Favero, Pascal Frossard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task arithmetic has recently emerged as a cost-effective and scalable
approach to edit pre-trained models directly in weight space: By adding the
fine-tuned weights of different tasks, the model's performance can be improved
on these tasks, while negating them leads to task forgetting. Yet, our
understanding of the effectiveness of task arithmetic and its underlying
principles remains limited. We present a comprehensive study of task arithmetic
in vision-language models and show that weight disentanglement is the crucial
factor that makes it effective. This property arises during pre-training and
manifests when distinct directions in weight space govern separate, localized
regions in function space associated with the tasks. Notably, we show that
fine-tuning models in their tangent space by linearizing them amplifies weight
disentanglement. This leads to substantial performance improvements across
multiple task arithmetic benchmarks and diverse models. Building on these
findings, we provide theoretical and empirical analyses of the neural tangent
kernel (NTK) of these models and establish a compelling link between task
arithmetic and the spatial localization of the NTK eigenfunctions. Overall, our
work uncovers novel insights into the fundamental mechanisms of task arithmetic
and offers a more reliable and effective approach to edit pre-trained models
through the NTK linearization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Aggregate Multi-Scale Context for Instance Segmentation in
  Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.11057v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.11057v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Liu, Huifang Li, Chao Hu, Shuang Luo, Yan Luo, Chang Wen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of instance segmentation in remote sensing images, aiming at
performing per-pixel labeling of objects at instance level, is of great
importance for various civil applications. Despite previous successes, most
existing instance segmentation methods designed for natural images encounter
sharp performance degradations when they are directly applied to top-view
remote sensing images. Through careful analysis, we observe that the challenges
mainly come from the lack of discriminative object features due to severe scale
variations, low contrasts, and clustered distributions. In order to address
these problems, a novel context aggregation network (CATNet) is proposed to
improve the feature extraction process. The proposed model exploits three
lightweight plug-and-play modules, namely dense feature pyramid network
(DenseFPN), spatial context pyramid (SCP), and hierarchical region of interest
extractor (HRoIE), to aggregate global visual context at feature, spatial, and
instance domains, respectively. DenseFPN is a multi-scale feature propagation
module that establishes more flexible information flows by adopting inter-level
residual connections, cross-level dense connections, and feature re-weighting
strategy. Leveraging the attention mechanism, SCP further augments the features
by aggregating global spatial context into local regions. For each instance,
HRoIE adaptively generates RoI features for different downstream tasks.
Extensive evaluations of the proposed scheme on iSAID, DIOR, NWPU VHR-10, and
HRSID datasets demonstrate that the proposed approach outperforms
state-of-the-arts under similar computational costs. Source code and
pre-trained models are available at https://github.com/yeliudev/CATNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VeriCompress: A Tool to Streamline the Synthesis of Verified Robust
  Compressed Neural Networks from Scratch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09945v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09945v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sawinder Kaur, Yi Xiao, Asif Salekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI's widespread integration has led to neural networks (NNs) deployment on
edge and similar limited-resource platforms for safety-critical scenarios. Yet,
NN's fragility raises concerns about reliable inference. Moreover, constrained
platforms demand compact networks. This study introduces VeriCompress, a tool
that automates the search and training of compressed models with robustness
guarantees. These models are well-suited for safety-critical applications and
adhere to predefined architecture and size limitations, making them deployable
on resource-restricted platforms. The method trains models 2-3 times faster
than the state-of-the-art approaches, surpassing relevant baseline approaches
by average accuracy and robustness gains of 15.1 and 9.8 percentage points,
respectively. When deployed on a resource-restricted generic platform, these
models require 5-8 times less memory and 2-4 times less inference time than
models used in verified robustness literature. Our comprehensive evaluation
across various model architectures and datasets, including MNIST, CIFAR, SVHN,
and a relevant pedestrian detection dataset, showcases VeriCompress's capacity
to identify compressed verified robust models with reduced computation overhead
compared to current standards. This underscores its potential as a valuable
tool for end users, such as developers of safety-critical applications on edge
or Internet of Things platforms, empowering them to create suitable models for
safety-critical, resource-constrained platforms in their respective domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code is available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress, add more experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Automated Pipeline for Tumour-Infiltrating Lymphocyte Scoring in
  Breast Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam J Shephard, Mostafa Jahanifar, Ruoyu Wang, Muhammad Dawood, Simon Graham, Kastytis Sidlauskas, Syed Ali Khurram, Nasir M Rajpoot, Shan E Ahmed Raza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tumour-infiltrating lymphocytes (TILs) are considered as a valuable
prognostic markers in both triple-negative and human epidermal growth factor
receptor 2 (HER2) positive breast cancer. In this study, we introduce an
innovative deep learning pipeline based on the Efficient-UNet architecture to
predict the TILs score for breast cancer whole-slide images (WSIs). We first
segment tumour and stromal regions in order to compute a tumour bulk mask. We
then detect TILs within the tumour-associated stroma, generating a TILs score
by closely mirroring the pathologist's workflow. Our method exhibits
state-of-the-art performance in segmenting tumour/stroma areas and TILs
detection, as demonstrated by internal cross-validation on the TiGER Challenge
training dataset and evaluation on the final leaderboards. Additionally, our
TILs score proves competitive in predicting survival outcomes within the same
challenge, underscoring the clinical relevance and potential of our automated
TILs scoring pipeline as a breast cancer prognostic tool.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Scalable Computer Vision to Automate High-throughput Semiconductor
  Characterization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14408v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14408v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander E. Siemenn, Eunice Aissi, Fang Sheng, Armi Tiihonen, Hamide Kavak, Basita Das, Tonio Buonassisi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-throughput materials synthesis methods have risen in popularity due to
their potential to accelerate the design and discovery of novel functional
materials, such as solution-processed semiconductors. After synthesis, key
material properties must be measured and characterized to validate discovery
and provide feedback to optimization cycles. However, with the boom in
development of high-throughput synthesis tools that champion production rates
up to $10^4$ samples per hour with flexible form factors, most sample
characterization methods are either slow (conventional rates of $10^1$ samples
per hour, approximately 1000x slower) or rigid (e.g., designed for
standard-size microplates), resulting in a bottleneck that impedes the
materials-design process. To overcome this challenge, we propose a set of
automated material property characterization (autocharacterization) tools that
leverage the adaptive, parallelizable, and scalable nature of computer vision
to accelerate the throughput of characterization by 85x compared to the
non-automated workflow. We demonstrate a generalizable composition mapping tool
for high-throughput synthesized binary material systems as well as two scalable
autocharacterization algorithms that (1) autonomously compute the band gap of
200 unique compositions in 6 minutes and (2) autonomously compute the degree of
degradation in 200 unique compositions in 20 minutes, generating ultra-high
compositional resolution trends of band gap and stability. We demonstrate that
the developed band gap and degradation detection autocharacterization methods
achieve 98.5% accuracy and 96.9% accuracy, respectively, on the
FA$_{1-x}$MA$_{x}$PbI$_3$, $0\leq x \leq 1$ perovskite semiconductor system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript 18 pages; Supplemental 20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XPert: Peripheral Circuit & Neural Architecture Co-search for Area and
  Energy-efficient Xbar-based Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Moitra, Abhiroop Bhattacharjee, Youngeun Kim, Priyadarshini Panda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The hardware-efficiency and accuracy of Deep Neural Networks (DNNs)
implemented on In-memory Computing (IMC) architectures primarily depend on the
DNN architecture and the peripheral circuit parameters. It is therefore
essential to holistically co-search the network and peripheral parameters to
achieve optimal performance. To this end, we propose XPert, which co-searches
network architecture in tandem with peripheral parameters such as the type and
precision of analog-to-digital converters, crossbar column sharing and the
layer-specific input precision using an optimization-based design space
exploration. Compared to VGG16 baselines, XPert achieves 10.24x (4.7x) lower
EDAP, 1.72x (1.62x) higher TOPS/W,1.93x (3x) higher TOPS/mm2 at 92.46% (56.7%)
accuracy for CIFAR10 (TinyImagenet) datasets. The code for this paper is
available at https://github.com/Intelligent-Computing-Lab-Yale/XPert.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Design and Automation Conference (DAC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WEAR: An Outdoor Sports <span class="highlight-title">Dataset</span> for Wearable and Egocentric Activity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05088v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05088v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marius Bock, Hilde Kuehne, Kristof Van Laerhoven, Michael Moeller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though research has shown the complementarity of camera- and inertial-based
data, datasets which offer both egocentric video and inertial-based sensor data
remain scarce. In this paper, we introduce WEAR, an outdoor sports dataset for
both vision- and inertial-based human activity recognition (HAR). The dataset
comprises data from 18 participants performing a total of 18 different workout
activities with untrimmed inertial (acceleration) and camera (egocentric video)
data recorded at 10 different outside locations. Unlike previous egocentric
datasets, WEAR provides a challenging prediction scenario marked by purposely
introduced activity variations as well as an overall small information overlap
across modalities. Benchmark results obtained using each modality separately
show that each modality interestingly offers complementary strengths and
weaknesses in their prediction performance. Further, in light of the recent
success of temporal action localization models following the architecture
design of the ActionFormer, we demonstrate their versatility by applying them
in a plain fashion using vision, inertial and combined (vision + inertial)
features as input. Results demonstrate both the applicability of vision-based
temporal action localization models for inertial data and fusing both
modalities by means of simple concatenation, with the combined approach (vision
+ inertial features) being able to produce the highest mean average precision
and close-to-best F1-score. The dataset and code to reproduce experiments is
publicly available via: https://mariusbock.github.io/wear/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detect Every Thing with Few Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhang, Yuting Wang, Abdeslam Boularias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-set object detection aims at detecting arbitrary categories beyond those
seen during training. Most recent advancements have adopted the open-vocabulary
paradigm, utilizing vision-language backbones to represent categories with
language. In this paper, we introduce DE-ViT, an open-set object detector that
employs vision-only DINOv2 backbones and learns new categories through example
images instead of language. To improve general detection ability, we transform
multi-classification tasks into binary classification tasks while bypassing
per-class inference, and propose a novel region propagation technique for
localization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shot
object detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms the
open-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViT
surpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot and
one-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabulary
SoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available at
https://github.com/mlzxy/devit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Optimizers Can Learn Adversarially Robust Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhang, Zhiqi Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have shone in a variety of domains and attracted
increasing attention from both the security and the privacy communities. One
important yet worrying question is: Will training models under the differential
privacy (DP) constraint have an unfavorable impact on their adversarial
robustness? While previous works have postulated that privacy comes at the cost
of worse robustness, we give the first theoretical analysis to show that DP
models can indeed be robust and accurate, even sometimes more robust than their
naturally-trained non-private counterparts. We observe three key factors that
influence the privacy-robustness-accuracy tradeoff: (1) hyper-parameters for DP
optimizers are critical; (2) pre-training on public data significantly
mitigates the accuracy and robustness drop; (3) choice of DP optimizers makes a
difference. With these factors set properly, we achieve 90\% natural accuracy,
72\% robust accuracy ($+9\%$ than the non-private model) under $l_2(0.5)$
attack, and 69\% robust accuracy ($+16\%$ than the non-private model) with
pre-trained SimCLRv2 model under $l_\infty(4/255)$ attack on CIFAR10 with
$\epsilon=2$. In fact, we show both theoretically and empirically that DP
models are Pareto optimal on the accuracy-robustness tradeoff. Empirically, the
robustness of DP models is consistently observed across various datasets and
models. We believe our encouraging results are a significant step towards
training models that are private as well as robust.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Influencer Videos: Unboxing the Mystique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.12311v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.12311v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Rajaram, Puneet Manchanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influencer marketing has become a very popular tool to reach customers.
Despite the rapid growth in influencer videos, there has been little research
on the effectiveness of their constituent features in explaining video
engagement. We study YouTube influencers and analyze their unstructured video
data across text, audio and images using an "interpretable deep learning"
framework that accomplishes both goals of prediction and interpretation. Our
prediction-based approach analyzes unstructured data and finds that "what is
said" in words (text) is more influential than "how it is said" in imagery
(images) or acoustics (audio). Our novel interpretation-based approach is
implemented after completion of model prediction by analyzing the same source
of unstructured data to measure importance attributed to the video features. We
eliminate several spurious relationships in two steps, identifying a subset of
relationships which are confirmed using theory. We uncover novel findings that
establish distinct associations for measures of shallow and deep engagement
based on the dual-system framework of human thinking. Our approach is validated
using simulated data, and we discuss the learnings from our findings for
influencers and brands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, Online Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decodable and Sample Invariant Continuous Object Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dehao Yuan, Furong Huang, Cornelia Fermüller, Yiannis Aloimonos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Hyper-Dimensional Function Encoding (HDFE). Given samples of a
continuous object (e.g. a function), HDFE produces an explicit vector
representation of the given object, invariant to the sample distribution and
density. Sample distribution and density invariance enables HDFE to
consistently encode continuous objects regardless of their sampling, and
therefore allows neural networks to receive continuous objects as inputs for
machine learning tasks, such as classification and regression. Besides, HDFE
does not require any training and is proved to map the object into an organized
embedding space, which facilitates the training of the downstream tasks. In
addition, the encoding is decodable, which enables neural networks to regress
continuous objects by regressing their encodings. Therefore, HDFE serves as an
interface for processing continuous objects.
  We apply HDFE to function-to-function mapping, where vanilla HDFE achieves
competitive performance as the state-of-the-art algorithm. We apply HDFE to
point cloud surface normal estimation, where a simple replacement from PointNet
to HDFE leads to immediate 12% and 15% error reductions in two benchmarks. In
addition, by integrating HDFE into the PointNet-based SOTA network, we improve
the SOTA baseline by 2.5% and 1.7% in the same benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shortcut Learning in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.07780v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.07780v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has triggered the current rise of artificial intelligence and
is the workhorse of today's machine intelligence. Numerous success stories have
rapidly spread all over science, industry and society, but its limitations have
only recently come into focus. In this perspective we seek to distill how many
of deep learning's problems can be seen as different symptoms of the same
underlying problem: shortcut learning. Shortcuts are decision rules that
perform well on standard benchmarks but fail to transfer to more challenging
testing conditions, such as real-world scenarios. Related issues are known in
Comparative Psychology, Education and Linguistics, suggesting that shortcut
learning may be a common characteristic of learning systems, biological and
artificial alike. Based on these observations, we develop a set of
recommendations for model interpretation and benchmarking, highlighting recent
advances in machine learning to improve robustness and transferability from the
lab to real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>perspective article published at Nature Machine Intelligence
  (https://doi.org/10.1038/s42256-020-00257-z)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning: Applications and the Road Forward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L. Hayes, Eyke Hüllermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu, Andreas S. Tolias, Joost van de Weijer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars, Gido M. van de Ven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning is a sub-field of machine learning, which aims to allow
machine learning models to continuously learn on new data, by accumulating
knowledge without forgetting what was learned in the past. In this work, we
take a step back, and ask: "Why should one care about continual learning in the
first place?". We set the stage by surveying recent continual learning papers
published at three major machine learning conferences, and show that
memory-constrained settings dominate the field. Then, we discuss five open
problems in machine learning, and even though they seem unrelated to continual
learning at first sight, we show that continual learning will inevitably be
part of their solution. These problems are model-editing, personalization,
on-device learning, faster (re-)training and reinforcement learning. Finally,
by comparing the desiderata from these unsolved problems and the current
assumptions in continual learning, we highlight and discuss four future
directions for continual learning research. We hope that this work offers an
interesting perspective on the future of continual learning, while displaying
its potential value and the paths we have to pursue in order to make it
successful. This work is the result of the many discussions the authors had at
the Dagstuhl seminar on Deep Continual Learning, in March 2023.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCL-VI: <span class="highlight-title">Self-supervised</span> Context Learning for Visual Inspection of
  Industrial Defects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Haiming Yao, Wenyong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unsupervised visual inspection of defects in industrial products poses a
significant challenge due to substantial variations in product surfaces.
Current unsupervised models struggle to strike a balance between detecting
texture and object defects, lacking the capacity to discern latent
representations and intricate features. In this paper, we present a novel
self-supervised learning algorithm designed to derive an optimal encoder by
tackling the renowned jigsaw puzzle. Our approach involves dividing the target
image into nine patches, tasking the encoder with predicting the relative
position relationships between any two patches to extract rich semantics.
Subsequently, we introduce an affinity-augmentation method to accentuate
differences between normal and abnormal latent representations. Leveraging the
classic support vector data description algorithm yields final detection
results. Experimental outcomes demonstrate that our proposed method achieves
outstanding detection and segmentation performance on the widely used MVTec AD
dataset, with rates of 95.8% and 96.8%, respectively, establishing a
state-of-the-art benchmark for both texture and object defects. Comprehensive
experimentation underscores the effectiveness of our approach in diverse
industrial applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-LLaVA: Learning United Visual Representation by Alignment Before
  Projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Large Vision-Language Model (LVLM) has enhanced the performance of
various downstream tasks in visual-language understanding. Most existing
approaches encode images and videos into separate feature spaces, which are
then fed as inputs to large language models. However, due to the lack of
unified tokenization for images and videos, namely misalignment before
projection, it becomes challenging for a Large Language Model (LLM) to learn
multi-modal interactions from several poor projection layers. In this work, we
unify visual representation into the language feature space to advance the
foundational LLM towards a unified LVLM. As a result, we establish a simple but
robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images
and videos, mutually enhancing each other. Video-LLaVA achieves superior
performances on a broad range of 9 image benchmarks across 5 image
question-answering datasets and 4 image benchmark toolkits. Additionally, our
Video-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on
MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive
experiments demonstrate that Video-LLaVA mutually benefits images and videos
within a unified visual representation, outperforming models designed
specifically for images or videos. We aim for this work to provide modest
insights into the multi-modal inputs for the LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open Sesame! Universal Black Box Jailbreaking of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01446v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01446v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raz Lapid, Ron Langberg, Moshe Sipper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), designed to provide helpful and safe responses,
often rely on alignment techniques to align with user intent and social
guidelines. Unfortunately, this alignment can be exploited by malicious actors
seeking to manipulate an LLM's outputs for unintended purposes. In this paper
we introduce a novel approach that employs a genetic algorithm (GA) to
manipulate LLMs when model architecture and parameters are inaccessible. The GA
attack works by optimizing a universal adversarial prompt that -- when combined
with a user's query -- disrupts the attacked model's alignment, resulting in
unintended and potentially harmful outputs. Our novel approach systematically
reveals a model's limitations and vulnerabilities by uncovering instances where
its responses deviate from expected behavior. Through extensive experiments we
demonstrate the efficacy of our technique, thus contributing to the ongoing
discussion on responsible AI development by providing a diagnostic tool for
evaluating and enhancing alignment of LLMs with human intent. To our knowledge
this is the first automated universal black box jailbreak attack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UMAAF: Unveiling Aesthetics via Multifarious Attributes of Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Li, Yitian Wan, Xingjiao Wu, Junjie Xu, Cheng Jin, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing prevalence of smartphones and websites, Image Aesthetic
Assessment (IAA) has become increasingly crucial. While the significance of
attributes in IAA is widely recognized, many attribute-based methods lack
consideration for the selection and utilization of aesthetic attributes. Our
initial step involves the acquisition of aesthetic attributes from both intra-
and inter-perspectives. Within the intra-perspective, we extract the direct
visual attributes of images, constituting the absolute attribute. In the
inter-perspective, our focus lies in modeling the relative score relationships
between images within the same sequence, forming the relative attribute. Then,
to better utilize image attributes in aesthetic assessment, we propose the
Unified Multi-attribute Aesthetic Assessment Framework (UMAAF) to model both
absolute and relative attributes of images. For absolute attributes, we
leverage multiple absolute-attribute perception modules and an
absolute-attribute interacting network. The absolute-attribute perception
modules are first pre-trained on several absolute-attribute learning tasks and
then used to extract corresponding absolute attribute features. The
absolute-attribute interacting network adaptively learns the weight of diverse
absolute-attribute features, effectively integrating them with generic
aesthetic features from various absolute-attribute perspectives and generating
the aesthetic prediction. To model the relative attribute of images, we
consider the relative ranking and relative distance relationships between
images in a Relative-Relation Loss function, which boosts the robustness of the
UMAAF. Furthermore, UMAAF achieves state-of-the-art performance on TAD66K and
AVA datasets, and multiple experiments demonstrate the effectiveness of each
module and the model's alignment with human preference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyoung Park, Jin Kim, Hyeongjun Kwon, Ilhoon Yoon, Kwanghoon Sohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the inevitability of domain shifts during inference in real-world
applications, test-time adaptation (TTA) is essential for model adaptation
after deployment. However, the real-world scenario of continuously changing
target distributions presents challenges including catastrophic forgetting and
error accumulation. Existing TTA methods for non-stationary domain shifts,
while effective, incur excessive computational load, making them impractical
for on-device settings. In this paper, we introduce a layer-wise auto-weighting
algorithm for continual and gradual TTA that autonomously identifies layers for
preservation or concentrated adaptation. By leveraging the Fisher Information
Matrix (FIM), we first design the learning weight to selectively focus on
layers associated with log-likelihood changes while preserving unrelated ones.
Then, we further propose an exponential min-max scaler to make certain layers
nearly frozen while mitigating outliers. This minimizes forgetting and error
accumulation, leading to efficient adaptation to non-stationary target
distribution. Experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C show our
method outperforms conventional continual and gradual TTA approaches while
significantly reducing computational load, highlighting the importance of
FIM-based learning weight in adapting to continuously or gradually shifting
target domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-level Geometric Optimization for Regularised Constrained Linear
  Inverse Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04934v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04934v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Müller, Stefania Petra, Matthias Zisler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a geometric multilevel optimization approach that smoothly
incorporates box constraints. Given a box constrained optimization problem, we
consider a hierarchy of models with varying discretization levels. Finer models
are accurate but expensive to compute, while coarser models are less accurate
but cheaper to compute. When working at the fine level, multilevel optimisation
computes the search direction based on a coarser model which speeds up updates
at the fine level. Moreover, exploiting geometry induced by the hierarchy the
feasibility of the updates is preserved. In particular, our approach extends
classical components of multigrid methods like restriction and prolongation to
the Riemannian structure of our constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YolOOD: Utilizing Object Detection Concepts for Multi-Label
  Out-of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alon Zolfi, Guy Amit, Amit Baras, Satoru Koda, Ikuya Morikawa, Yuval Elovici, Asaf Shabtai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection has attracted a large amount of attention
from the machine learning research community in recent years due to its
importance in deployed systems. Most of the previous studies focused on the
detection of OOD samples in the multi-class classification task. However, OOD
detection in the multi-label classification task, a more common real-world use
case, remains an underexplored domain. In this research, we propose YolOOD - a
method that utilizes concepts from the object detection domain to perform OOD
detection in the multi-label classification task. Object detection models have
an inherent ability to distinguish between objects of interest
(in-distribution) and irrelevant objects (e.g., OOD objects) in images that
contain multiple objects belonging to different class categories. These
abilities allow us to convert a regular object detection model into an image
classifier with inherent OOD detection capabilities with just minor changes. We
compare our approach to state-of-the-art OOD detection methods and demonstrate
YolOOD's ability to outperform these methods on a comprehensive suite of
in-distribution and OOD benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupling Dynamic Monocular Videos for Dynamic View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01716v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01716v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng You, Junhui Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of dynamic view synthesis from dynamic monocular videos, i.e.,
synthesizing novel views for free viewpoints given a monocular video of a
dynamic scene captured by a moving camera, mainly lies in accurately modeling
the dynamic objects of a scene using limited 2D frames, each with a varying
timestamp and viewpoint. Existing methods usually require pre-processed 2D
optical flow and depth maps by off-the-shelf methods to supervise the network,
making them suffer from the inaccuracy of the pre-processed supervision and the
ambiguity when lifting the 2D information to 3D. In this paper, we tackle this
challenge in an unsupervised fashion. Specifically, we decouple the motion of
the dynamic objects into object motion and camera motion, respectively
regularized by proposed unsupervised surface consistency and patch-based
multi-view constraints. The former enforces the 3D geometric surfaces of moving
objects to be consistent over time, while the latter regularizes their
appearances to be consistent across different viewpoints. Such a fine-grained
motion formulation can alleviate the learning difficulty for the network, thus
enabling it to produce not only novel views with higher quality but also more
accurate scene flows and depth than existing methods requiring extra
supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yake Wei, Ruoxuan Feng, Zihe Wang, Di Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One primary topic of multi-modal learning is to jointly incorporate
heterogeneous information from different modalities. However, most models often
suffer from unsatisfactory multi-modal cooperation, which could not jointly
utilize all modalities well. Some methods are proposed to identify and enhance
the worse learnt modality, but are often hard to provide the fine-grained
observation of multi-modal cooperation at sample-level with theoretical
support. Hence, it is essential to reasonably observe and improve the
fine-grained cooperation between modalities, especially when facing realistic
scenarios where the modality discrepancy could vary across different samples.
To this end, we introduce a fine-grained modality valuation metric to evaluate
the contribution of each modality at sample-level. Via modality valuation, we
regretfully observe that the multi-modal model tends to rely on one specific
modality, resulting in other modalities being low-contributing. We further
analyze this issue and improve cooperation between modalities by enhancing the
discriminative ability of low-contributing modalities in a targeted manner.
Overall, our methods reasonably observe the fine-grained uni-modal contribution
at sample-level and achieve considerable improvement on different multi-modal
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing Domain Gap for Continual Domain Adaptation in Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10396v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10396v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh-Dzung Doan, Bach Long Nguyen, Surabhi Gupta, Ian Reid, Markus Wagner, Tat-Jun Chin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To ensure reliable object detection in autonomous systems, the detector must
be able to adapt to changes in appearance caused by environmental factors such
as time of day, weather, and seasons. Continually adapting the detector to
incorporate these changes is a promising solution, but it can be
computationally costly. Our proposed approach is to selectively adapt the
detector only when necessary, using new data that does not have the same
distribution as the current training data. To this end, we investigate three
popular metrics for domain gap evaluation and find that there is a correlation
between the domain gap and detection accuracy. Therefore, we apply the domain
gap as a criterion to decide when to adapt the detector. Our experiments show
that our approach has the potential to improve the efficiency of the detector's
operation in real-world scenarios, where environmental conditions change in a
cyclical manner, without sacrificing the overall performance of the detector.
Our code is publicly available at https://github.com/dadung/DGE-CDA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVIU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pink: Unveiling the Power of Referential Comprehension for Multi-modal
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Xuan, Qingpei Guo, Ming Yang, Shiliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities
in various multi-modal tasks. Nevertheless, their performance in fine-grained
image understanding tasks is still limited. To address this issue, this paper
proposes a new framework to enhance the fine-grained image understanding
abilities of MLLMs. Specifically, we present a new method for constructing the
instruction tuning dataset at a low cost by leveraging annotations in existing
datasets. A self-consistent bootstrapping method is also introduced to extend
existing dense object annotations into high-quality
referring-expression-bounding-box pairs. These methods enable the generation of
high-quality instruction data which includes a wide range of fundamental
abilities essential for fine-grained image perception. Moreover, we argue that
the visual encoder should be tuned during instruction tuning to mitigate the
gap between full image perception and fine-grained image perception.
Experimental results demonstrate the superior performance of our method. For
instance, our model exhibits a 5.2% accuracy improvement over Qwen-VL on GQA
and surpasses the accuracy of Kosmos-2 by 24.7% on RefCOCO_val. We also attain
the top rank on the leaderboard of MMBench. This promising performance is
achieved by training on only publicly available data, making it easily
reproducible. The models, datasets, and codes are publicly available at
https://github.com/SY-Xuan/Pink.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image augmentation with conformal mappings for a convolutional neural
  network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05258v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05258v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oona Rainio, Mohamed M. S. Nasser, Matti Vuorinen, Riku Klén
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For augmentation of the square-shaped image data of a convolutional neural
network (CNN), we introduce a new method, in which the original images are
mapped onto a disk with a conformal mapping, rotated around the center of this
disk and mapped under such a M\"obius transformation that preserves the disk,
and then mapped back onto their original square shape. This process does not
result the loss of information caused by removing areas from near the edges of
the original images unlike the typical transformations used in the data
augmentation for a CNN. We offer here the formulas of all the mappings needed
together with detailed instructions how to write a code for transforming the
images. The new method is also tested with simulated data and, according the
results, using this method to augment the training data of 10 images into 40
images decreases the amount of the error in the predictions by a CNN for a test
set of 160 images in a statistically significant way (p-value=0.0360).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColonMapper: topological mapping and localization for colonoscopy <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05546v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05546v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Morlana, Juan D. Tardós, J. M. M. Montiel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a topological mapping and localization system able to operate on
real human colonoscopies, despite significant shape and illumination changes.
The map is a graph where each node codes a colon location by a set of real
images, while edges represent traversability between nodes. For close-in-time
images, where scene changes are minor, place recognition can be successfully
managed with the recent transformers-based local feature matching algorithms.
However, under long-term changes -- such as different colonoscopies of the same
patient -- feature-based matching fails. To address this, we train on real
colonoscopies a deep global descriptor achieving high recall with significant
changes in the scene. The addition of a Bayesian filter boosts the accuracy of
long-term place recognition, enabling relocalization in a previously built map.
Our experiments show that ColonMapper is able to autonomously build a map and
localize against it in two important use cases: localization within the same
colonoscopy or within different colonoscopies of the same patient. Code will be
available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-Calibrated VAE with Test Time Free Lunch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Luo, Siya Qiu, Xingjian Tao, Yujun Cai, Jing Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel generative model that utilizes a
conditional Energy-Based Model (EBM) for enhancing Variational Autoencoder
(VAE), termed Energy-Calibrated VAE (EC-VAE). Specifically, VAEs often suffer
from blurry generated samples due to the lack of a tailored training on the
samples generated in the generative direction. On the other hand, EBMs can
generate high-quality samples but require expensive Markov Chain Monte Carlo
(MCMC) sampling. To address these issues, we introduce a conditional EBM for
calibrating the generative direction of VAE during training, without requiring
it for the generation at test time. In particular, we train EC-VAE upon both
the input data and the calibrated samples with adaptive weight to enhance
efficacy while avoiding MCMC sampling at test time. Furthermore, we extend the
calibration idea of EC-VAE to variational learning and normalizing flows, and
apply EC-VAE to an additional application of zero-shot image restoration via
neural transport prior and range-null theory. We evaluate the proposed method
with two applications, including image generation and zero-shot image
restoration, and the experimental results show that our method achieves the
state-of-the-art performance over single-step non-adversarial generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Composite Score for Anomaly Detection in Imbalanced Real-World
  Industrial <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnaud Bougaham, Mohammed El Adoui, Isabelle Linden, Benoît Frénay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the industrial sector has evolved towards its fourth
revolution. The quality control domain is particularly interested in advanced
machine learning for computer vision anomaly detection. Nevertheless, several
challenges have to be faced, including imbalanced datasets, the image
complexity, and the zero-false-negative (ZFN) constraint to guarantee the
high-quality requirement. This paper illustrates a use case for an industrial
partner, where Printed Circuit Board Assembly (PCBA) images are first
reconstructed with a Vector Quantized Generative Adversarial Network (VQGAN)
trained on normal products. Then, several multi-level metrics are extracted on
a few normal and abnormal images, highlighting anomalies through reconstruction
differences. Finally, a classifer is trained to build a composite anomaly score
thanks to the metrics extracted. This three-step approach is performed on the
public MVTec-AD datasets and on the partner PCBA dataset, where it achieves a
regular accuracy of 95.69% and 87.93% under the ZFN constraint.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version of the article has been accepted for publication, after
  peer review and is subject to Springer Nature AM terms of use, but is not the
  Version of Record and does not reflect post-acceptance improvements, or any
  corrections. The Version of Record is available online at:
  https://doi.org/10.1007/s10994-023-06415-9</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vocabulary-free Image Classification <span class="chip">NeurIPS2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Conti, Enrico Fini, Massimiliano Mancini, Paolo Rota, Yiming Wang, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large vision-language models have revolutionized the image
classification paradigm. Despite showing impressive zero-shot capabilities, a
pre-defined set of categories, a.k.a. the vocabulary, is assumed at test time
for composing the textual prompts. However, such assumption can be impractical
when the semantic context is unknown and evolving. We thus formalize a novel
task, termed as Vocabulary-free Image Classification (VIC), where we aim to
assign to an input image a class that resides in an unconstrained
language-induced semantic space, without the prerequisite of a known
vocabulary. VIC is a challenging task as the semantic space is extremely large,
containing millions of concepts, with hard-to-discriminate fine-grained
categories. In this work, we first empirically verify that representing this
semantic space by means of an external vision-language database is the most
effective way to obtain semantically relevant content for classifying the
image. We then propose Category Search from External Databases (CaSED), a
method that exploits a pre-trained vision-language model and an external
vision-language database to address VIC in a training-free manner. CaSED first
extracts a set of candidate categories from captions retrieved from the
database based on their semantic similarity to the image, and then assigns to
the image the best matching candidate category according to the same
vision-language model. Experiments on benchmark datasets validate that CaSED
outperforms other complex vision-language frameworks, while being efficient
with much fewer parameters, paving the way for future research in this
direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS2023, 19 pages, 8 figures, code is available at
  https://github.com/altndrr/vic</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised discovery of Interpretable Visual Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caroline Mazini Rodrigues, Nicolas Boutry, Laurent Najman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing interpretability of deep-learning models to non-experts, while
fundamental for a responsible real-world usage, is challenging. Attribution
maps from xAI techniques, such as Integrated Gradients, are a typical example
of a visualization technique containing a high level of information, but with
difficult interpretation. In this paper, we propose two methods, Maximum
Activation Groups Extraction (MAGE) and Multiscale Interpretable Visualization
(Ms-IV), to explain the model's decision, enhancing global interpretability.
MAGE finds, for a given CNN, combinations of features which, globally, form a
semantic meaning, that we call concepts. We group these similar feature
patterns by clustering in ``concepts'', that we visualize through Ms-IV. This
last method is inspired by Occlusion and Sensitivity analysis (incorporating
causality), and uses a novel metric, called Class-aware Order Correlation
(CaOC), to globally evaluate the most important image regions according to the
model's decision space. We compare our approach to xAI methods such as LIME and
Integrated Gradients. Experimental results evince the Ms-IV higher localization
and faithfulness values. Finally, qualitative evaluation of combined MAGE and
Ms-IV demonstrates humans' ability to agree, based on the visualization, with
the decision of clusters' concepts; and, to detect, among a given set of
networks, the existence of bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Hourglass Network for Semantic Segmentation of High
  Resolution Aerial Imagery <span class="chip">ICIP 2019</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1810.12813v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1810.12813v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Li, Youzuo Lin, Emily Schultz-Fellenz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation for aerial imagery is a challenging and important
problem in remotely sensed imagery analysis. In recent years, with the success
of deep learning, various convolutional neural network (CNN) based models have
been developed. However, due to the varying sizes of the objects and imbalanced
class labels, it can be challenging to obtain accurate pixel-wise semantic
segmentation results. To address those challenges, we develop a novel semantic
segmentation method and call it Contextual Hourglass Network. In our method, in
order to improve the robustness of the prediction, we design a new contextual
hourglass module which incorporates attention mechanism on processed
low-resolution featuremaps to exploit the contextual semantics. We further
exploit the stacked encoder-decoder structure by connecting multiple contextual
hourglass modules from end to end. This architecture can effectively extract
rich multi-scale features and add more feedback loops for better learning
contextual semantics through intermediate supervision. To demonstrate the
efficacy of our semantic segmentation method, we test it on Potsdam and
Vaihingen datasets. Through the comparisons to other baseline methods, our
method yields the best results on overall performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICIP 2019,
  https://cmsworkshops.com/ICIP2019/Papers/AcceptedPapers.asp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Class-Incremental Learning For Real-World Food Classification <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddeshwar Raghavan, Jiangpeng He, Fengqing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Food image classification is essential for monitoring health and tracking
dietary in image-based dietary assessment methods. However, conventional
systems often rely on static datasets with fixed classes and uniform
distribution. In contrast, real-world food consumption patterns, shaped by
cultural, economic, and personal influences, involve dynamic and evolving data.
Thus, require the classification system to cope with continuously evolving
data. Online Class Incremental Learning (OCIL) addresses the challenge of
learning continuously from a single-pass data stream while adapting to the new
knowledge and reducing catastrophic forgetting. Experience Replay (ER) based
OCIL methods store a small portion of previous data and have shown encouraging
performance. However, most existing OCIL works assume that the distribution of
encountered data is perfectly balanced, which rarely happens in real-world
scenarios. In this work, we explore OCIL for real-world food image
classification by first introducing a probabilistic framework to simulate
realistic food consumption scenarios. Subsequently, we present an attachable
Dynamic Model Update (DMU) module designed for existing ER methods, which
enables the selection of relevant images for model training, addressing
challenges arising from data repetition and imbalanced sample occurrences
inherent in realistic food consumption patterns within the OCIL framework. Our
performance evaluation demonstrates significant enhancements compared to
established ER methods, showing great potential for lifelong learning in
real-world food image classification scenarios. The code of our method is
publicly accessible at
\href{https://gitlab.com/viper-purdue/OCIL-real-world-food-image-classification}{https://gitlab.com/viper-purdue/OCIL-real-world-food-image-classification}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robot Hand-Eye Calibration using Structure-from-Motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Andreff, Radu Horaud, Bernard Espiau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose a new flexible method for hand-eye calibration. The
vast majority of existing hand-eye calibration techniques requires a
calibration rig which is used in conjunction with camera pose estimation
methods. Instead, we combine structure-from-motion with known robot motions and
we show that the solution can be obtained in linear form. The latter solves for
both the hand-eye parameters and for the unknown scale factor inherent with
structure-from-motion methods. The algebraic analysis that is made possible
with such a linear formulation allows to investigate not only the well known
case of general screw motions but also such singular motions as pure
translations, pure rotations, and planar motions. In essence, the robot-mounted
camera looks to an unknown rigid layout, tracks points over an image sequence
and estimates the camera-to-robot relationship. Such a self calibration process
is relevant for unmanned vehicles, robots working in remote places, and so
forth. We conduct a large number of experiments which validate the quality of
the method by comparing it with existing ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Counterfactual Data Augmentation Under Confounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abbavaram Gowtham Reddy, Saketh Bachu, Saloni Dash, Charchit Sharma, Amit Sharma, Vineeth N Balasubramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual data augmentation has recently emerged as a method to mitigate
confounding biases in the training data. These biases, such as spurious
correlations, arise due to various observed and unobserved confounding
variables in the data generation process. In this paper, we formally analyze
how confounding biases impact downstream classifiers and present a causal
viewpoint to the solutions based on counterfactual data augmentation. We
explore how removing confounding biases serves as a means to learn invariant
features, ultimately aiding in generalization beyond the observed data
distribution. Additionally, we present a straightforward yet powerful algorithm
for generating counterfactual images, which effectively mitigates the influence
of confounding effects on downstream classifiers. Through experiments on MNIST
variants and the CelebA datasets, we demonstrate how our simple augmentation
method helps existing state-of-the-art methods achieve good results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PFENet++: Boosting Few-shot Semantic Segmentation with the
  Noise-filtered Context-aware Prior Mask 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.13788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.13788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoliu Luo, Zhuotao Tian, Taiping Zhang, Bei Yu, Yuan Yan Tang, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we revisit the prior mask guidance proposed in ``Prior Guided
Feature Enrichment Network for Few-Shot Segmentation''. The prior mask serves
as an indicator that highlights the region of interests of unseen categories,
and it is effective in achieving better performance on different frameworks of
recent studies. However, the current method directly takes the maximum
element-to-element correspondence between the query and support features to
indicate the probability of belonging to the target class, thus the broader
contextual information is seldom exploited during the prior mask generation. To
address this issue, first, we propose the Context-aware Prior Mask (CAPM) that
leverages additional nearby semantic cues for better locating the objects in
query images. Second, since the maximum correlation value is vulnerable to
noisy features, we take one step further by incorporating a lightweight Noise
Suppression Module (NSM) to screen out the unnecessary responses, yielding
high-quality masks for providing the prior knowledge. Both two contributions
are experimentally shown to have substantial practical merit, and the new model
named PFENet++ significantly outperforms the baseline PFENet as well as all
other competitors on three challenging benchmarks PASCAL-5$^i$, COCO-20$^i$ and
FSS-1000. The new state-of-the-art performance is achieved without compromising
the efficiency, manifesting the potential for being a new strong baseline in
few-shot semantic segmentation. Our code will be available at
https://github.com/luoxiaoliu/PFENet2Plus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contribute equally and are listed in
  alphabetical order</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self supervised convolutional kernel based handcrafted feature
  harmonization: Enhanced left ventricle hypertension disease phenotyping on
  echocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jina Lee, Youngtaek Hong, Dawun Jeong, Yeonggul Jang, Sihyeon Jeong, Taekgeun Jung, Yeonyee E. Yoon, Inki Moon, Seung-Ah Lee, Hyuk-Jae Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiomics, a medical imaging technique, extracts quantitative handcrafted
features from images to predict diseases. Harmonization in those features
ensures consistent feature extraction across various imaging devices and
protocols. Methods for harmonization include standardized imaging protocols,
statistical adjustments, and evaluating feature robustness. Myocardial diseases
such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD)
are diagnosed via echocardiography, but variable imaging settings pose
challenges. Harmonization techniques are crucial for applying handcrafted
features in disease diagnosis in such scenario. Self-supervised learning (SSL)
enhances data understanding within limited datasets and adapts to diverse data
settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying
superior performance in various tasks. This study focuses on convolutional
filters within SSL, using them as preprocessing to convert images into feature
maps for handcrafted feature harmonization. Our proposed method excelled in
harmonization evaluation and exhibited superior LVH classification performance
compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DistNet2D: Leveraging long-range temporal information for efficient
  segmentation and tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Ollion, Martin Maliet, Caroline Giuglaris, Elise Vacher, Maxime Deforet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting long tracks and lineages from videomicroscopy requires an
extremely low error rate, which is challenging on complex datasets of dense or
deforming cells. Leveraging temporal context is key to overcoming this
challenge. We propose DistNet2D, a new deep neural network (DNN) architecture
for 2D cell segmentation and tracking that leverages both mid- and long-term
temporal information. DistNet2D considers seven frames at the input and uses a
post-processing procedure that exploits information from the entire video to
correct segmentation errors. DistNet2D outperforms two recent methods on two
experimental datasets, one containing densely packed bacterial cells and the
other containing eukaryotic cells. It is integrated into an ImageJ-based
graphical user interface for 2D data visualization, curation, and training.
Finally, we demonstrate the performance of DistNet2D on correlating the size
and shape of cells with their transport properties over large statistics, for
both bacterial and eukaryotic cells.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 5 figures, 18 supp figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated
  Class Incremental Learning for Vision Tasks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07784v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07784v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Babakniya, Zalan Fabian, Chaoyang He, Mahdi Soltanolkotabi, Salman Avestimehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models often suffer from forgetting previously learned
information when trained on new data. This problem is exacerbated in federated
learning (FL), where the data is distributed and can change independently for
each user. Many solutions are proposed to resolve this catastrophic forgetting
in a centralized setting. However, they do not apply directly to FL because of
its unique complexities, such as privacy concerns and resource limitations. To
overcome these challenges, this paper presents a framework for
$\textbf{federated class incremental learning}$ that utilizes a generative
model to synthesize samples from past distributions. This data can be later
exploited alongside the training data to mitigate catastrophic forgetting. To
preserve privacy, the generative model is trained on the server using data-free
methods at the end of each task without requesting data from clients. Moreover,
our solution does not demand the users to store old data or models, which gives
them the freedom to join/leave the training at any time. Additionally, we
introduce SuperImageNet, a new regrouping of the ImageNet dataset specifically
tailored for federated continual learning. We demonstrate significant
improvements compared to existing baselines through extensive experiments on
multiple datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2023. arXiv admin note: text overlap with
  arXiv:2307.00497</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and
  Repulsive UDF <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Lionar, Xiangyu Xu, Min Lin, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remarkable progress has been made in 3D reconstruction from single-view RGB-D
inputs. MCC is the current state-of-the-art method in this field, which
achieves unprecedented success by combining vision Transformers with
large-scale training. However, we identified two key limitations of MCC: 1) The
Transformer decoder is inefficient in handling large number of query points; 2)
The 3D representation struggles to recover high-fidelity details. In this
paper, we propose a new approach called NU-MCC that addresses these
limitations. NU-MCC includes two key innovations: a Neighborhood decoder and a
Repulsive Unsigned Distance Function (Repulsive UDF). First, our Neighborhood
decoder introduces center points as an efficient proxy of input visual
features, allowing each query point to only attend to a small neighborhood.
This design not only results in much faster inference speed but also enables
the exploitation of finer-scale visual features for improved recovery of 3D
textures. Second, our Repulsive UDF is a novel alternative to the occupancy
field used in MCC, significantly improving the quality of 3D object
reconstruction. Compared to standard UDFs that suffer from holes in results,
our proposed Repulsive UDF can achieve more complete surface reconstruction.
Experimental results demonstrate that NU-MCC is able to learn a strong 3D
representation, significantly advancing the state of the art in single-view 3D
reconstruction. Particularly, it outperforms MCC by 9.7% in terms of the
F1-score on the CO3D-v2 dataset with more than 5x faster running speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023. Project page: https://numcc.github.io/ Code:
  https://github.com/sail-sg/numcc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Yan, Delin Qu, Dong Wang, Dan Xu, Zhigang Wang, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D
Gaussian representation in the Simultaneous Localization and Mapping (SLAM)
system. It facilitates a better balance between efficiency and accuracy.
Compared to recent SLAM methods employing neural implicit representations, our
method utilizes a real-time differentiable splatting rendering pipeline that
offers significant speedup to map optimization and RGB-D re-rendering.
Specifically, we propose an adaptive expansion strategy that adds new or
deletes noisy 3D Gaussian in order to efficiently reconstruct new observed
scene geometry and improve the mapping of previously observed areas. This
strategy is essential to extend 3D Gaussian representation to reconstruct the
whole scene rather than synthesize a static object in existing methods.
Moreover, in the pose tracking process, an effective coarse-to-fine technique
is designed to select reliable 3D Gaussian representations to optimize camera
pose, resulting in runtime reduction and robust estimation. Our method achieves
competitive performance compared with existing state-of-the-art real-time
methods on the Replica, TUM-RGBD datasets. The source code will be released
soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Formalizing and Evaluating Requirements of Perception Systems for
  Automated Vehicles using Spatio-Temporal Perception Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.14372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.14372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Hekmatnejad, Bardh Hoxha, Jyotirmoy V. Deshmukh, Yezhou Yang, Georgios Fainekos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated vehicles (AV) heavily depend on robust perception systems. Current
methods for evaluating vision systems focus mainly on frame-by-frame
performance. Such evaluation methods appear to be inadequate in assessing the
performance of a perception subsystem when used within an AV. In this paper, we
present a logic -- referred to as Spatio-Temporal Perception Logic (STPL) --
which utilizes both spatial and temporal modalities. STPL enables reasoning
over perception data using spatial and temporal operators. One major advantage
of STPL is that it facilitates basic sanity checks on the functional
performance of the perception system, even without ground-truth data in some
cases. We identify a fragment of STPL which is efficiently monitorable offline
in polynomial time. Finally, we present a range of specifications for AV
perception systems to highlight the types of requirements that can be expressed
and analyzed through offline monitoring with STPL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 11 figures, 6 tables, 4 algorithms, 2 appendixes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene
  Graphs with Weak Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiani Huang, Ziyang Li, Mayur Naik, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose LASER, a neuro-symbolic approach to learn semantic video
representations that capture rich spatial and temporal properties in video data
by leveraging high-level logic specifications. In particular, we formulate the
problem in terms of alignment between raw videos and spatio-temporal logic
specifications. The alignment algorithm leverages a differentiable symbolic
reasoner and a combination of contrastive, temporal, and semantics losses. It
effectively and efficiently trains low-level perception models to extract
fine-grained video representation in the form of a spatio-temporal scene graph
that conforms to the desired high-level specification. In doing so, we explore
a novel methodology that weakly supervises the learning of video semantic
representations through logic specifications. We evaluate our method on two
datasets with rich spatial and temporal specifications:
20BN-Something-Something and MUGEN. We demonstrate that our method learns
better fine-grained video semantics than existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Sub-Pixel Disparity Distribution for Light Field Depth
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09688v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09688v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Chao, Xuechun Wang, Yingqian Wang, Guanghui Wang, Fuqing Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Light field (LF) depth estimation plays a crucial role in many LF-based
applications. Existing LF depth estimation methods consider depth estimation as
a regression problem, where a pixel-wise L1 loss is employed to supervise the
training process. However, the disparity map is only a sub-space projection
(i.e., an expectation) of the disparity distribution, which is essential for
models to learn. In this paper, we propose a simple yet effective method to
learn the sub-pixel disparity distribution by fully utilizing the power of deep
networks, especially for LF of narrow baselines. We construct the cost volume
at the sub-pixel level to produce a finer disparity distribution and design an
uncertainty-aware focal loss to supervise the predicted disparity distribution
toward the ground truth. Extensive experimental results demonstrate the
effectiveness of our method.Our method significantly outperforms recent
state-of-the-art LF depth algorithms on the HCI 4D LF Benchmark in terms of all
four accuracy metrics (i.e., BadPix 0.01, BadPix 0.03, BadPix 0.07, and MSE
$\times$100). The code and model of the proposed method are available at
\url{https://github.com/chaowentao/SubFocal}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Computational Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LDP: Language-driven Dual-Pixel Image Defocus Deblurring Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09815v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09815v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Liyuan Pan, Yan Yang, Richard Hartley, Miaomiao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering sharp images from dual-pixel (DP) pairs with disparity-dependent
blur is a challenging task.~Existing blur map-based deblurring methods have
demonstrated promising results. In this paper, we propose, to the best of our
knowledge, the first framework that introduces the contrastive language-image
pre-training framework (CLIP) to accurately estimate the blur map from a DP
pair unsupervisedly. To achieve this, we first carefully design text prompts to
enable CLIP to understand blur-related geometric prior knowledge from the DP
pair. Then, we propose a format to input a stereo DP pair to CLIP without any
fine-tuning, despite the fact that CLIP is pre-trained on monocular images.
Given the estimated blur map, we introduce a blur-prior attention block, a
blur-weighting loss, and a blur-aware loss to recover the all-in-focus image.
Our method achieves state-of-the-art performance in extensive experiments (see
Fig.~\ref{fig:teaser}).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Defect Detection and Classification Method for Advanced IC
  Nodes by Using Slicing Aided Hyper Inference with Refinement Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vic De Ridder, Bappaditya Dey, Victor Blanco, Sandip Halder, Bartel Van Waeyenberge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In semiconductor manufacturing, lithography has often been the manufacturing
step defining the smallest possible pattern dimensions. In recent years,
progress has been made towards high-NA (Numerical Aperture) EUVL
(Extreme-Ultraviolet-Lithography) paradigm, which promises to advance pattern
shrinking (2 nm node and beyond). However, a significant increase in stochastic
defects and the complexity of defect detection becomes more pronounced with
high-NA. Present defect inspection techniques (both non-machine learning and
machine learning based), fail to achieve satisfactory performance at high-NA
dimensions. In this work, we investigate the use of the Slicing Aided Hyper
Inference (SAHI) framework for improving upon current techniques. Using SAHI,
inference is performed on size-increased slices of the SEM images. This leads
to the object detector's receptive field being more effective in capturing
small defect instances. First, the performance on previously investigated
semiconductor datasets is benchmarked across various configurations, and the
SAHI approach is demonstrated to substantially enhance the detection of small
defects, by approx. 2x. Afterwards, we also demonstrated application of SAHI
leads to flawless detection rates on a new test dataset, with scenarios not
encountered during training, whereas previous trained models failed. Finally,
we formulate an extension of SAHI that does not significantly reduce
true-positive predictions while eliminating false-positive predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, to be presented at International Conference on
  Machine Intelligence with Applications (ICMIA), and to be published in
  conference proceedings by AIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Event-RGBD Neural SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delin Qu, Chi Yan, Dong Wang, Jie Yin, Dan Xu, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural SLAM has achieved remarkable progress recently. Nevertheless,
existing methods face significant challenges in non-ideal scenarios, such as
motion blur or lighting variation, which often leads to issues like convergence
failures, localization drifts, and distorted mapping. To address these
challenges, we propose $\textbf{EN-SLAM}$, the first event-RGBD implicit neural
SLAM framework, which effectively leverages the high rate and high dynamic
range advantages of event data for tracking and mapping. Specifically, EN-SLAM
proposes a differentiable CRF (Camera Response Function) rendering technique to
generate distinct RGB and event camera data via a shared radiance field, which
is optimized by learning a unified implicit representation with the captured
event and RGBD supervision. Moreover, based on the temporal difference property
of events, we propose a temporal aggregating optimization strategy for the
event joint tracking and global bundle adjustment, capitalizing on the
consecutive difference constraints of events, significantly enhancing tracking
accuracy and robustness. Finally, we construct the simulated dataset
$\textbf{DEV-Indoors}$ and real captured dataset $\textbf{DEV-Reals}$
containing 6 scenes, 17 sequences with practical motion blur and lighting
changes for evaluations. Experimental results show that our method outperforms
the SOTA methods in both tracking ATE and mapping ACC with a real-time $17$ FPS
in various challenging environments. The code and dataset will be released
soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance Segmentation under Occlusions via Location-aware Copy-Paste
  Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Son Nguyen, Mikel Lainsa, Hung Dao, Daeyoung Kim, Giang Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Occlusion is a long-standing problem in computer vision, particularly in
instance segmentation. ACM MMSports 2023 DeepSportRadar has introduced a
dataset that focuses on segmenting human subjects within a basketball context
and a specialized evaluation metric for occlusion scenarios. Given the modest
size of the dataset and the highly deformable nature of the objects to be
segmented, this challenge demands the application of robust data augmentation
techniques and wisely-chosen deep learning architectures. Our work (ranked 1st
in the competition) first proposes a novel data augmentation technique, capable
of generating more training samples with wider distribution. Then, we adopt a
new architecture - Hybrid Task Cascade (HTC) framework with CBNetV2 as backbone
and MaskIoU head to improve segmentation performance. Furthermore, we employ a
Stochastic Weight Averaging (SWA) training strategy to improve the model's
generalization. As a result, we achieve a remarkable occlusion score (OM) of
0.533 on the challenge dataset, securing the top-1 position on the leaderboard.
Source code is available at this
https://github.com/nguyendinhson-kaist/MMSports23-Seg-AutoID.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProtoCLIP: Prototypical Contrastive Language Image <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10996v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10996v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Chen, Zhao Wu, Fan Liu, Zaiquan Yang, Huaxi Huang, Ying Tan, Erjin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language Image Pretraining (CLIP) has received widespread
attention, since its learned representations can be transferred well to various
downstream tasks. During the training process of the CLIP model, the InfoNCE
objective aligns positive image-text pairs and separates negative ones. We show
an underlying representation grouping effect during this process: the InfoNCE
objective indirectly groups semantically similar representations together via
randomly emerged within-modal anchors. Based on this understanding, in this
paper, Prototypical Contrastive Language Image Pretraining (ProtoCLIP) is
introduced to enhance such grouping by boosting its efficiency and increasing
its robustness against the modality gap. Specifically, ProtoCLIP sets up
prototype-level discrimination between image and text spaces, which efficiently
transfers higher-level structural knowledge. Further, Prototypical Back
Translation (PBT) is proposed to decouple representation grouping from
representation alignment, resulting in effective learning of meaningful
representations under large modality gap. The PBT also enables us to introduce
additional external teachers with richer prior language knowledge. ProtoCLIP is
trained with an online episodic training strategy, which makes it can be scaled
up to unlimited amounts of data. We train our ProtoCLIP on Conceptual Captions
and achieved an +5.81% ImageNet linear probing improvement and an +2.01%
ImageNet zero-shot classification improvement. On the larger YFCC-15M dataset,
ProtoCLIP matches the performance of CLIP with 33% of training time. Codes are
available at https://github.com/megvii-research/protoclip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revealing the preference for correcting separated aberrations in joint
  optic-image design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04342v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04342v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwen Zhou, Shiqi Chen, Zheng Ren, Wenguan Zhang, Jiapu Yan, Huajun Feng, Qi Li, Yueting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The joint design of the optical system and the downstream algorithm is a
challenging and promising task. Due to the demand for balancing the global
optimal of imaging systems and the computational cost of physical simulation,
existing methods cannot achieve efficient joint design of complex systems such
as smartphones and drones. In this work, starting from the perspective of the
optical design, we characterize the optics with separated aberrations.
Additionally, to bridge the hardware and software without gradients, an image
simulation system is presented to reproduce the genuine imaging procedure of
lenses with large field-of-views. As for aberration correction, we propose a
network to perceive and correct the spatially varying aberrations and validate
its superiority over state-of-the-art methods. Comprehensive experiments reveal
that the preference for correcting separated aberrations in joint design is as
follows: longitudinal chromatic aberration, lateral chromatic aberration,
spherical aberration, field curvature, and coma, with astigmatism coming last.
Drawing from the preference, a 10% reduction in the total track length of the
consumer-level mobile phone lens module is accomplished. Moreover, this
procedure spares more space for manufacturing deviations, realizing
extreme-quality enhancement of computational photography. The optimization
paradigm provides innovative insight into the practical joint design of
sophisticated optical systems and post-processing algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scale-aware competition network for palmprint recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11354v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11354v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengrui Gao, Ziyuan Yang, Min Zhu, Andrew Beng Jin Teoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Palmprint biometrics garner heightened attention in palm-scanning payment and
social security due to their distinctive attributes. However, prevailing
methodologies singularly prioritize texture orientation, neglecting the
significant texture scale dimension. We design an innovative network for
concurrently extracting intra-scale and inter-scale features to redress this
limitation. This paper proposes a scale-aware competitive network (SAC-Net),
which includes the Inner-Scale Competition Module (ISCM) and the Across-Scale
Competition Module (ASCM) to capture texture characteristics related to
orientation and scale. ISCM efficiently integrates learnable Gabor filters and
a self-attention mechanism to extract rich orientation data and discern
textures with long-range discriminative properties. Subsequently, ASCM
leverages a competitive strategy across various scales to effectively
encapsulate the competitive texture scale elements. By synergizing ISCM and
ASCM, our method adeptly characterizes palmprint features. Rigorous
experimentation across three benchmark datasets unequivocally demonstrates our
proposed approach's exceptional recognition performance and resilience relative
to state-of-the-art alternatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Plastic and Stable Exemplar-Free Incremental Learning: A
  Dual-Learner Framework with Cumulative Parameter Averaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenju Sun, Qingyong Li, Wen Wang, Yangli-ao Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dilemma between plasticity and stability presents a significant challenge
in Incremental Learning (IL), especially in the exemplar-free scenario where
accessing old-task samples is strictly prohibited during the learning of a new
task. A straightforward solution to this issue is learning and storing an
independent model for each task, known as Single Task Learning (STL). Despite
the linear growth in model storage with the number of tasks in STL, we
empirically discover that averaging these model parameters can potentially
preserve knowledge across all tasks. Inspired by this observation, we propose a
Dual-Learner framework with Cumulative Parameter Averaging (DLCPA). DLCPA
employs a dual-learner design: a plastic learner focused on acquiring new-task
knowledge and a stable learner responsible for accumulating all learned
knowledge. The knowledge from the plastic learner is transferred to the stable
learner via cumulative parameter averaging. Additionally, several task-specific
classifiers work in cooperation with the stable learner to yield the final
prediction. Specifically, when learning a new task, these modules are updated
in a cyclic manner: i) the plastic learner is initially optimized using a
self-supervised loss besides the supervised loss to enhance the feature
extraction robustness; ii) the stable learner is then updated with respect to
the plastic learner in a cumulative parameter averaging manner to maintain its
task-wise generalization; iii) the task-specific classifier is accordingly
optimized to align with the stable learner. Experimental results on CIFAR-100
and Tiny-ImageNet show that DLCPA outperforms several state-of-the-art
exemplar-free baselines in both Task-IL and Class-IL settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking the Backward Propagation for Adversarial Transferability <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12685v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12685v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosen Wang, Kangheng Tong, Kun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer-based attacks generate adversarial examples on the surrogate model,
which can mislead other black-box models without access, making it promising to
attack real-world applications. Recently, several works have been proposed to
boost adversarial transferability, in which the surrogate model is usually
overlooked. In this work, we identify that non-linear layers (e.g., ReLU,
max-pooling, etc.) truncate the gradient during backward propagation, making
the gradient w.r.t. input image imprecise to the loss function. We hypothesize
and empirically validate that such truncation undermines the transferability of
adversarial examples. Based on these findings, we propose a novel method called
Backward Propagation Attack (BPA) to increase the relevance between the
gradient w.r.t. input image and loss function so as to generate adversarial
examples with higher transferability. Specifically, BPA adopts a non-monotonic
function as the derivative of ReLU and incorporates softmax with temperature to
smooth the derivative of max-pooling, thereby mitigating the information loss
during the backward propagation of gradients. Empirical results on the ImageNet
dataset demonstrate that not only does our method substantially boost the
adversarial transferability, but it is also general to existing transfer-based
attacks. Code is available at https://github.com/Trustworthy-AI-Group/RPA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extraction and Summarization of Explicit Video Content using Multi-Modal
  Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaunak Joshi, Raghav Gaggar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increase in video-sharing platforms across the internet, it is
difficult for humans to moderate the data for explicit content. Hence, an
automated pipeline to scan through video data for explicit content has become
the need of the hour. We propose a novel pipeline that uses multi-modal deep
learning to first extract the explicit segments of input videos and then
summarize their content using text to determine its age appropriateness and age
rating. We also evaluate our pipeline's effectiveness in the end using standard
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Attention-<span class="highlight-title">Prompt</span>ed Prediction and Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Zhang, Siyi Gu, Bo Pan, Guangji Bai, Xiaofeng Yang, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanation(attention)-guided learning is a method that enhances a model's
predictive power by incorporating human understanding during the training
phase. While attention-guided learning has shown promising results, it often
involves time-consuming and computationally expensive model retraining. To
address this issue, we introduce the attention-prompted prediction technique,
which enables direct prediction guided by the attention prompt without the need
for model retraining. However, this approach presents several challenges,
including: 1) How to incorporate the visual attention prompt into the model's
decision-making process and leverage it for future predictions even in the
absence of a prompt? and 2) How to handle the incomplete information from the
visual attention prompt? To tackle these challenges, we propose a novel
framework called Visual Attention-Prompted Prediction and Learning, which
seamlessly integrates visual attention prompts into the model's decision-making
process and adapts to images both with and without attention prompts for
prediction. To address the incomplete information of the visual attention
prompt, we introduce a perturbation-based attention map modification method.
Additionally, we propose an optimization-based mask aggregation method with a
new weight learning function for adaptive perturbed annotation aggregation in
the attention map modification process. Our overall framework is designed to
learn in an attention-prompt guided multi-task manner to enhance future
predictions even for samples without attention prompts and trained in an
alternating manner for better convergence. Extensive experiments conducted on
two datasets demonstrate the effectiveness of our proposed framework in
enhancing predictions for samples, both with and without provided prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoCLIP: Clip-Inspired Alignment between Locations and Images for
  Effective Worldwide Geo-localization <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vicente Vivanco Cepeda, Gaurav Kumar Nayak, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Worldwide Geo-localization aims to pinpoint the precise location of images
taken anywhere on Earth. This task has considerable challenges due to immense
variation in geographic landscapes. The image-to-image retrieval-based
approaches fail to solve this problem on a global scale as it is not feasible
to construct a large gallery of images covering the entire world. Instead,
existing approaches divide the globe into discrete geographic cells,
transforming the problem into a classification task. However, their performance
is limited by the predefined classes and often results in inaccurate
localizations when an image's location significantly deviates from its class
center. To overcome these limitations, we propose GeoCLIP, a novel
CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between
the image and its corresponding GPS locations. GeoCLIP's location encoder
models the Earth as a continuous function by employing positional encoding
through random Fourier features and constructing a hierarchical representation
that captures information at varying resolutions to yield a semantically rich
high-dimensional feature suitable to use even beyond geo-localization. To the
best of our knowledge, this is the first work employing GPS encoding for
geo-localization. We demonstrate the efficacy of our method via extensive
experiments and ablations on benchmark datasets. We achieve competitive
performance with just 20% of training data, highlighting its effectiveness even
in limited-data settings. Furthermore, we qualitatively demonstrate
geo-localization using a text query by leveraging CLIP backbone of our image
encoder. The project webpage is available at:
https://vicentevivan.github.io/GeoCLIP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and
  3D Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01121v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01121v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Rollo, Gennaro Raiola, Andrea Zunino, Nikolaos Tsagarakis, Arash Ajoudani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric navigation is nowadays a well-established field of robotics and the
research focus is shifting towards higher-level scene understanding, such as
Semantic Mapping. When a robot needs to interact with its environment, it must
be able to comprehend the contextual information of its surroundings. This work
focuses on classifying and localising objects within a map, which is under
construction (SLAM) or already built. To further explore this direction, we
propose a framework that can autonomously detect and localize predefined
objects in a known environment using a multi-modal sensor fusion approach
(combining RGB and depth data from an RGB-D camera and a lidar). The framework
consists of three key elements: understanding the environment through RGB data,
estimating depth through multi-modal sensor fusion, and managing artifacts
(i.e., filtering and stabilizing measurements). The experiments show that the
proposed framework can accurately detect 98% of the objects in the real sample
environment, without post-processing, while 85% and 80% of the objects were
mapped using the single RGBD camera or RGB + lidar setup respectively. The
comparison with single-sensor (camera or lidar) experiments is performed to
show that sensor fusion allows the robot to accurately detect near and far
obstacles, which would have been noisy or imprecise in a purely visual or
laser-based approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 11th European Conference on Mobile Robots (ECMR) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized super-resolution 4D Flow MRI $\unicode{x2013}$ using
  ensemble learning to extend across the cardiovascular system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Ericsson, Adam Hjalmarsson, Muhammad Usman Akbar, Edward Ferdian, Mia Bonini, Brandon Hardy, Jonas Schollenberger, Maria Aristova, Patrick Winter, Nicholas Burris, Alexander Fyrdahl, Andreas Sigfridsson, Susanne Schnell, C. Alberto Figueroa, David Nordsletten, Alistair A. Young, David Marlevi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive
measurement technique capable of quantifying blood flow across the
cardiovascular system. While practical use is limited by spatial resolution and
image noise, incorporation of trained super-resolution (SR) networks has
potential to enhance image quality post-scan. However, these efforts have
predominantly been restricted to narrowly defined cardiovascular domains, with
limited exploration of how SR performance extends across the cardiovascular
system; a task aggravated by contrasting hemodynamic conditions apparent across
the cardiovasculature. The aim of our study was to explore the generalizability
of SR 4D Flow MRI using a combination of heterogeneous training sets and
dedicated ensemble learning. With synthetic training data generated across
three disparate domains (cardiac, aortic, cerebrovascular), varying
convolutional base and ensemble learners were evaluated as a function of domain
and architecture, quantifying performance on both in-silico and acquired
in-vivo data from the same three domains. Results show that both bagging and
stacking ensembling enhance SR performance across domains, accurately
predicting high-resolution velocities from low-resolution input data in-silico.
Likewise, optimized networks successfully recover native resolution velocities
from downsampled in-vivo data, as well as show qualitative potential in
generating denoised SR-images from clinical level input data. In conclusion,
our work presents a viable approach for generalized SR 4D Flow MRI, with
ensemble learning extending utility across various clinical areas of interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Challenges and Perspectives of Foundation Models for Medical
  Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoting Zhang, Dimitris Metaxas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article discusses the opportunities, applications and future directions
of large-scale pre-trained models, i.e., foundation models, for analyzing
medical images. Medical foundation models have immense potential in solving a
wide range of downstream tasks, as they can help to accelerate the development
of accurate and robust models, reduce the large amounts of required labeled
data, preserve the privacy and confidentiality of patient data. Specifically,
we illustrate the "spectrum" of medical foundation models, ranging from general
vision models, modality-specific models, to organ/task-specific models,
highlighting their challenges, opportunities and applications. We also discuss
how foundation models can be leveraged in downstream medical tasks to enhance
the accuracy and efficiency of medical image analysis, leading to more precise
diagnosis and treatment decisions.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSMeD: Bridging the <span class="highlight-title">Dataset</span> Gap in Automated Citation Screening for
  Systematic Literature <span class="highlight-title">Review</span>s <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wojciech Kusa, Oscar E. Mendoza, Matthias Samwald, Petr Knoth, Allan Hanbury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systematic literature reviews (SLRs) play an essential role in summarising,
synthesising and validating scientific evidence. In recent years, there has
been a growing interest in using machine learning techniques to automate the
identification of relevant studies for SLRs. However, the lack of standardised
evaluation datasets makes comparing the performance of such automated
literature screening systems difficult. In this paper, we analyse the citation
screening evaluation datasets, revealing that many of the available datasets
are either too small, suffer from data leakage or have limited applicability to
systems treating automated literature screening as a classification task, as
opposed to, for example, a retrieval or question-answering task. To address
these challenges, we introduce CSMeD, a meta-dataset consolidating nine
publicly released collections, providing unified access to 325 SLRs from the
fields of medicine and computer science. CSMeD serves as a comprehensive
resource for training and evaluating the performance of automated citation
screening models. Additionally, we introduce CSMeD-FT, a new dataset designed
explicitly for evaluating the full text publication screening task. To
demonstrate the utility of CSMeD, we conduct experiments and establish
baselines on new datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inter<span class="highlight-title">Prompt</span>: Interpretable <span class="highlight-title">Prompt</span>ing for Interrelated Interpersonal Risk
  Factors in Reddit Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MSVPJ Sathvik, Surjodeep Sarkar, Chandni Saxena, Sunghwan Sohn, Muskan Garg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health professionals and clinicians have observed the upsurge of
mental disorders due to Interpersonal Risk Factors (IRFs). To simulate the
human-in-the-loop triaging scenario for early detection of mental health
disorders, we recognized textual indications to ascertain these IRFs : Thwarted
Belongingness (TBe) and Perceived Burdensomeness (PBu) within personal
narratives. In light of this, we use N-shot learning with GPT-3 model on the
IRF dataset, and underscored the importance of fine-tuning GPT-3 model to
incorporate the context-specific sensitivity and the interconnectedness of
textual cues that represent both IRFs.
  In this paper, we introduce an Interpretable Prompting (InterPrompt)} method
to boost the attention mechanism by fine-tuning the GPT-3 model. This allows a
more sophisticated level of language modification by adjusting the pre-trained
weights. Our model learns to detect usual patterns and underlying connections
across both the IRFs, which leads to better system-level explainability and
trustworthiness. The results of our research demonstrate that all four variants
of GPT-3 model, when fine-tuned with InterPrompt, perform considerably better
as compared to the baseline methods, both in terms of classification and
explanation generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear-time online visibility graph transformation algorithm: for both
  natural and horizontal visibility criteria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Huang, Yong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visibility graph (VG) transformation is a technique used to convert a time
series into a graph based on specific visibility criteria. It has attracted
increasing interest in the fields of time series analysis, forecasting, and
classification. Optimizing the VG transformation algorithm to accelerate the
process is a critical aspect of VG-related research, as it enhances the
applicability of VG transformation in latency-sensitive areas and conserves
computational resources. In the real world, many time series are presented in
the form of data streams. Despite the proposal of the concept of VG's online
functionality, previous studies have not thoroughly explored the acceleration
of VG transformation by leveraging the characteristics of data streams. In this
paper, we propose that an efficient online VG algorithm should adhere to two
criteria and develop a linear-time method, termed the LOT framework, for both
natural and horizontal visibility graph transformations in data stream
scenarios. Experiments are conducted on two datasets, comparing our approach
with five existing methods as baselines. The results demonstrate the validity
and promising computational efficiency of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Language Models for Tour Itinerary Recommendation <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ngai Lam Ho, Kwan Hui Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tour itinerary recommendation involves planning a sequence of relevant
Point-of-Interest (POIs), which combines challenges from the fields of both
Operations Research (OR) and Recommendation Systems (RS). As an OR problem,
there is the need to maximize a certain utility (e.g., popularity of POIs in
the tour) while adhering to some constraints (e.g., maximum time for the tour).
As a RS problem, it is heavily related to problem or filtering or ranking a
subset of POIs that are relevant to a user and recommending it as part of an
itinerary. In this paper, we explore the use of language models for the task of
tour itinerary recommendation and planning. This task has the unique
requirement of recommending personalized POIs relevant to users and planning
these POIs as an itinerary that satisfies various constraints. We discuss some
approaches in this area, such as using word embedding techniques like Word2Vec
and GloVe for learning POI embeddings and transformer-based techniques like
BERT for generating
  itineraries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PMAI23 @IJCAI 2023 2nd International Workshop on Process Management
  in the AI era</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Large Language Models for Personalized and Explainable
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Recommender Systems(RS) have witnessed a transformative
shift with the advent of Large Language Models(LLMs) in the field of Natural
Language Processing(NLP). These models such as OpenAI's GPT-3.5/4, Llama from
Meta, have demonstrated unprecedented capabilities in understanding and
generating human-like text. This has led to a paradigm shift in the realm of
personalized and explainable recommendations, as LLMs offer a versatile toolset
for processing vast amounts of textual data to enhance user experiences. To
provide a comprehensive understanding of the existing LLM-based recommendation
systems, this survey aims to analyze how RS can benefit from LLM-based
methodologies. Furthermore, we describe major challenges in Personalized
Explanation Generating(PEG) tasks, which are cold-start problems, unfairness
and bias problems in RS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Ordinary Differential Equations-based method for
  Collaborative Filtering <span class="chip">ICDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Xu, Yuanjie Zhu, Weizhi Zhang, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolution Networks (GCNs) are widely considered state-of-the-art for
collaborative filtering. Although several GCN-based methods have been proposed
and achieved state-of-the-art performance in various tasks, they can be
computationally expensive and time-consuming to train if too many layers are
created. However, since the linear GCN model can be interpreted as a
differential equation, it is possible to transfer it to an ODE problem. This
inspired us to address the computational limitations of GCN-based models by
designing a simple and efficient NODE-based model that can skip some GCN layers
to reach the final state, thus avoiding the need to create many layers. In this
work, we propose a Graph Neural Ordinary Differential Equation-based method for
Collaborative Filtering (GODE-CF). This method estimates the final embedding by
utilizing the information captured by one or two GCN layers. To validate our
approach, we conducted experiments on multiple datasets. The results
demonstrate that our model outperforms competitive baselines, including
GCN-based models and other state-of-the-art CF methods. Notably, our proposed
GODE-CF model has several advantages over traditional GCN-based models. It is
simple, efficient, and has a fast training time, making it a practical choice
for real-world situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting LLMs for Efficient, Personalized Information Retrieval: Methods
  and Implications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samira Ghodratnama, Mehrdad Zakershahrak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) heralds a pivotal shift in online
user interactions with information. Traditional Information Retrieval (IR)
systems primarily relied on query-document matching, whereas LLMs excel in
comprehending and generating human-like text, thereby enriching the IR
experience significantly. While LLMs are often associated with chatbot
functionalities, this paper extends the discussion to their explicit
application in information retrieval. We explore methodologies to optimize the
retrieval process, select optimal models, and effectively scale and orchestrate
LLMs, aiming for cost-efficiency and enhanced result accuracy. A notable
challenge, model hallucination-where the model yields inaccurate or
misinterpreted data-is addressed alongside other model-specific hurdles. Our
discourse extends to crucial considerations including user privacy, data
optimization, and the necessity for system clarity and interpretability.
Through a comprehensive examination, we unveil not only innovative strategies
for integrating Language Models (LLMs) with Information Retrieval (IR) systems,
but also the consequential considerations that underline the need for a
balanced approach aligned with user-centric principles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equipping <span class="highlight-title">Pretrain</span>ed Unconditional Music <span class="highlight-title">Transformer</span>s with Instrument
  and Genre Controls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihan Xu, Julian McAuley, Shlomo Dubnov, Hao-Wen Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ''pretraining-and-finetuning'' paradigm has become a norm for training
domain-specific models in natural language processing and computer vision. In
this work, we aim to examine this paradigm for symbolic music generation
through leveraging the largest ever symbolic music dataset sourced from the
MuseScore forum. We first pretrain a large unconditional transformer model
using 1.5 million songs. We then propose a simple technique to equip this
pretrained unconditional music transformer model with instrument and genre
controls by finetuning the model with additional control tokens. Our proposed
representation offers improved high-level controllability and expressiveness
against two existing representations. The experimental results show that the
proposed model can successfully generate music with user-specified instruments
and genre. In a subjective listening test, the proposed model outperforms the
pretrained baseline model in terms of coherence, harmony, arrangement and
overall quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't forget private retrieval: distributed private similarity search
  for large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Zyskind, Tobin South, Alex Pentland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the flexible capabilities of large language models (LLMs) allow them to
answer a range of queries based on existing learned knowledge, information
retrieval to augment generation is an important tool to allow LLMs to answer
questions on information not included in pre-training data. Such private
information is increasingly being generated in a wide array of distributed
contexts by organizations and individuals. Performing such information
retrieval using neural embeddings of queries and documents always leaked
information about queries and database content unless both were stored locally.
We present Private Retrieval Augmented Generation (PRAG), an approach that uses
multi-party computation (MPC) to securely transmit queries to a distributed set
of servers containing a privately constructed database to return top-k and
approximate top-k documents. This is a first-of-its-kind approach to dense
information retrieval that ensures no server observes a client's query or can
see the database content. The approach introduces a novel MPC friendly protocol
for inverted file approximate search (IVF) that allows for fast document search
over distributed and private data in sublinear communication complexity. This
work presents new avenues through which data for use in LLMs can be accessed
and used without needing to centralize or forgo privacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attribute-Aware Deep Hashing with Self-Consistency for Large-Scale
  Fine-Grained Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiu-Shen Wei, Yang Shen, Xuhao Sun, Peng Wang, Yuxin Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our work focuses on tackling large-scale fine-grained image retrieval as
ranking the images depicting the concept of interests (i.e., the same
sub-category labels) highest based on the fine-grained details in the query. It
is desirable to alleviate the challenges of both fine-grained nature of small
inter-class variations with large intra-class variations and explosive growth
of fine-grained data for such a practical task. In this paper, we propose
attribute-aware hashing networks with self-consistency for generating
attribute-aware hash codes to not only make the retrieval process efficient,
but also establish explicit correspondences between hash codes and visual
attributes. Specifically, based on the captured visual representations by
attention, we develop an encoder-decoder structure network of a reconstruction
task to unsupervisedly distill high-level attribute-specific vectors from the
appearance-specific visual representations without attribute annotations. Our
models are also equipped with a feature decorrelation constraint upon these
attribute vectors to strengthen their representative abilities. Then, driven by
preserving original entities' similarity, the required hash codes can be
generated from these attribute-specific vectors and thus become
attribute-aware. Furthermore, to combat simplicity bias in deep hashing, we
consider the model design from the perspective of the self-consistency
principle and propose to further enhance models' self-consistency by equipping
an additional image reconstruction path. Comprehensive quantitative experiments
under diverse empirical settings on six fine-grained retrieval datasets and two
generic retrieval datasets show the superiority of our models over competing
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relphormer: Relational Graph <span class="highlight-title">Transformer</span> for Knowledge Graph
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10852v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10852v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Bi, Siyuan Cheng, Jing Chen, Xiaozhuan Liang, Feiyu Xiong, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have achieved remarkable performance in widespread fields,
including natural language processing, computer vision and graph mining.
However, vanilla Transformer architectures have not yielded promising
improvements in the Knowledge Graph (KG) representations, where the
translational distance paradigm dominates this area. Note that vanilla
Transformer architectures struggle to capture the intrinsically heterogeneous
structural and semantic information of knowledge graphs. To this end, we
propose a new variant of Transformer for knowledge graph representations dubbed
Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample
contextualized sub-graph sequences as the input to alleviate the heterogeneity
issue. We propose a novel structure-enhanced self-attention mechanism to encode
the relational information and keep the semantic information within entities
and relations. Moreover, we utilize masked knowledge modeling for general
knowledge graph representation learning, which can be applied to various
KG-based tasks including knowledge graph completion, question answering, and
recommendation. Experimental results on six datasets show that Relphormer can
obtain better performance compared with baselines. Code is available in
https://github.com/zjunlp/Relphormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurocomputing 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Budgeted Embedding Table For Recommender Systems <span class="chip">WSDM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14884v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14884v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunke Qu, Tong Chen, Quoc Viet Hung Nguyen, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  At the heart of contemporary recommender systems (RSs) are latent factor
models that provide quality recommendation experience to users. These models
use embedding vectors, which are typically of a uniform and fixed size, to
represent users and items. As the number of users and items continues to grow,
this design becomes inefficient and hard to scale. Recent lightweight embedding
methods have enabled different users and items to have diverse embedding sizes,
but are commonly subject to two major drawbacks. Firstly, they limit the
embedding size search to optimizing a heuristic balancing the recommendation
quality and the memory complexity, where the trade-off coefficient needs to be
manually tuned for every memory budget requested. The implicitly enforced
memory complexity term can even fail to cap the parameter usage, making the
resultant embedding table fail to meet the memory budget strictly. Secondly,
most solutions, especially reinforcement learning based ones derive and
optimize the embedding size for each each user/item on an instance-by-instance
basis, which impedes the search efficiency. In this paper, we propose Budgeted
Embedding Table (BET), a novel method that generates table-level actions (i.e.,
embedding sizes for all users and items) that is guaranteed to meet
pre-specified memory budgets. Furthermore, by leveraging a set-based action
formulation and engaging set representation learning, we present an innovative
action search strategy powered by an action fitness predictor that efficiently
evaluates each table-level action. Experiments have shown state-of-the-art
performance on two real-world datasets when BET is paired with three popular
recommender models under different memory budgets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WSDM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-optimized Contrastive Learning for Sequential Recommendation <span class="chip">SIGIR2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07763v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07763v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuyuan Qin, Huanhuan Yuan, Pengpeng Zhao, Junhua Fang, Fuzhen Zhuang, Guanfeng Liu, Victor Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Learning (CL) performances as a rising approach to address the
challenge of sparse and noisy recommendation data. Although having achieved
promising results, most existing CL methods only perform either hand-crafted
data or model augmentation for generating contrastive pairs to find a proper
augmentation operation for different datasets, which makes the model hard to
generalize. Additionally, since insufficient input data may lead the encoder to
learn collapsed embeddings, these CL methods expect a relatively large number
of training data (e.g., large batch size or memory bank) to contrast. However,
not all contrastive pairs are always informative and discriminative enough for
the training processing. Therefore, a more general CL-based recommendation
model called Meta-optimized Contrastive Learning for sequential Recommendation
(MCLRec) is proposed in this work. By applying both data augmentation and
learnable model augmentation operations, this work innovates the standard CL
framework by contrasting data and model augmented views for adaptively
capturing the informative features hidden in stochastic data augmentation.
Moreover, MCLRec utilizes a meta-learning manner to guide the updating of the
model augmenters, which helps to improve the quality of contrastive pairs
without enlarging the amount of input data. Finally, a contrastive
regularization term is considered to encourage the augmentation model to
generate more informative augmented views and avoid too similar contrastive
pairs within the meta updating. The experimental results on commonly used
datasets validate the effectiveness of MCLRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 Pages,8 figures,SIGIR2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03488v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03488v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derek Lilienthal, Paul Mello, Magdalini Eirinaki, Stas Tiomkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recommender systems have become an integral component of the Web
experience, their heavy reliance on user data raises privacy and security
concerns. Substituting user data with synthetic data can address these
concerns, but accurately replicating these real-world datasets has been a
notoriously challenging problem. Recent advancements in generative AI have
demonstrated the impressive capabilities of diffusion models in generating
realistic data across various domains. In this work we introduce a Score-based
Diffusion Recommendation Module (SDRM), which captures the intricate patterns
of real-world datasets required for training highly accurate recommender
systems. SDRM allows for the generation of synthetic data that can replace
existing datasets to preserve user privacy, or augment existing datasets to
address excessive data sparsity. Our method outperforms competing baselines
such as generative adversarial networks, variational autoencoders, and recently
proposed diffusion models in synthesizing various datasets to replace or
augment the original data by an average improvement of 4.30% in Recall@$k$ and
4.65% in NDCG@$k$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChoralSynth: Synthetic <span class="highlight-title">Dataset</span> of Choral Singing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jyoti Narang, Viviana De La Vega, Xavier Lizarraga, Oscar Mayor, Hector Parra, Jordi Janer, Xavier Serra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Choral singing, a widely practiced form of ensemble singing, lacks
comprehensive datasets in the realm of Music Information Retrieval (MIR)
research, due to challenges arising from the requirement to curate multitrack
recordings. To address this, we devised a novel methodology, leveraging
state-of-the-art synthesizers to create and curate quality renditions. The
scores were sourced from Choral Public Domain Library(CPDL). This work is done
in collaboration with a diverse team of musicians, software engineers and
researchers. The resulting dataset, complete with its associated metadata, and
methodology is released as part of this work, opening up new avenues for
exploration and advancement in the field of singing voice research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset Link: https://doi.org/10.5281/zenodo.10137883</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">169</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-guided Shape-from-Template: Monocular Video Perception through
  Neural Surrogate Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Stotko, Nils Wandel, Reinhard Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction of dynamic scenes is a long-standing problem in computer
graphics and increasingly difficult the less information is available.
Shape-from-Template (SfT) methods aim to reconstruct a template-based geometry
from RGB images or video sequences, often leveraging just a single monocular
camera without depth information, such as regular smartphone recordings.
Unfortunately, existing reconstruction methods are either unphysical and noisy
or slow in optimization. To solve this problem, we propose a novel SfT
reconstruction algorithm for cloth using a pre-trained neural surrogate model
that is fast to evaluate, stable, and produces smooth reconstructions due to a
regularizing physics simulation. Differentiable rendering of the simulated mesh
enables pixel-wise comparisons between the reconstruction and a target video
sequence that can be used for a gradient-based optimization procedure to
extract not only shape information but also physical parameters such as
stretching, shearing, or bending stiffness of the cloth. This allows to retain
a precise, stable, and smooth reconstructed geometry while reducing the runtime
by a factor of 400-500 compared to $\phi$-SfT, a state-of-the-art physics-based
SfT approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanistically analyzing the effects of fine-tuning on procedurally
  defined tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rocktäschel, David Scott Krueger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large pre-trained models has become the de facto strategy for
developing both task-specific and general-purpose machine learning systems,
including developing models that are safe to deploy. Despite its clear
importance, there has been minimal work that explains how fine-tuning alters
the underlying capabilities learned by a model during pretraining: does
fine-tuning yield entirely novel capabilities or does it just modulate existing
ones? We address this question empirically in synthetic, controlled settings
where we can use mechanistic interpretability tools (e.g., network pruning and
probing) to understand how the model's underlying capabilities are changing. We
perform an extensive analysis of the effects of fine-tuning in these settings,
and show that: (i) fine-tuning rarely alters the underlying model capabilities;
(ii) a minimal transformation, which we call a 'wrapper', is typically learned
on top of the underlying model capabilities, creating the illusion that they
have been modified; and (iii) further fine-tuning on a task where such hidden
capabilities are relevant leads to sample-efficient 'revival' of the
capability, i.e., the model begins reusing these capability after only a few
gradient steps. This indicates that practitioners can unintentionally remove a
model's safety wrapper merely by fine-tuning it on a, e.g., superficially
unrelated, downstream task. We additionally perform analysis on language models
trained on the TinyStories dataset to support our claims in a more realistic
setup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimality in Mean Estimation: Beyond Worst-Case, Beyond Sub-Gaussian,
  and Beyond $1+α$ Moments <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung Dang, Jasper C. H. Lee, Maoyuan Song, Paul Valiant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is growing interest in improving our algorithmic understanding of
fundamental statistical problems such as mean estimation, driven by the goal of
understanding the limits of what we can extract from valuable data. The state
of the art results for mean estimation in $\mathbb{R}$ are 1) the optimal
sub-Gaussian mean estimator by [LV22], with the tight sub-Gaussian constant for
all distributions with finite but unknown variance, and 2) the analysis of the
median-of-means algorithm by [BCL13] and a lower bound by [DLLO16],
characterizing the big-O optimal errors for distributions for which only a
$1+\alpha$ moment exists for $\alpha \in (0,1)$. Both results, however, are
optimal only in the worst case. We initiate the fine-grained study of the mean
estimation problem: Can algorithms leverage useful features of the input
distribution to beat the sub-Gaussian rate, without explicit knowledge of such
features?
  We resolve this question with an unexpectedly nuanced answer: "Yes in limited
regimes, but in general no". For any distribution $p$ with a finite mean, we
construct a distribution $q$ whose mean is well-separated from $p$'s, yet $p$
and $q$ are not distinguishable with high probability, and $q$ further
preserves $p$'s moments up to constants. The main consequence is that no
reasonable estimator can asymptotically achieve better than the sub-Gaussian
error rate for any distribution, matching the worst-case result of [LV22]. More
generally, we introduce a new definitional framework to analyze the
fine-grained optimality of algorithms, which we call "neighborhood optimality",
interpolating between the unattainably strong "instance optimality" and the
trivially weak "admissibility" definitions. Applying the new framework, we show
that median-of-means is neighborhood optimal, up to constant factors. It is
open to find a neighborhood-optimal estimator without constant factor
slackness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, to appear in NeurIPS 2023. Abstract shortened to fit arXiv
  limit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Impairment and Disease Severity Using AI Models Trained on
  Healthy Subjects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Yu, Aakash Kaku, Kangning Liu, Avinash Parnandi, Emily Fokas, Anita Venkatesan, Natasha Pandit, Rajesh Ranganath, Heidi Schambra, Carlos Fernandez-Granda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic assessment of impairment and disease severity is a key challenge in
data-driven medicine. We propose a novel framework to address this challenge,
which leverages AI models trained exclusively on healthy individuals. The
COnfidence-Based chaRacterization of Anomalies (COBRA) score exploits the
decrease in confidence of these models when presented with impaired or diseased
patients to quantify their deviation from the healthy population. We applied
the COBRA score to address a key limitation of current clinical evaluation of
upper-body impairment in stroke patients. The gold-standard Fugl-Meyer
Assessment (FMA) requires in-person administration by a trained assessor for
30-45 minutes, which restricts monitoring frequency and precludes physicians
from adapting rehabilitation protocols to the progress of each patient. The
COBRA score, computed automatically in under one minute, is shown to be
strongly correlated with the FMA on an independent test cohort for two
different data modalities: wearable sensors ($\rho = 0.845$, 95% CI
[0.743,0.908]) and video ($\rho = 0.746$, 95% C.I [0.594, 0.847]). To
demonstrate the generalizability of the approach to other conditions, the COBRA
score was also applied to quantify severity of knee osteoarthritis from
magnetic-resonance imaging scans, again achieving significant correlation with
an independent clinical assessment ($\rho = 0.644$, 95% C.I [0.585,0.696]).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-resolution Image-based Malware Classification using Multiple
  Instance Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Peters, Hikmat Farhat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel method of classifying malware into families using
high-resolution greyscale images and multiple instance learning to overcome
adversarial binary enlargement. Current methods of visualisation-based malware
classification largely rely on lossy transformations of inputs such as resizing
to handle the large, variable-sized images. Through empirical analysis and
experimentation, it is shown that these approaches cause crucial information
loss that can be exploited. The proposed solution divides the images into
patches and uses embedding-based multiple instance learning with a
convolutional neural network and an attention aggregation function for
classification. The implementation is evaluated on the Microsoft Malware
Classification dataset and achieves accuracies of up to $96.6\%$ on
adversarially enlarged samples compared to the baseline of $22.8\%$. The Python
code is available online at https://github.com/timppeters/MIL-Malware-Images .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 13 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfOcc: <span class="highlight-title">Self-Supervised</span> Vision-Based 3D Occupancy Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhui Huang, Wenzhao Zheng, Borui Zhang, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D occupancy prediction is an important task for the robustness of
vision-centric autonomous driving, which aims to predict whether each point is
occupied in the surrounding 3D space. Existing methods usually require 3D
occupancy labels to produce meaningful results. However, it is very laborious
to annotate the occupancy status of each voxel. In this paper, we propose
SelfOcc to explore a self-supervised way to learn 3D occupancy using only video
sequences. We first transform the images into the 3D space (e.g., bird's eye
view) to obtain 3D representation of the scene. We directly impose constraints
on the 3D representations by treating them as signed distance fields. We can
then render 2D images of previous and future frames as self-supervision signals
to learn the 3D representations. We propose an MVS-embedded strategy to
directly optimize the SDF-induced weights with multiple depth proposals. Our
SelfOcc outperforms the previous best method SceneRF by 58.7% using a single
frame as input on SemanticKITTI and is the first self-supervised work that
produces reasonable 3D occupancy for surround cameras on Occ3D. SelfOcc
produces high-quality depth and achieves state-of-the-art results on novel
depth synthesis, monocular depth estimation, and surround-view depth estimation
on the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code:
https://github.com/huang-yh/SelfOcc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/huang-yh/SelfOcc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Optimise Wind Farms with Graph <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyi Li, Arnaud Robert, A. Aldo Faisal, Matthew D. Piggott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a novel data-driven model capable of providing accurate
predictions for the power generation of all wind turbines in wind farms of
arbitrary layout, yaw angle configurations and wind conditions. The proposed
model functions by encoding a wind farm into a fully-connected graph and
processing the graph representation through a graph transformer. The graph
transformer surrogate is shown to generalise well and is able to uncover latent
structural patterns within the graph representation of wind farms. It is
demonstrated how the resulting surrogate model can be used to optimise yaw
angle configurations using genetic algorithms, achieving similar levels of
accuracy to industrially-standard wind farm simulation tools while only taking
a fraction of the computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Transformation for IoT Time-Series Data: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duygu Altunkaya, Feyza Yildirim Okay, Suat Ozdemir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of the Internet of Things (IoT), where smartphones, built-in
systems, wireless sensors, and nearly every smart device connect through local
networks or the internet, billions of smart things communicate with each other
and generate vast amounts of time-series data. As IoT time-series data is
high-dimensional and high-frequency, time-series classification or regression
has been a challenging issue in IoT. Recently, deep learning algorithms have
demonstrated superior performance results in time-series data classification in
many smart and intelligent IoT applications. However, it is hard to explore the
hidden dynamic patterns and trends in time-series. Recent studies show that
transforming IoT data into images improves the performance of the learning
model. In this paper, we present a review of these studies which use image
transformation/encoding techniques in IoT domain. We examine the studies
according to their encoding techniques, data types, and application areas.
Lastly, we emphasize the challenges and future dimensions of image
transformation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Content Augmented Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Gholamzadeh Nasrabadi, AmirHossein Kashani, Pegah Zahedi, Mostafa Haghir Chehreghani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, graph neural networks (GNNs) have become a popular tool for
solving various problems over graphs. In these models, the link structure of
the graph is typically exploited and nodes' embeddings are iteratively updated
based on adjacent nodes. Nodes' contents are used solely in the form of feature
vectors, served as nodes' first-layer embeddings. However, the filters or
convolutions, applied during iterations/layers to these initial embeddings lead
to their impact diminish and contribute insignificantly to the final
embeddings. In order to address this issue, in this paper we propose augmenting
nodes' embeddings by embeddings generating from their content, at higher GNN
layers. More precisely, we propose models wherein a structural embedding using
a GNN and a content embedding are computed for each node. These two are
combined using a combination layer to form the embedding of a node at a given
layer. We suggest methods such as using an auto-encoder or building a content
graph, to generate content embeddings. In the end, by conducting experiments
over several real-world datasets, we demonstrate the high accuracy and
performance of our models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Graph Classification Techniques Under Low Data Constraints: A
  Comprehensive Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kush Kothari, Bhavya Mehta, Reshmika Nambiar, Seema Shrawne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey paper presents a brief overview of recent research on graph data
augmentation and few-shot learning. It covers various techniques for graph data
augmentation, including node and edge perturbation, graph coarsening, and graph
generation, as well as the latest developments in few-shot learning, such as
meta-learning and model-agnostic meta-learning. The paper explores these areas
in depth and delves into further sub classifications. Rule based approaches and
learning based approaches are surveyed under graph augmentation techniques.
Few-Shot Learning on graphs is also studied in terms of metric learning
techniques and optimization-based techniques. In all, this paper provides an
extensive array of techniques that can be employed in solving graph processing
problems faced in low-data scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Random Sampling: A Theoretical and Empirical Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Cui, Ashish Mittal, Songtao Lu, Wei Zhang, George Saon, Brian Kingsbury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft random sampling (SRS) is a simple yet effective approach for efficient
training of large-scale deep neural networks when dealing with massive data.
SRS selects a subset uniformly at random with replacement from the full data
set in each epoch. In this paper, we conduct a theoretical and empirical
analysis of SRS. First, we analyze its sampling dynamics including data
coverage and occupancy. Next, we investigate its convergence with non-convex
objective functions and give the convergence rate. Finally, we provide its
generalization performance. We empirically evaluate SRS for image recognition
on CIFAR10 and automatic speech recognition on Librispeech and an in-house
payload dataset to demonstrate its effectiveness. Compared to existing
coreset-based data selection methods, SRS offers a better accuracy-efficiency
trade-off. Especially on real-world industrial scale data sets, it is shown to
be a powerful training strategy with significant speedup and competitive
performance with almost no additional computing cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attacking Motion Planners Using Adversarial Perception Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Sadeghi, Nicholas A. Lord, John Redford, Romain Mueller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving (AD) systems are often built and tested in a modular
fashion, where the performance of different modules is measured using
task-specific metrics. These metrics should be chosen so as to capture the
downstream impact of each module and the performance of the system as a whole.
For example, high perception quality should enable prediction and planning to
be performed safely. Even though this is true in general, we show here that it
is possible to construct planner inputs that score very highly on various
perception quality metrics but still lead to planning failures. In an analogy
to adversarial attacks on image classifiers, we call such inputs
\textbf{adversarial perception errors} and show they can be systematically
constructed using a simple boundary-attack algorithm. We demonstrate the
effectiveness of this algorithm by finding attacks for two different black-box
planners in several urban and highway driving scenarios using the CARLA
simulator. Finally, we analyse the properties of these attacks and show that
they are isolated in the input space of the planner, and discuss their
implications for AD system deployment and testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ minimax: Efficient Baselines for Autocurricula in JAX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minqi Jiang, Michael Dennis, Edward Grefenstette, Tim Rocktäschel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised environment design (UED) is a form of automatic curriculum
learning for training robust decision-making agents to zero-shot transfer into
unseen environments. Such autocurricula have received much interest from the RL
community. However, UED experiments, based on CPU rollouts and GPU model
updates, have often required several weeks of training. This compute
requirement is a major obstacle to rapid innovation for the field. This work
introduces the minimax library for UED training on accelerated hardware. Using
JAX to implement fully-tensorized environments and autocurriculum algorithms,
minimax allows the entire training loop to be compiled for hardware
acceleration. To provide a petri dish for rapid experimentation, minimax
includes a tensorized grid-world based on MiniGrid, in addition to reusable
abstractions for conducting autocurricula in procedurally-generated
environments. With these components, minimax provides strong UED baselines,
including new parallelized variants, which achieve over 120$\times$ speedups in
wall time compared to previous implementations when training with equal batch
sizes. The minimax library is available under the Apache 2.0 license at
https://github.com/facebookresearch/minimax.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ALOE 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attacks of fairness in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Rance, Filip Svoboda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning is an important emerging distributed training paradigm
that keeps data private on clients. It is now well understood that by
controlling only a small subset of FL clients, it is possible to introduce a
backdoor to a federated learning model, in the presence of certain attributes.
In this paper, we present a new type of attack that compromises the fairness of
the trained model. Fairness is understood to be the attribute-level performance
distribution of a trained model. It is particularly salient in domains where,
for example, skewed accuracy discrimination between subpopulations could have
disastrous consequences. We find that by employing a threat model similar to
that of a backdoor attack, an attacker is able to influence the aggregated
model to have an unfair performance distribution between any given set of
attributes. Furthermore, we find that this attack is possible by controlling
only a single client. While combating naturally induced unfairness in FL has
previously been discussed in depth, its artificially induced kind has been
neglected. We show that defending against attacks on fairness should be a
critical consideration in any situation where unfairness in a trained model
could benefit a user who participated in its training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regression-Based Analysis of Multimodal Single-Cell Data Integration
  Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhavya Mehta, Nirmit Deliwala, Madhav Chandane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal single-cell technologies enable the simultaneous collection of
diverse data types from individual cells, enhancing our understanding of
cellular states. However, the integration of these datatypes and modeling the
interrelationships between modalities presents substantial computational and
analytical challenges in disease biomarker detection and drug discovery.
Established practices rely on isolated methodologies to investigate individual
molecular aspects separately, often resulting in inaccurate analyses. To
address these obstacles, distinct Machine Learning Techniques are leveraged,
each of its own kind to model the co-variation of DNA to RNA, and finally to
surface proteins in single cells during hematopoietic stem cell development,
which simplifies understanding of underlying cellular mechanisms and immune
responses. Experiments conducted on a curated subset of a 300,000-cell time
course dataset, highlights the exceptional performance of Echo State Networks,
boasting a remarkable state-of-the-art correlation score of 0.94 and 0.895 on
Multi-omic and CiteSeq datasets. Beyond the confines of this study, these
findings hold promise for advancing comprehension of cellular differentiation
and function, leveraging the potential of Machine Learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Text Classification with Wasserstein Independence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibaud Leteno, Antoine Gourru, Charlotte Laclau, Rémi Emonet, Christophe Gravier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group fairness is a central research topic in text classification, where
reaching fair treatment between sensitive groups (e.g. women vs. men) remains
an open challenge. This paper presents a novel method for mitigating biases in
neural text classification, agnostic to the model architecture. Considering the
difficulty to distinguish fair from unfair information in a text encoder, we
take inspiration from adversarial training to induce Wasserstein independence
between representations learned to predict our target label and the ones
learned to predict some sensitive attribute. Our approach provides two
significant advantages. Firstly, it does not require annotations of sensitive
attributes in both testing and training data. This is more suitable for
real-life scenarios compared to existing methods that require annotations of
sensitive attributes at train time. Second, our approach exhibits a comparable
or better fairness-accuracy trade-off compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Out-of-Distribution Coverage of Combining Split Conformal
  Prediction and Bayesian Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Scemama, Ariel Kapusta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian deep learning and conformal prediction are two methods that have
been used to convey uncertainty and increase safety in machine learning
systems. We focus on combining Bayesian deep learning with split conformal
prediction and how this combination effects out-of-distribution coverage;
particularly in the case of multiclass image classification. We suggest that if
the model is generally underconfident on the calibration set, then the
resultant conformal sets may exhibit worse out-of-distribution coverage
compared to simple predictive credible sets. Conversely, if the model is
overconfident on the calibration set, the use of conformal prediction may
improve out-of-distribution coverage. We evaluate prediction sets as a result
of combining split conformal methods and neural networks trained with (i)
stochastic gradient descent, (ii) deep ensembles, and (iii) mean-field
variational inference. Our results suggest that combining Bayesian deep
learning models with split conformal prediction can, in some cases, cause
unintended consequences such as reducing out-of-distribution coverage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Managing ML-Based Application Non-Functional Behavior: A Multi-Model
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Anisetti, Claudio A. Ardagna, Nicola Bena, Ernesto Damiani, Paolo G. Panero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern applications are increasingly driven by Machine Learning (ML) models
whose non-deterministic behavior is affecting the entire application life cycle
from design to operation. The pervasive adoption of ML is urgently calling for
approaches that guarantee a stable non-functional behavior of ML-based
applications over time and across model changes. To this aim, non-functional
properties of ML models, such as privacy, confidentiality, fairness, and
explainability, must be monitored, verified, and maintained. This need is even
more pressing when modern applications operate in the edge-cloud continuum,
increasing their complexity and dynamicity. Existing approaches mostly focus on
i) implementing classifier selection solutions according to the functional
behavior of ML models, ii) finding new algorithmic solutions to this need, such
as continuous re-training. In this paper, we propose a multi-model approach
built on dynamic classifier selection, where multiple ML models showing similar
non-functional properties are made available to the application and one model
is selected over time according to (dynamic and unpredictable) contextual
changes. Our solution goes beyond the state of the art by providing an
architectural and methodological approach that continuously guarantees a stable
non-functional behavior of ML-based applications, is applicable to different ML
models, and is driven by non-functional properties assessed on the models
themselves. It consists of a two-step process working during application
operation, where model assessment verifies non-functional properties of ML
models trained and selected at development time, and model substitution
guarantees a continuous and stable support of non-functional properties. We
experimentally evaluate our solution in a real-world scenario focusing on
non-functional property fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Reweighting Guided by Wasserstein Distance for Bias
  Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Zhao, Simone Fabbrizzi, Paula Reyero Lobo, Siamak Ghodsi, Klaus Broelemann, Steffen Staab, Gjergji Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unequal representation of different groups in a sample population can
lead to discrimination of minority groups when machine learning models make
automated decisions. To address these issues, fairness-aware machine learning
jointly optimizes two (or more) metrics aiming at predictive effectiveness and
low unfairness. However, the inherent under-representation of minorities in the
data makes the disparate treatment of subpopulations less noticeable and
difficult to deal with during learning. In this paper, we propose a novel
adversarial reweighting method to address such \emph{representation bias}. To
balance the data distribution between the majority and the minority groups, our
approach deemphasizes samples from the majority group. To minimize empirical
risk, our method prefers samples from the majority group that are close to the
minority group as evaluated by the Wasserstein distance. Our theoretical
analysis shows the effectiveness of our adversarial reweighting approach.
Experiments demonstrate that our approach mitigates bias without sacrificing
classification accuracy, outperforming related state-of-the-art methods on
image and tabular benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse
  Multiview Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Albanis, Nikolaos Zioulis, Kostas Kolomvatsos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing smooth motions from videos using markerless techniques typically
involves complex processes such as temporal constraints, multiple stages with
data-driven regression and optimization, and bundle solving over temporal
windows. These processes can be inefficient and require tuning multiple
objectives across stages. In contrast, BundleMoCap introduces a novel and
efficient approach to this problem. It solves the motion capture task in a
single stage, eliminating the need for temporal smoothness objectives while
still delivering smooth motions. BundleMoCap outperforms the state-of-the-art
without increasing complexity. The key concept behind BundleMoCap is manifold
interpolation between latent keyframes. By relying on a local manifold
smoothness assumption, we can efficiently solve a bundle of frames using a
single code. Additionally, the method can be implemented as a sliding window
optimization and requires only the first frame to be properly initialized,
reducing the overall computational burden. BundleMoCap's strength lies in its
ability to achieve high-quality motion capture results with simplicity and
efficiency. More details can be found at https://moverseai.github.io/bundle/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in European Conference on Visual Media Production (CVMP
  '23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretation of the <span class="highlight-title">Transformer</span> and Improvement of the Extractor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been over six years since the Transformer architecture was put
forward. Surprisingly, the vanilla Transformer architecture is still widely
used today. One reason is that the lack of deep understanding and comprehensive
interpretation of the Transformer architecture makes it more challenging to
improve the Transformer architecture. In this paper, we first interpret the
Transformer architecture comprehensively in plain words based on our
understanding and experiences. The interpretations are further proved and
verified. These interpretations also cover the Extractor, a family of drop-in
replacements for the multi-head self-attention in the Transformer architecture.
Then, we propose an improvement on a type of the Extractor that outperforms the
self-attention, without introducing additional trainable parameters.
Experimental results demonstrate that the improved Extractor performs even
better, showing a way to improve the Transformer architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Left-Right Wearable Sensors (IMUs) Consistency Matching for
  HAR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominique Nshimyimana, Vitor Fortes Rey, Paul Lukowic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning algorithms are improving rapidly, but annotating training
data remains a bottleneck for many applications. In this paper, we show how
real data can be used for self-supervised learning without any transformations
by taking advantage of the symmetry present in the activities. Our approach
involves contrastive matching of two different sensors (left and right wrist or
leg-worn IMUs) to make representations of co-occurring sensor data more similar
and those of non-co-occurring sensor data more different. We test our approach
on the Opportunity and MM-Fit datasets. In MM-Fit we show significant
improvement over the baseline supervised and self-supervised method SimCLR,
while for Opportunity there is significant improvement over the supervised
baseline and slight improvement when compared to SimCLR. Moreover, our method
improves supervised baselines even when using only a small amount of the data
for training. Future work should explore under which conditions our method is
beneficial for human activity recognition systems and other related
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ABC 2023. The 5th International Conference on Activity
  and Behavior Computing September 7th - 9th, 2023 in Kaiserslautern, Germany
  (Hybrid)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a more inductive world for drug repurposing approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesus de la Fuente, Guillermo Serrano, Uxía Veleiro, Mikel Casals, Laura Vera, Marija Pizurica, Antonio Pineda-Lucena, Idoia Ochoa, Silve Vicent, Olivier Gevaert, Mikel Hernaez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug-target interaction (DTI) prediction is a challenging, albeit essential
task in drug repurposing. Learning on graph models have drawn special attention
as they can significantly reduce drug repurposing costs and time commitment.
However, many current approaches require high-demanding additional information
besides DTIs that complicates their evaluation process and usability.
Additionally, structural differences in the learning architecture of current
models hinder their fair benchmarking. In this work, we first perform an
in-depth evaluation of current DTI datasets and prediction models through a
robust benchmarking process, and show that DTI prediction methods based on
transductive models lack generalization and lead to inflated performance when
evaluated as previously done in the literature, hence not being suited for drug
repurposing approaches. We then propose a novel biologically-driven strategy
for negative edge subsampling and show through in vitro validation that newly
discovered interactions are indeed true. We envision this work as the
underpinning for future fair benchmarking and robust model design. All
generated resources and tools are publicly available as a python package.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSVEP-DAN: A Data Alignment Network for SSVEP-based Brain Computer
  Interfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sung-Yu Chen, Chi-Min Chang, Kuan-Jung Chiang, Chun-Shu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steady-state visual-evoked potential (SSVEP)-based brain-computer interfaces
(BCIs) offer a non-invasive means of communication through high-speed speller
systems. However, their efficiency heavily relies on individual training data
obtained during time-consuming calibration sessions. To address the challenge
of data insufficiency in SSVEP-based BCIs, we present SSVEP-DAN, the first
dedicated neural network model designed for aligning SSVEP data across
different domains, which can encompass various sessions, subjects, or devices.
Our experimental results across multiple cross-domain scenarios demonstrate
SSVEP-DAN's capability to transform existing source SSVEP data into
supplementary calibration data, significantly enhancing SSVEP decoding accuracy
in scenarios with limited calibration data. We envision SSVEP-DAN as a catalyst
for practical SSVEP-based BCI applications with minimal calibration. The source
codes in this work are available at: https://github.com/CECNL/SSVEP-DAN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Carbohydrate NMR chemical shift predictions using E(3) equivariant graph
  neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Bånkestad, Keven M. Dorst, Göran Widmalm, Jerk Rönnols
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Carbohydrates, vital components of biological systems, are well-known for
their structural diversity. Nuclear Magnetic Resonance (NMR) spectroscopy plays
a crucial role in understanding their intricate molecular arrangements and is
essential in assessing and verifying the molecular structure of organic
molecules. An important part of this process is to predict the NMR chemical
shift from the molecular structure. This work introduces a novel approach that
leverages E(3) equivariant graph neural networks to predict carbohydrate NMR
spectra. Notably, our model achieves a substantial reduction in mean absolute
error, up to threefold, compared to traditional models that rely solely on
two-dimensional molecular structure. Even with limited data, the model excels,
highlighting its robustness and generalization capabilities. The implications
are far-reaching and go beyond an advanced understanding of carbohydrate
structures and spectral interpretation. For example, it could accelerate
research in pharmaceutical applications, biochemistry, and structural biology,
offering a faster and more reliable analysis of molecular structures.
Furthermore, our approach is a key step towards a new data-driven era in
spectroscopy, potentially influencing spectroscopic techniques beyond NMR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedDRO: Federated Compositional Optimization for Distributionally Robust
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Khanduri, Chengyin Li, Rafi Ibn Sultan, Yao Qiang, Joerg Kliewer, Dongxiao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, compositional optimization (CO) has gained popularity because of
its applications in distributionally robust optimization (DRO) and many other
machine learning problems. Large-scale and distributed availability of data
demands the development of efficient federated learning (FL) algorithms for
solving CO problems. Developing FL algorithms for CO is particularly
challenging because of the compositional nature of the objective. Moreover,
current state-of-the-art methods to solve such problems rely on large batch
gradients (depending on the solution accuracy) not feasible for most practical
settings. To address these challenges, in this work, we propose efficient
FedAvg-type algorithms for solving non-convex CO in the FL setting. We first
establish that vanilla FedAvg is not suitable to solve distributed CO problems
because of the data heterogeneity in the compositional objective at each client
which leads to the amplification of bias in the local compositional gradient
estimates. To this end, we propose a novel FL framework FedDRO that utilizes
the DRO problem structure to design a communication strategy that allows FedAvg
to control the bias in the estimation of the compositional gradient. A key
novelty of our work is to develop solution accuracy-independent algorithms that
do not require large batch gradients (and function evaluations) for solving
federated CO problems. We establish $\mathcal{O}(\epsilon^{-2})$ sample and
$\mathcal{O}(\epsilon^{-3/2})$ communication complexity in the FL setting while
achieving linear speedup with the number of clients. We corroborate our
theoretical findings with empirical studies on large-scale DRO problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 Pages, 6 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Careful Selection and Thoughtful Discarding: Graph Explicit Pooling
  Utilizing Discarded Nodes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Liu, Wenhang Yu, Kuang Gao, Xueqi Ma, Yibing Zhan, Jia Wu, Bo Du, Wenbin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph pooling has been increasingly recognized as crucial for Graph Neural
Networks (GNNs) to facilitate hierarchical graph representation learning.
Existing graph pooling methods commonly consist of two stages: selecting
top-ranked nodes and discarding the remaining to construct coarsened graph
representations. However, this paper highlights two key issues with these
methods: 1) The process of selecting nodes to discard frequently employs
additional Graph Convolutional Networks or Multilayer Perceptrons, lacking a
thorough evaluation of each node's impact on the final graph representation and
subsequent prediction tasks. 2) Current graph pooling methods tend to directly
discard the noise segment (dropped) of the graph without accounting for the
latent information contained within these elements. To address the first issue,
we introduce a novel Graph Explicit Pooling (GrePool) method, which selects
nodes by explicitly leveraging the relationships between the nodes and final
representation vectors crucial for classification. The second issue is
addressed using an extended version of GrePool (i.e., GrePool+), which applies
a uniform loss on the discarded nodes. This addition is designed to augment the
training process and improve classification accuracy. Furthermore, we conduct
comprehensive experiments across 12 widely used datasets to validate our
proposed method's effectiveness, including the Open Graph Benchmark datasets.
Our experimental results uniformly demonstrate that GrePool outperforms 14
baseline methods for most datasets. Likewise, implementing GrePool+ enhances
GrePool's performance without incurring additional computational costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, 4 tables. Submitting to Science China
  Information Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Joint Graph Learning and Multivariate Time Series
  Forecasting <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juhyeon Kim, Hyungeun Lee, Seungwon Yu, Ung Hwang, Wooyul Jung, Miseon Park, Kijung Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series is prevalent in many scientific and industrial
domains. Modeling multivariate signals is challenging due to their long-range
temporal dependencies and intricate interactions--both direct and indirect. To
confront these complexities, we introduce a method of representing multivariate
signals as nodes in a graph with edges indicating interdependency between them.
Specifically, we leverage graph neural networks (GNN) and attention mechanisms
to efficiently learn the underlying relationships within the time series data.
Moreover, we suggest employing hierarchical signal decompositions running over
the graphs to capture multiple spatial dependencies. The effectiveness of our
proposed model is evaluated across various real-world benchmark datasets
designed for long-term forecasting tasks. The results consistently showcase the
superiority of our model, achieving an average 23\% reduction in mean squared
error (MSE) compared to existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Temporal Graph Learning Workshop @ NeurIPS 2023, New Orleans, United
  States</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Algorithmic Information Theory and Machine Learning: A New
  Approach to Kernel Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boumediene Hamzi, Marcus Hutter, Houman Owhadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) and Algorithmic Information Theory (AIT) look at
Complexity from different points of view. We explore the interface between AIT
and Kernel Methods (that are prevalent in ML) by adopting an AIT perspective on
the problem of learning kernels from data, in kernel ridge regression, through
the method of Sparse Kernel Flows. In particular, by looking at the differences
and commonalities between Minimal Description Length (MDL) and Regularization
in Machine Learning (RML), we prove that the method of Sparse Kernel Flows is
the natural approach to adopt to learn kernels from data. This paper shows that
it is not necessary to use the statistical route to derive Sparse Kernel Flows
and that one can directly work with code-lengths and complexities that are
concepts that show up in AIT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An earlier version of this paper appeared at
  https://www.researchgate.net/publication/371875631_A_note_on_learning_kernels_from_data_from_an_Algorithmic_Information_Theoretic_point_of_view.
  arXiv admin note: text overlap with arXiv:2111.13037, arXiv:2007.05074</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Koopman Learning with Episodic Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William T. Redman, Dean Huang, Maria Fonoberova, Igor Mezić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Koopman operator theory, a data-driven dynamical systems framework, has found
significant success in learning models from complex, real-world data sets,
enabling state-of-the-art prediction and control. The greater interpretability
and lower computational costs of these models, compared to traditional machine
learning methodologies, make Koopman learning an especially appealing approach.
Despite this, little work has been performed on endowing Koopman learning with
the ability to learn from its own mistakes. To address this, we equip Koopman
methods - developed for predicting non-stationary time-series - with an
episodic memory mechanism, enabling global recall of (or attention to) periods
in time where similar dynamics previously occurred. We find that a basic
implementation of Koopman learning with episodic memory leads to significant
improvements in prediction on synthetic and real-world data. Our framework has
considerable potential for expansion, allowing for future advances, and opens
exciting new directions for Koopman learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralised Q-Learning for Multi-Agent Markov Decision Processes with
  a Satisfiability Criterion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keshav P. Keval, Vivek S. Borkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a reinforcement learning algorithm to solve a
multi-agent Markov decision process (MMDP). The goal, inspired by Blackwell's
Approachability Theorem, is to lower the time average cost of each agent to
below a pre-specified agent-specific bound. For the MMDP, we assume the state
dynamics to be controlled by the joint actions of agents, but the per-stage
costs to only depend on the individual agent's actions. We combine the
Q-learning algorithm for a weighted combination of the costs of each agent,
obtained by a gossip algorithm with the Metropolis-Hastings or Multiplicative
Weights formalisms to modulate the averaging matrix of the gossip. We use
multiple timescales in our algorithm and prove that under mild conditions, it
approximately achieves the desired bounds for each of the agents. We also
demonstrate the empirical performance of this algorithm in the more general
setting of MMDPs having jointly controlled per-stage costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New Type Of Upper And Lower Bounds On Right-Tail Probabilities Of
  Continuous Random Variables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola Zlatanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, I present a completely new type of upper and lower bounds on
the right-tail probabilities of continuous random variables with unbounded
support and with semi-bounded support from the left. The presented upper and
lower right-tail bounds depend only on the probability density function (PDF),
its first derivative, and two parameters that are used for tightening the
bounds. These tail bounds hold under certain conditions that depend on the PDF,
its first and second derivatives, and the two parameters. The new tail bounds
are shown to be tight for a wide range of continuous random variables via
numerical examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TouchSDF: A DeepSDF Approach for 3D Shape Reconstruction using
  Vision-Based Tactile Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mauro Comi, Yijiong Lin, Alex Church, Alessio Tonioni, Laurence Aitchison, Nathan F. Lepora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans rely on their visual and tactile senses to develop a comprehensive 3D
understanding of their physical environment. Recently, there has been a growing
interest in exploring and manipulating objects using data-driven approaches
that utilise high-resolution vision-based tactile sensors. However, 3D shape
reconstruction using tactile sensing has lagged behind visual shape
reconstruction because of limitations in existing techniques, including the
inability to generalise over unseen shapes, the absence of real-world testing,
and limited expressive capacity imposed by discrete representations. To address
these challenges, we propose TouchSDF, a Deep Learning approach for tactile 3D
shape reconstruction that leverages the rich information provided by a
vision-based tactile sensor and the expressivity of the implicit neural
representation DeepSDF. Our technique consists of two components: (1) a
Convolutional Neural Network that maps tactile images into local meshes
representing the surface at the touch location, and (2) an implicit neural
function that predicts a signed distance function to extract the desired 3D
shape. This combination allows TouchSDF to reconstruct smooth and continuous 3D
shapes from tactile inputs in simulation and real-world settings, opening up
research avenues for robust 3D-aware representations and improved multimodal
perception in robotics. Code and supplementary material are available at:
https://touchsdf.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning-based detection of morphological features associated with
  hypoxia in H&E breast cancer whole slide images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petru Manescu, Joseph Geradts, Delmiro Fernandez-Reyes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypoxia occurs when tumour cells outgrow their blood supply, leading to
regions of low oxygen levels within the tumour. Calculating hypoxia levels can
be an important step in understanding the biology of tumours, their clinical
progression and response to treatment. This study demonstrates a novel
application of deep learning to evaluate hypoxia in the context of breast
cancer histomorphology. More precisely, we show that Weakly Supervised Deep
Learning (WSDL) models can accurately detect hypoxia associated features in
routine Hematoxylin and Eosin (H&E) whole slide images (WSI). We trained and
evaluated a deep Multiple Instance Learning model on tiles from WSI H&E tissue
from breast cancer primary sites (n=240) obtaining on average an AUC of 0.87 on
a left-out test set. We also showed significant differences between features of
hypoxic and normoxic tissue regions as distinguished by the WSDL models. Such
DL hypoxia H&E WSI detection models could potentially be extended to other
tumour types and easily integrated into the pathology workflow without
requiring additional costly assays.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChronoPscychosis: Temporal Segmentation and Its Impact on Schizophrenia
  Classification Using Motor Activity Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pradnya Rajendra Jadhav, Raviprasad Aduri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schizophrenia is a complicated mental illness characterized by a broad
spectrum of symptoms affecting cognition, behavior, and emotion. The task of
identifying reliable biomarkers to classify Schizophrenia accurately continues
to be a challenge in the field of psychiatry. We investigate the temporal
patterns within the motor activity data as a potential key to enhancing the
categorization of individuals with Schizophrenia, using the dataset having
motor activity recordings of 22 Schizophrenia patients and 32 control subjects.
The dataset contains per-minute motor activity measurements collected for an
average of 12.7 days in a row for each participant. We dissect each day into
segments (Twelve, Eight, six, four, three, and two parts) and evaluate their
impact on classification. We employ sixteen statistical features within these
temporal segments and train them on Seven machine learning models to get deeper
insights. LightGBM model outperforms the other six models. Our results indicate
that the temporal segmentation significantly improves the classification, with
AUC-ROC = 0.93, F1 score = 0.84( LightGBM- without any segmentation) and
AUC-ROC = 0.98, F1 score = 0.93( LightGBM- with segmentation). Distinguishing
between diurnal and nocturnal segments amplifies the differences between
Schizophrenia patients and controls. However, further subdivisions into smaller
time segments do not affect the AUC- ROC significantly. Morning, afternoon,
evening, and night partitioning gives similar classification performance to
day-night partitioning. These findings are valuable as they indicate that
extensive temporal classification beyond distinguishing between day and night
does not yield substantial results, offering an efficient approach for further
classification, early diagnosis, and monitoring of Schizophrenia.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Source-Free Target Adaptation with Vision <span class="highlight-title">Transformer</span>s
  Leveraging Domain Representation Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gauransh Sawhney, Daksh Dave, Adeel Ahmed, Jiechao Gao, Khalid Saleem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Domain Adaptation (UDA) methods facilitate knowledge transfer
from a labeled source domain to an unlabeled target domain, navigating the
obstacle of domain shift. While Convolutional Neural Networks (CNNs) are a
staple in UDA, the rise of Vision Transformers (ViTs) provides new avenues for
domain generalization. This paper presents an innovative method to bolster ViT
performance in source-free target adaptation, beginning with an evaluation of
how key, query, and value elements affect ViT outcomes. Experiments indicate
that altering the key component has negligible effects on Transformer
performance. Leveraging this discovery, we introduce Domain Representation
Images (DRIs), feeding embeddings through the key element. DRIs act as
domain-specific markers, effortlessly merging with the training regimen. To
assess our method, we perform target adaptation tests on the Cross Instance DRI
source-only (SO) control. We measure the efficacy of target adaptation with and
without DRIs, against existing benchmarks like SHOT-B* and adaptations via
CDTrans. Findings demonstrate that excluding DRIs offers limited gains over
SHOT-B*, while their inclusion in the key segment boosts average precision
promoting superior domain generalization. This research underscores the vital
role of DRIs in enhancing ViT efficiency in UDA scenarios, setting a precedent
for further domain adaptation explorations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine-Guided Discovery of a Real-World Rogue Wave Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dion Häfner, Johannes Gemmrich, Markus Jochum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Big data and large-scale machine learning have had a profound impact on
science and engineering, particularly in fields focused on forecasting and
prediction. Yet, it is still not clear how we can use the superior pattern
matching abilities of machine learning models for scientific discovery. This is
because the goals of machine learning and science are generally not aligned. In
addition to being accurate, scientific theories must also be causally
consistent with the underlying physical process and allow for human analysis,
reasoning, and manipulation to advance the field. In this paper, we present a
case study on discovering a new symbolic model for oceanic rogue waves from
data using causal analysis, deep learning, parsimony-guided model selection,
and symbolic regression. We train an artificial neural network on causal
features from an extensive dataset of observations from wave buoys, while
selecting for predictive performance and causal invariance. We apply symbolic
regression to distill this black-box model into a mathematical equation that
retains the neural network's predictive capabilities, while allowing for
interpretation in the context of existing wave theory. The resulting model
reproduces known behavior, generates well-calibrated probabilities, and
achieves better predictive scores on unseen data than current theory. This
showcases how machine learning can facilitate inductive scientific discovery,
and paves the way for more accurate rogue wave forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Moderating Model Marketplaces: Platform Governance Puzzles for AI
  Intermediaries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Gorwa, Michael Veale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The AI development community is increasingly making use of hosting
intermediaries such as Hugging Face provide easy access to user-uploaded models
and training data. These model marketplaces lower technical deployment barriers
for hundreds of thousands of users, yet can be used in numerous potentially
harmful and illegal ways. In this article, we explain ways in which AI systems,
which can both `contain' content and be open-ended tools, present one of the
trickiest platform governance challenges seen to date. We provide case studies
of several incidents across three illustrative platforms -- Hugging Face,
GitHub and Civitai -- to examine how model marketplaces moderate models.
Building on this analysis, we outline important (and yet nevertheless limited)
practices that industry has been developing to respond to moderation demands:
licensing, access and use restrictions, automated content moderation, and open
policy development. While the policy challenge at hand is a considerable one,
we conclude with some ideas as to how platforms could better mobilize resources
to act as a careful, fair, and proportionate regulatory access point.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BEND: Benchmarking DNA Language Models on biologically meaningful tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederikke Isa Marin, Felix Teufel, Marc Horrender, Dennis Madsen, Dennis Pultz, Ole Winther, Wouter Boomsma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The genome sequence contains the blueprint for governing cellular processes.
While the availability of genomes has vastly increased over the last decades,
experimental annotation of the various functional, non-coding and regulatory
elements encoded in the DNA sequence remains both expensive and challenging.
This has sparked interest in unsupervised language modeling of genomic DNA, a
paradigm that has seen great success for protein sequence data. Although
various DNA language models have been proposed, evaluation tasks often differ
between individual works, and might not fully recapitulate the fundamental
challenges of genome annotation, including the length, scale and sparsity of
the data. In this study, we introduce BEND, a Benchmark for DNA language
models, featuring a collection of realistic and biologically meaningful
downstream tasks defined on the human genome. We find that embeddings from
current DNA LMs can approach performance of expert methods on some tasks, but
only capture limited information about long-range features. BEND is available
at https://github.com/frederikkemarin/BEND.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure, 3 tables, code available at
  https://github.com/frederikkemarin/BEND</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Sampling of Categorical Distributions Using the
  CatLog-Derivative Trick 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennert De Smet, Emanuele Sansone, Pedro Zuidberg Dos Martires
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Categorical random variables can faithfully represent the discrete and
uncertain aspects of data as part of a discrete latent variable model. Learning
in such models necessitates taking gradients with respect to the parameters of
the categorical probability distributions, which is often intractable due to
their combinatorial nature. A popular technique to estimate these otherwise
intractable gradients is the Log-Derivative trick. This trick forms the basis
of the well-known REINFORCE gradient estimator and its many extensions. While
the Log-Derivative trick allows us to differentiate through samples drawn from
categorical distributions, it does not take into account the discrete nature of
the distribution itself. Our first contribution addresses this shortcoming by
introducing the CatLog-Derivative trick - a variation of the Log-Derivative
trick tailored towards categorical distributions. Secondly, we use the
CatLog-Derivative trick to introduce IndeCateR, a novel and unbiased gradient
estimator for the important case of products of independent categorical
distributions with provably lower variance than REINFORCE. Thirdly, we
empirically show that IndeCateR can be efficiently implemented and that its
gradient estimates have significantly lower bias and variance for the same
number of samples compared to the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Elliptical Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Bånkestad, Jens Sjölund, Jalil Taghia, Thomas B. Schöon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present elliptical processes, a family of non-parametric probabilistic
models that subsume Gaussian processes and Student's t processes. This
generalization includes a range of new heavy-tailed behaviors while retaining
computational tractability. Elliptical processes are based on a representation
of elliptical distributions as a continuous mixture of Gaussian distributions.
We parameterize this mixture distribution as a spline normalizing flow, which
we train using variational inference. The proposed form of the variational
posterior enables a sparse variational elliptical process applicable to
large-scale problems. We highlight advantages compared to Gaussian processes
through regression and classification experiments. Elliptical processes can
supersede Gaussian processes in several settings, including cases where the
likelihood is non-Gaussian or when accurate tail modeling is essential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 15 figures, appendix 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Summary of the DISPLACE Challenge 2023 -- DIarization of SPeaker and
  LAnguage in Conversational Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikha Baghel, Shreyas Ramoji, Somil Jain, Pratik Roy Chowdhuri, Prachi Singh, Deepu Vijayasenan, Sriram Ganapathy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-lingual societies, where multiple languages are spoken in a small
geographic vicinity, informal conversations often involve mix of languages.
Existing speech technologies may be inefficient in extracting information from
such conversations, where the speech data is rich in diversity with multiple
languages and speakers. The DISPLACE (DIarization of SPeaker and LAnguage in
Conversational Environments) challenge constitutes an open-call for evaluating
and bench-marking the speaker and language diarization technologies on this
challenging condition. The challenge entailed two tracks: Track-1 focused on
speaker diarization (SD) in multilingual situations while, Track-2 addressed
the language diarization (LD) in a multi-speaker scenario. Both the tracks were
evaluated using the same underlying audio data. To facilitate this evaluation,
a real-world dataset featuring multilingual, multi-speaker conversational
far-field speech was recorded and distributed. Furthermore, a baseline system
was made available for both SD and LD task which mimicked the state-of-art in
these tasks. The challenge garnered a total of $42$ world-wide registrations
and received a total of $19$ combined submissions for Track-1 and Track-2. This
paper describes the challenge, details of the datasets, tasks, and the baseline
system. Additionally, the paper provides a concise overview of the submitted
systems in both tracks, with an emphasis given to the top performing systems.
The paper also presents insights and future perspectives for SD and LD tasks,
focusing on the key challenges that the systems need to overcome before
wide-spread commercial deployment on such conversations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Neural Networks for Neuroimaging in Parkinson's Disease:
  Is Preprocessing Needed? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco J. Martinez-Murcia, Juan M. Górriz, Javier Ramírez, Andrés Ortiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial and intensity normalization are nowadays a prerequisite for
neuroimaging analysis. Influenced by voxel-wise and other univariate
comparisons, where these corrections are key, they are commonly applied to any
type of analysis and imaging modalities. Nuclear imaging modalities such as
PET-FDG or FP-CIT SPECT, a common modality used in Parkinson's Disease
diagnosis, are especially dependent on intensity normalization. However, these
steps are computationally expensive and furthermore, they may introduce
deformations in the images, altering the information contained in them.
Convolutional Neural Networks (CNNs), for their part, introduce position
invariance to pattern recognition, and have been proven to classify objects
regardless of their orientation, size, angle, etc. Therefore, a question
arises: how well can CNNs account for spatial and intensity differences when
analysing nuclear brain imaging? Are spatial and intensity normalization still
needed? To answer this question, we have trained four different CNN models
based on well-established architectures, using or not different spatial and
intensity normalization preprocessing. The results show that a sufficiently
complex model such as our three-dimensional version of the ALEXNET can
effectively account for spatial differences, achieving a diagnosis accuracy of
94.1% with an area under the ROC curve of 0.984. The visualization of the
differences via saliency maps shows that these models are correctly finding
patterns that match those found in the literature, without the need of applying
any complex spatial normalization procedure. However, the intensity
normalization -- and its type -- is revealed as very influential in the results
and accuracy of the trained model, and therefore must be well accounted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Anomaly Detection using Masked Latent Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daesoo Lee, Sara Malacarne, Erlend Aune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel time series anomaly detection method that achieves
excellent detection accuracy while offering a superior level of explainability.
Our proposed method, TimeVQVAE-AD, leverages masked generative modeling adapted
from the cutting-edge time series generation method known as TimeVQVAE. The
prior model is trained on the discrete latent space of a time-frequency domain.
Notably, the dimensional semantics of the time-frequency domain are preserved
in the latent space, enabling us to compute anomaly scores across different
frequency bands, which provides a better insight into the detected anomalies.
Additionally, the generative nature of the prior model allows for sampling
likely normal states for detected anomalies, enhancing the explainability of
the detected anomalies through counterfactuals. Our experimental evaluation on
the UCR Time Series Anomaly archive demonstrates that TimeVQVAE-AD
significantly surpasses the existing methods in terms of detection accuracy and
explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Learning Functions with Varying Number of Minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Oniani, Yanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have proven effective at In-Context Learning
(ICL), an ability that allows them to create predictors from labeled examples.
Few studies have explored the interplay between ICL and specific properties of
functions it attempts to approximate. In our study, we use a formal framework
to explore ICL and propose a new task of approximating functions with varying
number of minima. We implement a method that allows for producing functions
with given inputs as minima. We find that increasing the number of minima
degrades ICL performance. At the same time, our evaluation shows that ICL
outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster
than 2NN in all settings. We validate the findings through a set of few-shot
experiments across various hyperparameter configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An efficient likelihood-free Bayesian inference method based on
  sequential neural posterior estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Xiong, Xiliang Yang, Sanguo Zhang, Zhijian He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential neural posterior estimation (SNPE) techniques have been recently
proposed for dealing with simulation-based models with intractable likelihoods.
Unlike approximate Bayesian computation, SNPE techniques learn the posterior
from sequential simulation using neural network-based conditional density
estimators. This paper reclaims SNPE-B proposed by Lueckmann et al. (2017),
which suffers from inefficiency and slow inference due to inefficient
utilization of simulated data and high variance of parameter updates. To
address these issues, we firstly introduce a concentrated loss function based
on an adaptive calibration kernel that reweights the simulated data
appropriately to improve the data efficiency. Moreover, we provide a
theoretical analysis of the variance of associated Monte Carlo estimators.
Based on this analysis, we then propose several variance reduction techniques
to further accelerate the process of learning. Numerical experiments
demonstrate that our method outperforms the original method together with other
existing competitors on certain tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse Problems with Learned Forward Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Arridge, Andreas Hauptmann, Yury Korolev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving inverse problems requires knowledge of the forward operator, but
accurate models can be computationally expensive and hence cheaper variants are
desired that do not compromise reconstruction quality. This chapter reviews
reconstruction methods in inverse problems with learned forward operators that
follow two different paradigms. The first one is completely agnostic to the
forward operator and learns its restriction to the subspace spanned by the
training data. The framework of regularisation by projection is then used to
find a reconstruction. The second one uses a simplified model of the physics of
the measurement process and only relies on the training data to learn a model
correction. We present the theory of these two approaches and compare them
numerically. A common theme emerges: both methods require, or at least benefit
from, training data not only for the forward operator, but also for its
adjoint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network Pruning by Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhang Zhang, Ruyi Tao, Jiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in the parameters of deep learning models has led to
significant costs, challenging computational efficiency and model
interpretability. In this paper, we introduce a novel and straightforward
neural network pruning framework that incorporates the Gumbel-Softmax
technique. This framework enables the simultaneous optimization of a network's
weights and topology in an end-to-end process using stochastic gradient
descent. Empirical results demonstrate its exceptional compression capability,
maintaining high accuracy on the MNIST dataset with only 0.15\% of the original
network parameters. Moreover, our framework enhances neural network
interpretability, not only by allowing easy extraction of feature importance
directly from the pruned network but also by enabling visualization of feature
symmetry and the pathways of information propagation from features to outcomes.
Although the pruning strategy is learned through deep learning, it is
surprisingly intuitive and understandable, focusing on selecting key
representative features and exploiting data patterns to achieve extreme sparse
pruning. We believe our method opens a promising new avenue for deep learning
pruning and the creation of interpretable machine learning systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALPHA: AnomaLous Physiological Health Assessment Using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiankai Tang, Kegang Wang, Hongming Hu, Xiyuxing Zhang, Peiyu Wang, Xin Liu, Yuntao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study concentrates on evaluating the efficacy of Large Language Models
(LLMs) in healthcare, with a specific focus on their application in personal
anomalous health monitoring. Our research primarily investigates the
capabilities of LLMs in interpreting and analyzing physiological data obtained
from FDA-approved devices. We conducted an extensive analysis using anomalous
physiological data gathered in a simulated low-air-pressure plateau
environment. This allowed us to assess the precision and reliability of LLMs in
understanding and evaluating users' health status with notable specificity. Our
findings reveal that LLMs exhibit exceptional performance in determining
medical indicators, including a Mean Absolute Error (MAE) of less than 1 beat
per minute for heart rate and less than 1% for oxygen saturation (SpO2).
Furthermore, the Mean Absolute Percentage Error (MAPE) for these evaluations
remained below 1%, with the overall accuracy of health assessments surpassing
85%. In image analysis tasks, such as interpreting photoplethysmography (PPG)
data, our specially adapted GPT models demonstrated remarkable proficiency,
achieving less than 1 bpm error in cycle count and 7.28 MAE for heart rate
estimation. This study highlights LLMs' dual role as health data analysis tools
and pivotal elements in advanced AI health assistants, offering personalized
health insights and recommendations within the future health assistant
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Polylog-Approximate Low-Cost Hierarchical Clustering <span class="chip">NeurIPS '23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Knittel, Max Springer, John Dickerson, MohammadTaghi Hajiaghayi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in fair machine learning, and particularly clustering, has been
crucial in recent years given the many ethical controversies that modern
intelligent systems have posed. Ahmadian et al. [2020] established the study of
fairness in \textit{hierarchical} clustering, a stronger, more structured
variant of its well-known flat counterpart, though their proposed algorithm
that optimizes for Dasgupta's [2016] famous cost function was highly
theoretical. Knittel et al. [2023] then proposed the first practical fair
approximation for cost, however they were unable to break the
polynomial-approximate barrier they posed as a hurdle of interest. We break
this barrier, proposing the first truly polylogarithmic-approximate low-cost
fair hierarchical clustering, thus greatly bridging the gap between the best
fair and vanilla hierarchical clustering approximations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS '23 (16 pages, 5 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Reinforcement Learning based on Decomposition: A
  taxonomy and framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Felten, El-Ghazali Talbi, Grégoire Danoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective reinforcement learning (MORL) extends traditional RL by
seeking policies making different compromises among conflicting objectives. The
recent surge of interest in MORL has led to diverse studies and solving
methods, often drawing from existing knowledge in multi-objective optimization
based on decomposition (MOO/D). Yet, a clear categorization based on both RL
and MOO/D is lacking in the existing literature. Consequently, MORL researchers
face difficulties when trying to classify contributions within a broader
context due to the absence of a standardized taxonomy. To tackle such an issue,
this paper introduces Multi-Objective Reinforcement Learning based on
Decomposition (MORL/D), a novel methodology bridging RL and MOO literature. A
comprehensive taxonomy for MORL/D is presented, providing a structured
foundation for categorizing existing and potential MORL works. The introduced
taxonomy is then used to scrutinize MORL research, enhancing clarity and
conciseness through well-defined categorization. Moreover, a flexible framework
derived from the taxonomy is introduced. This framework accommodates diverse
instantiations using tools from both RL and MOO/D. Implementation across
various configurations demonstrates its versatility, assessed against benchmark
problems. Results indicate MORL/D instantiations achieve comparable performance
with significantly greater versatility than current state-of-the-art
approaches. By presenting the taxonomy and framework, this paper offers a
comprehensive perspective and a unified vocabulary for MORL. This not only
facilitates the identification of algorithmic contributions but also lays the
groundwork for novel research avenues in MORL, contributing to the continued
advancement of this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at JAIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heuristics for Detecting CoinJoin Transactions on the Bitcoin Blockchain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Schnoering, Michalis Vazirgiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research delves into the intricacies of Bitcoin, a decentralized
peer-to-peer network, and its associated blockchain, which records all
transactions since its inception. While this ensures integrity and
transparency, the transparent nature of Bitcoin potentially compromises users'
privacy rights. To address this concern, users have adopted CoinJoin, a method
that amalgamates multiple transaction intents into a single, larger transaction
to bolster transactional privacy. This process complicates individual
transaction tracing and disrupts many established blockchain analysis
heuristics. Despite its significance, limited research has been conducted on
identifying CoinJoin transactions. Particularly noteworthy are varied CoinJoin
implementations such as JoinMarket, Wasabi, and Whirlpool, each presenting
distinct challenges due to their unique transaction structures. This study
delves deeply into the open-source implementations of these protocols, aiming
to develop refined heuristics for identifying their transactions on the
blockchain. Our exhaustive analysis covers transactions up to block 760,000,
offering a comprehensive insight into CoinJoin transactions and their
implications for Bitcoin blockchain analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields <span class="chip">WACV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Yi Gong, Yuan Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Neural radiance fields (NeRF) have enabled high-fidelity
scene reconstruction for novel view synthesis. However, NeRF requires hundreds
of network evaluations per pixel to approximate a volume rendering integral,
making it slow to train. Caching NeRFs into explicit data structures can
effectively enhance rendering speed but at the cost of higher memory usage. To
address these issues, we present Hyb-NeRF, a novel neural radiance field with a
multi-resolution hybrid encoding that achieves efficient neural modeling and
fast rendering, which also allows for high-quality novel view synthesis. The
key idea of Hyb-NeRF is to represent the scene using different encoding
strategies from coarse-to-fine resolution levels. Hyb-NeRF exploits
memory-efficiency learnable positional features at coarse resolutions and the
fast optimization speed and local details of hash-based feature grids at fine
resolutions. In addition, to further boost performance, we embed cone
tracing-based features in our learnable positional encoding that eliminates
encoding ambiguity and reduces aliasing artifacts. Extensive experiments on
both synthetic and real-world datasets show that Hyb-NeRF achieves faster
rendering speed with better rending quality and even a lower memory footprint
in comparison to previous state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaskFlow: Object-Aware Motion Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aria Ahmadi, David R. Walton, Tim Atherton, Cagatay Dikici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel motion estimation method, MaskFlow, that is capable of
estimating accurate motion fields, even in very challenging cases with small
objects, large displacements and drastic appearance changes. In addition to
lower-level features, that are used in other Deep Neural Network (DNN)-based
motion estimation methods, MaskFlow draws from object-level features and
segmentations. These features and segmentations are used to approximate the
objects' translation motion field. We propose a novel and effective way of
incorporating the incomplete translation motion field into a subsequent motion
estimation network for refinement and completion. We also produced a new
challenging synthetic dataset with motion field ground truth, and also provide
extra ground truth for the object-instance matchings and corresponding
segmentation masks. We demonstrate that MaskFlow outperforms state of the art
methods when evaluated on our new challenging dataset, whilst still producing
comparable results on the popular FlyingThings3D benchmark dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing FPGA Technology for Enhanced Biomedical Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisanur Alici, Kayode Inadagbo, Murat Isik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research delves into sophisticated neural network frameworks like
Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long
Short-Term Memory Networks (LSTMs), and Deep Belief Networks (DBNs) for
improved analysis of ECG signals via Field Programmable Gate Arrays (FPGAs).
The MIT-BIH Arrhythmia Database serves as the foundation for training and
evaluating our models, with added Gaussian noise to heighten the algorithms'
resilience. The developed architectures incorporate various layers for specific
processing and categorization functions, employing strategies such as the
EarlyStopping callback and Dropout layer to prevent overfitting. Additionally,
this paper details the creation of a tailored Tensor Compute Unit (TCU)
accelerator for the PYNQ Z1 platform. It provides a thorough methodology for
implementing FPGA-based machine learning, encompassing the configuration of the
Tensil toolchain in Docker, selection of architectures, PS-PL configuration,
and the compilation and deployment of models. By evaluating performance
indicators like latency and throughput, we showcase the efficacy of FPGAs in
advanced biomedical computing. This study ultimately serves as a comprehensive
guide to optimizing neural network operations on FPGAs across various fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Biomedical Circuits and Systems.
  arXiv admin note: substantial text overlap with arXiv:2307.07914</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifier Calibration with ROC-Regularized Isotonic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eugene Berta, Francis Bach, Michael Jordan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibration of machine learning classifiers is necessary to obtain reliable
and interpretable predictions, bridging the gap between model confidence and
actual probabilities. One prominent technique, isotonic regression (IR), aims
at calibrating binary classifiers by minimizing the cross entropy on a
calibration set via monotone transformations. IR acts as an adaptive binning
procedure, which allows achieving a calibration error of zero, but leaves open
the issue of the effect on performance. In this paper, we first prove that IR
preserves the convex hull of the ROC curve -- an essential performance metric
for binary classifiers. This ensures that a classifier is calibrated while
controlling for overfitting of the calibration set. We then present a novel
generalization of isotonic regression to accommodate classifiers with K
classes. Our method constructs a multidimensional adaptive binning scheme on
the probability simplex, again achieving a multi-class calibration error equal
to zero. We regularize this algorithm by imposing a form of monotony that
preserves the K-dimensional ROC surface of the classifier. We show empirically
that this general monotony criterion is effective in striking a balance between
reducing cross entropy loss and avoiding overfitting of the calibration set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Enough? A map of the current limitations of the requirements to
  have "fair'' algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Castelnovo, Nicole Inverardi, Gabriele Nanino, Ilaria Giuseppina Penco, Daniele Regoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the recent years, the raise in the usage and efficiency of Artificial
Intelligence and, more in general, of Automated Decision-Making systems has
brought with it an increasing and welcome awareness of the risks associated
with such systems. One of such risks is that of perpetuating or even amplifying
bias and unjust disparities present in the data from which many of these
systems learn to adjust and optimise their decisions. This awareness has on one
side encouraged several scientific communities to come up with more and more
appropriate ways and methods to assess, quantify, and possibly mitigate such
biases and disparities. On the other hand, it has prompted more and more layers
of society, including policy makers, to call for ``fair'' algorithms. We
believe that while a lot of excellent and multidisciplinary research is
currently being conducted, what is still fundamentally missing is the awareness
that having ``fair'' algorithms is per s\'e a nearly meaningless requirement,
that needs to be complemented with a lot of additional societal choices to
become actionable. Namely, there is a hiatus between what the society is
demanding from Automated Decision-Making systems, and what this demand actually
means in real-world scenarios. In this work, we outline the key features of
such a hiatus, and pinpoint a list of fundamental ambiguities and attention
points that we as a society must address in order to give a concrete meaning to
the increasing demand of fairness in Automated Decision-Making systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looped <span class="highlight-title">Transformer</span>s are Better at Learning Learning Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Yang, Kangwook Lee, Robert Nowak, Dimitris Papailiopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have demonstrated effectiveness in \emph{in-context solving}
data-fitting problems from various (latent) models, as reported by Garg et al.
However, the absence of an inherent iterative structure in the transformer
architecture presents a challenge in emulating the iterative algorithms, which
are commonly employed in traditional machine learning methods. To address this,
we propose the utilization of \emph{looped} transformer architecture and its
associated training methodology, with the aim of incorporating iterative
characteristics into the transformer architectures. Experimental results
suggest that the looped transformer achieves performance comparable to the
standard transformer in solving various data-fitting problems, while utilizing
less than 10\% of the parameter count.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Board-to-Board: Evaluating Moonboard Grade Prediction Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Petashvili, Matthew Rodda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bouldering is a sport where athletes aim to climb up an obstacle using a set
of defined holds called a route. Typically routes are assigned a grade to
inform climbers of its difficulty and allow them to more easily track their
progression. However, the variation in individual climbers technical and
physical attributes and many nuances of an individual route make grading a
difficult and often biased task. In this work, we apply classical and
deep-learning modelling techniques to the 2016, 2017 and 2019 Moonboard
datasets, achieving state of the art grade prediction performance with 0.87 MAE
and 1.12 RMSE. We achieve this performance on a feature-set that does not
require decomposing routes into individual moves, which is a method common in
literature and introduces bias. We also demonstrate the generalization
capability of this model between editions and introduce a novel vision-based
method of grade prediction. While the generalization performance of these
techniques is below human level performance currently, we propose these methods
as a basis for future work. Such a tool could be implemented in pre-existing
mobile applications and would allow climbers to better track their progress and
assess new routes with reduced bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ nach0: Multimodal Natural and Chemical Languages Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Micha Livne, Zulfat Miftahutdinov, Elena Tutubalina, Maksim Kuznetsov, Daniil Polykovskiy, Annika Brundyn, Aastha Jhunjhunwala, Anthony Costa, Alex Aliper, Alex Zhavoronkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have substantially driven scientific progress in
various domains, and many papers have demonstrated their ability to tackle
complex problems with creative solutions. Our paper introduces a new foundation
model, nach0, capable of solving various chemical and biological tasks:
biomedical question answering, named entity recognition, molecular generation,
molecular synthesis, attributes prediction, and others. nach0 is a multi-domain
and multi-task encoder-decoder LLM pre-trained on unlabeled text from
scientific literature, patents, and molecule strings to incorporate a range of
chemical and linguistic knowledge. We employed instruction tuning, where
specific task-related instructions are utilized to fine-tune nach0 for the
final set of tasks. To train nach0 effectively, we leverage the NeMo framework,
enabling efficient parallel optimization of both base and large model versions.
Extensive experiments demonstrate that our model outperforms state-of-the-art
baselines on single-domain and cross-domain tasks. Furthermore, it can generate
high-quality outputs in molecular and textual formats, showcasing its
effectiveness in multi-domain setups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Nature Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Graph Meets Large Language Model: Progress and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph plays a significant role in representing and analyzing complex
relationships in real-world applications such as citation networks, social
networks, and biological data. Recently, Large Language Models (LLMs), which
have achieved tremendous success in various domains, have also been leveraged
in graph-related tasks to surpass traditional Graph Neural Networks (GNNs)
based methods and yield state-of-the-art performance. In this survey, we first
present a comprehensive review and analysis of existing methods that integrate
LLMs with graphs. First of all, we propose a new taxonomy, which organizes
existing methods into three categories based on the role (i.e., enhancer,
predictor, and alignment component) played by LLMs in graph-related tasks. Then
we systematically survey the representative methods along the three categories
of the taxonomy. Finally, we discuss the remaining limitations of existing
studies and highlight promising avenues for future research. The relevant
papers are summarized and will be consistently updated at:
https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress; 13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Infinite forecast combinations based on Dirichlet process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinuo Ren, Feng Li, Yanfei Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecast combination integrates information from various sources by
consolidating multiple forecast results from the target time series. Instead of
the need to select a single optimal forecasting model, this paper introduces a
deep learning ensemble forecasting model based on the Dirichlet process.
Initially, the learning rate is sampled with three basis distributions as
hyperparameters to convert the infinite mixture into a finite one. All
checkpoints are collected to establish a deep learning sub-model pool, and
weight adjustment and diversity strategies are developed during the combination
process. The main advantage of this method is its ability to generate the
required base learners through a single training process, utilizing the
decaying strategy to tackle the challenge posed by the stochastic nature of
gradient descent in determining the optimal learning rate. To ensure the
method's generalizability and competitiveness, this paper conducts an empirical
analysis using the weekly dataset from the M4 competition and explores
sensitivity to the number of models to be combined. The results demonstrate
that the ensemble model proposed offers substantial improvements in prediction
accuracy and stability compared to a single benchmark model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Post-Training Quantization with Low-precision Minifloats and Integers on
  FPGAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Aggarwal, Alessandro Pappalardo, Hans Jakob Damsgaard, Giuseppe Franco, Thomas B. Preußer, Michaela Blott, Tulika Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-Training Quantization (PTQ) is a powerful technique for model
compression, reducing the precision of neural networks without additional
training overhead. Recent works have investigated adopting 8-bit floating-point
quantization (FP8) in the context of PTQ for model inference. However, the
exploration of floating-point formats smaller than 8 bits and their comparison
with integer quantization remains relatively limited. In this work, we present
minifloats, which are reduced-precision floating-point formats capable of
further reducing the memory footprint, latency, and energy cost of a model
while approaching full-precision model accuracy. Our work presents a novel PTQ
design-space exploration, comparing minifloat and integer quantization schemes
across a range of 3 to 8 bits for both weights and activations. We examine the
applicability of various PTQ techniques to minifloats, including weight
equalization, bias correction, SmoothQuant, gradient-based learned rounding,
and the GPTQ method. Our experiments validate the effectiveness of
low-precision minifloats when compared to their integer counterparts across a
spectrum of accuracy-precision trade-offs on a set of reference deep learning
vision workloads. Finally, we evaluate our results against an FPGA-based
hardware cost model, showing that integer quantization often remains the
Pareto-optimal option, given its relatively smaller hardware resource
footprint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning via Consensus Mechanism on Heterogeneous Data: A New
  Perspective on Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Zheng, Tiandi Ye, Xiang Li, Ming Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) on heterogeneous data (non-IID data) has recently
received great attention. Most existing methods focus on studying the
convergence guarantees for the global objective. While these methods can
guarantee the decrease of the global objective in each communication round,
they fail to ensure risk decrease for each client. In this paper, to address
the problem,we propose FedCOME, which introduces a consensus mechanism to
enforce decreased risk for each client after each training round. In
particular, we allow a slight adjustment to a client's gradient on the server
side, which generates an acute angle between the corrected gradient and the
original ones of other clients. We theoretically show that the consensus
mechanism can guarantee the convergence of the global objective. To generalize
the consensus mechanism to the partial participation FL scenario, we devise a
novel client sampling strategy to select the most representative clients for
the global data distribution. Training on these selected clients with the
consensus mechanism could empirically lead to risk decrease for clients that
are not selected. Finally, we conduct extensive experiments on four benchmark
datasets to show the superiority of FedCOME against other state-of-the-art
methods in terms of effectiveness, efficiency and fairness. For
reproducibility, we make our source code publicly available at:
\url{https://github.com/fedcome/fedcome}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Random Linear Projections Loss for Hyperplane-Based Optimization in
  Regression Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shyam Venkatasubramanian, Ahmed Aloui, Vahid Tarokh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their popularity across a wide range of domains, regression neural
networks are prone to overfitting complex datasets. In this work, we propose a
loss function termed Random Linear Projections (RLP) loss, which is empirically
shown to mitigate overfitting. With RLP loss, the distance between sets of
hyperplanes connecting fixed-size subsets of the neural network's
feature-prediction pairs and feature-label pairs is minimized. The intuition
behind this loss derives from the notion that if two functions share the same
hyperplanes connecting all subsets of feature-label pairs, then these functions
must necessarily be equivalent. Our empirical studies, conducted across
benchmark datasets and representative synthetic examples, demonstrate the
improvements of the proposed RLP loss over mean squared error (MSE).
Specifically, neural networks trained with the RLP loss achieve better
performance while requiring fewer data samples and are more robust to additive
noise. We provide theoretical analysis supporting our empirical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Language Models for Tour Itinerary Recommendation <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ngai Lam Ho, Kwan Hui Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tour itinerary recommendation involves planning a sequence of relevant
Point-of-Interest (POIs), which combines challenges from the fields of both
Operations Research (OR) and Recommendation Systems (RS). As an OR problem,
there is the need to maximize a certain utility (e.g., popularity of POIs in
the tour) while adhering to some constraints (e.g., maximum time for the tour).
As a RS problem, it is heavily related to problem or filtering or ranking a
subset of POIs that are relevant to a user and recommending it as part of an
itinerary. In this paper, we explore the use of language models for the task of
tour itinerary recommendation and planning. This task has the unique
requirement of recommending personalized POIs relevant to users and planning
these POIs as an itinerary that satisfies various constraints. We discuss some
approaches in this area, such as using word embedding techniques like Word2Vec
and GloVe for learning POI embeddings and transformer-based techniques like
BERT for generating
  itineraries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PMAI23 @IJCAI 2023 2nd International Workshop on Process Management
  in the AI era</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing <span class="highlight-title">Transformer</span> Architecture in Long-Context Large Language
  Models: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, Xiaoxing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the bomb ignited by ChatGPT, Transformer-based Large Language Models
(LLMs) have paved a revolutionary path toward Artificial General Intelligence
(AGI) and have been applied in diverse areas as knowledge bases, human
interfaces, and dynamic agents. However, a prevailing limitation exists: many
current LLMs, constrained by resources, are primarily pre-trained on shorter
texts, rendering them less effective for longer-context prompts, commonly
encountered in real-world settings. In this paper, we present a comprehensive
survey focusing on the advancement of model architecture in Transformer-based
LLMs to optimize long-context capabilities across all stages from pre-training
to inference. We firstly delineate and analyze the problems of handling
long-context input and output with the current Transformer-based models. Then,
we mainly offer a holistic taxonomy to navigate the landscape of Transformer
upgrades on architecture to solve these problems. Afterward, we provide the
investigation on wildly used evaluation necessities tailored for long-context
LLMs, including datasets, metrics, and baseline models, as well as some amazing
optimization toolkits like libraries, systems, and compilers to augment LLMs'
efficiency and efficacy across different stages. Finally, we further discuss
the predominant challenges and potential avenues for future research in this
domain. Additionally, we have established a repository where we curate relevant
literature with real-time updates at
https://github.com/Strivin0311/long-llms-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Diffusion For Aerial Object Detection <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Jian, Fuxun Yu, Simranjit Singh, Dimitrios Stamoulis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerial object detection is a challenging task, in which one major obstacle
lies in the limitations of large-scale data collection and the long-tail
distribution of certain classes. Synthetic data offers a promising solution,
especially with recent advances in diffusion-based methods like stable
diffusion (SD). However, the direct application of diffusion methods to aerial
domains poses unique challenges: stable diffusion's optimization for rich
ground-level semantics doesn't align with the sparse nature of aerial objects,
and the extraction of post-synthesis object coordinates remains problematic. To
address these challenges, we introduce a synthetic data augmentation framework
tailored for aerial images. It encompasses sparse-to-dense region of interest
(ROI) extraction to bridge the semantic gap, fine-tuning the diffusion model
with low-rank adaptation (LORA) to circumvent exhaustive retraining, and
finally, a Copy-Paste method to compose synthesized objects with backgrounds,
providing a nuanced approach to aerial object detection through synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023 Synthetic Data Generation with Generative AI
  workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Ordinary Differential Equations-based method for
  Collaborative Filtering <span class="chip">ICDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Xu, Yuanjie Zhu, Weizhi Zhang, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolution Networks (GCNs) are widely considered state-of-the-art for
collaborative filtering. Although several GCN-based methods have been proposed
and achieved state-of-the-art performance in various tasks, they can be
computationally expensive and time-consuming to train if too many layers are
created. However, since the linear GCN model can be interpreted as a
differential equation, it is possible to transfer it to an ODE problem. This
inspired us to address the computational limitations of GCN-based models by
designing a simple and efficient NODE-based model that can skip some GCN layers
to reach the final state, thus avoiding the need to create many layers. In this
work, we propose a Graph Neural Ordinary Differential Equation-based method for
Collaborative Filtering (GODE-CF). This method estimates the final embedding by
utilizing the information captured by one or two GCN layers. To validate our
approach, we conducted experiments on multiple datasets. The results
demonstrate that our model outperforms competitive baselines, including
GCN-based models and other state-of-the-art CF methods. Notably, our proposed
GODE-CF model has several advantages over traditional GCN-based models. It is
simple, efficient, and has a fast training time, making it a practical choice
for real-world situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Political Orientation of Social Media Posts: An Extended
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadia Kamal, Brenner Little, Jade Gullic, Trevor Harms, Kristin Olofsson, Arunkumar Bagavathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing machine learning models to characterize political polarization on
online social media presents significant challenges. These challenges mainly
stem from various factors such as the lack of annotated data, presence of noise
in social media datasets, and the sheer volume of data. The common research
practice typically examines the biased structure of online user communities for
a given topic or qualitatively measuring the impacts of polarized topics on
social media. However, there is limited work focusing on analyzing polarization
at the ground-level, specifically in the social media posts themselves. Such
existing analysis heavily relies on annotated data, which often requires
laborious human labeling, offers labels only to specific problems, and lacks
the ability to determine the near-future bias state of a social media
conversations. Understanding the degree of political orientation conveyed in
social media posts is crucial for quantifying the bias of online user
communities and investigating the spread of polarized content. In this work, we
first introduce two heuristic methods that leverage on news media bias and post
content to label social media posts. Next, we compare the efficacy and quality
of heuristically labeled dataset with a randomly sampled human-annotated
dataset. Additionally, we demonstrate that current machine learning models can
exhibit improved performance in predicting political orientation of social
media posts, employing both traditional supervised learning and few-shot
learning setups. We conduct experiments using the proposed heuristic methods
and machine learning approaches to predict the political orientation of posts
collected from two social media forums with diverse political ideologies: Gab
and Twitter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IEKM: A Model Incorporating External Keyword Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Luo, Qin Li, Zhao Yan, Mengliang Rao, Yunbo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A customer service platform system with a core text semantic similarity (STS)
task faces two urgent challenges: Firstly, one platform system needs to adapt
to different domains of customers, i.e., different domains adaptation (DDA).
Secondly, it is difficult for the model of the platform system to distinguish
sentence pairs that are literally close but semantically different, i.e., hard
negative samples. In this paper, we propose an incorporation external keywords
matrices model (IEKM) to address these challenges. The model uses external
tools or dictionaries to construct external matrices and fuses them to the
self-attention layers of the Transformer structure through gating units, thus
enabling flexible corrections to the model results. We evaluate the method on
multiple datasets and the results show that our method has improved performance
on all datasets. To demonstrate that our method can effectively solve all the
above challenges, we conduct a flexible correction experiment, which results in
an increase in the F1 value from 56.61 to 73.53. Our code will be publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Power grid operational risk assessment using graph neural network
  surrogates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yadong Zhang, Pranav M Karve, Sankaran Mahadevan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the utility of graph neural networks (GNNs) as proxies of
power grid operational decision-making algorithms (optimal power flow (OPF) and
security-constrained unit commitment (SCUC)) to enable rigorous quantification
of the operational risk. To conduct principled risk analysis, numerous Monte
Carlo (MC) samples are drawn from the (foretasted) probability distributions of
spatio-temporally correlated stochastic grid variables. The corresponding OPF
and SCUC solutions, which are needed to quantify the risk, are generated using
traditional OPF and SCUC solvers to generate data for training GNN model(s).
The GNN model performance is evaluated in terms of the accuracy of predicting
quantities of interests (QoIs) derived from the decision variables in OPF and
SCUC. Specifically, we focus on thermal power generation and load shedding at
system and individual zone level. We also perform reliability and risk
quantification based on GNN predictions and compare with that obtained from
OPF/SCUC solutions. Our results demonstrate that GNNs are capable of providing
fast and accurate prediction of QoIs and thus can be good surrogate models for
OPF and SCUC. The excellent accuracy of GNN-based reliability and risk
assessment further suggests that GNN surrogate has the potential to be applied
in real-time and hours-ahead risk quantification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript submitted to IEEE PES GM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discovering Effective Policies for Land-Use Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Risto Miikkulainen, Olivier Francon, Daniel Young, Elliot Meyerson, Babak Hodjat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How areas of land are allocated for different uses, such as forests, urban,
and agriculture, has a large effect on carbon balance, and therefore climate
change. Based on available historical data on changes in land use and a
simulation of carbon emissions/absorption, a surrogate model can be learned
that makes it possible to evaluate the different options available to
decision-makers efficiently. An evolutionary search process can then be used to
discover effective land-use policies for specific locations. Such a system was
built on the Project Resilience platform and evaluated with the Land-Use
Harmonization dataset and the BLUE simulator. It generates Pareto fronts that
trade off carbon impact and amount of change customized to different locations,
thus providing a potentially useful tool for land-use planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting subtle macroscopic changes in a finite temperature classical
  scalar field with machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiming Yang, Yutong Zheng, Jiahong Zhou, Huiyu Li, Jun Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to detect macroscopic changes is important for probing the
behaviors of experimental many-body systems from the classical to the quantum
realm. Although abrupt changes near phase boundaries can easily be detected,
subtle macroscopic changes are much more difficult to detect as the changes can
be obscured by noise. In this study, as a toy model for detecting subtle
macroscopic changes in many-body systems, we try to differentiate scalar field
samples at varying temperatures. We compare different methods for making such
differentiations, from physics method, statistics method, to AI method. Our
finding suggests that the AI method outperforms both the statistical method and
the physics method in its sensitivity. Our result provides a proof-of-concept
that AI can potentially detect macroscopic changes in many-body systems that
elude physical measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping "Brain Coral" Regions on Mars using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle A. Pearson, Eldar Noe, Daniel Zhao, Alphan Altinok, Alex Morgan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the main objectives of the Mars Exploration Program is to search for
evidence of past or current life on the planet. To achieve this, Mars
exploration has been focusing on regions that may have liquid or frozen water.
A set of critical areas may have seen cycles of ice thawing in the relatively
recent past in response to periodic changes in the obliquity of Mars. In this
work, we use convolutional neural networks to detect surface regions containing
"Brain Coral" terrain, a landform on Mars whose similarity in morphology and
scale to sorted stone circles on Earth suggests that it may have formed as a
consequence of freeze/thaw cycles. We use large images (~100-1000 megapixels)
from the Mars Reconnaissance Orbiter to search for these landforms at
resolutions close to a few tens of centimeters per pixel (~25--50 cm). Over
52,000 images (~28 TB) were searched (~5% of the Martian surface) where we
found detections in over 200 images. To expedite the processing we leverage a
classifier network (prior to segmentation) in the Fourier domain that can take
advantage of JPEG compression by leveraging blocks of coefficients from a
discrete cosine transform in lieu of decoding the entire image at the full
spatial resolution. The hybrid pipeline approach maintains ~93% accuracy while
cutting down on ~95% of the total processing time compared to running the
segmentation network at the full resolution on every image. The timely
processing of big data sets helps inform mission operations, geologic surveys
to prioritize candidate landing sites, avoid hazardous areas, or map the
spatial extent of certain terrain. The segmentation masks and source code are
available on Github for the community to explore and build upon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication, seeking comments from the community. Code
  available: https://github.com/pearsonkyle/Mars-Brain-Coral-Network</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A note on estimating the dimension from a random geometric graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caelan Atamanchuk, Luc Devroye, Gabor Lugosi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Let $G_n$ be a random geometric graph with vertex set $[n]$ based on $n$
i.i.d.\ random vectors $X_1,\ldots,X_n$ drawn from an unknown density $f$ on
$\R^d$. An edge $(i,j)$ is present when $\|X_i -X_j\| \le r_n$, for a given
threshold $r_n$ possibly depending upon $n$, where $\| \cdot \|$ denotes
Euclidean distance. We study the problem of estimating the dimension $d$ of the
underlying space when we have access to the adjacency matrix of the graph but
do not know $r_n$ or the vectors $X_i$. The main result of the paper is that
there exists an estimator of $d$ that converges to $d$ in probability as $n \to
\infty$ for all densities with $\int f^5 < \infty$ whenever $n^{3/2} r_n^d \to
\infty$ and $r_n = o(1)$. The conditions allow very sparse graphs since when
$n^{3/2} r_n^d \to 0$, the graph contains isolated edges only, with high
probability. We also show that, without any condition on the density, a
consistent estimator of $d$ exists when $n r_n^d \to \infty$ and $r_n = o(1)$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel OCT mosaicking pipeline with Feature- and Pixel-based registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Wang, Hao Li, Dewei Hu, Yuankai K. Tao, Ipek Oguz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution Optical Coherence Tomography (OCT) images are crucial for
ophthalmology studies but are limited by their relatively narrow field of view
(FoV). Image mosaicking is a technique for aligning multiple overlapping images
to obtain a larger FoV. Current mosaicking pipelines often struggle with
substantial noise and considerable displacement between the input sub-fields.
In this paper, we propose a versatile pipeline for stitching multi-view
OCT/OCTA \textit{en face} projection images. Our method combines the strengths
of learning-based feature matching and robust pixel-based registration to align
multiple images effectively. Furthermore, we advance the application of a
trained foundational model, Segment Anything Model (SAM), to validate
mosaicking results in an unsupervised manner. The efficacy of our pipeline is
validated using an in-house dataset and a large public dataset, where our
method shows superior performance in terms of both accuracy and computational
efficiency. We also made our evaluation tool for image mosaicking and the
corresponding pipeline publicly available at
\url{https://github.com/MedICL-VU/OCT-mosaicking}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-fidelity Bayesian Optimization in Engineering Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bach Do, Ruda Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resided at the intersection of multi-fidelity optimization (MFO) and Bayesian
optimization (BO), MF BO has found a niche in solving expensive engineering
design optimization problems, thanks to its advantages in incorporating
physical and mathematical understandings of the problems, saving resources,
addressing exploitation-exploration trade-off, considering uncertainty, and
processing parallel computing. The increasing number of works dedicated to MF
BO suggests the need for a comprehensive review of this advanced optimization
technique. In this paper, we survey recent developments of two essential
ingredients of MF BO: Gaussian process (GP) based MF surrogates and acquisition
functions. We first categorize the existing MF modeling methods and MFO
strategies to locate MF BO in a large family of surrogate-based optimization
and MFO algorithms. We then exploit the common properties shared between the
methods from each ingredient of MF BO to describe important GP-based MF
surrogate models and review various acquisition functions. By doing so, we
expect to provide a structured understanding of MF BO. Finally, we attempt to
reveal important aspects that require further research for applications of MF
BO in solving intricate yet important design optimization problems, including
constrained optimization, high-dimensional optimization, optimization under
uncertainty, and multi-objective optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do we listen to what we are told? An empirical study on human behaviour
  during the COVID-19 pandemic: neural networks vs. regression analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Heluo, Kexin Wang, Charles W. Robson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we contribute the first visual open-source empirical study on
human behaviour during the COVID-19 pandemic, in order to investigate how
compliant a general population is to mask-wearing-related public-health policy.
Object-detection-based convolutional neural networks, regression analysis and
multilayer perceptrons are combined to analyse visual data of the Viennese
public during 2020. We find that mask-wearing-related government regulations
and public-transport announcements encouraged correct mask-wearing-behaviours
during the COVID-19 pandemic. Importantly, changes in announcement and
regulation contents led to heterogeneous effects on people's behaviour.
Comparing the predictive power of regression analysis and neural networks, we
demonstrate that the latter produces more accurate predictions of population
reactions during the COVID-19 pandemic. Our use of regression modelling also
allows us to unearth possible causal pathways underlying societal behaviour.
Since our findings highlight the importance of appropriate communication
contents, our results will facilitate more effective non-pharmaceutical
interventions to be developed in future. Adding to the literature, we
demonstrate that regression modelling and neural networks are not mutually
exclusive but instead complement each other.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synaptic Sampling of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James B. Aimone, William Severa, J. Darby Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic artificial neural networks offer intriguing prospects for
enabling the uncertainty of artificial intelligence methods to be described
explicitly in their function; however, the development of techniques that
quantify uncertainty by well-understood methods such as Monte Carlo sampling
has been limited by the high costs of stochastic sampling on deterministic
computing hardware. Emerging computing systems that are amenable to
hardware-level probabilistic computing, such as those that leverage stochastic
devices, may make probabilistic neural networks more feasible in the
not-too-distant future. This paper describes the scANN technique --
\textit{sampling (by coinflips) artificial neural networks} -- which enables
neural networks to be sampled directly by treating the weights as Bernoulli
coin flips. This method is natively well suited for probabilistic computing
techniques that focus on tunable stochastic devices, nearly matches fully
deterministic performance while also describing the uncertainty of correct and
incorrect neural network outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, accepted to 2023 IEEE International Conference on Rebooting
  Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Favour: FAst Variance Operator for Uncertainty Rating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas D. Ahle, Sahar Karimi, Peter Tak Peter Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Neural Networks (BNN) have emerged as a crucial approach for
interpreting ML predictions. By sampling from the posterior distribution, data
scientists may estimate the uncertainty of an inference. Unfortunately many
inference samples are often needed, the overhead of which greatly hinder BNN's
wide adoption. To mitigate this, previous work proposed propagating the first
and second moments of the posterior directly through the network. However, on
its own this method is even slower than sampling, so the propagated variance
needs to be approximated such as assuming independence between neural nodes.
The resulting trade-off between quality and inference time did not match even
plain Monte Carlo sampling.
  Our contribution is a more principled variance propagation framework based on
"spiked covariance matrices", which smoothly interpolates between quality and
inference time. This is made possible by a new fast algorithm for updating a
diagonal-plus-low-rank matrix approximation under various operations. We tested
our algorithm against sampling based MC Dropout and Variational Inference on a
number of downstream uncertainty themed tasks, such as calibration and
out-of-distribution testing. We find that Favour is as fast as performing 2-3
inference samples, while matching the performance of 10-100 samples.
  In summary, this work enables the use of BNN in the realm of performance
critical tasks where they have previously been out of reach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DMLR: Data-centric Machine Learning Research -- Past, Present and Future <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Oala, Manil Maskey, Lilith Bat-Leah, Alicia Parrish, Nezihe Merve Gürel, Tzu-Sheng Kuo, Yang Liu, Rotem Dror, Danilo Brajovic, Xiaozhe Yao, Max Bartolo, William A Gaviria Rojas, Ryan Hileman, Rainier Aliment, Michael W. Mahoney, Meg Risdal, Matthew Lease, Wojciech Samek, Debojyoti Dutta, Curtis G Northcutt, Cody Coleman, Braden Hancock, Bernard Koch, Girmaw Abebe Tadesse, Bojan Karlaš, Ahmed Alaa, Adji Bousso Dieng, Natasha Noy, Vijay Janapa Reddi, James Zou, Praveen Paritosh, Mihaela van der Schaar, Kurt Bollacker, Lora Aroyo, Ce Zhang, Joaquin Vanschoren, Isabelle Guyon, Peter Mattson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drawing from discussions at the inaugural DMLR workshop at ICML 2023 and
meetings prior, in this report we outline the relevance of community engagement
and infrastructure development for the creation of next-generation public
datasets that will advance machine learning science. We chart a path forward as
a collective effort to sustain the creation and maintenance of these datasets
and methods towards positive scientific, societal and business impact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This editorial report accompanies the inaugural Data-centric Machine
  Learning Research (DMLR) Workshop that took place at ICML 2023
  https://dmlr.ai/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Multimodal Surface Registration with Geometric Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed A. Suliman, Logan Z. J. Williams, Abdulah Fawaz, Emma C. Robinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces GeoMorph, a novel geometric deep-learning framework
designed for image registration of cortical surfaces. The registration process
consists of two main steps. First, independent feature extraction is performed
on each input surface using graph convolutions, generating low-dimensional
feature representations that capture important cortical surface
characteristics. Subsequently, features are registered in a deep-discrete
manner to optimize the overlap of common structures across surfaces by learning
displacements of a set of control points. To ensure smooth and biologically
plausible deformations, we implement regularization through a deep conditional
random field implemented with a recurrent neural network. Experimental results
demonstrate that GeoMorph surpasses existing deep-learning methods by achieving
improved alignment with smoother deformations. Furthermore, GeoMorph exhibits
competitive performance compared to classical frameworks. Such versatility and
robustness suggest strong potential for various neuroscience applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast and Interpretable Mortality Risk Scores for Critical Care Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chloe Qinyu Zhu, Muhang Tian, Lesia Semenova, Jiachang Liu, Jack Xu, Joseph Scarpa, Cynthia Rudin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prediction of mortality in intensive care unit (ICU) patients is an important
task in critical care medicine. Prior work in creating mortality risk models
falls into two major categories: domain-expert-created scoring systems, and
black box machine learning (ML) models. Both of these have disadvantages: black
box models are unacceptable for use in hospitals, whereas manual creation of
models (including hand-tuning of logistic regression parameters) relies on
humans to perform high-dimensional constrained optimization, which leads to a
loss in performance. In this work, we bridge the gap between accurate black box
models and hand-tuned interpretable models. We build on modern interpretable ML
techniques to design accurate and interpretable mortality risk scores. We
leverage the largest existing public ICU monitoring datasets, namely the MIMIC
III and eICU datasets. By evaluating risk across medical centers, we are able
to study generalization across domains. In order to customize our risk score
models, we develop a new algorithm, GroupFasterRisk, which has several
important benefits: (1) it uses hard sparsity constraint, allowing users to
directly control the number of features; (2) it incorporates group sparsity to
allow more cohesive models; (3) it allows for monotonicity correction on models
for including domain knowledge; (4) it produces many equally-good models at
once, which allows domain experts to choose among them. GroupFasterRisk creates
its risk scores within hours, even on the large datasets we study here.
GroupFasterRisk's risk scores perform better than risk scores currently used in
hospitals, and have similar prediction performance to black box ML models
(despite being much sparser). Because GroupFasterRisk produces a variety of
risk scores and handles constraints, it allows design flexibility, which is the
key enabler of practical and trustworthy model creation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyTorch Geometric Signed Directed: A Software Package on Graph Neural
  Networks for Signed and Directed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.10793v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.10793v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan He, Xitong Zhang, Junjie Huang, Benedek Rozemberczki, Mihai Cucuringu, Gesine Reinert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Networks are ubiquitous in many real-world applications (e.g., social
networks encoding trust/distrust relationships, correlation networks arising
from time series data). While many networks are signed or directed, or both,
there is a lack of unified software packages on graph neural networks (GNNs)
specially designed for signed and directed networks. In this paper, we present
PyTorch Geometric Signed Directed (PyGSD), a software package which fills this
gap. Along the way, we evaluate the implemented methods with experiments with a
view to providing insights into which method to choose for a given task. The
deep learning framework consists of easy-to-use GNN models, synthetic and
real-world data, as well as task-specific evaluation metrics and loss functions
for signed and directed networks. As an extension library for PyG, our proposed
software is maintained with open-source releases, detailed documentation,
continuous integration, unit tests and code coverage checks. The GitHub
repository of the library is
https://github.com/SherylHYX/pytorch_geometric_signed_directed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LoG 2023. 27 pages in total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offline Imitation from Observation via Primal Wasserstein State
  Occupancy Matching <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yan, Alexander G. Schwing, Yu-xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, arbitrary interactions with the environment can
often be costly, and actions of expert demonstrations are not always available.
To reduce the need for both, Offline Learning from Observations (LfO) is
extensively studied, where the agent learns to solve a task with only expert
states and \textit{task-agnostic} non-expert state-action pairs. The
state-of-the-art DIstribution Correction Estimation (DICE) methods minimize the
state occupancy divergence between the learner and expert policies. However,
they are limited to either $f$-divergences (KL and $\chi^2$) or Wasserstein
distance with Rubinstein duality, the latter of which constrains the underlying
distance metric crucial to the performance of Wasserstein-based solutions. To
address this problem, we propose Primal Wasserstein DICE (PW-DICE), which
minimizes the primal Wasserstein distance between the expert and learner state
occupancies with a pessimistic regularizer and leverages a contrastively
learned distance as the underlying metric for the Wasserstein distance.
Theoretically, we prove that our framework is a generalization of the
state-of-the-art, SMODICE, and unifies $f$-divergence and Wasserstein
minimization. Empirically, we find that PW-DICE improves upon several
state-of-the-art methods on multiple testbeds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages. Accepted to the Optimal Transport and Machine Learning
  Workshop at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topological properties of basins of attraction and expressiveness of
  width bounded neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.04923v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.04923v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hans-Peter Beise, Steve Dias Da Cruz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Radhakrishnan et al. [2020], the authors empirically show that
autoencoders trained with usual SGD methods shape out basins of attraction
around their training data. We consider network functions of width not
exceeding the input dimension and prove that in this situation basins of
attraction are bounded and their complement cannot have bounded components. Our
conditions in these results are met in several experiments of the latter work
and we thus address a question posed therein. We also show that under some more
restrictive conditions the basins of attraction are path-connected. The
tightness of the conditions in our results is demonstrated by means of several
examples. Finally, the arguments used to prove the above results allow us to
derive a root cause why scalar-valued neural network functions that fulfill our
bounded width condition are not dense in spaces of continuous functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task Arithmetic in the Tangent Space: Improved Editing of <span class="highlight-title">Pre-Train</span>ed
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12827v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12827v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Ortiz-Jimenez, Alessandro Favero, Pascal Frossard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task arithmetic has recently emerged as a cost-effective and scalable
approach to edit pre-trained models directly in weight space: By adding the
fine-tuned weights of different tasks, the model's performance can be improved
on these tasks, while negating them leads to task forgetting. Yet, our
understanding of the effectiveness of task arithmetic and its underlying
principles remains limited. We present a comprehensive study of task arithmetic
in vision-language models and show that weight disentanglement is the crucial
factor that makes it effective. This property arises during pre-training and
manifests when distinct directions in weight space govern separate, localized
regions in function space associated with the tasks. Notably, we show that
fine-tuning models in their tangent space by linearizing them amplifies weight
disentanglement. This leads to substantial performance improvements across
multiple task arithmetic benchmarks and diverse models. Building on these
findings, we provide theoretical and empirical analyses of the neural tangent
kernel (NTK) of these models and establish a compelling link between task
arithmetic and the spatial localization of the NTK eigenfunctions. Overall, our
work uncovers novel insights into the fundamental mechanisms of task arithmetic
and offers a more reliable and effective approach to edit pre-trained models
through the NTK linearization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Banach-Tarski Embeddings and <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Maher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new construction of embeddings of arbitrary recursive data
structures into high dimensional vectors. These embeddings provide an
interpretable model for the latent state vectors of transformers. We
demonstrate that these embeddings can be decoded to the original data structure
when the embedding dimension is sufficiently large. This decoding algorithm has
a natural implementation as a transformer. We also show that these embedding
vectors can be manipulated directly to perform computations on the underlying
data without decoding. As an example we present an algorithm that constructs
the embedded parse tree of an embedded token sequence using only vector
operations in embedding space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 7 figures. v2: Fixed order of matrix multiplication in
  section 2.4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compressive Fourier collocation methods for high-dimensional diffusion
  equations with periodic boundary conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01255v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01255v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqi Wang, Simone Brugiapaglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-dimensional Partial Differential Equations (PDEs) are a popular
mathematical modelling tool, with applications ranging from finance to
computational chemistry. However, standard numerical techniques for solving
these PDEs are typically affected by the curse of dimensionality. In this work,
we tackle this challenge while focusing on stationary diffusion equations
defined over a high-dimensional domain with periodic boundary conditions.
Inspired by recent progress in sparse function approximation in high
dimensions, we propose a new method called compressive Fourier collocation.
Combining ideas from compressive sensing and spectral collocation, our method
replaces the use of structured collocation grids with Monte Carlo sampling and
employs sparse recovery techniques, such as orthogonal matching pursuit and
$\ell^1$ minimization, to approximate the Fourier coefficients of the PDE
solution. We conduct a rigorous theoretical analysis showing that the
approximation error of the proposed method is comparable with the best $s$-term
approximation (with respect to the Fourier basis) to the solution. Using the
recently introduced framework of random sampling in bounded Riesz systems, our
analysis shows that the compressive Fourier collocation method mitigates the
curse of dimensionality with respect to the number of collocation points under
sufficient conditions on the regularity of the diffusion coefficient. We also
present numerical experiments that illustrate the accuracy and stability of the
method for the approximation of sparse and compressible solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Editing Personality for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative task focused on editing the personality
traits of Large Language Models (LLMs). This task seeks to adjust the models'
responses to opinion-related questions on specified topics since an
individual's personality often manifests in the form of their expressed
opinions, thereby showcasing different personality traits. Specifically, we
construct a new benchmark dataset PersonalityEdit to address this task. Drawing
on the theory in Social Psychology, we isolate three representative traits,
namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our
benchmark. We then gather data using GPT-4, generating responses that not only
align with a specified topic but also embody the targeted personality trait. We
conduct comprehensive experiments involving various baselines and discuss the
representation of personality behavior in LLMs. Our intriguing findings uncover
potential challenges of the proposed task, illustrating several remaining
issues. We anticipate that our work can provide the NLP community with
insights. Code and datasets will be released at
https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress, add more experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code is available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress, add more experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Automated Pipeline for Tumour-Infiltrating Lymphocyte Scoring in
  Breast Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam J Shephard, Mostafa Jahanifar, Ruoyu Wang, Muhammad Dawood, Simon Graham, Kastytis Sidlauskas, Syed Ali Khurram, Nasir M Rajpoot, Shan E Ahmed Raza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tumour-infiltrating lymphocytes (TILs) are considered as a valuable
prognostic markers in both triple-negative and human epidermal growth factor
receptor 2 (HER2) positive breast cancer. In this study, we introduce an
innovative deep learning pipeline based on the Efficient-UNet architecture to
predict the TILs score for breast cancer whole-slide images (WSIs). We first
segment tumour and stromal regions in order to compute a tumour bulk mask. We
then detect TILs within the tumour-associated stroma, generating a TILs score
by closely mirroring the pathologist's workflow. Our method exhibits
state-of-the-art performance in segmenting tumour/stroma areas and TILs
detection, as demonstrated by internal cross-validation on the TiGER Challenge
training dataset and evaluation on the final leaderboards. Additionally, our
TILs score proves competitive in predicting survival outcomes within the same
challenge, underscoring the clinical relevance and potential of our automated
TILs scoring pipeline as a breast cancer prognostic tool.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Adam Optimization for 16-bit Neural Networks Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16189v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16189v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyoung Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we address critical concerns related to the numerical
instability observed in 16-bit computations of machine learning models. Such
instability, particularly when employing popular optimization algorithms like
Adam, often leads to unstable training of deep neural networks. This not only
disrupts the learning process but also poses significant challenges in
deploying dependable models in real-world applications. Our investigation
identifies the epsilon hyperparameter as the primary source of this
instability. A nuanced exploration reveals that subtle adjustments to epsilon
within 16-bit computations can enhance the numerical stability of Adam,
enabling more stable training of 16-bit neural networks. We propose a novel,
dependable approach that leverages updates from the Adam optimizer to bolster
the stability of the learning process. Our contributions provide deeper
insights into optimization challenges in low-precision computations and offer
solutions to ensure the stability of deep neural network training, paving the
way for their dependable use in various applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BrainWash: A Poisoning Attack to Forget in Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Abbasi, Parsa Nooralinejad, Hamed Pirsiavash, Soheil Kolouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning has gained substantial attention within the deep learning
community, offering promising solutions to the challenging problem of
sequential learning. Yet, a largely unexplored facet of this paradigm is its
susceptibility to adversarial attacks, especially with the aim of inducing
forgetting. In this paper, we introduce "BrainWash," a novel data poisoning
method tailored to impose forgetting on a continual learner. By adding the
BrainWash noise to a variety of baselines, we demonstrate how a trained
continual learner can be induced to forget its previously learned tasks
catastrophically, even when using these continual learning baselines. An
important feature of our approach is that the attacker requires no access to
previous tasks' data and is armed merely with the model's current parameters
and the data belonging to the most recent task. Our extensive experiments
highlight the efficacy of BrainWash, showcasing degradation in performance
across various regularization-based continual learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging High-Level Synthesis and Large Language Models to Generate,
  Simulate, and Deploy a Uniform Random Number Generator Hardware Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James T. Meech
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new high-level synthesis methodology for using large language
model tools to generate hardware designs. The methodology uses exclusively
open-source tools excluding the large language model. As a case study, we use
our methodology to generate a permuted congruential random number generator
design with a wishbone interface. We verify the functionality and quality of
the random number generator design using large language model-generated
simulations and the Dieharder randomness test suite. We document all the large
language model chat logs, Python scripts, Verilog scripts, and simulation
results used in the case study. We believe that our method of hardware design
generation coupled with the open source silicon 130 nm design tools will
revolutionize application-specific integrated circuit design. Our methodology
significantly lowers the bar to entry when building domain-specific computing
accelerators for the Internet of Things and proof of concept prototypes for
later fabrication in more modern process nodes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Objective Optimization Using the R2 Utility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Tu, Nikolas Kantas, Robert M. Lee, Behrang Shafei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of multi-objective optimization is to identify a collection of
points which describe the best possible trade-offs between the multiple
objectives. In order to solve this vector-valued optimization problem,
practitioners often appeal to the use of scalarization functions in order to
transform the multi-objective problem into a collection of single-objective
problems. This set of scalarized problems can then be solved using traditional
single-objective optimization techniques. In this work, we formalise this
convention into a general mathematical framework. We show how this strategy
effectively recasts the original multi-objective optimization problem into a
single-objective optimization problem defined over sets. An appropriate class
of objective functions for this new problem is the R2 utility function, which
is defined as a weighted integral over the scalarized optimization problems. We
show that this utility function is a monotone and submodular set function,
which can be optimised effectively using greedy optimization algorithms. We
analyse the performance of these greedy algorithms both theoretically and
empirically. Our analysis largely focusses on Bayesian optimization, which is a
popular probabilistic framework for black-box optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is available at: https://github.com/benmltu/scalarize</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Maskable Stock Representation for Portfolio
  Management in Customizable Stock Pools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhang, Yilei Zhao, Shuo Sun, Jie Ying, Yonggang Xie, Zitao Song, Xinrun Wang, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Portfolio management (PM) is a fundamental financial trading task, which
explores the optimal periodical reallocation of capitals into different stocks
to pursue long-term profits. Reinforcement learning (RL) has recently shown its
potential to train profitable agents for PM through interacting with financial
markets. However, existing work mostly focuses on fixed stock pools, which is
inconsistent with investors' practical demand. Specifically, the target stock
pool of different investors varies dramatically due to their discrepancy on
market states and individual investors may temporally adjust stocks they desire
to trade (e.g., adding one popular stocks), which lead to customizable stock
pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny
change of the stock pool, which leads to high computational cost and unstable
performance. To tackle this challenge, we propose EarnMore, a rEinforcement
leARNing framework with Maskable stOck REpresentation to handle PM with CSPs
through one-shot training in a global stock pool (GSP). Specifically, we first
introduce a mechanism to mask out the representation of the stocks outside the
target pool. Second, we learn meaningful stock representations through a
self-supervised masking and reconstruction process. Third, a re-weighting
mechanism is designed to make the portfolio concentrate on favorable stocks and
neglect the stocks outside the target pool. Through extensive experiments on 8
subset stock pools of the US stock market, we demonstrate that EarnMore
significantly outperforms 14 state-of-the-art baselines in terms of 6 popular
financial metrics with over 40% improvement on profit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Clock and the Pizza: Two Stories in Mechanistic Explanation of
  Neural Networks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Do neural networks, trained on well-understood algorithmic tasks, reliably
rediscover known algorithms for solving those tasks? Several recent studies, on
tasks ranging from group arithmetic to in-context linear regression, have
suggested that the answer is yes. Using modular addition as a prototypical
problem, we show that algorithm discovery in neural networks is sometimes more
complex. Small changes to model hyperparameters and initializations can induce
the discovery of qualitatively different algorithms from a fixed training set,
and even parallel implementations of multiple such algorithms. Some networks
trained to perform modular addition implement a familiar Clock algorithm;
others implement a previously undescribed, less intuitive, but comprehensible
procedure which we term the Pizza algorithm, or a variety of even more complex
procedures. Our results show that even simple learning problems can admit a
surprising diversity of solutions, motivating the development of new tools for
characterizing the behavior of neural networks across their algorithmic phase
space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relphormer: Relational Graph <span class="highlight-title">Transformer</span> for Knowledge Graph
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10852v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10852v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Bi, Siyuan Cheng, Jing Chen, Xiaozhuan Liang, Feiyu Xiong, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have achieved remarkable performance in widespread fields,
including natural language processing, computer vision and graph mining.
However, vanilla Transformer architectures have not yielded promising
improvements in the Knowledge Graph (KG) representations, where the
translational distance paradigm dominates this area. Note that vanilla
Transformer architectures struggle to capture the intrinsically heterogeneous
structural and semantic information of knowledge graphs. To this end, we
propose a new variant of Transformer for knowledge graph representations dubbed
Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample
contextualized sub-graph sequences as the input to alleviate the heterogeneity
issue. We propose a novel structure-enhanced self-attention mechanism to encode
the relational information and keep the semantic information within entities
and relations. Moreover, we utilize masked knowledge modeling for general
knowledge graph representation learning, which can be applied to various
KG-based tasks including knowledge graph completion, question answering, and
recommendation. Experimental results on six datasets show that Relphormer can
obtain better performance compared with baselines. Code is available in
https://github.com/zjunlp/Relphormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurocomputing 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine-learning-accelerated simulations to enable automatic surface
  reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Du, James K. Damewood, Jaclyn R. Lunger, Reisel Millan, Bilge Yildiz, Lin Li, Rafael Gómez-Bombarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding material surfaces and interfaces is vital in applications like
catalysis or electronics. By combining energies from electronic structure with
statistical mechanics, ab initio simulations can in principle predict the
structure of material surfaces as a function of thermodynamic variables.
However, accurate energy simulations are prohibitive when coupled to the vast
phase space that must be statistically sampled. Here, we present a bi-faceted
computational loop to predict surface phase diagrams of multi-component
materials that accelerates both the energy scoring and statistical sampling
methods. Fast, scalable, and data-efficient machine learning interatomic
potentials are trained on high-throughput density-functional theory
calculations through closed-loop active learning. Markov-chain Monte Carlo
sampling in the semi-grand canonical ensemble is enabled by using virtual
surface sites. The predicted surfaces for GaN(0001), Si(111), and SrTiO3(001)
are in agreement with past work and suggest that the proposed strategy can
model complex material surfaces and discover previously unreported surface
terminations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages main, 15 figures/tables, 5 pages supplementary</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Optimizers Can Learn Adversarially Robust Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhang, Zhiqi Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have shone in a variety of domains and attracted
increasing attention from both the security and the privacy communities. One
important yet worrying question is: Will training models under the differential
privacy (DP) constraint have an unfavorable impact on their adversarial
robustness? While previous works have postulated that privacy comes at the cost
of worse robustness, we give the first theoretical analysis to show that DP
models can indeed be robust and accurate, even sometimes more robust than their
naturally-trained non-private counterparts. We observe three key factors that
influence the privacy-robustness-accuracy tradeoff: (1) hyper-parameters for DP
optimizers are critical; (2) pre-training on public data significantly
mitigates the accuracy and robustness drop; (3) choice of DP optimizers makes a
difference. With these factors set properly, we achieve 90\% natural accuracy,
72\% robust accuracy ($+9\%$ than the non-private model) under $l_2(0.5)$
attack, and 69\% robust accuracy ($+16\%$ than the non-private model) with
pre-trained SimCLRv2 model under $l_\infty(4/255)$ attack on CIFAR10 with
$\epsilon=2$. In fact, we show both theoretically and empirically that DP
models are Pareto optimal on the accuracy-robustness tradeoff. Empirically, the
robustness of DP models is consistently observed across various datasets and
models. We believe our encouraging results are a significant step towards
training models that are private as well as robust.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low Dimensional Invariant Embeddings for Universal Geometric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.02956v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.02956v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Dym, Steven J. Gortler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies separating invariants: mappings on $D$ dimensional domains
which are invariant to an appropriate group action, and which separate orbits.
The motivation for this study comes from the usefulness of separating
invariants in proving universality of equivariant neural network architectures.
  We observe that in several cases the cardinality of separating invariants
proposed in the machine learning literature is much larger than the dimension
$D$. As a result, the theoretical universal constructions based on these
separating invariants is unrealistically large. Our goal in this paper is to
resolve this issue.
  We show that when a continuous family of semi-algebraic separating invariants
is available, separation can be obtained by randomly selecting $2D+1 $ of these
invariants. We apply this methodology to obtain an efficient scheme for
computing separating invariants for several classical group actions which have
been studied in the invariant learning literature. Examples include matrix
multiplication actions on point clouds by permutations, rotations, and various
other linear groups.
  Often the requirement of invariant separation is relaxed and only generic
separation is required. In this case, we show that only $D+1$ invariants are
required. More importantly, generic invariants are often significantly easier
to compute, as we illustrate by discussing generic and full separation for
weighted graphs. Finally we outline an approach for proving that separating
invariants can be constructed also when the random parameters have finite
precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pairing-based graph neural network for simulating quantum materials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Luo, David D. Dai, Liang Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a pairing-based graph neural network for simulating quantum
many-body systems. Our architecture augments a BCS-type geminal wavefunction
with a generalized pair amplitude parameterized by a graph neural network.
Variational Monte Carlo with our neural network simultaneously provides an
accurate, flexible, and scalable method for simulating many-electron systems.
We apply this method to two-dimensional semiconductor electron-hole bilayers
and obtain accurate results on a variety of interaction-induced phases,
including the exciton Bose-Einstein condensate, electron-hole superconductor,
and bilayer Wigner crystal. Our study demonstrates the potential of
physically-motivated neural network wavefunctions for quantum materials
simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ survex: an R package for explaining machine learning survival models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikołaj Spytek, Mateusz Krzyziński, Sophie Hanna Langbein, Hubert Baniecki, Marvin N. Wright, Przemysław Biecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to their flexibility and superior performance, machine learning models
frequently complement and outperform traditional statistical survival models.
However, their widespread adoption is hindered by a lack of user-friendly tools
to explain their internal operations and prediction rationales. To tackle this
issue, we introduce the survex R package, which provides a cohesive framework
for explaining any survival model by applying explainable artificial
intelligence techniques. The capabilities of the proposed software encompass
understanding and diagnosing survival models, which can lead to their
improvement. By revealing insights into the decision-making process, such as
variable effects and importances, survex enables the assessment of model
reliability and the detection of biases. Thus, transparency and responsibility
may be promoted in sensitive areas, such as biomedical research and healthcare
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Influencer Videos: Unboxing the Mystique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.12311v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.12311v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Rajaram, Puneet Manchanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influencer marketing has become a very popular tool to reach customers.
Despite the rapid growth in influencer videos, there has been little research
on the effectiveness of their constituent features in explaining video
engagement. We study YouTube influencers and analyze their unstructured video
data across text, audio and images using an "interpretable deep learning"
framework that accomplishes both goals of prediction and interpretation. Our
prediction-based approach analyzes unstructured data and finds that "what is
said" in words (text) is more influential than "how it is said" in imagery
(images) or acoustics (audio). Our novel interpretation-based approach is
implemented after completion of model prediction by analyzing the same source
of unstructured data to measure importance attributed to the video features. We
eliminate several spurious relationships in two steps, identifying a subset of
relationships which are confirmed using theory. We uncover novel findings that
establish distinct associations for measures of shallow and deep engagement
based on the dual-system framework of human thinking. Our approach is validated
using simulated data, and we discuss the learnings from our findings for
influencers and brands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, Online Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shortcut Learning in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.07780v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.07780v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has triggered the current rise of artificial intelligence and
is the workhorse of today's machine intelligence. Numerous success stories have
rapidly spread all over science, industry and society, but its limitations have
only recently come into focus. In this perspective we seek to distill how many
of deep learning's problems can be seen as different symptoms of the same
underlying problem: shortcut learning. Shortcuts are decision rules that
perform well on standard benchmarks but fail to transfer to more challenging
testing conditions, such as real-world scenarios. Related issues are known in
Comparative Psychology, Education and Linguistics, suggesting that shortcut
learning may be a common characteristic of learning systems, biological and
artificial alike. Based on these observations, we develop a set of
recommendations for model interpretation and benchmarking, highlighting recent
advances in machine learning to improve robustness and transferability from the
lab to real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>perspective article published at Nature Machine Intelligence
  (https://doi.org/10.1038/s42256-020-00257-z)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning: Applications and the Road Forward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L. Hayes, Eyke Hüllermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu, Andreas S. Tolias, Joost van de Weijer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars, Gido M. van de Ven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning is a sub-field of machine learning, which aims to allow
machine learning models to continuously learn on new data, by accumulating
knowledge without forgetting what was learned in the past. In this work, we
take a step back, and ask: "Why should one care about continual learning in the
first place?". We set the stage by surveying recent continual learning papers
published at three major machine learning conferences, and show that
memory-constrained settings dominate the field. Then, we discuss five open
problems in machine learning, and even though they seem unrelated to continual
learning at first sight, we show that continual learning will inevitably be
part of their solution. These problems are model-editing, personalization,
on-device learning, faster (re-)training and reinforcement learning. Finally,
by comparing the desiderata from these unsolved problems and the current
assumptions in continual learning, we highlight and discuss four future
directions for continual learning research. We hope that this work offers an
interesting perspective on the future of continual learning, while displaying
its potential value and the paths we have to pursue in order to make it
successful. This work is the result of the many discussions the authors had at
the Dagstuhl seminar on Deep Continual Learning, in March 2023.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-channel Speech Separation Using Spatially Selective Deep
  Non-linear Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.12023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.12023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristina Tesch, Timo Gerkmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a multi-channel separation task with multiple speakers, we aim to recover
all individual speech signals from the mixture. In contrast to single-channel
approaches, which rely on the different spectro-temporal characteristics of the
speech signals, multi-channel approaches should additionally utilize the
different spatial locations of the sources for a more powerful separation
especially when the number of sources increases. To enhance the spatial
processing in a multi-channel source separation scenario, in this work, we
propose a deep neural network (DNN) based spatially selective filter (SSF) that
can be spatially steered to extract the speaker of interest by initializing a
recurrent neural network layer with the target direction. We compare the
proposed SSF with a common end-to-end direct separation (DS) approach trained
using utterance-wise permutation invariant training (PIT), which only
implicitly learns to perform spatial filtering. We show that the SSF has a
clear advantage over a DS approach with the same underlying network
architecture when there are more than two speakers in the mixture, which can be
attributed to a better use of the spatial information. Furthermore, we find
that the SSF generalizes much better to additional noise sources that were not
seen during training and to scenarios with speakers positioned at a similar
angle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attending to Graph <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Müller, Mikhail Galkin, Christopher Morris, Ladislav Rampášek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, transformer architectures for graphs emerged as an alternative to
established techniques for machine learning with graphs, such as
(message-passing) graph neural networks. So far, they have shown promising
empirical results, e.g., on molecular prediction datasets, often attributed to
their ability to circumvent graph neural networks' shortcomings, such as
over-smoothing and over-squashing. Here, we derive a taxonomy of graph
transformer architectures, bringing some order to this emerging field. We
overview their theoretical properties, survey structural and positional
encodings, and discuss extensions for important graph classes, e.g., 3D
molecular graphs. Empirically, we probe how well graph transformers can recover
various graph properties, how well they can deal with heterophilic graphs, and
to what extent they prevent over-squashing. Further, we outline open challenges
and research direction to stimulate future work. Our code is available at
https://github.com/luis-mueller/probing-graph-transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computing Approximate $\ell_p$ Sensitivities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swati Padmanabhan, David P. Woodruff, Qiuyi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works in dimensionality reduction for regression tasks have introduced
the notion of sensitivity, an estimate of the importance of a specific
datapoint in a dataset, offering provable guarantees on the quality of the
approximation after removing low-sensitivity datapoints via subsampling.
However, fast algorithms for approximating $\ell_p$ sensitivities, which we
show is equivalent to approximate $\ell_p$ regression, are known for only the
$\ell_2$ setting, in which they are termed leverage scores.
  In this work, we provide efficient algorithms for approximating $\ell_p$
sensitivities and related summary statistics of a given matrix. In particular,
for a given $n \times d$ matrix, we compute $\alpha$-approximation to its
$\ell_1$ sensitivities at the cost of $O(n/\alpha)$ sensitivity computations.
For estimating the total $\ell_p$ sensitivity (i.e. the sum of $\ell_p$
sensitivities), we provide an algorithm based on importance sampling of
$\ell_p$ Lewis weights, which computes a constant factor approximation to the
total sensitivity at the cost of roughly $O(\sqrt{d})$ sensitivity
computations. Furthermore, we estimate the maximum $\ell_1$ sensitivity, up to
a $\sqrt{d}$ factor, using $O(d)$ sensitivity computations. We generalize all
these results to $\ell_p$ norms for $p > 1$. Lastly, we experimentally show
that for a wide class of matrices in real-world datasets, the total sensitivity
can be quickly approximated and is significantly smaller than the theoretical
prediction, demonstrating that real-world datasets have low intrinsic effective
dimensionality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample Efficient Reward Augmentation in offline-to-online Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19805v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19805v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Zhang, Xiao Xiong, Zifeng Zhuang, Jinxin Liu, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline-to-online RL can make full use of pre-collected offline datasets to
initialize policies, resulting in higher sample efficiency and better
performance compared to only using online algorithms alone for policy training.
However, direct fine-tuning of the pre-trained policy tends to result in
sub-optimal performance. A primary reason is that conservative offline RL
methods diminish the agent's capability of exploration, thereby impacting
online fine-tuning performance. To encourage agent's exploration during online
fine-tuning and enhance the overall online fine-tuning performance, we propose
a generalized reward augmentation method called Sample Efficient Reward
Augmentation (SERA). Specifically, SERA encourages agent to explore by
computing Q conditioned entropy as intrinsic reward. The advantage of SERA is
that it can extensively utilize offline pre-trained Q to encourage agent
uniformly coverage of state space while considering the imbalance between the
distributions of high-value and low-value states. Additionally, SERA can be
effortlessly plugged into various RL algorithms to improve online fine-tuning
and ensure sustained asymptotic improvement. Moreover, extensive experimental
results demonstrate that when conducting offline-to-online problems, SERA
consistently and effectively enhances the performance of various offline
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 Figures, and 6 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Can We Train Deep Learning Models Across Clouds and Continents? An
  Experimental Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Isenko, Ruben Mayer, Hans-Arno Jacobsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep learning models in the cloud or on dedicated hardware is
expensive. A more cost-efficient option are hyperscale clouds offering spot
instances, a cheap but ephemeral alternative to on-demand resources. As spot
instance availability can change depending on the time of day, continent, and
cloud provider, it could be more cost-efficient to distribute resources over
the world. Still, it has not been investigated whether geo-distributed,
data-parallel spot deep learning training could be a more cost-efficient
alternative to centralized training.
  This paper aims to answer the question: Can deep learning models be
cost-efficiently trained on a global market of spot VMs spanning different data
centers and cloud providers? To provide guidance, we extensively evaluate the
cost and throughput implications of training in different zones, continents,
and clouds for representative CV, NLP and ASR models. To expand the current
training options further, we compare the scalability potential for hybrid-cloud
scenarios by adding cloud resources to on-premise hardware to improve training
throughput. Finally, we show how leveraging spot instance pricing enables a new
cost-efficient way to train models with multiple cheap VMs, trumping both more
centralized and powerful hardware and even on-demand cloud offerings at
competitive prices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Currently in review. Artifacts and Code:
  https://github.com/cirquit/hivemind-multi-cloud</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Federated Learning with Multi-branch Architecture <span class="chip">IJCNN 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07931v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07931v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junki Mori, Tomoyuki Yoshiyama, Furukawa Ryo, Isamu Teranishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a decentralized machine learning technique that
enables multiple clients to collaboratively train models without requiring
clients to reveal their raw data to each other. Although traditional FL trains
a single global model with average performance among clients, statistical data
heterogeneity across clients has resulted in the development of personalized FL
(PFL), which trains personalized models with good performance on each client's
data. A key challenge with PFL is how to facilitate clients with similar data
to collaborate more in a situation where each client has data from complex
distribution and cannot determine one another's distribution. In this paper, we
propose a new PFL method (pFedMB) using multi-branch architecture, which
achieves personalization by splitting each layer of a neural network into
multiple branches and assigning client-specific weights to each branch. We also
design an aggregation method to improve the communication efficiency and the
model performance, with which each branch is globally updated with weighted
averaging by client-specific weights assigned to the branch. pFedMB is simple
but effective in facilitating each client to share knowledge with similar
clients by adjusting the weights assigned to each branch. We experimentally
show that pFedMB performs better than the state-of-the-art PFL methods using
the CIFAR10 and CIFAR100 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IJCNN 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximating Two-Layer Feedforward Networks for Efficient <span class="highlight-title">Transformer</span>s <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10837v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10837v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to reduce compute and memory requirements of neural networks (NNs)
without sacrificing performance? Many recent works use sparse Mixtures of
Experts (MoEs) to build resource-efficient large language models (LMs). Here we
introduce several novel perspectives on MoEs, presenting a general framework
that unifies various methods to approximate two-layer NNs (e.g., feedforward
blocks of Transformers), including product-key memories (PKMs). Leveraging
insights from this framework, we propose methods to improve both MoEs and PKMs.
Unlike prior work that compares MoEs with dense baselines under the
compute-equal condition, our evaluation condition is parameter-equal, which is
crucial to properly evaluate LMs. We show that our MoEs are competitive with
the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two
different scales, while being much more resource efficient. This demonstrates
that MoEs are relevant not only to extremely large LMs but also to any-scale
resource-efficient LMs. Our code is public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyoung Park, Jin Kim, Hyeongjun Kwon, Ilhoon Yoon, Kwanghoon Sohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the inevitability of domain shifts during inference in real-world
applications, test-time adaptation (TTA) is essential for model adaptation
after deployment. However, the real-world scenario of continuously changing
target distributions presents challenges including catastrophic forgetting and
error accumulation. Existing TTA methods for non-stationary domain shifts,
while effective, incur excessive computational load, making them impractical
for on-device settings. In this paper, we introduce a layer-wise auto-weighting
algorithm for continual and gradual TTA that autonomously identifies layers for
preservation or concentrated adaptation. By leveraging the Fisher Information
Matrix (FIM), we first design the learning weight to selectively focus on
layers associated with log-likelihood changes while preserving unrelated ones.
Then, we further propose an exponential min-max scaler to make certain layers
nearly frozen while mitigating outliers. This minimizes forgetting and error
accumulation, leading to efficient adaptation to non-stationary target
distribution. Experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C show our
method outperforms conventional continual and gradual TTA approaches while
significantly reducing computational load, highlighting the importance of
FIM-based learning weight in adapting to continuously or gradually shifting
target domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with
  <span class="highlight-title">Transformer</span>s <span class="chip">ECAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07121v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07121v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Rauch, Raphael Schwinger, Moritz Wirth, Bernhard Sick, Sven Tomforde, Christoph Scholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a shift towards end-to-end learning in bird sound monitoring by
combining self-supervised (SSL) and deep active learning (DAL). Leveraging
transformer models, we aim to bypass traditional spectrogram conversions,
enabling direct raw audio processing. ActiveBird2Vec is set to generate
high-quality bird sound representations through SSL, potentially accelerating
the assessment of environmental changes and decision-making processes for wind
farms. Additionally, we seek to utilize the wide variety of bird vocalizations
through DAL, reducing the reliance on extensively labeled datasets by human
experts. We plan to curate a comprehensive set of tasks through Huggingface
Datasets, enhancing future comparability and reproducibility of bioacoustic
research. A comparative analysis between various transformer models will be
conducted to evaluate their proficiency in bird sound recognition tasks. We aim
to accelerate the progression of avian bioacoustic research and contribute to
more effective conservation strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted @AI4S ECAI2023. This is the author's version of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterogeneous Domain Adaptation with Positive and Unlabeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junki Mori, Ryo Furukawa, Isamu Teranishi, Jun Sakuma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterogeneous unsupervised domain adaptation (HUDA) is the most challenging
domain adaptation setting where the feature spaces of source and target domains
are heterogeneous, and the target domain has only unlabeled data. Existing HUDA
methods assume that both positive and negative examples are available in the
source domain, which may not be satisfied in some real applications. This paper
addresses a new challenging setting called positive and unlabeled heterogeneous
unsupervised domain adaptation (PU-HUDA), a HUDA setting where the source
domain only has positives. PU-HUDA can also be viewed as an extension of PU
learning where the positive and unlabeled examples are sampled from different
domains. A naive combination of existing HUDA and PU learning methods is
ineffective in PU-HUDA due to the gap in label distribution between the source
and target domains. To overcome this issue, we propose a novel method,
predictive adversarial domain adaptation (PADA), which can predict likely
positive examples from the unlabeled target data and simultaneously align the
feature spaces to reduce the distribution divergence between the whole source
data and the likely positive target data. PADA achieves this by a unified
adversarial training framework for learning a classifier to predict positive
examples and a feature transformer to transform the target feature space to
that of the source. Specifically, they are both trained to fool a common
discriminator that determines whether the likely positive examples are from the
target or source domain. We experimentally show that PADA outperforms several
baseline methods, such as the naive combination of HUDA and PU learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Big Data 2023 as a regular paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Trie of Rules: a fast data structure for the
  representation of association rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Kudriavtsev, Marija Bezbradica, Andrew McCarren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Association rule mining techniques can generate a large volume of sequential
data when implemented on transactional databases. Extracting insights from a
large set of association rules has been found to be a challenging process. When
examining a ruleset, the fundamental question is how to summarise and represent
meaningful mined knowledge efficiently. Many algorithms and strategies have
been developed to address issue of knowledge extraction; however, the
effectiveness of this process can be limited by the data structures. A better
data structure can sufficiently affect the speed of the knowledge extraction
process. This paper proposes a novel data structure, called the Trie of rules,
for storing a ruleset that is generated by association rule mining. The
resulting data structure is a prefix-tree graph structure made of pre-mined
rules. This graph stores the rules as paths within the prefix-tree in a way
that similar rules overlay each other. Each node in the tree represents a rule
where a consequent is this node, and an antecedent is a path from this node to
the root of the tree. The evaluation showed that the proposed representation
technique is promising. It compresses a ruleset with almost no data loss and
benefits in terms of time for basic operations such as searching for a specific
rule and sorting, which is the base for many knowledge discovery methods.
Moreover, our method demonstrated a significant improvement in traversing time,
achieving an 8-fold increase compared to traditional data structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 13 figures, preprint of journal article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> To Compress or Not to Compress- <span class="highlight-title">Self-Supervised</span> Learning and Information
  Theory: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09355v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09355v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravid Shwartz-Ziv, <span class="highlight-author">Yann LeCun</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks excel in supervised learning tasks but are constrained
by the need for extensive labeled data. Self-supervised learning emerges as a
promising alternative, allowing models to learn without explicit labels.
Information theory, and notably the information bottleneck principle, has been
pivotal in shaping deep neural networks. This principle focuses on optimizing
the trade-off between compression and preserving relevant information,
providing a foundation for efficient network design in supervised contexts.
However, its precise role and adaptation in self-supervised learning remain
unclear. In this work, we scrutinize various self-supervised learning
approaches from an information-theoretic perspective, introducing a unified
framework that encapsulates the \textit{self-supervised information-theoretic
learning problem}. We weave together existing research into a cohesive
narrative, delve into contemporary self-supervised methodologies, and spotlight
potential research avenues and inherent challenges. Additionally, we discuss
the empirical evaluation of information-theoretic quantities and their
estimation methods. Overall, this paper furnishes an exhaustive review of the
intersection of information theory, self-supervised learning, and deep neural
networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predict, Refine, Synthesize: Self-Guiding Diffusion Models for
  Probabilistic Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao Wang, Yuyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved state-of-the-art performance in generative
modeling tasks across various domains. Prior works on time series diffusion
models have primarily focused on developing conditional models tailored to
specific forecasting or imputation tasks. In this work, we explore the
potential of task-agnostic, unconditional diffusion models for several time
series applications. We propose TSDiff, an unconditionally-trained diffusion
model for time series. Our proposed self-guidance mechanism enables
conditioning TSDiff for downstream tasks during inference, without requiring
auxiliary networks or altering the training procedure. We demonstrate the
effectiveness of our method on three different time series tasks: forecasting,
refinement, and synthetic data generation. First, we show that TSDiff is
competitive with several task-specific conditional forecasting methods
(predict). Second, we leverage the learned implicit probability density of
TSDiff to iteratively refine the predictions of base forecasters with reduced
computational overhead over reverse diffusion (refine). Notably, the generative
performance of the model remains intact -- downstream forecasters trained on
synthetic samples from TSDiff outperform forecasters that are trained on
samples from other state-of-the-art generative time series models, occasionally
even outperforming models trained on real data (synthesize).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YolOOD: Utilizing Object Detection Concepts for Multi-Label
  Out-of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alon Zolfi, Guy Amit, Amit Baras, Satoru Koda, Ikuya Morikawa, Yuval Elovici, Asaf Shabtai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection has attracted a large amount of attention
from the machine learning research community in recent years due to its
importance in deployed systems. Most of the previous studies focused on the
detection of OOD samples in the multi-class classification task. However, OOD
detection in the multi-label classification task, a more common real-world use
case, remains an underexplored domain. In this research, we propose YolOOD - a
method that utilizes concepts from the object detection domain to perform OOD
detection in the multi-label classification task. Object detection models have
an inherent ability to distinguish between objects of interest
(in-distribution) and irrelevant objects (e.g., OOD objects) in images that
contain multiple objects belonging to different class categories. These
abilities allow us to convert a regular object detection model into an image
classifier with inherent OOD detection capabilities with just minor changes. We
compare our approach to state-of-the-art OOD detection methods and demonstrate
YolOOD's ability to outperform these methods on a comprehensive suite of
in-distribution and OOD benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LGL-BCI: A Lightweight Geometric Learning Framework for Motor
  Imagery-Based Brain-Computer Interfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianchao Lu, Yuzhe Tian, Yang Zhang, Jiaqi Ge, Quan Z. Sheng, Xi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-Computer Interfaces (BCIs) are a groundbreaking technology for
interacting with external devices using brain signals. Despite advancements,
electroencephalogram (EEG)-based Motor Imagery (MI) tasks face challenges like
amplitude and phase variability, and complex spatial correlations, with a need
for smaller model size and faster inference. This study introduces the LGL-BCI
framework, employing a Geometric Deep Learning Framework for EEG processing in
non-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD)
Manifold space. LGL-BCI offers robust EEG data representation and captures
spatial correlations. We propose an EEG channel selection solution via a
feature decomposition algorithm to reduce SPD matrix dimensionality, with a
lossless transformation boosting inference speed. Extensive experiments show
LGL-BCI's superior accuracy and efficiency compared to current solutions,
highlighting geometric deep learning's potential in MI-BCI applications. The
efficiency, assessed on two public EEG datasets and two real-world EEG devices,
significantly outperforms the state-of-the-art solution in accuracy ($82.54\%$
versus $62.22\%$) with fewer parameters (64.9M compared to 183.7M).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An actor-critic algorithm with policy gradients to solve the job shop
  scheduling problem using deep double recurrent agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.09076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.09076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Monaci, Valerio Agasucci, Giorgio Grani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing interest in integrating machine learning techniques and
optimization to solve challenging optimization problems. In this work, we
propose a deep reinforcement learning methodology for the job shop scheduling
problem (JSSP). The aim is to build up a greedy-like heuristic able to learn on
some distribution of JSSP instances, different in the number of jobs and
machines. The need for fast scheduling methods is well known, and it arises in
many areas, from transportation to healthcare. We model the JSSP as a Markov
Decision Process and then we exploit the efficacy of reinforcement learning to
solve the problem. We adopt an actor-critic scheme, where the action taken by
the agent is influenced by policy considerations on the state-value function.
The procedures are adapted to take into account the challenging nature of JSSP,
where the state and the action space change not only for every instance but
also after each decision. To tackle the variability in the number of jobs and
operations in the input, we modeled the agent using two incident LSTM models, a
special type of deep neural network. Experiments show the algorithm reaches
good solutions in a short time, proving that is possible to generate new greedy
heuristics just from learning-based methodologies. Benchmarks have been
generated in comparison with the commercial solver CPLEX. As expected, the
model can generalize, to some extent, to larger problems or instances
originated by a different distribution from the one used in training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Langevin dynamics based algorithm e-TH$\varepsilon$O POULA for
  stochastic optimization problems with discontinuous stochastic gradient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong-Young Lim, Ariel Neufeld, Sotirios Sabanis, Ying Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new Langevin dynamics based algorithm, called
e-TH$\varepsilon$O POULA, to solve optimization problems with discontinuous
stochastic gradients which naturally appear in real-world applications such as
quantile estimation, vector quantization, CVaR minimization, and regularized
optimization problems involving ReLU neural networks. We demonstrate both
theoretically and numerically the applicability of the e-TH$\varepsilon$O POULA
algorithm. More precisely, under the conditions that the stochastic gradient is
locally Lipschitz in average and satisfies a certain convexity at infinity
condition, we establish non-asymptotic error bounds for e-TH$\varepsilon$O
POULA in Wasserstein distances and provide a non-asymptotic estimate for the
expected excess risk, which can be controlled to be arbitrarily small. Three
key applications in finance and insurance are provided, namely, multi-period
portfolio optimization, transfer learning in multi-period portfolio
optimization, and insurance claim prediction, which involve neural networks
with (Leaky)-ReLU activation functions. Numerical experiments conducted using
real-world datasets illustrate the superior empirical performance of
e-TH$\varepsilon$O POULA compared to SGLD, TUSLA, ADAM, and AMSGrad in terms of
model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is TinyML Sustainable? Assessing the Environmental Impacts of Machine
  Learning on Microcontrollers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11899v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11899v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shvetank Prakash, Matthew Stewart, Colby Banbury, Mark Mazumder, Pete Warden, Brian Plancher, Vijay Janapa Reddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sustained growth of carbon emissions and global waste elicits significant
sustainability concerns for our environment's future. The growing Internet of
Things (IoT) has the potential to exacerbate this issue. However, an emerging
area known as Tiny Machine Learning (TinyML) has the opportunity to help
address these environmental challenges through sustainable computing practices.
TinyML, the deployment of machine learning (ML) algorithms onto low-cost,
low-power microcontroller systems, enables on-device sensor analytics that
unlocks numerous always-on ML applications. This article discusses both the
potential of these TinyML applications to address critical sustainability
challenges, as well as the environmental footprint of this emerging technology.
Through a complete life cycle analysis (LCA), we find that TinyML systems
present opportunities to offset their carbon emissions by enabling applications
that reduce the emissions of other sectors. Nevertheless, when globally scaled,
the carbon footprint of TinyML systems is not negligible, necessitating that
designers factor in environmental impact when formulating new devices. Finally,
we outline research directions to enable further sustainable contributions of
TinyML.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Communications of the ACM (CACM) November 2023 Issue</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yake Wei, Ruoxuan Feng, Zihe Wang, Di Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One primary topic of multi-modal learning is to jointly incorporate
heterogeneous information from different modalities. However, most models often
suffer from unsatisfactory multi-modal cooperation, which could not jointly
utilize all modalities well. Some methods are proposed to identify and enhance
the worse learnt modality, but are often hard to provide the fine-grained
observation of multi-modal cooperation at sample-level with theoretical
support. Hence, it is essential to reasonably observe and improve the
fine-grained cooperation between modalities, especially when facing realistic
scenarios where the modality discrepancy could vary across different samples.
To this end, we introduce a fine-grained modality valuation metric to evaluate
the contribution of each modality at sample-level. Via modality valuation, we
regretfully observe that the multi-modal model tends to rely on one specific
modality, resulting in other modalities being low-contributing. We further
analyze this issue and improve cooperation between modalities by enhancing the
discriminative ability of low-contributing modalities in a targeted manner.
Overall, our methods reasonably observe the fine-grained uni-modal contribution
at sample-level and achieve considerable improvement on different multi-modal
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Methods for Media Mix Modelling with shape and funnel effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Marin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, significant progress in generative AI has highlighted the
important role of physics-inspired models that utilize advanced mathematical
concepts based on fundamental physics principles to enhance artificial
intelligence capabilities. Among these models, those based on diffusion
equations have greatly improved image quality. This study aims to explore the
potential uses of Maxwell-Boltzmann equation, which forms the basis of the
kinetic theory of gases, and the Michaelis-Menten model in Marketing Mix
Modelling (MMM) applications. We propose incorporating these equations into
Hierarchical Bayesian models to analyse consumer behaviour in the context of
advertising. These equation sets excel in accurately describing the random
dynamics in complex systems like social interactions and consumer-advertising
interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Engineering with Regularity Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.05879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.05879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Chevyrev, Andris Gerasimovics, Hendrik Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the use of models from the theory of regularity structures as
features in machine learning tasks. A model is a polynomial function of a
space-time signal designed to well-approximate solutions to partial
differential equations (PDEs), even in low regularity regimes. Models can be
seen as natural multi-dimensional generalisations of signatures of paths; our
work therefore aims to extend the recent use of signatures in data science
beyond the context of time-ordered data. We provide a flexible definition of a
model feature vector associated to a space-time signal, along with two
algorithms which illustrate ways in which these features can be combined with
linear regression. We apply these algorithms in several numerical experiments
designed to learn solutions to PDEs with a given forcing and boundary data. Our
experiments include semi-linear parabolic and wave equations with forcing, and
Burgers' equation with no forcing. We find an advantage in favour of our
algorithms when compared to several alternative methods. Additionally, in the
experiment with Burgers' equation, we find non-trivial predictive power when
noise is added to the observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 7 figures, 7 tables. Improved presentation of model feature
  vector (Section 2) and experiments (Section 3). Added new experiment in 2D
  spatial domain (Section 3.1.2). To appear in Journal of Scientific Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyDaddy: A Python package for discovering stochastic dynamical equations
  from timeseries data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.02645v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.02645v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arshed Nabeel, Ashwin Karichannavar, Shuaib Palathingal, Jitesh Jhawar, David B. Brückner, Danny Raj M., Vishwesha Guttal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic differential equations (SDEs) are an important framework to model
dynamics with randomness, as is common in most biological systems. The inverse
problem of integrating these models with empirical data remains a major
challenge. Here, we present a software package, PyDaDDy (Python Library for
Data Driven Dynamics) that takes time series data as an input and outputs an
interpretable SDE. We achieve this by combining traditional approaches from
stochastic calculus literature with state-of-the-art equation discovery
techniques. We validate our approach on synthetic datasets, and demonstrate the
generality and applicability of the method on two real-world datasets of vastly
different spatiotemporal scales: (i) collective movement of fish school where
stochasticity plays a crucial role, and (ii) confined migration of a single
cell, primarily following a relaxed oscillation. We make the method available
as an easy-to-use, open-source Python package, PyDaddy (Python Library for Data
Driven Dynamics).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages (+ 9 page appendix), 6 figures (+ 8 appendix figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Composite Score for Anomaly Detection in Imbalanced Real-World
  Industrial <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnaud Bougaham, Mohammed El Adoui, Isabelle Linden, Benoît Frénay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the industrial sector has evolved towards its fourth
revolution. The quality control domain is particularly interested in advanced
machine learning for computer vision anomaly detection. Nevertheless, several
challenges have to be faced, including imbalanced datasets, the image
complexity, and the zero-false-negative (ZFN) constraint to guarantee the
high-quality requirement. This paper illustrates a use case for an industrial
partner, where Printed Circuit Board Assembly (PCBA) images are first
reconstructed with a Vector Quantized Generative Adversarial Network (VQGAN)
trained on normal products. Then, several multi-level metrics are extracted on
a few normal and abnormal images, highlighting anomalies through reconstruction
differences. Finally, a classifer is trained to build a composite anomaly score
thanks to the metrics extracted. This three-step approach is performed on the
public MVTec-AD datasets and on the partner PCBA dataset, where it achieves a
regular accuracy of 95.69% and 87.93% under the ZFN constraint.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version of the article has been accepted for publication, after
  peer review and is subject to Springer Nature AM terms of use, but is not the
  Version of Record and does not reflect post-acceptance improvements, or any
  corrections. The Version of Record is available online at:
  https://doi.org/10.1007/s10994-023-06415-9</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised discovery of Interpretable Visual Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caroline Mazini Rodrigues, Nicolas Boutry, Laurent Najman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing interpretability of deep-learning models to non-experts, while
fundamental for a responsible real-world usage, is challenging. Attribution
maps from xAI techniques, such as Integrated Gradients, are a typical example
of a visualization technique containing a high level of information, but with
difficult interpretation. In this paper, we propose two methods, Maximum
Activation Groups Extraction (MAGE) and Multiscale Interpretable Visualization
(Ms-IV), to explain the model's decision, enhancing global interpretability.
MAGE finds, for a given CNN, combinations of features which, globally, form a
semantic meaning, that we call concepts. We group these similar feature
patterns by clustering in ``concepts'', that we visualize through Ms-IV. This
last method is inspired by Occlusion and Sensitivity analysis (incorporating
causality), and uses a novel metric, called Class-aware Order Correlation
(CaOC), to globally evaluate the most important image regions according to the
model's decision space. We compare our approach to xAI methods such as LIME and
Integrated Gradients. Experimental results evince the Ms-IV higher localization
and faithfulness values. Finally, qualitative evaluation of combined MAGE and
Ms-IV demonstrates humans' ability to agree, based on the visualization, with
the decision of clusters' concepts; and, to detect, among a given set of
networks, the existence of bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empirical Risk Minimization with Relative Entropy Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06617v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06617v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir M. Perlaza, Gaetan Bisson, Iñaki Esnaola, Alain Jean-Marie, Stefano Rini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The empirical risk minimization (ERM) problem with relative entropy
regularization (ERM-RER) is investigated under the assumption that the
reference measure is a {\sigma}-finite measure, and not necessarily a
probability measure. Under this assumption, which leads to a generalization of
the ERM-RER problem allowing a larger degree of flexibility for incorporating
prior knowledge, numerous relevant properties are stated. Among these
properties, the solution to this problem, if it exists, is shown to be a unique
probability measure, often mutually absolutely continuous with the reference
measure. Such a solution exhibits a probably-approximately-correct guarantee
for the ERM problem independently of whether the latter possesses a solution.
For a fixed dataset, the empirical risk is shown to be a sub-Gaussian random
variable when the models are sampled from the solution to the ERM-RER problem.
The generalization capabilities of the solution to the ERM-RER problem (the
Gibbs algorithm) are studied via the sensitivity of the expected empirical risk
to deviations from such a solution towards alternative probability measures.
Finally, an interesting connection between sensitivity, generalization error,
and lautum information is established
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the the Transactions on Information Theory on June 12,
  2023. Also available as: Research Report, INRIA, No. RR-9454, Centre Inria
  d'Universit\'e C\^ote d'Azur, Sophia Antipolis, France, Feb., 2022 This
  version contains the revision for Transactions on Information Theory on
  November 21, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-shot backpropagation for multi-step prediction in physics-based
  system identification -- EXTENDED VERSION 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cesare Donati, Martina Mammarella, Fabrizio Dabbene, Carlo Novara, Constantino Lagoa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this paper is to present a novel physics-based framework for the
identification of dynamical systems, in which the physical and structural
insights are reflected directly into a backpropagation-based learning
algorithm. The main result is a method to compute in closed form the gradient
of a multi-step loss function, while enforcing physical properties and
constraints. The derived algorithm has been exploited to identify the unknown
inertia matrix of a space debris, and the results show the reliability of the
method in capturing the physical adherence of the estimated parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personas as a Way to Model Truthfulness in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18168v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18168v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung Kim, He He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are trained on vast amounts of text from the
internet, which contains both factual and misleading information about the
world. Can language models discern truth from falsehood in this contradicting
data? Expanding on the view that LLMs can model different communicative agents,
we present the persona hypothesis: LLMs can cluster agents into personas using
common features of their generations. For instance, a truthful persona is a
group of agents that are likely to produce truthful text and that share similar
features like formal writing styles and scientific references. By modeling this
persona, LLMs can generalize truthfulness beyond the specific contexts in which
each agent generated the training text. For example, the model can infer that
the agent ``Wikipedia'' will behave truthfully on topics that were only
generated by ``Science'' because they both belong to the truthful persona. We
show evidence for the persona hypothesis via two observations: (1) we can probe
whether a model's answer will be truthful before it is generated; (2)
finetuning a model on a set of facts improves its truthfulness on unseen
topics. Next, using arithmetics as a synthetic environment, we show that
language models can separate true and false statements, and generalize
truthfulness across agents; but only if agents in the training data share a
truthful generative process that enables the creation of a truthful persona.
Overall, our findings suggest that models can exploit hierarchical structures
in the data to learn abstract concepts like truthfulness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Counterfactual Data Augmentation Under Confounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abbavaram Gowtham Reddy, Saketh Bachu, Saloni Dash, Charchit Sharma, Amit Sharma, Vineeth N Balasubramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual data augmentation has recently emerged as a method to mitigate
confounding biases in the training data. These biases, such as spurious
correlations, arise due to various observed and unobserved confounding
variables in the data generation process. In this paper, we formally analyze
how confounding biases impact downstream classifiers and present a causal
viewpoint to the solutions based on counterfactual data augmentation. We
explore how removing confounding biases serves as a means to learn invariant
features, ultimately aiding in generalization beyond the observed data
distribution. Additionally, we present a straightforward yet powerful algorithm
for generating counterfactual images, which effectively mitigates the influence
of confounding effects on downstream classifiers. Through experiments on MNIST
variants and the CelebA datasets, we demonstrate how our simple augmentation
method helps existing state-of-the-art methods achieve good results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Labeling Oracles: What does it mean to steal ML models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avital Shafran, Ilia Shumailov, Murat A. Erdogdu, Nicolas Papernot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model extraction attacks are designed to steal trained models with only query
access, as is often provided through APIs that ML-as-a-Service providers offer.
ML models are expensive to train, in part because data is hard to obtain, and a
primary incentive for model extraction is to acquire a model while incurring
less cost than training from scratch. Literature on model extraction commonly
claims or presumes that the attacker is able to save on both data acquisition
and labeling costs. We show that the attacker often does not. This is because
current attacks implicitly rely on the adversary being able to sample from the
victim model's data distribution. We thoroughly evaluate factors influencing
the success of model extraction. We discover that prior knowledge of the
attacker, i.e. access to in-distribution data, dominates other factors like the
attack policy the adversary follows to choose which queries to make to the
victim model API. Thus, an adversary looking to develop an equally capable
model with a fixed budget has little practical incentive to perform model
extraction, since for the attack to work they need to collect in-distribution
data, saving only on the cost of labeling. With low labeling costs in the
current market, the usefulness of such attacks is questionable. Ultimately, we
demonstrate that the effect of prior knowledge needs to be explicitly decoupled
from the attack policy. To this end, we propose a benchmark to evaluate attack
policy directly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self supervised convolutional kernel based handcrafted feature
  harmonization: Enhanced left ventricle hypertension disease phenotyping on
  echocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jina Lee, Youngtaek Hong, Dawun Jeong, Yeonggul Jang, Sihyeon Jeong, Taekgeun Jung, Yeonyee E. Yoon, Inki Moon, Seung-Ah Lee, Hyuk-Jae Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiomics, a medical imaging technique, extracts quantitative handcrafted
features from images to predict diseases. Harmonization in those features
ensures consistent feature extraction across various imaging devices and
protocols. Methods for harmonization include standardized imaging protocols,
statistical adjustments, and evaluating feature robustness. Myocardial diseases
such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD)
are diagnosed via echocardiography, but variable imaging settings pose
challenges. Harmonization techniques are crucial for applying handcrafted
features in disease diagnosis in such scenario. Self-supervised learning (SSL)
enhances data understanding within limited datasets and adapts to diverse data
settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying
superior performance in various tasks. This study focuses on convolutional
filters within SSL, using them as preprocessing to convert images into feature
maps for handcrafted feature harmonization. Our proposed method excelled in
harmonization evaluation and exhibited superior LVH classification performance
compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting Black-box Machine Learning Models for High Dimensional
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.13405v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.13405v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Rezaul Karim, Md. Shajalal, Alex Graß, Till Döhmen, Sisay Adugna Chala, Alexander Boden, Christian Beecks, Stefan Decker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have been shown to outperform traditional machine
learning algorithms in a broad variety of application domains due to their
effectiveness in modeling complex problems and handling high-dimensional
datasets. Many real-life datasets, however, are of increasingly high
dimensionality, where a large number of features may be irrelevant for both
supervised and unsupervised learning tasks. The inclusion of such features
would not only introduce unwanted noise but also increase computational
complexity. Furthermore, due to high non-linearity and dependency among a large
number of features, DNN models tend to be unavoidably opaque and perceived as
black-box methods because of their not well-understood internal functioning.
Their algorithmic complexity is often simply beyond the capacities of humans to
understand the interplay among myriads of hyperparameters. A well-interpretable
model can identify statistically significant features and explain the way they
affect the model's outcome. In this paper, we propose an efficient method to
improve the interpretability of black-box models for classification tasks in
the case of high-dimensional datasets. First, we train a black-box model on a
high-dimensional dataset to learn the embeddings on which the classification is
performed. To decompose the inner working principles of the black-box model and
to identify top-k important features, we employ different probing and
perturbing techniques. We then approximate the behavior of the black-box model
by means of an interpretable surrogate model on the top-k feature space.
Finally, we derive decision rules and local explanations from the surrogate
model to explain individual decisions. Our approach outperforms
state-of-the-art methods like TabNet and XGboost when tested on different
datasets with varying dimensionality between 50 and 20,000 w.r.t metrics and
explainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is currently under review in a journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensor Train for Global Optimization Problems in Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05077v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05077v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhan Shetty, Teguh Lembono, Tobias Loew, Sylvain Calinon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The convergence of many numerical optimization techniques is highly dependent
on the initial guess given to the solver. To address this issue, we propose a
novel approach that utilizes tensor methods to initialize existing optimization
solvers near global optima. Our method does not require access to a database of
good solutions. We first transform the cost function, which depends on both
task parameters and optimization variables, into a probability density
function. Unlike existing approaches, the joint probability distribution of the
task parameters and optimization variables is approximated using the Tensor
Train model, which enables efficient conditioning and sampling. We treat the
task parameters as random variables, and for a given task, we generate samples
for decision variables from the conditional distribution to initialize the
optimization solver. Our method can produce multiple solutions (when they
exist) faster than existing methods. We first evaluate the approach on
benchmark functions for numerical optimization that are hard to solve using
gradient-based optimization solvers with a naive initialization. The results
show that the proposed method can generate samples close to global optima and
from multiple modes. We then demonstrate the generality and relevance of our
framework to robotics by applying it to inverse kinematics with obstacles and
motion planning problems with a 7-DoF manipulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated
  Class Incremental Learning for Vision Tasks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07784v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07784v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Babakniya, Zalan Fabian, Chaoyang He, Mahdi Soltanolkotabi, Salman Avestimehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models often suffer from forgetting previously learned
information when trained on new data. This problem is exacerbated in federated
learning (FL), where the data is distributed and can change independently for
each user. Many solutions are proposed to resolve this catastrophic forgetting
in a centralized setting. However, they do not apply directly to FL because of
its unique complexities, such as privacy concerns and resource limitations. To
overcome these challenges, this paper presents a framework for
$\textbf{federated class incremental learning}$ that utilizes a generative
model to synthesize samples from past distributions. This data can be later
exploited alongside the training data to mitigate catastrophic forgetting. To
preserve privacy, the generative model is trained on the server using data-free
methods at the end of each task without requesting data from clients. Moreover,
our solution does not demand the users to store old data or models, which gives
them the freedom to join/leave the training at any time. Additionally, we
introduce SuperImageNet, a new regrouping of the ImageNet dataset specifically
tailored for federated continual learning. We demonstrate significant
improvements compared to existing baselines through extensive experiments on
multiple datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2023. arXiv admin note: text overlap with
  arXiv:2307.00497</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ROOT-SGD: Sharp Nonasymptotics and Asymptotic Efficiency in a Single
  Algorithm <span class="chip">COLT 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.12690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.12690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Junchi Li, Wenlong Mou, Martin J. Wainwright, Michael I. Jordan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of solving strongly convex and smooth unconstrained
optimization problems using stochastic first-order algorithms. We devise a
novel algorithm, referred to as \emph{Recursive One-Over-T SGD} (\ROOTSGD),
based on an easily implementable, recursive averaging of past stochastic
gradients. We prove that it simultaneously achieves state-of-the-art
performance in both a finite-sample, nonasymptotic sense and an asymptotic
sense. On the nonasymptotic side, we prove risk bounds on the last iterate of
\ROOTSGD with leading-order terms that match the optimal statistical risk with
a unity pre-factor, along with a higher-order term that scales at the sharp
rate of $O(n^{-3/2})$ under the Lipschitz condition on the Hessian matrix. On
the asymptotic side, we show that when a mild, one-point Hessian continuity
condition is imposed, the rescaled last iterate of (multi-epoch) \ROOTSGD
converges asymptotically to a Gaussian limit with the Cram\'{e}r-Rao optimal
asymptotic covariance, for a broad range of step-size choices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera Ready, COLT 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Data-Algorithm Dependent Generalization: a Case Study on
  Overparameterized Linear Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Xu, Jiaye Teng, Yang Yuan, Andrew Chi-Chih Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the major open problems in machine learning is to characterize
generalization in the overparameterized regime, where most traditional
generalization bounds become inconsistent even for overparameterized linear
regression. In many scenarios, this failure can be attributed to obscuring the
crucial interplay between the training algorithm and the underlying data
distribution. This paper demonstrate that the generalization behavior of
overparameterized model should be analyzed in a both data-relevant and
algorithm-relevant manner. To make a formal characterization, We introduce a
notion called data-algorithm compatibility, which considers the generalization
behavior of the entire data-dependent training trajectory, instead of
traditional last-iterate analysis. We validate our claim by studying the
setting of solving overparameterized linear regression with gradient descent.
Specifically, we perform a data-dependent trajectory analysis and derive a
sufficient condition for compatibility in such a setting. Our theoretical
results demonstrate that if we take early stopping iterates into consideration,
generalization can hold with significantly weaker restrictions on the problem
instance than the previous last-iterate analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digital Twin Assisted Deep Reinforcement Learning for Online Admission
  Control in Sliced Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09299v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09299v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Tao, Wei Xu, Xiaohu You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of diverse wireless services in 5G and beyond has led to
the emergence of network slicing technologies. Among these, admission control
plays a crucial role in achieving service-oriented optimization goals through
the selective acceptance of service requests. Although deep reinforcement
learning (DRL) forms the foundation in many admission control approaches thanks
to its effectiveness and flexibility, initial instability with excessive
convergence delay of DRL models hinders their deployment in real-world
networks. We propose a digital twin (DT) accelerated DRL solution to address
this issue. Specifically, we first formulate the admission decision-making
process as a semi-Markov decision process, which is subsequently simplified
into an equivalent discrete-time Markov decision process to facilitate the
implementation of DRL methods. A neural network-based DT is established with a
customized output layer for queuing systems, trained through supervised
learning, and then employed to assist the training phase of the DRL model.
Extensive simulations show that the DT-accelerated DRL improves resource
utilization by over 40% compared to the directly trained state-of-the-art
dueling deep Q-learning model. This improvement is achieved while preserving
the model's capability to optimize the long-term rewards of the admission
process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene
  Graphs with Weak Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiani Huang, Ziyang Li, Mayur Naik, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose LASER, a neuro-symbolic approach to learn semantic video
representations that capture rich spatial and temporal properties in video data
by leveraging high-level logic specifications. In particular, we formulate the
problem in terms of alignment between raw videos and spatio-temporal logic
specifications. The alignment algorithm leverages a differentiable symbolic
reasoner and a combination of contrastive, temporal, and semantics losses. It
effectively and efficiently trains low-level perception models to extract
fine-grained video representation in the form of a spatio-temporal scene graph
that conforms to the desired high-level specification. In doing so, we explore
a novel methodology that weakly supervises the learning of video semantic
representations through logic specifications. We evaluate our method on two
datasets with rich spatial and temporal specifications:
20BN-Something-Something and MUGEN. We demonstrate that our method learns
better fine-grained video semantics than existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-term Causal Effects Estimation via Latent Surrogates Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04589v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04589v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichu Cai, Weilin Chen, Zeqin Yang, Shu Wan, Chen Zheng, Xiaoqing Yang, Jiecheng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating long-term causal effects based on short-term surrogates is a
significant but challenging problem in many real-world applications, e.g.,
marketing and medicine. Despite its success in certain domains, most existing
methods estimate causal effects in an idealistic and simplistic way - ignoring
the causal structure among short-term outcomes and treating all of them as
surrogates. However, such methods cannot be well applied to real-world
scenarios, in which the partially observed surrogates are mixed with their
proxies among short-term outcomes. To this end, we develop our flexible method,
Laser, to estimate long-term causal effects in the more realistic situation
that the surrogates are observed or have observed proxies.Given the
indistinguishability between the surrogates and proxies, we utilize
identifiable variational auto-encoder (iVAE) to recover the whole valid
surrogates on all the surrogates candidates without the need of distinguishing
the observed surrogates or the proxies of latent surrogates. With the help of
the recovered surrogates, we further devise an unbiased estimation of long-term
causal effects. Extensive experimental results on the real-world and
semi-synthetic datasets demonstrate the effectiveness of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic <span class="highlight-title">Review</span> of Aspect-based Sentiment Analysis (ABSA): Domains,
  Methods, and Trends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Cathy Hua, Paul Denny, Katerina Taskova, Jörg Wicker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based Sentiment Analysis (ABSA) is a type of fine-grained sentiment
analysis (SA) that identifies aspects and the associated opinions from a given
text. In the digital era, ABSA gained increasing popularity and applications in
mining opinionated text data to obtain insights and support decisions. ABSA
research employs linguistic, statistical, and machine-learning approaches and
utilises resources such as labelled datasets, aspect and sentiment lexicons and
ontology. By its nature, ABSA is domain-dependent and can be sensitive to the
impact of misalignment between the resource and application domains. However,
to our knowledge, this topic has not been explored by the existing ABSA
literature reviews. In this paper, we present a Systematic Literature Review
(SLR) of ABSA studies with a focus on the research application domain, dataset
domain, and the research methods to examine their relationships and identify
trends over time. Our results suggest a number of potential systemic issues in
the ABSA research literature, including the predominance of the
``product/service review'' dataset domain among the majority of studies that
did not have a specific research application domain, coupled with the
prevalence of dataset-reliant methods such as supervised machine learning. This
review makes a number of unique contributions to the ABSA research field: 1) To
our knowledge, it is the first SLR that links the research domain, dataset
domain, and research method through a systematic perspective; 2) it is one of
the largest scoped SLR on ABSA, with 519 eligible studies filtered from 4191
search results without time constraint; and 3) our review methodology adopted
an innovative automatic filtering process based on PDF-mining, which enhanced
screening quality and reliability. Suggestions and our review limitations are
also discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exponentially Faster Language Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Belcak, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models only really need to use an exponential fraction of their
neurons for individual inferences. As proof, we present UltraFastBERT, a BERT
variant that uses 0.3% of its neurons during inference while performing on par
with similar BERT models. UltraFastBERT selectively engages just 12 out of 4095
neurons for each layer inference. This is achieved by replacing feedforward
networks with fast feedforward networks (FFFs). While no truly efficient
implementation currently exists to unlock the full acceleration potential of
conditional neural execution, we provide high-level CPU code achieving 78x
speedup over the optimized baseline feedforward implementation, and a PyTorch
implementation delivering 40x speedup over the equivalent batched feedforward
inference. We publish our training code, benchmarking setup, and model weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verified Compositional Neuro-Symbolic Control for Stochastic Systems
  with Temporal Logic Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Wang, Kaiyuan Tan, Zihe Sun, Yiannis Kantaros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several methods have been proposed recently to learn neural network (NN)
controllers for autonomous agents, with unknown and stochastic dynamics, tasked
with complex missions captured by Linear Temporal Logic (LTL). Due to the
sample-inefficiency of the majority of these works, compositional learning
methods have been proposed decomposing the LTL specification into smaller
sub-tasks. Then, separate controllers are learned and composed to satisfy the
original task. A key challenge within these approaches is that they often lack
safety guarantees or the provided guarantees are impractical. This paper aims
to address this challenge. Particularly, we consider autonomous systems with
unknown and stochastic dynamics and LTL-encoded tasks. We assume that the
system is equipped with a finite set of base skills modeled by trained NN
feedback controllers. Our goal is to check if there exists a temporal
composition of the trained NN controllers - and if so, to compute it - that
will yield a composite system behavior that satisfies the assigned LTL task
with probability one. We propose a new approach that relies on a novel
integration of automata theory and data-driven reachability analysis tools for
NN-controlled stochastic systems. The resulting neuro-symbolic controller
allows the agent to generate safe behaviors for unseen complex temporal logic
tasks in a zero-shot fashion by leveraging its base skills. We show correctness
of the proposed method and we provide conditions under which it is complete. To
the best of our knowledge, this is the first work that designs verified
temporal compositions of NN controllers for unknown and stochastic systems.
Finally, we provide extensive numerical simulations and hardware experiments on
robot navigation tasks to demonstrate the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper was withdrawn as it did not include the correct author
  list, credit was given to the wrong author</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better with Less: A Data-Active Perspective on <span class="highlight-title">Pre-Train</span>ing Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarong Xu, Renhong Huang, Xin Jiang, Yuxuan Cao, Carl Yang, Chunping Wang, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training on graph neural networks (GNNs) aims to learn transferable
knowledge for downstream tasks with unlabeled data, and it has recently become
an active research area. The success of graph pre-training models is often
attributed to the massive amount of input data. In this paper, however, we
identify the curse of big data phenomenon in graph pre-training: more training
data do not necessarily lead to better downstream performance. Motivated by
this observation, we propose a better-with-less framework for graph
pre-training: fewer, but carefully chosen data are fed into a GNN model to
enhance pre-training. The proposed pre-training pipeline is called the
data-active graph pre-training (APT) framework, and is composed of a graph
selector and a pre-training model. The graph selector chooses the most
representative and instructive data points based on the inherent properties of
graphs as well as predictive uncertainty. The proposed predictive uncertainty,
as feedback from the pre-training model, measures the confidence level of the
model in the data. When fed with the chosen data, on the other hand, the
pre-training model grasps an initial understanding of the new, unseen data, and
at the same time attempts to remember the knowledge learned from previous data.
Therefore, the integration and interaction between these two components form a
unified framework (APT), in which graph pre-training is performed in a
progressive and iterative way. Experiment results show that the proposed APT is
able to obtain an efficient pre-training model with fewer training data and
better downstream performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multifidelity Deep Operator Networks For Data-Driven and
  Physics-Informed Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.09157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.09157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amanda A. Howard, Mauro Perego, George E. Karniadakis, Panos Stinis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operator learning for complex nonlinear systems is increasingly common in
modeling multi-physics and multi-scale systems. However, training such
high-dimensional operators requires a large amount of expensive, high-fidelity
data, either from experiments or simulations. In this work, we present a
composite Deep Operator Network (DeepONet) for learning using two datasets with
different levels of fidelity to accurately learn complex operators when
sufficient high-fidelity data is not available. Additionally, we demonstrate
that the presence of low-fidelity data can improve the predictions of
physics-informed learning with DeepONets. We demonstrate the new multi-fidelity
training in diverse examples, including modeling of the ice-sheet dynamics of
the Humboldt glacier, Greenland, using two different fidelity models and also
using the same physical model at two different resolutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stacked networks improve physics-informed training: applications to
  neural networks and deep operator networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amanda A Howard, Sarah H Murphy, Shady E Ahmed, Panos Stinis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks and operator networks have shown promise for
effectively solving equations modeling physical systems. However, these
networks can be difficult or impossible to train accurately for some systems of
equations. We present a novel multifidelity framework for stacking
physics-informed neural networks and operator networks that facilitates
training. We successively build a chain of networks, where the output at one
step can act as a low-fidelity input for training the next step, gradually
increasing the expressivity of the learned model. The equations imposed at each
step of the iterative process can be the same or different (akin to simulated
annealing). The iterative (stacking) nature of the proposed method allows us to
progressively learn features of a solution that are hard to learn directly.
Through benchmark problems including a nonlinear pendulum, the wave equation,
and the viscous Burgers equation, we show how stacking can be used to improve
the accuracy and reduce the required size of physics-informed neural networks
and operator networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised and Unsupervised Deep Learning Approaches for EEG Seizure
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zakary Georgis-Yap, Milos R. Popovic, Shehroz S. Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Epilepsy affects more than 50 million people worldwide, making it one of the
world's most prevalent neurological diseases. The main symptom of epilepsy is
seizures, which occur abruptly and can cause serious injury or death. The
ability to predict the occurrence of an epileptic seizure could alleviate many
risks and stresses people with epilepsy face. We formulate the problem of
detecting preictal (or pre-seizure) with reference to normal EEG as a precursor
to incoming seizure. To this end, we developed several supervised deep learning
approaches model to identify preictal EEG from normal EEG. We further develop
novel unsupervised deep learning approaches to train the models on only normal
EEG, and detecting pre-seizure EEG as an anomalous event. These deep learning
models were trained and evaluated on two large EEG seizure datasets in a
person-specific manner. We found that both supervised and unsupervised
approaches are feasible; however, their performance varies depending on the
patient, approach and architecture. This new line of research has the potential
to develop therapeutic interventions and save human lives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Reinforcement Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning is an essential paradigm for solving sequential
decision problems under uncertainty. Despite many remarkable achievements in
recent decades, applying reinforcement learning methods in the real world
remains challenging. One of the main obstacles is that reinforcement learning
agents lack a fundamental understanding of the world and must therefore learn
from scratch through numerous trial-and-error interactions. They may also face
challenges in providing explanations for their decisions and generalizing the
acquired knowledge. Causality, however, offers a notable advantage as it can
formalize knowledge in a systematic manner and leverage invariance for
effective knowledge transfer. This has led to the emergence of causal
reinforcement learning, a subfield of reinforcement learning that seeks to
enhance existing algorithms by incorporating causal relationships into the
learning process. In this survey, we comprehensively review the literature on
causal reinforcement learning. We first introduce the basic concepts of
causality and reinforcement learning, and then explain how causality can
address core challenges in non-causal reinforcement learning. We categorize and
systematically review existing causal reinforcement learning approaches based
on their target problems and methodologies. Finally, we outline open issues and
future directions in this emerging field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data is often loadable in short depth: Quantum circuits from tensor
  networks for finance, images, fluids, and proteins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Jumade, Nicolas PD Sawaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though there has been substantial progress in developing quantum algorithms
to study classical datasets, the cost of simply loading classical data is an
obstacle to quantum advantage. When the amplitude encoding is used, loading an
arbitrary classical vector requires up to exponential circuit depths with
respect to the number of qubits. Here, we address this "input problem" with two
contributions. First, we introduce a circuit compilation method based on tensor
network (TN) theory. Our method -- AMLET (Automatic Multi-layer Loader
Exploiting TNs) -- proceeds via careful construction of a specific TN topology
and can be tailored to arbitrary circuit depths. Second, we perform numerical
experiments on real-world classical data from four distinct areas: finance,
images, fluid mechanics, and proteins. To the best of our knowledge, this is
the broadest numerical analysis to date of loading classical data into a
quantum computer. Consistent with other recent work in this area, the required
circuit depths are often several orders of magnitude lower than the
exponentially-scaling general loading algorithm would require. Besides
introducing a more efficient loading algorithm, this work demonstrates that
many classical datasets are loadable in depths that are much shorter than
previously expected, which has positive implications for speeding up classical
workloads on quantum computers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03488v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03488v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derek Lilienthal, Paul Mello, Magdalini Eirinaki, Stas Tiomkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recommender systems have become an integral component of the Web
experience, their heavy reliance on user data raises privacy and security
concerns. Substituting user data with synthetic data can address these
concerns, but accurately replicating these real-world datasets has been a
notoriously challenging problem. Recent advancements in generative AI have
demonstrated the impressive capabilities of diffusion models in generating
realistic data across various domains. In this work we introduce a Score-based
Diffusion Recommendation Module (SDRM), which captures the intricate patterns
of real-world datasets required for training highly accurate recommender
systems. SDRM allows for the generation of synthetic data that can replace
existing datasets to preserve user privacy, or augment existing datasets to
address excessive data sparsity. Our method outperforms competing baselines
such as generative adversarial networks, variational autoencoders, and recently
proposed diffusion models in synthesizing various datasets to replace or
augment the original data by an average improvement of 4.30% in Recall@$k$ and
4.65% in NDCG@$k$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Absolute Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiye Zhao, Feihan Li, Yifan Sun, Rui Chen, Tianhao Wei, Changliu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, trust region on-policy reinforcement learning has achieved
impressive results in addressing complex control tasks and gaming scenarios.
However, contemporary state-of-the-art algorithms within this category
primarily emphasize improvement in expected performance, lacking the ability to
control over the worst-case performance outcomes. To address this limitation,
we introduce a novel objective function; by optimizing which, it will lead to
guaranteed monotonic improvement in the lower bound of near-total performance
samples (absolute performance). Considering this groundbreaking theoretical
advancement, we then refine this theoretically grounded algorithm through a
series of approximations, resulting in a practical solution called Absolute
Policy Optimization (APO). Our experiments demonstrate the effectiveness of our
approach across challenging continuous control benchmark tasks and extend its
applicability to mastering Atari games. Our findings reveal that APO
significantly outperforms state-of-the-art policy gradient algorithms,
resulting in substantial improvements in both expected performance and
worst-case performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I submitted this article to Journal of Machine Learning Research. The
  manuscript will go under a major revision and I don't want the reviewer know
  who I am. I will re-upload after JMLR review released</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extraction and Summarization of Explicit Video Content using Multi-Modal
  Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaunak Joshi, Raghav Gaggar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increase in video-sharing platforms across the internet, it is
difficult for humans to moderate the data for explicit content. Hence, an
automated pipeline to scan through video data for explicit content has become
the need of the hour. We propose a novel pipeline that uses multi-modal deep
learning to first extract the explicit segments of input videos and then
summarize their content using text to determine its age appropriateness and age
rating. We also evaluate our pipeline's effectiveness in the end using standard
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Randomized Approach for Tight Privacy Accounting <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen T. Wang, Saeed Mahloujifar, Tong Wu, Ruoxi Jia, Prateek Mittal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bounding privacy leakage over compositions, i.e., privacy accounting, is a
key challenge in differential privacy (DP). The privacy parameter ($\eps$ or
$\delta$) is often easy to estimate but hard to bound. In this paper, we
propose a new differential privacy paradigm called estimate-verify-release
(EVR), which addresses the challenges of providing a strict upper bound for
privacy parameter in DP compositions by converting an estimate of privacy
parameter into a formal guarantee. The EVR paradigm first estimates the privacy
parameter of a mechanism, then verifies whether it meets this guarantee, and
finally releases the query output based on the verification result. The core
component of the EVR is privacy verification. We develop a randomized privacy
verifier using Monte Carlo (MC) technique. Furthermore, we propose an MC-based
DP accountant that outperforms existing DP accounting techniques in terms of
accuracy and efficiency. Our empirical evaluation shows the newly proposed EVR
paradigm improves the utility-privacy tradeoff for privacy-preserving machine
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why Shallow Networks Struggle with Approximating and Learning High
  Frequency: A Numerical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijun Zhang, Hongkai Zhao, Yimin Zhong, Haomin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, a comprehensive numerical study involving analysis and
experiments shows why a two-layer neural network has difficulties handling high
frequencies in approximation and learning when machine precision and
computation cost are important factors in real practice. In particular, the
following basic computational issues are investigated: (1) the minimal
numerical error one can achieve given a finite machine precision, (2) the
computation cost to achieve a given accuracy, and (3) stability with respect to
perturbations. The key to the study is the conditioning of the representation
and its learning dynamics. Explicit answers to the above questions with
numerical verifications are presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoCLIP: Clip-Inspired Alignment between Locations and Images for
  Effective Worldwide Geo-localization <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vicente Vivanco Cepeda, Gaurav Kumar Nayak, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Worldwide Geo-localization aims to pinpoint the precise location of images
taken anywhere on Earth. This task has considerable challenges due to immense
variation in geographic landscapes. The image-to-image retrieval-based
approaches fail to solve this problem on a global scale as it is not feasible
to construct a large gallery of images covering the entire world. Instead,
existing approaches divide the globe into discrete geographic cells,
transforming the problem into a classification task. However, their performance
is limited by the predefined classes and often results in inaccurate
localizations when an image's location significantly deviates from its class
center. To overcome these limitations, we propose GeoCLIP, a novel
CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between
the image and its corresponding GPS locations. GeoCLIP's location encoder
models the Earth as a continuous function by employing positional encoding
through random Fourier features and constructing a hierarchical representation
that captures information at varying resolutions to yield a semantically rich
high-dimensional feature suitable to use even beyond geo-localization. To the
best of our knowledge, this is the first work employing GPS encoding for
geo-localization. We demonstrate the efficacy of our method via extensive
experiments and ablations on benchmark datasets. We achieve competitive
performance with just 20% of training data, highlighting its effectiveness even
in limited-data settings. Furthermore, we qualitatively demonstrate
geo-localization using a text query by leveraging CLIP backbone of our image
encoder. The project webpage is available at:
https://vicentevivan.github.io/GeoCLIP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Taylor-Approximated Gradients to Improve the Frank-Wolfe Method
  for Empirical Risk Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.13933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.13933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikai Xiong, Robert M. Freund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Frank-Wolfe method has become increasingly useful in statistical and
machine learning applications, due to the structure-inducing properties of the
iterates, and especially in settings where linear minimization over the
feasible set is more computationally efficient than projection. In the setting
of Empirical Risk Minimization -- one of the fundamental optimization problems
in statistical and machine learning -- the computational effectiveness of
Frank-Wolfe methods typically grows linearly in the number of data observations
$n$. This is in stark contrast to the case for typical stochastic projection
methods. In order to reduce this dependence on $n$, we look to second-order
smoothness of typical smooth loss functions (least squares loss and logistic
loss, for example) and we propose amending the Frank-Wolfe method with Taylor
series-approximated gradients, including variants for both deterministic and
stochastic settings. Compared with current state-of-the-art methods in the
regime where the optimality tolerance $\varepsilon$ is sufficiently small, our
methods are able to simultaneously reduce the dependence on large $n$ while
obtaining optimal convergence rates of Frank-Wolfe methods, in both the convex
and non-convex settings. We also propose a novel adaptive step-size approach
for which we have computational guarantees. Last of all, we present
computational experiments which show that our methods exhibit very significant
speed-ups over existing methods on real-world datasets for both convex and
non-convex binary classification problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancements in Generative AI: A Comprehensive <span class="highlight-title">Review</span> of GANs, <span class="highlight-title">GPT</span>,
  Autoencoders, Diffusion Model, and <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Staphord Bengesi, Hoda El-Sayed, Md Kamruzzaman Sarker, Yao Houkpati, John Irungu, Timothy Oladunni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The launch of ChatGPT has garnered global attention, marking a significant
milestone in the field of Generative Artificial Intelligence. While Generative
AI has been in effect for the past decade, the introduction of ChatGPT has
ignited a new wave of research and innovation in the AI domain. This surge in
interest has led to the development and release of numerous cutting-edge tools,
such as Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox,
among others. These tools exhibit remarkable capabilities, encompassing tasks
ranging from text generation and music composition, image creation, video
production, code generation, and even scientific work. They are built upon
various state-of-the-art models, including Stable Diffusion, transformer models
like GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial
networks. This advancement in Generative AI presents a wealth of exciting
opportunities and, simultaneously, unprecedented challenges. Throughout this
paper, we have explored these state-of-the-art models, the diverse array of
tasks they can accomplish, the challenges they pose, and the promising future
of Generative Artificial Intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HINT: Healthy Influential-Noise based Training to Defend against Data
  Poisoning Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08549v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08549v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh-Hao Van, Alycia N. Carey, Xintao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While numerous defense methods have been proposed to prohibit potential
poisoning attacks from untrusted data sources, most research works only defend
against specific attacks, which leaves many avenues for an adversary to
exploit. In this work, we propose an efficient and robust training approach to
defend against data poisoning attacks based on influence functions, named
Healthy Influential-Noise based Training. Using influence functions, we craft
healthy noise that helps to harden the classification model against poisoning
attacks without significantly affecting the generalization ability on test
data. In addition, our method can perform effectively when only a subset of the
training data is modified, instead of the current method of adding noise to all
examples that has been used in several previous works. We conduct comprehensive
evaluations over two image datasets with state-of-the-art poisoning attacks
under different realistic attack scenarios. Our empirical results show that
HINT can efficiently protect deep learning models against the effect of both
untargeted and targeted poisoning attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epsilon*: Privacy Metric for Machine Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diana M. Negoescu, Humberto Gonzalez, Saad Eddin Al Orjany, Jilei Yang, Yuliia Lut, Rahul Tandra, Xiaowen Zhang, Xinyi Zheng, Zach Douglas, Vidita Nolkha, Parvez Ahammad, Gennady Samorodnitsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Epsilon*, a new privacy metric for measuring the privacy risk of
a single model instance prior to, during, or after deployment of privacy
mitigation strategies. The metric requires only black-box access to model
predictions, does not require training data re-sampling or model re-training,
and can be used to measure the privacy risk of models not trained with
differential privacy. Epsilon* is a function of true positive and false
positive rates in a hypothesis test used by an adversary in a membership
inference attack. We distinguish between quantifying the privacy loss of a
trained model instance, which we refer to as empirical privacy, and quantifying
the privacy loss of the training mechanism which produces this model instance.
Existing approaches in the privacy auditing literature provide lower bounds for
the latter, while our metric provides an empirical lower bound for the former
by relying on an (${\epsilon}$, ${\delta}$)-type of quantification of the
privacy of the trained model instance. We establish a relationship between
these lower bounds and show how to implement Epsilon* to avoid numerical and
noise amplification instability. We further show in experiments on benchmark
public data sets that Epsilon* is sensitive to privacy risk mitigation by
training with differential privacy (DP), where the value of Epsilon* is reduced
by up to 800% compared to the Epsilon* values of non-DP trained baseline
models. This metric allows privacy auditors to be independent of model owners,
and enables visualizing the privacy-utility landscape to make informed
decisions regarding the trade-offs between model privacy and utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable Generation of EHR Time Series via Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15290v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15290v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhang Tian, Bernie Chen, Allan Guo, Shiyi Jiang, Anru R. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHRs) are rich sources of patient-level data,
including laboratory tests, medications, and diagnoses, offering valuable
resources for medical data analysis. However, concerns about privacy often
restrict access to EHRs, hindering downstream analysis. Researchers have
explored various methods for generating privacy-preserving EHR data. In this
study, we introduce a new method for generating diverse and realistic synthetic
EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We
conducted experiments on six datasets, comparing our proposed method with eight
existing methods. Our results demonstrate that our approach significantly
outperforms all existing methods in terms of data utility while requiring less
training effort. Our approach also enhances downstream medical data analysis by
providing diverse and realistic synthetic EHR data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inferring Actual Treatment Pathways from Patient Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Wilkins-Caruana, Madhushi Bandara, Katarzyna Musial, Daniel Catchpoole, Paul J. Kennedy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Treatment pathways are step-by-step plans outlining the recommended medical
care for specific diseases; they get revised when different treatments are
found to improve patient outcomes. Examining health records is an important
part of this revision process, but inferring patients' actual treatments from
health data is challenging due to complex event-coding schemes and the absence
of pathway-related annotations. This study aims to infer the actual treatment
steps for a particular patient group from administrative health records (AHR) -
a common form of tabular healthcare data - and address several technique- and
methodology-based gaps in treatment pathway-inference research. We introduce
Defrag, a method for examining AHRs to infer the real-world treatment steps for
a particular patient group. Defrag learns the semantic and temporal meaning of
healthcare event sequences, allowing it to reliably infer treatment steps from
complex healthcare data. To our knowledge, Defrag is the first
pathway-inference method to utilise a neural network (NN), an approach made
possible by a novel, self-supervised learning objective. We also developed a
testing and validation framework for pathway inference, which we use to
characterise and evaluate Defrag's pathway inference ability and compare
against baselines. We demonstrate Defrag's effectiveness by identifying
best-practice pathway fragments for breast cancer, lung cancer, and melanoma in
public healthcare records. Additionally, we use synthetic data experiments to
demonstrate the characteristics of the Defrag method, and to compare Defrag to
several baselines where it significantly outperforms non-NN-based methods.
Defrag significantly outperforms several existing pathway-inference methods and
offers an innovative and effective approach for inferring treatment pathways
from AHRs. Open-source code is provided to encourage further research in this
area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stability and Generalization of Stochastic Compositional Gradient
  Descent Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Yang, Xiyuan Wei, Tianbao Yang, Yiming Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many machine learning tasks can be formulated as a stochastic compositional
optimization (SCO) problem such as reinforcement learning, AUC maximization,
and meta-learning, where the objective function involves a nested composition
associated with an expectation. While a significant amount of studies has been
devoted to studying the convergence behavior of SCO algorithms, there is little
work on understanding their generalization, i.e., how these learning algorithms
built from training examples would behave on future test examples. In this
paper, we provide the stability and generalization analysis of stochastic
compositional gradient descent algorithms through the lens of algorithmic
stability in the framework of statistical learning theory. Firstly, we
introduce a stability concept called compositional uniform stability and
establish its quantitative relation with generalization for SCO problems. Then,
we establish the compositional uniform stability results for two popular
stochastic compositional gradient descent algorithms, namely SCGD and SCSC.
Finally, we derive dimension-independent excess risk bounds for SCGD and SCSC
by trade-offing their stability results and optimization errors. To the best of
our knowledge, these are the first-ever-known results on stability and
generalization analysis of stochastic compositional gradient descent
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safety-aware Causal Representation for Trustworthy Reinforcement
  Learning in Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohong Lin, Wenhao Ding, Zuxin Liu, Yaru Niu, Jiacheng Zhu, Yuming Niu, Ding Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of autonomous driving, the Learning from Demonstration (LfD)
paradigm has exhibited notable efficacy in addressing sequential
decision-making problems. However, consistently achieving safety in varying
traffic contexts, especially in safety-critical scenarios, poses a significant
challenge due to the long-tailed and unforeseen scenarios absent from offline
datasets. In this paper, we introduce the saFety-aware strUctured Scenario
representatION (FUSION), a pioneering methodology conceived to facilitate the
learning of an adaptive end-to-end driving policy by leveraging structured
scenario information. FUSION capitalizes on the causal relationships between
decomposed reward, cost, state, and action space, constructing a framework for
structured sequential reasoning under dynamic traffic environments. We conduct
rigorous evaluations in two typical real-world settings of distribution shift
in autonomous vehicles, demonstrating the good balance between safety cost and
utility reward of FUSION compared to contemporary state-of-the-art safety-aware
LfD baselines. Empirical evidence under diverse driving scenarios attests that
FUSION significantly enhances the safety and generalizability of autonomous
driving agents, even in the face of challenging and unseen environments.
Furthermore, our ablation studies reveal noticeable improvements in the
integration of causal representation into the safe offline RL problem.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with
  Spatially Relation Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Chu, Zhedong Zheng, Wei Ji, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drone navigation through natural language commands remains a significant
challenge due to the lack of publicly available multi-modal datasets and the
intricate demands of fine-grained visual-text alignment. In response to this
pressing need, we present a new human-computer interaction annotation benchmark
called GeoText-1652, meticulously curated through a robust Large Language Model
(LLM)-based data generation framework and the expertise of pre-trained vision
models. This new dataset seamlessly extends the existing image dataset, \ie,
University-1652, with spatial-aware text annotations, encompassing intricate
image-text-bounding box associations. Besides, we introduce a new optimization
objective to leverage fine-grained spatial associations, called blending
spatial matching, for region-level spatial relation matching. Extensive
experiments reveal that our approach maintains an exceptional recall rate under
varying description complexities. This underscores the promising potential of
our approach in elevating drone control and navigation through the seamless
integration of natural language commands in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HierSpeech++: Bridging the Gap between Semantic and Acoustic
  Representation of Speech by Hierarchical Variational Inference for Zero-shot
  Speech Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sang-Hoon Lee, Ha-Yeong Choi, Seung-Bin Kim, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM)-based speech synthesis has been widely adopted in
zero-shot speech synthesis. However, they require a large-scale data and
possess the same limitations as previous autoregressive speech models,
including slow inference speed and lack of robustness. This paper proposes
HierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech
(TTS) and voice conversion (VC). We verified that hierarchical speech synthesis
frameworks could significantly improve the robustness and expressiveness of the
synthetic speech. Furthermore, we significantly improve the naturalness and
speaker similarity of synthetic speech even in zero-shot speech synthesis
scenarios. For text-to-speech, we adopt the text-to-vec framework, which
generates a self-supervised speech representation and an F0 representation
based on text representations and prosody prompts. Then, HierSpeech++ generates
speech from the generated vector, F0, and voice prompt. We further introduce a
high-efficient speech super-resolution framework from 16 kHz to 48 kHz. The
experimental results demonstrated that the hierarchical variational autoencoder
could be a strong zero-shot speech synthesizer given that it outperforms
LLM-based and diffusion-based models. Moreover, we achieved the first
human-level quality zero-shot speech synthesis. Audio samples and source code
are available at https://github.com/sh-lee-prml/HierSpeechpp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CASR: Refining Action Segmentation via Magrinalizing Frame-levle Causal
  Relationships 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keqing Du, Xinyu Yang, Hang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating deep learning and causal discovery has increased the
interpretability of Temporal Action Segmentation (TAS) tasks. However,
frame-level causal relationships exist many complicated noises outside the
segment-level, making it infeasible to directly express macro action semantics.
Thus, we propose \textit{\textbf{Causal Abstraction Segmentation Refiner
(CASR)}}, which can refine TAS results from various models by enhancing video
causality in marginalizing frame-level casual relationships. Specifically, we
define the equivalent frame-level casual model and segment-level causal model,
so that the causal adjacency matrix constructed from marginalized frame-level
causal relationships has the ability to represent the segmnet-level causal
relationships. CASR works out by reducing the difference in the causal
adjacency matrix between we constructed and pre-segmentation results of
backbone models. In addition, we propose a novel evaluation metric Causal Edit
Distance (CED) to evaluate the causal interpretability. Extensive experimental
results on mainstream datasets indicate that CASR significantly surpasses
existing various methods in action segmentation performance, as well as in
causal explainability and generalization. Our code will be available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equipping <span class="highlight-title">Pretrain</span>ed Unconditional Music <span class="highlight-title">Transformer</span>s with Instrument
  and Genre Controls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihan Xu, Julian McAuley, Shlomo Dubnov, Hao-Wen Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ''pretraining-and-finetuning'' paradigm has become a norm for training
domain-specific models in natural language processing and computer vision. In
this work, we aim to examine this paradigm for symbolic music generation
through leveraging the largest ever symbolic music dataset sourced from the
MuseScore forum. We first pretrain a large unconditional transformer model
using 1.5 million songs. We then propose a simple technique to equip this
pretrained unconditional music transformer model with instrument and genre
controls by finetuning the model with additional control tokens. Our proposed
representation offers improved high-level controllability and expressiveness
against two existing representations. The experimental results show that the
proposed model can successfully generate music with user-specified instruments
and genre. In a subjective listening test, the proposed model outperforms the
pretrained baseline model in terms of coherence, harmony, arrangement and
overall quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attribute-Aware Deep Hashing with Self-Consistency for Large-Scale
  Fine-Grained Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiu-Shen Wei, Yang Shen, Xuhao Sun, Peng Wang, Yuxin Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our work focuses on tackling large-scale fine-grained image retrieval as
ranking the images depicting the concept of interests (i.e., the same
sub-category labels) highest based on the fine-grained details in the query. It
is desirable to alleviate the challenges of both fine-grained nature of small
inter-class variations with large intra-class variations and explosive growth
of fine-grained data for such a practical task. In this paper, we propose
attribute-aware hashing networks with self-consistency for generating
attribute-aware hash codes to not only make the retrieval process efficient,
but also establish explicit correspondences between hash codes and visual
attributes. Specifically, based on the captured visual representations by
attention, we develop an encoder-decoder structure network of a reconstruction
task to unsupervisedly distill high-level attribute-specific vectors from the
appearance-specific visual representations without attribute annotations. Our
models are also equipped with a feature decorrelation constraint upon these
attribute vectors to strengthen their representative abilities. Then, driven by
preserving original entities' similarity, the required hash codes can be
generated from these attribute-specific vectors and thus become
attribute-aware. Furthermore, to combat simplicity bias in deep hashing, we
consider the model design from the perspective of the self-consistency
principle and propose to further enhance models' self-consistency by equipping
an additional image reconstruction path. Comprehensive quantitative experiments
under diverse empirical settings on six fine-grained retrieval datasets and two
generic retrieval datasets show the superiority of our models over competing
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yake Wei, Ruoxuan Feng, Zihe Wang, Di Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One primary topic of multi-modal learning is to jointly incorporate
heterogeneous information from different modalities. However, most models often
suffer from unsatisfactory multi-modal cooperation, which could not jointly
utilize all modalities well. Some methods are proposed to identify and enhance
the worse learnt modality, but are often hard to provide the fine-grained
observation of multi-modal cooperation at sample-level with theoretical
support. Hence, it is essential to reasonably observe and improve the
fine-grained cooperation between modalities, especially when facing realistic
scenarios where the modality discrepancy could vary across different samples.
To this end, we introduce a fine-grained modality valuation metric to evaluate
the contribution of each modality at sample-level. Via modality valuation, we
regretfully observe that the multi-modal model tends to rely on one specific
modality, resulting in other modalities being low-contributing. We further
analyze this issue and improve cooperation between modalities by enhancing the
discriminative ability of low-contributing modalities in a targeted manner.
Overall, our methods reasonably observe the fine-grained uni-modal contribution
at sample-level and achieve considerable improvement on different multi-modal
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring User Perceptions of Virtual Reality Scene Design in Metaverse
  Learning Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahatara Ferdousi, Mohammed Faisal, Fedwa Laamarti, Chunsheng Yang, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaverse learning environments allow for a seamless and intuitive transition
between activities compared to Virtual Reality (VR) learning environments, due
to their interconnected design. The design of VR scenes is important for
creating effective learning experiences in the Metaverse. However, there is
limited research on the impact of different design elements on user's learning
experiences in VR scenes. To address this, a study was conducted with 16
participants who interacted with two VR scenes, each with varying design
elements such as style, color, texture, object, and background, while watching
a short tutorial. Participant rankings of the scenes for learning were obtained
using a seven-point Likert scale, and the Mann-Whitney U test was used to
validate differences in preference between the scenes. The results showed a
significant difference in preference between the scenes. Further analysis using
the NASA TLX questionnaire was conducted to examine the impact of this
difference on cognitive load, and participant feedback was also considered. The
study emphasizes the importance of careful VR scene design to improve the
user's learning experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages,3 figures, accepted to present at IEEE 42nd International
  Conference on Consumer Electronics</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-11-20T00:00:00Z">2023-11-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">57</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient
  Language Model Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Guo, Philip Greengard, Eric P. Xing, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a simple approach for memory-efficient adaptation of pretrained
language models. Our approach uses an iterative algorithm to decompose each
pretrained matrix into a high-precision low-rank component and a
memory-efficient quantized component. During finetuning, the quantized
component remains fixed and only the low-rank component is updated. We present
an integer linear programming formulation of the quantization component which
enables dynamic configuration of quantization parameters (e.g., bit-width,
block size) for each matrix given an overall target memory budget. We further
explore a data-aware version of the algorithm which uses an approximation of
the Fisher information matrix to weight the reconstruction objective during
matrix decomposition. Experiments on adapting RoBERTa and LLaMA-2 (7B and 70B)
demonstrate that our low-rank plus quantized matrix decomposition approach
(LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and moreover enables
more aggressive quantization. For example, on the OpenAssistant benchmark
LQ-LoRA is able to learn a 2.5-bit LLaMA-2 model that is competitive with a
model finetuned with 4-bit QLoRA. When finetuned on a language modeling
calibration dataset, LQ-LoRA can also be used for model compression; in this
setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when
including the low-rank components and requires 27GB of GPU memory) is
competitive with the original model in full precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPQA: A Graduate-Level Google-Proof Q&A Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R. Bowman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GPQA, a challenging dataset of 448 multiple-choice questions
written by domain experts in biology, physics, and chemistry. We ensure that
the questions are high-quality and extremely difficult: experts who have or are
pursuing PhDs in the corresponding domains reach 65% accuracy (74% when
discounting clear mistakes the experts identified in retrospect), while highly
skilled non-expert validators only reach 34% accuracy, despite spending on
average over 30 minutes with unrestricted access to the web (i.e., the
questions are "Google-proof"). The questions are also difficult for
state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving
39% accuracy. If we are to use future AI systems to help us answer very hard
questions, for example, when developing new scientific knowledge, we need to
develop scalable oversight methods that enable humans to supervise their
outputs, which may be difficult even if the supervisors are themselves skilled
and knowledgeable. The difficulty of GPQA both for skilled non-experts and
frontier AI systems should enable realistic scalable oversight experiments,
which we hope can help devise ways for human experts to reliably get truthful
information from AI systems that surpass human capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>-4V(ision) for Robotics: Multimodal Task Planning from Human
  Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a pipeline that enhances a general-purpose Vision Language
Model, GPT-4V(ision), by integrating observations of human actions to
facilitate robotic manipulation. This system analyzes videos of humans
performing tasks and creates executable robot programs that incorporate
affordance insights. The computation starts by analyzing the videos with GPT-4V
to convert environmental and action details into text, followed by a
GPT-4-empowered task planner. In the following analyses, vision systems
reanalyze the video with the task plan. Object names are grounded using an
open-vocabulary object detector, while focus on the hand-object relation helps
to detect the moment of grasping and releasing. This spatiotemporal grounding
allows the vision systems to further gather affordance data (e.g., grasp type,
way points, and body postures). Experiments across various scenarios
demonstrate this method's efficacy in achieving real robots' operations from
human demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are
available at this project page:
https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures, 1 table. Last updated on November 20th, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H-COAL: Human Correction of AI-Generated Labels for Biomedical Named
  Entity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojing Duan, John P. Lalor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of machine learning models for NLP tasks,
collecting high-fidelity labels from AI models is a realistic possibility.
Firms now make AI available to customers via predictions as a service (PaaS).
This includes PaaS products for healthcare. It is unclear whether these labels
can be used for training a local model without expensive annotation checking by
in-house experts. In this work, we propose a new framework for Human Correction
of AI-Generated Labels (H-COAL). By ranking AI-generated outputs, one can
selectively correct labels and approach gold standard performance (100% human
labeling) with significantly less human effort. We show that correcting 5% of
labels can close the AI-human performance gap by up to 64% relative
improvement, and correcting 20% of labels can close the performance gap by up
to 86% relative improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at Conference on Information Systems and Technology (CIST)
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Potential and Limitations of Few-Shot In-Context Learning to
  Generate Metamorphic Specifications for Tax Preparation Software <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dananjay Srinivas, Rohan Das, Saeid Tizpaz-Niari, Ashutosh Trivedi, Maria Leonor Pacheco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the ever-increasing complexity of income tax laws in the United
States, the number of US taxpayers filing their taxes using tax preparation
software (henceforth, tax software) continues to increase. According to the
U.S. Internal Revenue Service (IRS), in FY22, nearly 50% of taxpayers filed
their individual income taxes using tax software. Given the legal consequences
of incorrectly filing taxes for the taxpayer, ensuring the correctness of tax
software is of paramount importance. Metamorphic testing has emerged as a
leading solution to test and debug legal-critical tax software due to the
absence of correctness requirements and trustworthy datasets. The key idea
behind metamorphic testing is to express the properties of a system in terms of
the relationship between one input and its slightly metamorphosed twinned
input. Extracting metamorphic properties from IRS tax publications is a tedious
and time-consuming process. As a response, this paper formulates the task of
generating metamorphic specifications as a translation task between properties
extracted from tax documents - expressed in natural language - to a contrastive
first-order logic form. We perform a systematic analysis on the potential and
limitations of in-context learning with Large Language Models(LLMs) for this
task, and outline a research agenda towards automating the generation of
metamorphic specifications for tax preparation software.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Proceedings of the Natural Legal Language Processing
  Workshop, EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-aware Neural Machine Translation for English-Japanese Business
  Scene Dialogues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumire Honda, Patrick Fernandes, Chrysoula Zerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable advancements in machine translation, the current
sentence-level paradigm faces challenges when dealing with highly-contextual
languages like Japanese. In this paper, we explore how context-awareness can
improve the performance of the current Neural Machine Translation (NMT) models
for English-Japanese business dialogues translation, and what kind of context
provides meaningful information to improve translation. As business dialogue
involves complex discourse phenomena but offers scarce training resources, we
adapted a pretrained mBART model, finetuning on multi-sentence dialogue data,
which allows us to experiment with different contexts. We investigate the
impact of larger context sizes and propose novel context tokens encoding
extra-sentential information, such as speaker turn and scene type. We make use
of Conditional Cross-Mutual Information (CXMI) to explore how much of the
context the model uses and generalise CXMI to study the impact of the
extra-sentential context. Overall, we find that models leverage both preceding
sentences and extra-sentential context (with CXMI increasing with context size)
and we provide a more focused analysis on honorifics translation. Regarding
translation quality, increased source-side context paired with scene and
speaker information improves the model performance compared to previous work
and our context-agnostic baselines, measured in BLEU and COMET metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MT Summit 2023, research track, link to paper in proceedings:
  https://aclanthology.org/2023.mtsummit-research.23/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Training Distributions with Scalable Online Bilevel
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Grangier, Pierre Ablin, Awni Hannun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large neural networks pretrained on web-scale corpora are central to modern
machine learning. In this paradigm, the distribution of the large,
heterogeneous pretraining data rarely matches that of the application domain.
This work considers modifying the pretraining distribution in the case where
one has a small sample of data reflecting the targeted test conditions. We
propose an algorithm motivated by a recent formulation of this setting as an
online, bilevel optimization problem. With scalability in mind, our algorithm
prioritizes computing gradients at training points which are likely to most
improve the loss on the targeted distribution. Empirically, we show that in
some cases this approach is beneficial over existing strategies from the domain
adaptation literature but may not succeed in other cases. We propose a simple
test to evaluate when our approach can be expected to work well and point
towards further research to address current limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Analysis of Substantiation in Scientific Peer <span class="highlight-title">Review</span>s <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzhu Guo, Guokan Shang, Virgile Rennard, Michalis Vazirgiannis, Chloé Clavel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing amount of problematic peer reviews in top AI conferences,
the community is urgently in need of automatic quality control measures. In
this paper, we restrict our attention to substantiation -- one popular quality
aspect indicating whether the claims in a review are sufficiently supported by
evidence -- and provide a solution automatizing this evaluation process. To
achieve this goal, we first formulate the problem as claim-evidence pair
extraction in scientific peer reviews, and collect SubstanReview, the first
annotated dataset for this task. SubstanReview consists of 550 reviews from NLP
conferences annotated by domain experts. On the basis of this dataset, we train
an argument mining system to automatically analyze the level of substantiation
in peer reviews. We also perform data analysis on the SubstanReview dataset to
obtain meaningful insights on peer reviewing quality in NLP conferences over
recent years.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinanceBench: A New Benchmark for Financial Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, Bertie Vidgen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FinanceBench is a first-of-its-kind test suite for evaluating the performance
of LLMs on open book financial question answering (QA). It comprises 10,231
questions about publicly traded companies, with corresponding answers and
evidence strings. The questions in FinanceBench are ecologically valid and
cover a diverse set of scenarios. They are intended to be clear-cut and
straightforward to answer to serve as a minimum performance standard. We test
16 state of the art model configurations (including GPT-4-Turbo, Llama2 and
Claude2, with vector stores and long context prompts) on a sample of 150 cases
from FinanceBench, and manually review their answers (n=2,400). The cases are
available open-source. We show that existing LLMs have clear limitations for
financial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly
answered or refused to answer 81% of questions. While augmentation techniques
such as using longer context window to feed in relevant evidence improve
performance, they are unrealistic for enterprise settings due to increased
latency and cannot support larger financial documents. We find that all models
examined exhibit weaknesses, such as hallucinations, that limit their
suitability for use by enterprises.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset is available at:
  https://huggingface.co/datasets/PatronusAI/financebench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs as Visual Explainers: Advancing Image Classification with Evolving
  Visual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhao Han, Le Zhuo, Yue Liao, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) offer a promising paradigm for image
classification by comparing the similarity between images and class embeddings.
A critical challenge lies in crafting precise textual representations for class
names. While previous studies have leveraged recent advancements in large
language models (LLMs) to enhance these descriptors, their outputs often suffer
from ambiguity and inaccuracy. We identify two primary causes: 1) The prevalent
reliance on textual interactions with LLMs, leading to a mismatch between the
generated text and the visual content in VLMs' latent space - a phenomenon we
term the "explain without seeing" dilemma. 2) The oversight of the inter-class
relationships, resulting in descriptors that fail to differentiate similar
classes effectively. To address these issues, we propose a novel image
classification framework combining VLMs with LLMs, named Iterative Optimization
with Visual Feedback. In particular, our method develops an LLM-based agent,
employing an evolutionary optimization strategy to refine class descriptors.
Crucially, we incorporate visual feedback from VLM classification metrics,
thereby guiding the optimization process with concrete visual data. Our method
leads to improving accuracy on a wide range of image classification benchmarks,
with 3.47\% average gains over state-of-the-art methods. We also highlight the
resulting descriptions serve as explainable and robust features that can
consistently improve the performance across various backbone models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Valid and Natural Adversarial Examples with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based natural language processing (NLP) models, particularly
pre-trained language models (PLMs), have been revealed to be vulnerable to
adversarial attacks. However, the adversarial examples generated by many
mainstream word-level adversarial attack models are neither valid nor natural,
leading to the loss of semantic maintenance, grammaticality, and human
imperceptibility. Based on the exceptional capacity of language understanding
and generation of large language models (LLMs), we propose LLM-Attack, which
aims at generating both valid and natural adversarial examples with LLMs. The
method consists of two stages: word importance ranking (which searches for the
most vulnerable words) and word synonym replacement (which substitutes them
with their synonyms obtained from LLMs). Experimental results on the Movie
Review (MR), IMDB, and Yelp Review Polarity datasets against the baseline
adversarial attack models illustrate the effectiveness of LLM-Attack, and it
outperforms the baselines in human and GPT-4 evaluation by a significant
margin. The model can generate adversarial examples that are typically valid
and natural, with the preservation of semantic meaning, grammaticality, and
human imperceptibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evil Geniuses: Delving into the Safety of LLM-based Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, Hang Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in large language models (LLMs) have led to a
resurgence in LLM-based agents, which demonstrate impressive human-like
behaviors and cooperative capabilities in various interactions and strategy
formulations. However, evaluating the safety of LLM-based agents remains a
complex challenge. This paper elaborately conducts a series of manual jailbreak
prompts along with a virtual chat-powered evil plan development team, dubbed
Evil Geniuses, to thoroughly probe the safety aspects of these agents. Our
investigation reveals three notable phenomena: 1) LLM-based agents exhibit
reduced robustness against malicious attacks. 2) the attacked agents could
provide more nuanced responses. 3) the detection of the produced improper
responses is more challenging. These insights prompt us to question the
effectiveness of LLM-based attacks on agents, highlighting vulnerabilities at
various levels and within different role specializations within the
system/agent of LLM-based agents. Extensive evaluation and discussion reveal
that LLM-based agents face significant challenges in safety and yield insights
for future research. Our code is available at
https://github.com/T1aNS1R/Evil-Geniuses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deepparse : An Extendable, and Fine-Tunable State-Of-The-Art Library for
  Parsing Multinational Street Addresses <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Beauchemin, Marouane Yassine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting an address into meaningful components, also known as address
parsing, is an essential step in many applications from record linkage to
geocoding and package delivery. Consequently, a lot of work has been dedicated
to develop accurate address parsing techniques, with machine learning and
neural network methods leading the state-of-the-art scoreboard. However, most
of the work on address parsing has been confined to academic endeavours with
little availability of free and easy-to-use open-source solutions.
  This paper presents Deepparse, a Python open-source, extendable, fine-tunable
address parsing solution under LGPL-3.0 licence to parse multinational
addresses using state-of-the-art deep learning algorithms and evaluated on over
60 countries. It can parse addresses written in any language and use any
address standard. The pre-trained model achieves average $99~\%$ parsing
accuracies on the countries used for training with no pre-processing nor
post-processing needed. Moreover, the library supports fine-tuning with new
data to generate a custom address parser.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in EMNLP 2024 NLP-OSS workshop. arXiv admin note: text
  overlap with arXiv:2006.16152, arXiv:2112.04008</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Use Large Language Models for Text Coding: The Case of Fatherhood
  Roles in Public Policy Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Lupo, Oscar Magnusson, Dirk Hovy, Elin Naurin, Lena Wängnerud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) like GPT-3 and GPT-4 have
opened up new opportunities for text analysis in political science. They
promise automation with better results and less programming. In this study, we
evaluate LLMs on three original coding tasks of non-English political science
texts, and we provide a detailed description of a general workflow for using
LLMs for text coding in political science research. Our use case offers a
practical guide for researchers looking to incorporate LLMs into their research
on text analysis. We find that, when provided with detailed label definitions
and coding examples, an LLM can be as good as or even better than a human
annotator while being much faster (up to hundreds of times), considerably
cheaper (costing up to 60% less than human coding), and much easier to scale to
large amounts of text. Overall, LLMs present a viable option for most text
coding projects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ System 2 Attention (is something you might need too) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Weston, Sainbayar Sukhbaatar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft attention in Transformer-based Large Language Models (LLMs) is
susceptible to incorporating irrelevant information from the context into its
latent representations, which adversely affects next token generations. To help
rectify these issues, we introduce System 2 Attention (S2A), which leverages
the ability of LLMs to reason in natural language and follow instructions in
order to decide what to attend to. S2A regenerates the input context to only
include the relevant portions, before attending to the regenerated context to
elicit the final response. In experiments, S2A outperforms standard
attention-based LLMs on three tasks containing opinion or irrelevant
information, QA, math word problems and longform generation, where S2A
increases factuality and objectivity, and decreases sycophancy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Grammatical Error Correction Via Multi-Task Training and
  Optimized Training Schedule <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Bout, Alexander Podolskiy, Sergey Nikolenko, Irina Piontkovskaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in neural grammatical error correction (GEC) is hindered by the lack
of annotated training data. Sufficient amounts of high-quality manually
annotated data are not available, so recent research has relied on generating
synthetic data, pretraining on it, and then fine-tuning on real datasets;
performance gains have been achieved either by ensembling or by using huge
pretrained models such as XXL-T5 as the backbone. In this work, we explore an
orthogonal direction: how to use available data more efficiently. First, we
propose auxiliary tasks that exploit the alignment between the original and
corrected sentences, such as predicting a sequence of corrections. We formulate
each task as a sequence-to-sequence problem and perform multi-task training.
Second, we discover that the order of datasets used for training and even
individual instances within a dataset may have important effects on the final
performance, so we set out to find the best training schedule. Together, these
two ideas lead to significant improvements, producing results that improve
state of the art with much smaller models; in particular, we outperform the
best models based on T5-XXL (11B parameters) with a BART-based model (400M
parameters).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Igniting Language Intelligence: The Hitchhiker's Guide From
  Chain-of-Thought Reasoning to Language Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, Hai Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have dramatically enhanced the field of language
intelligence, as demonstrably evidenced by their formidable empirical
performance across a spectrum of complex reasoning tasks. Additionally,
theoretical proofs have illuminated their emergent reasoning capabilities,
providing a compelling showcase of their advanced cognitive abilities in
linguistic contexts. Critical to their remarkable efficacy in handling complex
reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning
techniques, obliging them to formulate intermediate steps en route to deriving
an answer. The CoT reasoning approach has not only exhibited proficiency in
amplifying reasoning performance but also in enhancing interpretability,
controllability, and flexibility. In light of these merits, recent research
endeavors have extended CoT reasoning methodologies to nurture the development
of autonomous language agents, which adeptly adhere to language instructions
and execute actions within varied environments. This survey paper orchestrates
a thorough discourse, penetrating vital research dimensions, encompassing: (i)
the foundational mechanics of CoT techniques, with a focus on elucidating the
circumstances and justification behind its efficacy; (ii) the paradigm shift in
CoT; and (iii) the burgeoning of language agents fortified by CoT approaches.
Prospective research avenues envelop explorations into generalization,
efficiency, customization, scaling, and safety. This paper caters to a wide
audience, including beginners seeking comprehensive knowledge of CoT reasoning
and language agents, as well as experienced researchers interested in
foundational mechanics and engaging in cutting-edge discussions on these
topics. A repository for the related papers is available at
https://github.com/Zoeyyao27/CoT-Igniting-Agent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Boundaries: A Comprehensive <span class="highlight-title">Survey</span> of Transferable Attacks on AI
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangjing Wang, Ce Zhou, Yuanda Wang, Bocheng Chen, Hanqing Guo, Qiben Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) systems such as autonomous vehicles, facial
recognition, and speech recognition systems are increasingly integrated into
our daily lives. However, despite their utility, these AI systems are
vulnerable to a wide range of attacks such as adversarial, backdoor, data
poisoning, membership inference, model inversion, and model stealing attacks.
In particular, numerous attacks are designed to target a particular model or
system, yet their effects can spread to additional targets, referred to as
transferable attacks. Although considerable efforts have been directed toward
developing transferable attacks, a holistic understanding of the advancements
in transferable attacks remains elusive. In this paper, we comprehensively
explore learning-based attacks from the perspective of transferability,
particularly within the context of cyber-physical security. We delve into
different domains -- the image, text, graph, audio, and video domains -- to
highlight the ubiquitous and pervasive nature of transferable attacks. This
paper categorizes and reviews the architecture of existing attacks from various
viewpoints: data, process, model, and system. We further examine the
implications of transferable attacks in practical scenarios such as autonomous
driving, speech recognition, and large language models (LLMs). Additionally, we
outline the potential research directions to encourage efforts in exploring the
landscape of transferable attacks. This survey offers a holistic understanding
of the prevailing transferable attacks and their impacts across different
domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungil Kong, Junmo Lee, Jeongmin Kim, Beomjeong Kim, Jihoon Park, Dohee Kong, Changheon Lee, Sangjin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel method for modeling numerous speakers, which
enables expressing the overall characteristics of speakers in detail like a
trained multi-speaker model without additional training on the target speaker's
dataset. Although various works with similar purposes have been actively
studied, their performance has not yet reached that of trained multi-speaker
models due to their fundamental limitations. To overcome previous limitations,
we propose effective methods for feature learning and representing target
speakers' speech characteristics by discretizing the features and conditioning
them to a speech synthesis model. Our method obtained a significantly higher
similarity mean opinion score (SMOS) in subjective similarity evaluation than
seen speakers of a best-performing multi-speaker model, even with unseen
speakers. The proposed method also outperforms a zero-shot method by
significant margins. Furthermore, our method shows remarkable performance in
generating new artificial speakers. In addition, we demonstrate that the
encoded latent features are sufficiently informative to reconstruct an original
speaker's speech completely. It implies that our method can be used as a
general methodology to encode and reconstruct speakers' characteristics in
various tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control in Hybrid Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Rüdel, Jochen L. Leidner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customer data typically is held in database systems, which can be seen as
rule-based knowledge base, whereas businesses increasingly want to benefit from
the capabilities of large, pre-trained language models.
  In this technical report, we describe a case study of how a commercial rule
engine and an integrated neural chatbot may be integrated, and what level of
control that particular integration mode leads to. We also discuss alternative
ways (including past ways realized in other systems) how researchers strive to
maintain control and avoid what has recently been called model "hallucination".
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Low-rank Adaptation of <span class="highlight-title">Pre-train</span>ed Language Models <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained large language models in a parameter-efficient manner
is widely studied for its effectiveness and efficiency. The popular method of
low-rank adaptation (LoRA) offers a notable approach, hypothesizing that the
adaptation process is intrinsically low-dimensional. Although LoRA has
demonstrated commendable performance, it is implemented with a fixed and
unalterable intrinsic rank that might not always be the ideal choice.
Recognizing the need for more flexible adaptation, we extend the methodology of
LoRA to an innovative approach we call sparse low-rank adaptation (SoRA) that
enables dynamic adjustments to the intrinsic rank during the adaptation
process. We achieve this through the incorporation of a gate unit optimized
with proximal gradient method in the training stage, controlling the
cardinality of rank under the sparsity of the gate. In the subsequent inference
stage, we eliminate the parameter blocks corresponding to the zeroed-out ranks,
to reduce each SoRA module back to a concise yet rank-optimal LoRA. Our
approach strengthens the representation power of LoRA by initializing it with a
higher rank, while efficiently taming a temporarily increased number of
parameters via updating in a sparse way. We further introduce a sparsifying
scheduler for SoRA, aiming to examine the impact of the number of non-zero
parameters on the model's memorization and generalization. Our experimental
results demonstrate that SoRA can outperform other baselines even with 70%
retained parameters and 70% training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refactoring Programs Using Large Language Models with Few-Shot Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Shirafuji, Yusuke Oda, Jun Suzuki, Makoto Morishita, Yutaka Watanobe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A less complex and more straightforward program is a crucial factor that
enhances its maintainability and makes writing secure and bug-free programs
easier. However, due to its heavy workload and the risks of breaking the
working programs, programmers are reluctant to do code refactoring, and thus,
it also causes the loss of potential learning experiences. To mitigate this, we
demonstrate the application of using a large language model (LLM), GPT-3.5, to
suggest less complex versions of the user-written Python program, aiming to
encourage users to learn how to write better programs. We propose a method to
leverage the prompting with few-shot examples of the LLM by selecting the
best-suited code refactoring examples for each target programming problem based
on the prior evaluation of prompting with the one-shot example. The
quantitative evaluation shows that 95.68% of programs can be refactored by
generating 10 candidates each, resulting in a 17.35% reduction in the average
cyclomatic complexity and a 25.84% decrease in the average number of lines
after filtering only generated programs that are semantically correct.
Furthermore, the qualitative evaluation shows outstanding capability in code
formatting, while unnecessary behaviors such as deleting or translating
comments are also observed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures, accepted to the 30th Asia-Pacific Software
  Engineering Conference (APSEC 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse
  Biomedical Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Luo, Jinzhong Ning, Yingwen Zhao, Zhijun Wang, Zeyuan Ding, Peng Chen, Weiru Fu, Qinyu Han, Guangtao Xu, Yunzhi Qiu, Dinghao Pan, Jiru Li, Hao Li, Wenduo Feng, Senbo Tu, Yuqi Liu, Zhihao Yang, Jian Wang, Yuanyuan Sun, Hongfei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have shown promising
results across a variety of natural language processing (NLP) tasks. The
application of LLMs to specific domains, such as biomedicine, has achieved
increased attention. However, most biomedical LLMs focus on enhancing
performance in monolingual biomedical question answering and conversation
tasks. To further investigate the effectiveness of the LLMs on diverse
biomedical NLP tasks in different languages, we present Taiyi, a bilingual
(English and Chinese) fine-tuned LLM for diverse biomedical tasks. In this
work, we first curated a comprehensive collection of 140 existing biomedical
text mining datasets across over 10 task types. Subsequently, a two-stage
strategy is proposed for supervised fine-tuning to optimize the model
performance across varied tasks. Experimental results on 13 test sets covering
named entity recognition, relation extraction, text classification, question
answering tasks demonstrate Taiyi achieves superior performance compared to
general LLMs. The case study involving additional biomedical NLP tasks further
shows Taiyi's considerable potential for bilingual biomedical multi-tasking.
The source code, datasets, and model for Taiyi are freely available at
https://github.com/DUTIR-BioNLP/Taiyi-LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing the Length Bias Problem in Document-Level Neural Machine
  Translation <span class="chip">EMNLP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuocheng Zhang, Shuhao Gu, Min Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-level neural machine translation (DNMT) has shown promising results
by incorporating more context information. However, this approach also
introduces a length bias problem, whereby DNMT suffers from significant
translation quality degradation when decoding documents that are much shorter
or longer than the maximum sequence length during training. %i.e., the length
bias problem. To solve the length bias problem, we propose to improve the DNMT
model in training method, attention mechanism, and decoding strategy. Firstly,
we propose to sample the training data dynamically to ensure a more uniform
distribution across different sequence lengths. Then, we introduce a
length-normalized attention mechanism to aid the model in focusing on target
information, mitigating the issue of attention divergence when processing
longer sequences. Lastly, we propose a sliding window strategy during decoding
that integrates as much context information as possible without exceeding the
maximum sequence length. The experimental results indicate that our method can
bring significant improvements on several open datasets, and further analysis
shows that our method can significantly alleviate the length bias problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Filling the Image Information Gap for VQA: <span class="highlight-title">Prompt</span>ing Large Language
  Models to Proactively Ask Questions <span class="chip">EMNLP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Wang, Chi Chen, Peng Li, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate impressive reasoning ability and the
maintenance of world knowledge not only in natural language tasks, but also in
some vision-language tasks such as open-domain knowledge-based visual question
answering (OK-VQA). As images are invisible to LLMs, researchers convert images
to text to engage LLMs into the visual question reasoning procedure. This leads
to discrepancies between images and their textual representations presented to
LLMs, which consequently impedes final reasoning performance. To fill the
information gap and better leverage the reasoning capability, we design a
framework that enables LLMs to proactively ask relevant questions to unveil
more details in the image, along with filters for refining the generated
information. We validate our idea on OK-VQA and A-OKVQA. Our method
continuously boosts the performance of baselines methods by an average gain of
2.15% on OK-VQA, and achieves consistent improvements across different LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How well Chat<span class="highlight-title">GPT</span> understand Malaysian English? An Evaluation on Named
  Entity Recognition and Relation Extraction <span class="chip">EMNLP
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohan Raj Chanthran, Lay-Ki Soon, Huey Fang Ong, Bhawani Selvaretnam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, ChatGPT has attracted a lot of interest from both researchers and
the general public. While the performance of ChatGPT in named entity
recognition and relation extraction from Standard English texts is
satisfactory, it remains to be seen if it can perform similarly for Malaysian
English. Malaysian English is unique as it exhibits morphosyntactic and
semantical adaptation from local contexts. In this study, we assess ChatGPT's
capability in extracting entities and relations from the Malaysian English News
(MEN) dataset. We propose a three-step methodology referred to as
\textbf{\textit{educate-predict-evaluate}}. The performance of ChatGPT is
assessed using F1-Score across 18 unique prompt settings, which were carefully
engineered for a comprehensive review. From our evaluation, we found that
ChatGPT does not perform well in extracting entities from Malaysian English
news articles, with the highest F1-Score of 0.497. Further analysis shows that
the morphosyntactic adaptation in Malaysian English caused the limitation.
However, interestingly, this morphosyntactic adaptation does not impact the
performance of ChatGPT for relation extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Generation, Evaluation & Metrics (GEM) Workshop at EMNLP
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KBioXLM: A Knowledge-anchored Biomedical Multilingual <span class="highlight-title">Pretrain</span>ed
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Geng, Xu Yan, Ziqiang Cao, Juntao Li, Wenjie Li, Sujian Li, Xinjie Zhou, Yang Yang, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most biomedical pretrained language models are monolingual and cannot handle
the growing cross-lingual requirements. The scarcity of non-English domain
corpora, not to mention parallel data, poses a significant hurdle in training
multilingual biomedical models. Since knowledge forms the core of
domain-specific corpora and can be translated into various languages
accurately, we propose a model called KBioXLM, which transforms the
multilingual pretrained model XLM-R into the biomedical domain using a
knowledge-anchored approach. We achieve a biomedical multilingual corpus by
incorporating three granularity knowledge alignments (entity, fact, and passage
levels) into monolingual corpora. Then we design three corresponding training
tasks (entity masking, relation masking, and passage relation prediction) and
continue training on top of the XLM-R model to enhance its domain cross-lingual
ability. To validate the effectiveness of our model, we translate the English
benchmarks of multiple tasks into Chinese. Experimental results demonstrate
that our model significantly outperforms monolingual and multilingual
pretrained models in cross-lingual zero-shot and few-shot scenarios, achieving
improvements of up to 10+ points. Our code is publicly available at
https://github.com/ngwlh-gl/KBioXLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring <span class="highlight-title">Prompt</span>ing Large Language Models as Explainable Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ghazaleh Mahmoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the IUST NLP Lab submission to the Prompting Large
Language Models as Explainable Metrics Shared Task at the Eval4NLP 2023
Workshop on Evaluation & Comparison of NLP Systems. We have proposed a
zero-shot prompt-based strategy for explainable evaluation of the summarization
task using Large Language Models (LLMs). The conducted experiments demonstrate
the promising potential of LLMs as evaluation metrics in Natural Language
Processing (NLP), particularly in the field of summarization. Both few-shot and
zero-shot approaches are employed in these experiments. The performance of our
best provided prompts achieved a Kendall correlation of 0.477 with human
evaluations in the text summarization task on the test data. Code and results
are publicly available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Eval4NLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context
  Learning <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanyu Long, Wenya Wang, Sinno Jialin Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have showcased their capability with few-shot
inference known as in-context learning. However, in-domain demonstrations are
not always readily available in real scenarios, leading to cross-domain
in-context learning. Besides, LLMs are still facing challenges in long-tail
knowledge in unseen and unfamiliar domains. The above limitations demonstrate
the necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study
the UDA problem under an in-context learning setting to adapt language models
from the source domain to the target domain without any target labels. The core
idea is to retrieve a subset of cross-domain elements that are the most similar
to the query, and elicit language model to adapt in an in-context manner by
learning both target domain distribution and the discriminative task signal
simultaneously with the augmented cross-domain in-context examples. We devise
different prompting and training strategies, accounting for different LM
architectures to learn the target distribution via language modeling. With
extensive experiments on Sentiment Analysis (SA) and Named Entity Recognition
(NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer
and demonstrate significant improvements over baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-teacher Distillation for Multilingual Spelling Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingfen Zhang, Xuan Guo, Sravan Bodapati, Christopher Potts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate spelling correction is a critical step in modern search interfaces,
especially in an era of mobile devices and speech-to-text interfaces. For
services that are deployed around the world, this poses a significant challenge
for multilingual NLP: spelling errors need to be caught and corrected in all
languages, and even in queries that use multiple languages. In this paper, we
tackle this challenge using multi-teacher distillation. On our approach, a
monolingual teacher model is trained for each language/locale, and these
individual models are distilled into a single multilingual student model
intended to serve all languages/locales. In experiments using open-source data
as well as user data from a worldwide search service, we show that this leads
to highly effective spelling correction models that can meet the tight latency
requirements of deployed services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span> in Data Science: A Practical Exploration of Model Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathalia Nascimento, Cristina Tavares, Paulo Alencar, Donald Cowan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is an increasing interest in leveraging Large Language Models (LLMs)
for managing structured data and enhancing data science processes. Despite the
potential benefits, this integration poses significant questions regarding
their reliability and decision-making methodologies. It highlights the
importance of various factors in the model selection process, including the
nature of the data, problem type, performance metrics, computational resources,
interpretability vs accuracy, assumptions about data, and ethical
considerations. Our objective is to elucidate and express the factors and
assumptions guiding GPT-4's model selection recommendations. We employ a
variability model to depict these factors and use toy datasets to evaluate both
the model and the implementation of the identified heuristics. By contrasting
these outcomes with heuristics from other platforms, our aim is to determine
the effectiveness and distinctiveness of GPT-4's methodology. This research is
committed to advancing our comprehension of AI decision-making processes,
especially in the realm of model selection within data science. Our efforts are
directed towards creating AI systems that are more transparent and
comprehensible, contributing to a more responsible and efficient practice in
data science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages. To appear in IEEE BigData 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token-Level Adversarial <span class="highlight-title">Prompt</span> Detection Based on Perplexity Measures
  and Contextual Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengmian Hu, Gang Wu, Saayan Mitra, Ruiyi Zhang, Tong Sun, Heng Huang, Vishy Swaminathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLM) have emerged as pivotal tools in
various applications. However, these models are susceptible to adversarial
prompt attacks, where attackers can carefully curate input strings that lead to
undesirable outputs. The inherent vulnerability of LLMs stems from their
input-output mechanisms, especially when presented with intensely
out-of-distribution (OOD) inputs. This paper proposes a token-level detection
method to identify adversarial prompts, leveraging the LLM's capability to
predict the next token's probability. We measure the degree of the model's
perplexity and incorporate neighboring token information to encourage the
detection of contiguous adversarial prompt sequences. As a result, we propose
two methods: one that identifies each token as either being part of an
adversarial prompt or not, and another that estimates the probability of each
token being part of an adversarial prompt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta <span class="highlight-title">Prompt</span>ing for AGI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an in-depth exploration of Meta Prompting, a novel
technique that revolutionizes the way large language models (LLMs), multi-modal
foundation models, and AI systems approach problem-solving and data
interpretation. Meta Prompting, rooted in type theory and category theory,
prioritizes the structure and syntax of information, providing a unique
framework that transcends traditional content-focused methods. We delve into
the formal definitions of Meta Prompting, contrasting it with Few-Shot
Prompting, and highlight its applicability and superiority in various AI
applications.
  Key to this exploration is the expansion of Meta Prompting into the realm of
complex reasoning. Here, we demonstrate how this technique adeptly breaks down
intricate problems into manageable sub-problems, facilitating a step-by-step,
detailed approach to problem-solving. This method proves especially
advantageous in terms of token efficiency and offering a fair comparison in
problem-solving scenarios, standing out against few-shot example approaches.
  Furthermore, the paper breaks new ground by extending Meta Prompting into
multi-modal foundation model settings. This extension addresses the integration
of diverse data types, such as images, audio, and video, within the structured
framework of Meta Prompting, highlighting both the challenges and the vast
potential of this approach in handling complex, multi-faceted data (The code is
available at https://github.com/meta-prompting/meta-prompting).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What's left can't be right -- The remaining positional incompetence of
  contrastive vision-language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Hoehing, Ellen Rushe, Anthony Ventresque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive vision-language models like CLIP have been found to lack spatial
understanding capabilities. In this paper we discuss the possible causes of
this phenomenon by analysing both datasets and embedding space. By focusing on
simple left-right positional relations, we show that this behaviour is entirely
predictable, even with large-scale datasets, demonstrate that these relations
can be taught using synthetic data and show that this approach can generalise
well to natural images - improving the performance on left-right relations on
Visual Genome Relations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Corroborative and Contributive Attributions in Large Language
  Models <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodora Worledge, Judy Hanwen Shen, Nicole Meister, Caleb Winston, Carlos Guestrin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As businesses, products, and services spring up around large language models,
the trustworthiness of these models hinges on the verifiability of their
outputs. However, methods for explaining language model outputs largely fall
across two distinct fields of study which both use the term "attribution" to
refer to entirely separate techniques: citation generation and training data
attribution. In many modern applications, such as legal document generation and
medical question answering, both types of attributions are important. In this
work, we argue for and present a unified framework of large language model
attributions. We show how existing methods of different types of attribution
fall under the unified framework. We also use the framework to discuss
real-world use cases where one or both types of attributions are required. We
believe that this unified framework will guide the use case driven development
of systems that leverage both types of attribution, as well as the
standardization of their evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS ATTRIB Workshop 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Closed-Access Multilingual Embedding for Automatic Sentence
  Alignment in Low Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idris Abdulmumin, Auwal Abubakar Khalid, Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Lukman Jibril Aliyu, Babangida Sani, Bala Mairiga Abduljalil, Sani Ahmad Hassan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of qualitative parallel data in machine translation has long
been determined but it has always been very difficult to obtain such in
sufficient quantity for the majority of world languages, mainly because of the
associated cost and also the lack of accessibility to these languages. Despite
the potential for obtaining parallel datasets from online articles using
automatic approaches, forensic investigations have found a lot of
quality-related issues such as misalignment, and wrong language codes. In this
work, we present a simple but qualitative parallel sentence aligner that
carefully leveraged the closed-access Cohere multilingual embedding, a solution
that ranked second in the just concluded #CoHereAIHack 2023 Challenge (see
https://ai6lagos.devpost.com). The proposed approach achieved $94.96$ and
$54.83$ f1 scores on FLORES and MAFAND-MT, compared to $3.64$ and $0.64$ of
LASER respectively. Our method also achieved an improvement of more than 5 BLEU
scores over LASER, when the resulting datasets were used with MAFAND-MT dataset
to train translation models. Our code and data are available for research
purposes here (https://github.com/abumafrim/Cohere-Align).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the proceedings of ICCAIT 2023. 6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Learning by Model Feedback: The Dynamics of Iterative <span class="highlight-title">Prompt</span>ing
  with Midjourney <span class="chip">EMNLP23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shachar Don-Yehiya, Leshem Choshen, Omri Abend
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating images with a Text-to-Image model often requires multiple trials,
where human users iteratively update their prompt based on feedback, namely the
output image. Taking inspiration from cognitive work on reference games and
dialogue alignment, this paper analyzes the dynamics of the user prompts along
such iterations. We compile a dataset of iterative interactions of human users
with Midjourney. Our analysis then reveals that prompts predictably converge
toward specific traits along these iterations. We further study whether this
convergence is due to human users, realizing they missed important details, or
due to adaptation to the model's ``preferences'', producing better images for a
specific language style. We show initial evidence that both possibilities are
at play. The possibility that users adapt to the model's preference raises
concerns about reusing user data for further training. The prompts may be
biased towards the preferences of a specific model, rather than align with
human intentions and natural manner of expression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Ended Instructable Embodied Agents with Memory-Augmented Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Sarch, Yue Wu, Michael J. Tarr, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained and frozen large language models (LLMs) can effectively map
simple scene rearrangement instructions to programs over a robot's visuomotor
functions through appropriate few-shot example prompting. To parse open-domain
natural language and adapt to a user's idiosyncratic procedures, not known
during prompt engineering time, fixed prompts fall short. In this paper, we
introduce HELPER, an embodied agent equipped with an external memory of
language-program pairs that parses free-form human-robot dialogue into action
programs through retrieval-augmented LLM prompting: relevant memories are
retrieved based on the current dialogue, instruction, correction, or VLM
description, and used as in-context prompt examples for LLM querying. The
memory is expanded during deployment to include pairs of user's language and
action plans, to assist future inferences and personalize them to the user's
language and routines. HELPER sets a new state-of-the-art in the TEACh
benchmark in both Execution from Dialog History (EDH) and Trajectory from
Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for
TfD. Our models, code, and video results can be found in our project's website:
https://helper-agent-llm.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page with code & videos: https://helper-agent-llm.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal
  Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasilii A. Gromov, Nikita S. Borodin, Asel S. Yerbolova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The present paper introduces a novel object of study - a language fractal
structure. We hypothesize that a set of embeddings of all $n$-grams of a
natural language constitutes a representative sample of this fractal set. (We
use the term Hailonakea to refer to the sum total of all language fractal
structures, over all $n$). The paper estimates intrinsic (genuine) dimensions
of language fractal structures for the Russian and English languages. To this
end, we employ methods based on (1) topological data analysis and (2) a minimum
spanning tree of a data graph for a cloud of points considered (Steele
theorem). For both languages, for all $n$, the intrinsic dimensions appear to
be non-integer values (typical for fractal sets), close to 9 for both of the
Russian and English language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Varieties of Italy: Technology Challenges and Opportunities <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Ramponi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Italy is characterized by a one-of-a-kind linguistic diversity landscape in
Europe, which implicitly encodes local knowledge, cultural traditions, artistic
expressions and history of its speakers. However, most local languages and
dialects in Italy are at risk of disappearing within few generations. The NLP
community has recently begun to engage with endangered languages, including
those of Italy. Yet, most efforts assume that these varieties are
under-resourced language monoliths with an established written form and
homogeneous functions and needs, and thus highly interchangeable with each
other and with high-resource, standardized languages. In this paper, we
introduce the linguistic context of Italy and challenge the default
machine-centric assumptions of NLP for Italy's language varieties. We advocate
for a shift in the paradigm from machine-centric to speaker-centric NLP, and
provide recommendations and opportunities for work that prioritizes languages
and their speakers over technological advances. To facilitate the process, we
finally propose building a local community towards responsible, participatory
efforts aimed at supporting vitality of languages and dialects of Italy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TACL. This arXiv version is a pre-MIT Press publication
  version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Opinion Summarization Using Approximate Geodesics <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07496v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07496v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somnath Basu Roy Chowdhury, Nicholas Monath, Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Opinion summarization is the task of creating summaries capturing popular
opinions from user reviews. In this paper, we introduce Geodesic Summarizer
(GeoSumm), a novel system to perform unsupervised extractive opinion
summarization. GeoSumm involves an encoder-decoder based representation
learning model, that generates representations of text as a distribution over
latent semantic units. GeoSumm generates these representations by performing
dictionary learning over pre-trained text representations at multiple decoder
layers. We then use these representations to quantify the relevance of review
sentences using a novel approximate geodesic distance based scoring mechanism.
We use the relevance scores to identify popular opinions in order to compose
general and aspect-specific summaries. Our proposed model, GeoSumm, achieves
state-of-the-art performance on three opinion summarization datasets. We
perform additional experiments to analyze the functioning of our model and
showcase the generalization ability of {\X} across different domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEAL: Stable and Active Learning for Few-Shot <span class="highlight-title">Prompt</span>ing <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08358v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08358v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullatif Köksal, Timo Schick, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot classification has made great strides due to foundation models that,
through priming and prompting, are highly effective few-shot learners. However,
this approach has high variance both across different sets of few shots (data
selection) and across different finetuning runs (run variability). This is
problematic not only because it impedes the fair comparison of different
approaches, but especially because it makes few-shot learning too unreliable
for many real-world applications. To alleviate these issues, we make two
contributions for more stable and effective few-shot learning: First, we
propose novel ensembling methods and show that they substantially reduce run
variability. Second, we introduce a new active learning (AL) criterion for data
selection and present the first AL-based approach specifically tailored towards
prompt-based learning. In our experiments, we show that our combined method,
MEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),
improves overall performance of prompt-based finetuning by 2.3 points on five
diverse tasks. We publicly share our code and data splits in
https://github.com/akoksal/MEAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-Agnostic Bias Detection in Language Models with Bias Probing <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullatif Köksal, Omer Faruk Yalcin, Ahmet Akbiyik, M. Tahir Kilavuz, Anna Korhonen, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) are key components in NLP, but they contain
strong social biases. Quantifying these biases is challenging because current
methods focusing on fill-the-mask objectives are sensitive to slight changes in
input. To address this, we propose a bias probing technique called LABDet, for
evaluating social bias in PLMs with a robust and language-agnostic method. For
nationality as a case study, we show that LABDet `surfaces' nationality bias by
training a classifier on top of a frozen PLM on non-nationality sentiment
detection. We find consistent patterns of nationality bias across monolingual
PLMs in six languages that align with historical and political context. We also
show for English BERT that bias surfaced by LABDet correlates well with bias in
the pretraining data; thus, our work is one of the few studies that directly
links pretraining data to PLM behavior. Finally, we verify LABDet's reliability
and applicability to different templates and languages through an extensive set
of robustness checks. We publicly share our code and dataset in
https://github.com/akoksal/LABDet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15363v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15363v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL
task. However, the absence of a systematical benchmark inhibits the development
of designing effective, efficient and economic LLM-based Text-to-SQL solutions.
To address this challenge, in this paper, we first conduct a systematical and
extensive comparison over existing prompt engineering methods, including
question representation, example selection and example organization, and with
these experimental results, we elaborate their pros and cons. Based on these
findings, we propose a new integrated solution, named DAIL-SQL, which refreshes
the Spider leaderboard with 86.6% execution accuracy and sets a new bar. To
explore the potential of open-source LLM, we investigate them in various
scenarios, and further enhance their performance with supervised fine-tuning.
Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well
as the advantages and disadvantages of the supervised fine-tuning.
Additionally, towards an efficient and economic LLM-based Text-to-SQL solution,
we emphasize the token efficiency in prompt engineering and compare the prior
studies under this metric. We hope that our work provides a deeper
understanding of Text-to-SQL with LLMs, and inspires further investigations and
broad applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We have released code on https://github.com/BeachWang/DAIL-SQL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A novel approach to measuring patent claim scope based on probabilities
  obtained from (large) language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sébastien Ragot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes to measure the scope of a patent claim as the reciprocal
of the self-information contained in this claim. A probability of occurrence of
the claim is obtained from a language model and this probability is used to
compute the self-information. Grounded in information theory, this approach is
based on the assumption that an unlikely concept is more informative than a
usual concept, insofar as it is more surprising. In turn, the more surprising
the information required to defined the claim, the narrower its scope. Five
language models are considered, ranging from simplest models (each word or
character is assigned an identical probability) to intermediate models (using
average word or character frequencies), to a large language model (GPT2).
Interestingly, the scope resulting from the simplest language models is
proportional to the reciprocal of the number of words or characters involved in
the claim, a metric already used in previous works. Application is made to
multiple series of patent claims directed to distinct inventions, where each
series consists of claims devised to have a gradually decreasing scope. The
performance of the language models is assessed with respect to several ad hoc
tests. The more sophisticated the model, the better the results. I.e., the GPT2
probability model outperforms models based on word and character frequencies,
which themselves outdo the simplest models based on word or character counts.
Still, the character count appears to be a more reliable indicator than the
word count.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages, 8 tables, 6 figures. Substantial changes made to version 2:
  New section 4.1 added (including a new table); Minor normalization issue
  corrected in values listed in Appendix B; Content of former appendix C now
  moved to Section 3; and new Appendix C added. Minor changes made to version 3
  (style, typos, language)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Women Wearing Lipstick: Measuring the Bias Between an Object and Its
  Related Gender <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Sabir, Lluís Padró
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the impact of objects on gender bias in image
captioning systems. Our results show that only gender-specific objects have a
strong gender bias (e.g., women-lipstick). In addition, we propose a visual
semantic-based gender score that measures the degree of bias and can be used as
a plug-in for any image captioning system. Our experiments demonstrate the
utility of the gender score, since we observe that our score can measure the
bias relation between a caption and its related gender; therefore, our score
can be used as an additional metric to the existing Object Gender Co-Occ
approach. Code and data are publicly available at
\url{https://github.com/ahmedssabir/GenderScore}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attribution Patching Outperforms Automated Circuit Discovery <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaquib Syed, Can Rager, Arthur Conmy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated interpretability research has recently attracted attention as a
potential research direction that could scale explanations of neural network
behavior to large models. Existing automated circuit discovery work applies
activation patching to identify subnetworks responsible for solving specific
tasks (circuits). In this work, we show that a simple method based on
attribution patching outperforms all existing methods while requiring just two
forward passes and a backward pass. We apply a linear approximation to
activation patching to estimate the importance of each edge in the
computational subgraph. Using this approximation, we prune the least important
edges of the network. We survey the performance and limitations of this method,
finding that averaged over all tasks our method has greater AUC from circuit
recovery than other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 main paper pages, 6 additional pages. NeurIPS 2023 ATTRIB Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StyleTTS: A Style-Based Generative Model for Natural and Diverse
  Text-to-Speech Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Aaron Li, Cong Han, Nima Mesgarani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Speech (TTS) has recently seen great progress in synthesizing
high-quality speech owing to the rapid development of parallel TTS systems, but
producing speech with naturalistic prosodic variations, speaking styles and
emotional tones remains challenging. Moreover, since duration and speech are
generated separately, parallel TTS models still have problems finding the best
monotonic alignments that are crucial for naturalistic speech synthesis. Here,
we propose StyleTTS, a style-based generative model for parallel TTS that can
synthesize diverse speech with natural prosody from a reference speech
utterance. With novel Transferable Monotonic Aligner (TMA) and
duration-invariant data augmentation schemes, our method significantly
outperforms state-of-the-art models on both single and multi-speaker datasets
in subjective tests of speech naturalness and speaker similarity. Through
self-supervised learning of the speaking styles, our model can synthesize
speech with the same prosodic and emotional tone as any given reference speech
without the need for explicitly labeling these categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion
  and Adversarial Training with Large Speech Language Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Aaron Li, Cong Han, Vinay S. Raghavan, Gavin Mischler, Nima Mesgarani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that
leverages style diffusion and adversarial training with large speech language
models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its
predecessor by modeling styles as a latent random variable through diffusion
models to generate the most suitable style for the text without requiring
reference speech, achieving efficient latent diffusion while benefiting from
the diverse speech synthesis offered by diffusion models. Furthermore, we
employ large pre-trained SLMs, such as WavLM, as discriminators with our novel
differentiable duration modeling for end-to-end training, resulting in improved
speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker
LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by
native English speakers. Moreover, when trained on the LibriTTS dataset, our
model outperforms previous publicly available models for zero-shot speaker
adaptation. This work achieves the first human-level TTS on both single and
multispeaker datasets, showcasing the potential of style diffusion and
adversarial training with large SLMs. The audio demos and source code are
available at https://styletts2.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferring Procedural Knowledge across Commonsense Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13867v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13867v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Jiang, Filip Ilievski, Kaixin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stories about everyday situations are an essential part of human
communication, motivating the need to develop AI agents that can reliably
understand these stories. Despite the long list of supervised methods for story
completion and procedural understanding, current AI has no mechanisms to
automatically track and explain procedures in unseen stories. To bridge this
gap, we study the ability of AI models to transfer procedural knowledge to
novel narrative tasks in a transparent manner. We design LEAP: a comprehensive
framework that integrates state-of-the-art modeling architectures, training
regimes, and augmentation strategies based on both natural and synthetic
stories. To address the lack of densely annotated training data, we devise a
robust automatic labeler based on few-shot prompting to enhance the augmented
data. Our experiments with in- and out-of-domain tasks reveal insights into the
interplay of different architectures, training regimes, and augmentation
strategies. LEAP's labeler has a clear positive impact on out-of-domain
datasets, while the resulting dense annotation provides native explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Math Word Problems with Reexamination <span class="chip">NeurIPS2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Bin, Wenhao Shi, Yujuan Ding, Yang Yang, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Math word problem (MWP) solving aims to understand the descriptive math
problem and calculate the result, for which previous efforts are mostly devoted
to upgrade different technical modules. This paper brings a different
perspective of \textit{reexamination process} during training by introducing a
pseudo-dual task to enhance the MWP solving. We propose a pseudo-dual (PseDual)
learning scheme to model such process, which is model-agnostic thus can be
adapted to any existing MWP solvers. The pseudo-dual task is specifically
defined as filling the numbers in the expression back into the original word
problem with numbers masked. To facilitate the effective joint learning of the
two tasks, we further design a scheduled fusion strategy for the number
infilling task, which smoothly switches the input from the ground-truth math
expressions to the predicted ones. Our pseudo-dual learning scheme has been
tested and proven effective when being equipped in several representative MWP
solvers through empirical studies. \textit{The codes and trained models are
available at:} \url{https://github.com/steven640pixel/PsedualMWP}.
\end{abstract}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be appeared at NeurIPS2023 Workshop on MATH-AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the release of T\"ULU [Wang et al., 2023b], open resources for
instruction tuning have developed quickly, from better base models to new
finetuning techniques. We test and incorporate a number of these advances into
T\"ULU, resulting in T\"ULU 2, a suite of improved T\"ULU models for advancing
the understanding and best practices of adapting pretrained language models to
downstream tasks and user preferences. Concretely, we release: (1)
T\"ULU-V2-mix, an improved collection of high-quality instruction datasets; (2)
T\"ULU 2, LLAMA-2 models finetuned on the V2 mixture; (3) T\"ULU 2+DPO, T\"ULU
2 models trained with direct preference optimization (DPO), including the
largest DPO-trained model to date (T\"ULU 2+DPO 70B); (4) CODE T\"ULU 2, CODE
LLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its
instruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple
perspectives shows that the T\"ULU 2 suite achieves state-of-the-art
performance among open models and matches or exceeds the performance of
GPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data,
training and evaluation code to facilitate future open efforts on adapting
large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report; fixed zephyr numbers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective Proxy for Human Labeling: Ensemble Disagreement Scores in
  Large Language Models for Industrial NLP <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Du, Laksh Advani, Yashmeet Gambhir, Daniel J Perry, Prashant Shiralkar, Zhengzheng Xing, Aaron Colak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant capability to
generalize across a large number of NLP tasks. For industry applications, it is
imperative to assess the performance of the LLM on unlabeled production data
from time to time to validate for a real-world setting. Human labeling to
assess model error requires considerable expense and time delay. Here we
demonstrate that ensemble disagreement scores work well as a proxy for human
labeling for language models in zero-shot, few-shot, and fine-tuned settings,
per our evaluation on keyphrase extraction (KPE) task. We measure fidelity of
the results by comparing to true error measured from human labeled ground
truth. We contrast with the alternative of using another LLM as a source of
machine labels, or silver labels. Results across various languages and domains
show disagreement scores provide a better estimation of model performance with
mean average error (MAE) as low as 0.4% and on average 13.8% better than using
silver labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version for 2023 EMNLP (The Third Workshop on Natural
  Language Generation, Evaluation, and Metrics (GEM))</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Landmark Attention: Random-Access Infinite Context Length for
  <span class="highlight-title">Transformer</span>s <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirkeivan Mohtashami, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Transformers have shown remarkable success in natural language
processing, their attention mechanism's large memory requirements have limited
their ability to handle longer contexts. Prior approaches, such as recurrent
memory or retrieval-based augmentation, have either compromised the
random-access flexibility of attention (i.e., the capability to select any
token in the entire context) or relied on separate mechanisms for relevant
context retrieval, which may not be compatible with the model's attention. In
this paper, we present a novel approach that allows access to the complete
context while retaining random-access flexibility, closely resembling running
attention on the entire context. Our method uses a landmark token to represent
each block of the input and trains the attention to use it for selecting
relevant blocks, enabling retrieval of blocks directly through the attention
mechanism instead of by relying on a separate mechanism. Our approach
seamlessly integrates with specialized data structures and the system's memory
hierarchy, enabling processing of arbitrarily long context lengths. We
demonstrate that our method can obtain comparable performance with
Transformer-XL while significantly reducing the number of retrieved tokens in
each step. Finally, we show that fine-tuning LLaMA 7B with our method
successfully extends its context length capacity to over 32k tokens, allowing
for inference at the context lengths of GPT-4. We release the implementation of
landmark attention and the code to reproduce our experiments at
https://github.com/epfml/landmark-attention/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at NeurIPS 2023 - 37th Conference on
  Neural Information Processing Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lost in the Middle: How Language Models Use Long Contexts <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03172v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03172v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent language models have the ability to take long contexts as input,
relatively little is known about how well they use longer context. We analyze
the performance of language models on two tasks that require identifying
relevant information in their input contexts: multi-document question answering
and key-value retrieval. We find that performance can degrade significantly
when changing the position of relevant information, indicating that current
language models do not robustly make use of information in long input contexts.
In particular, we observe that performance is often highest when relevant
information occurs at the beginning or end of the input context, and
significantly degrades when models must access relevant information in the
middle of long contexts, even for explicitly long-context models. Our analysis
provides a better understanding of how language models use their input context
and provides new evaluation protocols for future long-context language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 16 figures. Accepted for publication in Transactions of the
  Association for Computational Linguistics (TACL), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Contamination Quiz: A Tool to Detect and Estimate Contamination in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06233v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06233v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahriar Golchin, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the Data Contamination Quiz, a simple and effective approach to
detect data contamination in large language models (LLMs) and estimate the
amount of it. Specifically, we frame data contamination detection as a series
of multiple-choice questions. We devise a quiz format wherein three perturbed
versions of each dataset instance are created. These changes only include
word-level perturbations, replacing words with their contextual synonyms,
ensuring both the semantic and sentence structure remain exactly the same as
the original instance. Together with the original instance, these perturbed
versions constitute the choices in the quiz. Given that the only distinguishing
signal among these choices is the exact wording, an LLM, when tasked with
identifying the original instance from the choices, opts for the original if it
has memorized it in its pre-training phase--a trait intrinsic to LLMs. A
dataset partition is then marked as contaminated if the LLM's performance on
the quiz surpasses what random chance suggests. Our evaluation spans seven
datasets and their respective splits (train and test/validation) on two
state-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the
pre-training data, our results suggest that our approach not only enhances the
detection of data contamination but also provides an accurate estimation of its
extent, even when the contamination signal is weak.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v1.2 preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Antibody Design for Complementary Chain Pairing Sequences
  through Encoder-Decoder Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02748v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02748v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon K. S. Chu, Kathy Y. Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current protein language models (pLMs) predominantly focus on single-chain
protein sequences and often have not accounted for constraints on generative
design imposed by protein-protein interactions. To address this gap, we present
paired Antibody T5 (pAbT5), an encoder-decoder model to generate complementary
heavy or light chain from its pairing partner. We show that our model respects
conservation in framework regions and variability in hypervariable domains,
demonstrated by agreement with sequence alignment and variable-length CDR
loops. We also show that our model captures chain pairing preferences through
the recovery of ground-truth chain type and gene families. Our results showcase
the potential of pAbT5 in generative antibody design, incorporating biological
constraints from chain pairing preferences.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">137</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hourglass Tokenizer for Efficient <span class="highlight-title">Transformer</span>-Based 3D Human Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Our source code will be open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, Kai Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a Pose-Free Large Reconstruction Model (PF-LRM) for reconstructing
a 3D object from a few unposed images even with little visual overlap, while
simultaneously estimating the relative camera poses in ~1.3 seconds on a single
A100 GPU. PF-LRM is a highly scalable method utilizing the self-attention
blocks to exchange information between 3D object tokens and 2D image tokens; we
predict a coarse point cloud for each view, and then use a differentiable
Perspective-n-Point (PnP) solver to obtain camera poses. When trained on a huge
amount of multi-view posed data of ~1M objects, PF-LRM shows strong
cross-dataset generalization ability, and outperforms baseline methods by a
large margin in terms of pose prediction accuracy and 3D reconstruction quality
on various unseen evaluation datasets. We also demonstrate our model's
applicability in downstream text/image-to-3D task with fast feed-forward
inference. Our project website is at: https://totoro97.github.io/pf-lrm .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://totoro97.github.io/pf-lrm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>-4V(ision) for Robotics: Multimodal Task Planning from Human
  Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a pipeline that enhances a general-purpose Vision Language
Model, GPT-4V(ision), by integrating observations of human actions to
facilitate robotic manipulation. This system analyzes videos of humans
performing tasks and creates executable robot programs that incorporate
affordance insights. The computation starts by analyzing the videos with GPT-4V
to convert environmental and action details into text, followed by a
GPT-4-empowered task planner. In the following analyses, vision systems
reanalyze the video with the task plan. Object names are grounded using an
open-vocabulary object detector, while focus on the hand-object relation helps
to detect the moment of grasping and releasing. This spatiotemporal grounding
allows the vision systems to further gather affordance data (e.g., grasp type,
way points, and body postures). Experiments across various scenarios
demonstrate this method's efficacy in achieving real robots' operations from
human demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are
available at this project page:
https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures, 1 table. Last updated on November 20th, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Lip Segmentation Techniques in Computer Vision: A Comparative
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro B. S. Masur, Francisco Braulio Oliveira, Lucas Moreira Medino, Emanuel Huber, Milene Haraguchi Padilha, Cassio de Alcantara, Renata Sellaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lip segmentation is crucial in computer vision, especially for lip reading.
Despite extensive face segmentation research, lip segmentation has received
limited attention. The aim of this study is to compare state-of-the-art lip
segmentation models using a standardized setting and a publicly available
dataset. Five techniques, namely EHANet, Mask2Former, BiSeNet V2, PIDNet, and
STDC1, are qualitatively selected based on their reported performance,
inference time, code availability, recency, and popularity. The CelebAMask-HQ
dataset, comprising manually annotated face images, is used to fairly assess
the lip segmentation performance of the selected models. Inference experiments
are conducted on a Raspberry Pi4 to emulate limited computational resources.
The results show that Mask2Former and EHANet have the best performances in
terms of mIoU score. BiSeNet V2 demonstrate competitive performance, while
PIDNet excels in recall but has lower precision. Most models present inference
time ranging from 1000 to around 3000 milliseconds on a Raspberry Pi4, with
PIDNet having the lowest mean inference time. This study provides a
comprehensive evaluation of lip segmentation models, highlighting their
performance and inference times. The findings contribute to the development of
lightweight techniques and establish benchmarks for future advances in lip
segmentation, especially in IoT and edge computing scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Categorizing the Visual Environment and Analyzing the Visual Attention
  of Dogs <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyas Sundara Raman, Madeline H. Pelgrim, Daphna Buchsbaum, Thomas Serre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dogs have a unique evolutionary relationship with humans and serve many
important roles e.g. search and rescue, blind assistance, emotional support.
However, few datasets exist to categorize visual features and objects available
to dogs, as well as how dogs direct their visual attention within their
environment. We collect and study a dataset with over 11,698 gazes to
categorize the objects available to be gazed at by 11 dogs in everyday outdoor
environments i.e. a walk around a college campus and urban area. We explore the
availability of these object categories and the visual attention of dogs over
these categories using a head mounted eye tracking apparatus. A small portion
(approx. 600 images or < 20% of total dataset) of the collected data is used to
fine tune a MaskRCNN for the novel image domain to segment objects present in
the scene, enabling further statistical analysis on the visual gaze tendencies
of dogs. The MaskRCNN, with eye tracking apparatus, serves as an end to end
model for automatically classifying the visual fixations of dogs. The fine
tuned MaskRCNN performs far better than chance. There are few individual
differences between the 11 dogs and we observe greater visual fixations on
buses, plants, pavement, and construction equipment. This work takes a step
towards understanding visual behavior of dogs and their interaction with the
physical world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures, 1 table, WACV CV4Smalls Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Previous Facial Action Units Knowledge for Emotion
  Recognition on Faces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro B. S. Masur, Willams Costa, Lucas S. Figueredo, Veronica Teichrieb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People naturally understand emotions, thus permitting a machine to do the
same could open new paths for human-computer interaction. Facial expressions
can be very useful for emotion recognition techniques, as these are the biggest
transmitters of non-verbal cues capable of being correlated with emotions.
Several techniques are based on Convolutional Neural Networks (CNNs) to extract
information in a machine learning process. However, simple CNNs are not always
sufficient to locate points of interest on the face that can be correlated with
emotions. In this work, we intend to expand the capacity of emotion recognition
techniques by proposing the usage of Facial Action Units (AUs) recognition
techniques to recognize emotions. This recognition will be based on the Facial
Action Coding System (FACS) and computed by a machine learning system. In
particular, our method expands over EmotiRAM, an approach for multi-cue emotion
recognition, in which we improve over their facial encoding module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Supervision Levels Trade-Offs for Infrared-Based People
  Counting <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Latortue, Moetez Kdayem, Fidel A Guerrero Peña, Eric Granger, Marco Pedersoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection models are commonly used for people counting (and
localization) in many applications but require a dataset with costly bounding
box annotations for training. Given the importance of privacy in people
counting, these models rely more and more on infrared images, making the task
even harder. In this paper, we explore how weaker levels of supervision can
affect the performance of deep person counting architectures for image
classification and point-level localization. Our experiments indicate that
counting people using a CNN Image-Level model achieves competitive results with
YOLO detectors and point-level models, yet provides a higher frame rate and a
similar amount of model parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDAR-HMR: 3D Human Mesh Recovery from LiDAR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Fan, Wenzhao Zheng, Jianjiang Feng, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, point cloud perception tasks have been garnering increasing
attention. This paper presents the first attempt to estimate 3D human body mesh
from sparse LiDAR point clouds. We found that the major challenge in estimating
human pose and mesh from point clouds lies in the sparsity, noise, and
incompletion of LiDAR point clouds. Facing these challenges, we propose an
effective sparse-to-dense reconstruction scheme to reconstruct 3D human mesh.
This involves estimating a sparse representation of a human (3D human pose) and
gradually reconstructing the body mesh. To better leverage the 3D structural
information of point clouds, we employ a cascaded graph transformer
(graphormer) to introduce point cloud features during sparse-to-dense
reconstruction. Experimental results on three publicly available databases
demonstrate the effectiveness of the proposed approach. Code:
https://github.com/soullessrobot/LiDAR-HMR/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/soullessrobot/LiDAR-HMR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SA-Med2D-20M <span class="highlight-title">Dataset</span>: Segment Anything in 2D Medical Imaging with 20
  Million masks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, Hui Sun, Min Zhu, Shaoting Zhang, Junjun He, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segment Anything Model (SAM) has achieved impressive results for natural
image segmentation with input prompts such as points and bounding boxes. Its
success largely owes to massive labeled training data. However, directly
applying SAM to medical image segmentation cannot perform well because SAM
lacks medical knowledge -- it does not use medical images for training. To
incorporate medical knowledge into SAM, we introduce SA-Med2D-20M, a
large-scale segmentation dataset of 2D medical images built upon numerous
public and private datasets. It consists of 4.6 million 2D medical images and
19.7 million corresponding masks, covering almost the whole body and showing
significant diversity. This paper describes all the datasets collected in
SA-Med2D-20M and details how to process these datasets. Furthermore,
comprehensive statistics of SA-Med2D-20M are presented to facilitate the better
use of our dataset, which can help the researchers build medical vision
foundation models or apply their models to downstream medical applications. We
hope that the large scale and diversity of SA-Med2D-20M can be leveraged to
develop medical artificial intelligence for enhancing diagnosis, medical image
analysis, knowledge sharing, and education. The data with the redistribution
license is publicly available at https://github.com/OpenGVLab/SAM-Med2D.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Can AutoML Do For Continual Learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Kilickaya, Joaquin Vanschoren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This position paper outlines the potential of AutoML for incremental
(continual) learning to encourage more research in this direction. Incremental
learning involves incorporating new data from a stream of tasks and
distributions to learn enhanced deep representations and adapt better to new
tasks. However, a significant limitation of incremental learners is that most
current techniques freeze the backbone architecture, hyperparameters, and the
order & structure of the learning tasks throughout the learning and adaptation
process. We strongly believe that AutoML offers promising solutions to address
these limitations, enabling incremental learning to adapt to more diverse
real-world tasks. Therefore, instead of directly proposing a new method, this
paper takes a step back by posing the question: "What can AutoML do for
incremental learning?" We outline three key areas of research that can
contribute to making incremental learners more dynamic, highlighting concrete
opportunities to apply AutoML methods in novel ways as well as entirely new
challenges for AutoML research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NNG-Mix: Improving Semi-supervised Anomaly Detection with Pseudo-anomaly
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dong, Gaëtan Frusque, Yue Zhao, Eleni Chatzi, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection (AD) is essential in identifying rare and often critical
events in complex systems, finding applications in fields such as network
intrusion detection, financial fraud detection, and fault detection in
infrastructure and industrial systems. While AD is typically treated as an
unsupervised learning task due to the high cost of label annotation, it is more
practical to assume access to a small set of labeled anomaly samples from
domain experts, as is the case for semi-supervised anomaly detection.
Semi-supervised and supervised approaches can leverage such labeled data,
resulting in improved performance. In this paper, rather than proposing a new
semi-supervised or supervised approach for AD, we introduce a novel algorithm
for generating additional pseudo-anomalies on the basis of the limited labeled
anomalies and a large volume of unlabeled data. This serves as an augmentation
to facilitate the detection of new anomalies. Our proposed algorithm, named
Nearest Neighbor Gaussian Mixup (NNG-Mix), efficiently integrates information
from both labeled and unlabeled data to generate pseudo-anomalies. We compare
the performance of this novel algorithm with commonly applied augmentation
techniques, such as Mixup and Cutout. We evaluate NNG-Mix by training various
existing semi-supervised and supervised anomaly detection algorithms on the
original training data along with the generated pseudo-anomalies. Through
extensive experiments on 57 benchmark datasets in ADBench, reflecting different
data types, we demonstrate that NNG-Mix outperforms other data augmentation
methods. It yields significant performance improvements compared to the
baselines trained exclusively on the original training data. Notably, NNG-Mix
yields up to 16.4%, 8.8%, and 8.0% improvements on Classical, CV, and NLP
datasets in ADBench. Our source code will be available at
https://github.com/donghao51/NNG-Mix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image is Worth Multiple Words: Multi-attribute Inversion for
  Constrained Text-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Agarwal, Srikrishna Karanam, Tripti Shukla, Balaji Vasan Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of constraining diffusion model outputs with a
user-supplied reference image. Our key objective is to extract multiple
attributes (e.g., color, object, layout, style) from this single reference
image, and then generate new samples with them. One line of existing work
proposes to invert the reference images into a single textual conditioning
vector, enabling generation of new samples with this learned token. These
methods, however, do not learn multiple tokens that are necessary to condition
model outputs on the multiple attributes noted above. Another line of
techniques expand the inversion space to learn multiple embeddings but they do
this only along the layer dimension (e.g., one per layer of the DDPM model) or
the timestep dimension (one for a set of timesteps in the denoising process),
leading to suboptimal attribute disentanglement. To address the aforementioned
gaps, the first contribution of this paper is an extensive analysis to
determine which attributes are captured in which dimension of the denoising
process. As noted above, we consider both the time-step dimension (in reverse
denoising) as well as the DDPM model layer dimension. We observe that often a
subset of these attributes are captured in the same set of model layers and/or
across same denoising timesteps. For instance, color and style are captured
across same U-Net layers, whereas layout and color are captured across same
timestep stages. Consequently, an inversion process that is designed only for
the time-step dimension or the layer dimension is insufficient to disentangle
all attributes. This leads to our second contribution where we design a new
multi-attribute inversion algorithm, MATTE, with associated
disentanglement-enhancing regularization losses, that operates across both
dimensions and explicitly leads to four disentangled tokens (color, style,
layout, and object).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization of Fitness Exercise Recognition from Doppler Measurements
  by Domain-adaption and Few-Shot Learning <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biying Fu, Naser Damer, Florian Kirchbuchner, Arjan Kuijper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In previous works, a mobile application was developed using an unmodified
commercial off-the-shelf smartphone to recognize whole-body exercises. The
working principle was based on the ultrasound Doppler sensing with the device
built-in hardware. Applying such a lab-environment trained model on realistic
application variations causes a significant drop in performance, and thus
decimate its applicability. The reason of the reduced performance can be
manifold. It could be induced by the user, environment, and device variations
in realistic scenarios. Such scenarios are often more complex and diverse,
which can be challenging to anticipate in the initial training data. To study
and overcome this issue, this paper presents a database with controlled and
uncontrolled subsets of fitness exercises. We propose two concepts to utilize
small adaption data to successfully improve model generalization in an
uncontrolled environment, increasing the recognition accuracy by two to six
folds compared to the baseline for different users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at International Conference on Pattern Recognition (ICPR)
  workshop 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning: Applications and the Road Forward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eli Verwimp, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L. Hayes, Eyke Hüllermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu, Andreas S. Tolias, Joost van de Weijer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars, Gido M. van de Ven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning is a sub-field of machine learning, which aims to allow
machine learning models to continuously learn on new data, by accumulating
knowledge without forgetting what was learned in the past. In this work, we
take a step back, and ask: "Why should one care about continual learning in the
first place?". We set the stage by surveying recent continual learning papers
published at three major machine learning conferences, and show that
memory-constrained settings dominate the field. Then, we discuss five open
problems in machine learning, and even though they seem unrelated to continual
learning at first sight, we show that continual learning will inevitably be
part of their solution. These problems are model-editing, personalization,
on-device learning, faster (re-)training and reinforcement learning. Finally,
by comparing the desiderata from these unsolved problems and the current
assumptions in continual learning, we highlight and discuss four future
directions for continual learning research. We hope that this work offers an
interesting perspective on the future of continual learning, while displaying
its potential value and the paths we have to pursue in order to make it
successful. This work is the result of the many discussions the authors had at
the Dagstuhl seminar on Deep Continual Learning, in March 2023.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs as Visual Explainers: Advancing Image Classification with Evolving
  Visual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhao Han, Le Zhuo, Yue Liao, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) offer a promising paradigm for image
classification by comparing the similarity between images and class embeddings.
A critical challenge lies in crafting precise textual representations for class
names. While previous studies have leveraged recent advancements in large
language models (LLMs) to enhance these descriptors, their outputs often suffer
from ambiguity and inaccuracy. We identify two primary causes: 1) The prevalent
reliance on textual interactions with LLMs, leading to a mismatch between the
generated text and the visual content in VLMs' latent space - a phenomenon we
term the "explain without seeing" dilemma. 2) The oversight of the inter-class
relationships, resulting in descriptors that fail to differentiate similar
classes effectively. To address these issues, we propose a novel image
classification framework combining VLMs with LLMs, named Iterative Optimization
with Visual Feedback. In particular, our method develops an LLM-based agent,
employing an evolutionary optimization strategy to refine class descriptors.
Crucially, we incorporate visual feedback from VLM classification metrics,
thereby guiding the optimization process with concrete visual data. Our method
leads to improving accuracy on a wide range of image classification benchmarks,
with 3.47\% average gains over state-of-the-art methods. We also highlight the
resulting descriptions serve as explainable and robust features that can
consistently improve the performance across various backbone models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying the Defective: Detecting Damaged Grains for Cereal
  Appearance Inspection <span class="chip">ECAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Fan, Yiwen Ding, Dongdong Fan, Yong Wu, Maurice Pagnucco, Yang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cereal grain plays a crucial role in the human diet as a major source of
essential nutrients. Grain Appearance Inspection (GAI) serves as an essential
process to determine grain quality and facilitate grain circulation and
processing. However, GAI is routinely performed manually by inspectors with
cumbersome procedures, which poses a significant bottleneck in smart
agriculture.
  In this paper, we endeavor to develop an automated GAI system:AI4GrainInsp.
By analyzing the distinctive characteristics of grain kernels, we formulate GAI
as a ubiquitous problem: Anomaly Detection (AD), in which healthy and edible
kernels are considered normal samples while damaged grains or unknown objects
are regarded as anomalies. We further propose an AD model, called AD-GAI, which
is trained using only normal samples yet can identify anomalies during
inference. Moreover, we customize a prototype device for data acquisition and
create a large-scale dataset including 220K high-quality images of wheat and
maize kernels. Through extensive experiments, AD-GAI achieves considerable
performance in comparison with advanced AD methods, and AI4GrainInsp has highly
consistent performance compared to human experts and excels at inspection
efficiency over 20x speedup. The dataset, code and models will be released at
https://github.com/hellodfan/AI4GrainInsp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECAI2023. https://github.com/hellodfan/AI4GrainInsp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SniffyArt: The <span class="highlight-title">Dataset</span> of Smelling Persons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Zinnen, Azhar Hussian, Hang Tran, Prathmesh Madhu, Andreas Maier, Vincent Christlein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smell gestures play a crucial role in the investigation of past smells in the
visual arts yet their automated recognition poses significant challenges. This
paper introduces the SniffyArt dataset, consisting of 1941 individuals
represented in 441 historical artworks. Each person is annotated with a tightly
fitting bounding box, 17 pose keypoints, and a gesture label. By integrating
these annotations, the dataset enables the development of hybrid classification
approaches for smell gesture recognition. The datasets high-quality human pose
estimation keypoints are achieved through the merging of five separate sets of
keypoint annotations per person. The paper also presents a baseline analysis,
evaluating the performance of representative algorithms for detection, keypoint
estimation, and classification tasks, showcasing the potential of combining
keypoint estimation with smell gesture classification. The SniffyArt dataset
lays a solid foundation for future research and the exploration of multi-task
approaches leveraging pose keypoints and person boxes to advance human gesture
and olfactory dimension analysis in historical artworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Faces (MTF) Data Set: A Legally and Ethically Compliant
  Collection of Face Images for Various Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rami Haffar, David Sánchez, Josep Domingo-Ferrer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human facial data hold tremendous potential to address a variety of
classification problems, including face recognition, age estimation, gender
identification, emotion analysis, and race classification. However, recent
privacy regulations, such as the EU General Data Protection Regulation and
others, have restricted the ways in which human images may be collected and
used for research. As a result, several previously published data sets
containing human faces have been removed from the internet due to inadequate
data collection methods that failed to meet privacy regulations. Data sets
consisting of synthetic data have been proposed as an alternative, but they
fall short of accurately representing the real data distribution. On the other
hand, most available data sets are labeled for just a single task, which limits
their applicability. To address these issues, we present the Multi-Task Faces
(MTF) image data set, a meticulously curated collection of face images designed
for various classification tasks, including face recognition, as well as race,
gender, and age classification. The MTF data set has been ethically gathered by
leveraging publicly available images of celebrities and strictly adhering to
copyright regulations. In this paper, we present this data set and provide
detailed descriptions of the followed data collection and processing
procedures. Furthermore, we evaluate the performance of five deep learning (DL)
models on the MTF data set across the aforementioned classification tasks.
Additionally, we compare the performance of DL models over the processed MTF
data and over raw data crawled from the internet. The reported results
constitute a baseline for further research employing these data. The MTF data
set can be accessed through the following link (please cite the present paper
if you use the data set): https://github.com/RamiHaf/MTF_data_set
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 2 figures, 9 Tables,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM-Eval: A General Evaluation on Video Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuailin Li, Yuang Zhang, Yucheng Zhao, Qiuyue Wang, Fan Jia, Yingfei Liu, Tiancai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid development of video Large Language Models (LLMs), a
comprehensive evaluation is still absent. In this paper, we introduce a unified
evaluation that encompasses multiple video tasks, including captioning,
question and answering, retrieval, and action recognition. In addition to
conventional metrics, we showcase how GPT-based evaluation can match human-like
performance in assessing response quality across multiple aspects. We propose a
simple baseline: Video-LLaVA, which uses a single linear projection and
outperforms existing video LLMs. Finally, we evaluate video LLMs beyond
academic datasets, which show encouraging recognition and reasoning
capabilities in driving scenarios with only hundreds of video-instruction pairs
for fine-tuning. We hope our work can serve as a unified evaluation for video
LLMs, and help expand more practical scenarios. The evaluation code will be
available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Li, Dingwen Zhang, Yalun Dai, Nian Liu, Lechao Cheng, Jingfeng Li, Jingdong Wang, Junwei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying NeRF to downstream perception tasks for scene understanding and
representation is becoming increasingly popular. Most existing methods treat
semantic prediction as an additional rendering task, \textit{i.e.}, the "label
rendering" task, to build semantic NeRFs. However, by rendering
semantic/instance labels per pixel without considering the contextual
information of the rendered image, these methods usually suffer from unclear
boundary segmentation and abnormal segmentation of pixels within an object. To
solve this problem, we propose Generalized Perception NeRF (GP-NeRF), a novel
pipeline that makes the widely used segmentation model and NeRF work compatibly
under a unified framework, for facilitating context-aware 3D scene perception.
To accomplish this goal, we introduce transformers to aggregate radiance as
well as semantic embedding fields jointly for novel views and facilitate the
joint volumetric rendering of both fields. In addition, we propose two
self-distillation mechanisms, i.e., the Semantic Distill Loss and the
Depth-Guided Semantic Distill Loss, to enhance the discrimination and quality
of the semantic field and the maintenance of geometric consistency. In
evaluation, we conduct experimental comparisons under two perception tasks
(\textit{i.e.} semantic and instance segmentation) using both synthetic and
real-world datasets. Notably, our method outperforms SOTA approaches by 6.94\%,
11.76\%, and 8.47\% on generalized semantic segmentation, finetuning semantic
segmentation, and instance segmentation, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LION : Empowering Multimodal Large Language Model with Dual-Level Visual
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have endowed LLMs with the ability
to perceive and understand multi-modal signals. However, most of the existing
MLLMs mainly adopt vision encoders pretrained on coarsely aligned image-text
pairs, leading to insufficient extraction and reasoning of visual knowledge. To
address this issue, we devise a dual-Level vIsual knOwledge eNhanced Multimodal
Large Language Model (LION), which empowers the MLLM by injecting visual
knowledge in two levels. 1) Progressive incorporation of fine-grained
spatial-aware visual knowledge. We design a vision aggregator cooperated with
region-level vision-language (VL) tasks to incorporate fine-grained
spatial-aware visual knowledge into the MLLM. To alleviate the conflict between
image-level and region-level VL tasks during incorporation, we devise a
dedicated stage-wise instruction-tuning strategy with mixture-of-adapters. This
progressive incorporation scheme contributes to the mutual promotion between
these two kinds of VL tasks. 2) Soft prompting of high-level semantic visual
evidence. We facilitate the MLLM with high-level semantic visual evidence by
leveraging diverse image tags. To mitigate the potential influence caused by
imperfect predicted tags, we propose a soft prompting method by embedding a
learnable token into the tailored text instruction. Comprehensive experiments
on several multi-modal benchmarks demonstrate the superiority of our model
(e.g., improvement of 5% accuracy on VSR and 3% CIDEr on TextCaps over
InstructBLIP, 5% accuracy on RefCOCOg over Kosmos-2).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. Project page:
  https://rshaojimmy.github.io/Projects/JiuTian-LION Code:
  https://github.com/rshaojimmy/JiuTian</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FATURA: A Multi-Layout Invoice Image <span class="highlight-title">Dataset</span> for Document Analysis and
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Limam, Marwa Dhiaf, Yousri Kessentini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document analysis and understanding models often require extensive annotated
data to be trained. However, various document-related tasks extend beyond mere
text transcription, requiring both textual content and precise bounding-box
annotations to identify different document elements. Collecting such data
becomes particularly challenging, especially in the context of invoices, where
privacy concerns add an additional layer of complexity. In this paper, we
introduce FATURA, a pivotal resource for researchers in the field of document
analysis and understanding. FATURA is a highly diverse dataset featuring
multi-layout, annotated invoice document images. Comprising $10,000$ invoices
with $50$ distinct layouts, it represents the largest openly accessible image
dataset of invoice documents known to date. We also provide comprehensive
benchmarks for various document analysis and understanding tasks and conduct
experiments under diverse training and evaluation scenarios. The dataset is
freely accessible at https://zenodo.org/record/8261508, empowering researchers
to advance the field of document analysis and understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asynchronous Bioplausible Neuron for Spiking Neural Networks for
  Event-Based Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanket Kachole, Hussain Sajwani, Fariborz Baghaei Naeini, Dimitrios Makris, Yahya Zweiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) offer a biologically inspired approach to
computer vision that can lead to more efficient processing of visual data with
reduced energy consumption. However, maintaining homeostasis within these
networks is challenging, as it requires continuous adjustment of neural
responses to preserve equilibrium and optimal processing efficiency amidst
diverse and often unpredictable input signals. In response to these challenges,
we propose the Asynchronous Bioplausible Neuron (ABN), a dynamic spike firing
mechanism to auto-adjust the variations in the input signal. Comprehensive
evaluation across various datasets demonstrates ABN's enhanced performance in
image classification and segmentation, maintenance of neural equilibrium, and
energy efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entangled View-Epipolar Information Aggregation for Generalizable Neural
  Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Min, Yawei Luo, Wei Yang, Yuesong Wang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizable NeRF can directly synthesize novel views across new scenes,
eliminating the need for scene-specific retraining in vanilla NeRF. A critical
enabling factor in these approaches is the extraction of a generalizable 3D
representation by aggregating source-view features. In this paper, we propose
an Entangled View-Epipolar Information Aggregation method dubbed EVE-NeRF.
Different from existing methods that consider cross-view and along-epipolar
information independently, EVE-NeRF conducts the view-epipolar feature
aggregation in an entangled manner by injecting the scene-invariant appearance
continuity and geometry consistency priors to the aggregation process. Our
approach effectively mitigates the potential lack of inherent geometric and
appearance constraint resulting from one-dimensional interactions, thus further
boosting the 3D representation generalizablity. EVE-NeRF attains
state-of-the-art performance across various evaluation scenarios. Extensive
experiments demonstate that, compared to prevailing single-dimensional
aggregation, the entangled network excels in the accuracy of 3D scene geometry
and appearance reconstruction.Our project page is
https://github.com/tatakai1/EVENeRF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kandinsky Conformal Prediction: Efficient Calibration of Image
  Segmentation Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joren Brunekreef, Eric Marcus, Ray Sheombarsing, Jan-Jakob Sonke, Jonas Teuwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation algorithms can be understood as a collection of pixel
classifiers, for which the outcomes of nearby pixels are correlated. Classifier
models can be calibrated using Inductive Conformal Prediction, but this
requires holding back a sufficiently large calibration dataset for computing
the distribution of non-conformity scores of the model's predictions. If one
only requires only marginal calibration on the image level, this calibration
set consists of all individual pixels in the images available for calibration.
However, if the goal is to attain proper calibration for each individual pixel
classifier, the calibration set consists of individual images. In a scenario
where data are scarce (such as the medical domain), it may not always be
possible to set aside sufficiently many images for this pixel-level
calibration. The method we propose, dubbed ``Kandinsky calibration'', makes use
of the spatial structure present in the distribution of natural images to
simultaneously calibrate the classifiers of ``similar'' pixels. This can be
seen as an intermediate approach between marginal (imagewise) and conditional
(pixelwise) calibration, where non-conformity scores are aggregated over
similar image regions, thereby making more efficient use of the images
available for calibration. We run experiments on segmentation algorithms
trained and calibrated on subsets of the public MS-COCO and Medical Decathlon
datasets, demonstrating that Kandinsky calibration method can significantly
improve the coverage. When compared to both pixelwise and imagewise calibration
on little data, the Kandinsky method achieves much lower coverage errors,
indicating the data efficiency of the Kandinsky calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Multispectral Segmentation with Representations Generated by
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dilith Jayakody, Thanuja Ambegoda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of multispectral image segmentation (segmentation of images with
numerous channels/bands, each capturing a specific range of wavelengths of
electromagnetic radiation) has been previously explored in contexts with large
amounts of labeled data. However, these models tend not to generalize well to
datasets of smaller size. In this paper, we propose a novel approach for
improving few-shot segmentation performance on multispectral images using
reinforcement learning to generate representations. These representations are
generated in the form of mathematical expressions between channels and are
tailored to the specific class being segmented. Our methodology involves
training an agent to identify the most informative expressions, updating the
dataset using these expressions, and then using the updated dataset to perform
segmentation. Due to the limited length of the expressions, the model receives
useful representations without any added risk of overfitting. We evaluate the
effectiveness of our approach on several multispectral datasets and demonstrate
its effectiveness in boosting the performance of segmentation algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Holistic Inverse Rendering of Complex Facade via Aerial 3D Scanning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Xie, Rengan Xie, Rong Li, Kai Huang, Pengju Qiao, Jingsen Zhu, Xu Yin, Qi Ye, Wei Hua, Yuchi Huo, Hujun Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we use multi-view aerial images to reconstruct the geometry,
lighting, and material of facades using neural signed distance fields (SDFs).
Without the requirement of complex equipment, our method only takes simple RGB
images captured by a drone as inputs to enable physically based and
photorealistic novel-view rendering, relighting, and editing. However, a
real-world facade usually has complex appearances ranging from diffuse rocks
with subtle details to large-area glass windows with specular reflections,
making it hard to attend to everything. As a result, previous methods can
preserve the geometry details but fail to reconstruct smooth glass windows or
verse vise. In order to address this challenge, we introduce three spatial- and
semantic-adaptive optimization strategies, including a semantic regularization
approach based on zero-shot segmentation techniques to improve material
consistency, a frequency-aware geometry regularization to balance surface
smoothness and details in different surfaces, and a visibility probe-based
scheme to enable efficient modeling of the local lighting in large-scale
outdoor environments. In addition, we capture a real-world facade aerial 3D
scanning image set and corresponding point clouds for training and
benchmarking. The experiment demonstrates the superior quality of our method on
facade holistic inverse rendering, novel view synthesis, and scene editing
compared to state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-View Graph Consistency Learning for Invariant Graph
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Chen, Zhiming Li, Hua Mao, Wai Lok Woo, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph representation learning is fundamental for analyzing graph-structured
data. Exploring invariant graph representations remains a challenge for most
existing graph representation learning methods. In this paper, we propose a
cross-view graph consistency learning (CGCL) method that learns invariant graph
representations for link prediction. First, two complementary augmented views
are derived from an incomplete graph structure through a bidirectional graph
structure augmentation scheme. This augmentation scheme mitigates the potential
information loss that is commonly associated with various data augmentation
techniques involving raw graph data, such as edge perturbation, node removal,
and attribute masking. Second, we propose a CGCL model that can learn invariant
graph representations. A cross-view training scheme is proposed to train the
proposed CGCL model. This scheme attempts to maximize the consistency
information between one augmented view and the graph structure reconstructed
from the other augmented view. Furthermore, we offer a comprehensive
theoretical CGCL analysis. This paper empirically and experimentally
demonstrates the effectiveness of the proposed CGCL method, achieving
competitive results on graph datasets in comparisons with several
state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized super-resolution 4D Flow MRI -- using ensemble learning to
  extend across the cardiovascular system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Ericsson, Adam Hjalmarsson, Muhammad Usman Akbar, Edward Ferdian, Mia Bonini, Brandon Hardy, Jonas Schollenberger, Maria Aristova, Patrick Winter, Nicholas Burris, Alexander Fyrdahl, Andreas Sigfridsson, Susanne Schnell, C. Alberto Figueroa, David Nordsletten, Alistair A. Young, David Marlevi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive
measurement technique capable of quantifying blood flow across the
cardiovascular system. While practical use is limited by spatial resolution and
image noise, incorporation of trained super-resolution (SR) networks has
potential to enhance image quality post-scan. However, these efforts have
predominantly been restricted to narrowly defined cardiovascular domains, with
limited exploration of how SR performance extends across the cardiovascular
system; a task aggravated by contrasting hemodynamic conditions apparent across
the cardiovasculature. The aim of our study was to explore the generalizability
of SR 4D Flow MRI using a combination of heterogeneous training sets and
dedicated ensemble learning. With synthetic training data generated across
three disparate domains (cardiac, aortic, cerebrovascular), varying
convolutional base and ensemble learners were evaluated as a function of domain
and architecture, quantifying performance on both in-silico and acquired
in-vivo data from the same three domains. Results show that both bagging and
stacking ensembling enhance SR performance across domains, accurately
predicting high-resolution velocities from low-resolution input data in-silico.
Likewise, optimized networks successfully recover native resolution velocities
from downsampled in-vivo data, as well as show qualitative potential in
generating denoised SR-images from clinical level input data. In conclusion,
our work presents a viable approach for generalized SR 4D Flow MRI, with
ensemble learning extending utility across various clinical areas of interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrackCLF: Automatic Pavement Crack Detection based on Closed-Loop
  Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Li, Zhun Fan, Ying Chen, Huibiao Lin, Laura Moretti, Giuseppe Loprencipe, Weihua Sheng, Kelvin C. P. Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic pavement crack detection is an important task to ensure the
functional performances of pavements during their service life. Inspired by
deep learning (DL), the encoder-decoder framework is a powerful tool for crack
detection. However, these models are usually open-loop (OL) systems that tend
to treat thin cracks as the background. Meanwhile, these models can not
automatically correct errors in the prediction, nor can it adapt to the changes
of the environment to automatically extract and detect thin cracks. To tackle
this problem, we embed closed-loop feedback (CLF) into the neural network so
that the model could learn to correct errors on its own, based on generative
adversarial networks (GAN). The resulting model is called CrackCLF and includes
the front and back ends, i.e. segmentation and adversarial network. The front
end with U-shape framework is employed to generate crack maps, and the back end
with a multi-scale loss function is used to correct higher-order
inconsistencies between labels and crack maps (generated by the front end) to
address open-loop system issues. Empirical results show that the proposed
CrackCLF outperforms others methods on three public datasets. Moreover, the
proposed CLF can be defined as a plug and play module, which can be embedded
into different neural network models to improve their performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DocPedia: Unleashing the Power of Large Multimodal Model in the
  Frequency Domain for Versatile Document Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang Li, Can Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents DocPedia, a novel large multimodal model (LMM) for
versatile OCR-free document understanding, capable of parsing images up to
2,560$\times$2,560 resolution. Unlike existing work either struggle with
high-resolution documents or give up the large language model thus vision or
language ability constrained, our DocPedia directly processes visual input in
the frequency domain rather than the pixel space. The unique characteristic
enables DocPedia to capture a greater amount of visual and textual information
using a limited number of visual tokens. To consistently enhance both
perception and comprehension abilities of our model, we develop a dual-stage
training strategy and enrich instructions/annotations of all training tasks
covering multiple document types. Extensive quantitative and qualitative
experiments conducted on various publicly available benchmarks confirm the
mutual benefits of jointly learning perception and comprehension tasks. The
results provide further evidence of the effectiveness and superior performance
of our DocPedia over other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot Hand-Eye Calibration using Structure-from-Motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Andreff, Bernard Espiau, Radu Horaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose a new flexible method for hand-eye calibration. The
vast majority of existing hand-eye calibration techniques requires a
calibration rig which is used in conjunction with camera pose estimation
methods. Instead, we combine structure-from-motion with known robot motions and
we show that the solution can be obtained in linear form. The latter solves for
both the hand-eye parameters and for the unknown scale factor inherent with
structure-from-motion methods. The algebraic analysis that is made possible
with such a linear formulation allows to investigate not only the well known
case of general screw motions but also such singular motions as pure
translations, pure rotations, and planar motions. In essence, the robot-mounted
camera looks to an unknown rigid layout, tracks points over an image sequence
and estimates the camera-to-robot relationship. Such a self calibration process
is relevant for unmanned vehicles, robots working in remote places, and so
forth. We conduct a large number of experiments which validate the quality of
the method by comparing it with existing ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Igniting Language Intelligence: The Hitchhiker's Guide From
  Chain-of-Thought Reasoning to Language Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, Hai Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have dramatically enhanced the field of language
intelligence, as demonstrably evidenced by their formidable empirical
performance across a spectrum of complex reasoning tasks. Additionally,
theoretical proofs have illuminated their emergent reasoning capabilities,
providing a compelling showcase of their advanced cognitive abilities in
linguistic contexts. Critical to their remarkable efficacy in handling complex
reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning
techniques, obliging them to formulate intermediate steps en route to deriving
an answer. The CoT reasoning approach has not only exhibited proficiency in
amplifying reasoning performance but also in enhancing interpretability,
controllability, and flexibility. In light of these merits, recent research
endeavors have extended CoT reasoning methodologies to nurture the development
of autonomous language agents, which adeptly adhere to language instructions
and execute actions within varied environments. This survey paper orchestrates
a thorough discourse, penetrating vital research dimensions, encompassing: (i)
the foundational mechanics of CoT techniques, with a focus on elucidating the
circumstances and justification behind its efficacy; (ii) the paradigm shift in
CoT; and (iii) the burgeoning of language agents fortified by CoT approaches.
Prospective research avenues envelop explorations into generalization,
efficiency, customization, scaling, and safety. This paper caters to a wide
audience, including beginners seeking comprehensive knowledge of CoT reasoning
and language agents, as well as experienced researchers interested in
foundational mechanics and engaging in cutting-edge discussions on these
topics. A repository for the related papers is available at
https://github.com/Zoeyyao27/CoT-Igniting-Agent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Boundaries: A Comprehensive <span class="highlight-title">Survey</span> of Transferable Attacks on AI
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangjing Wang, Ce Zhou, Yuanda Wang, Bocheng Chen, Hanqing Guo, Qiben Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) systems such as autonomous vehicles, facial
recognition, and speech recognition systems are increasingly integrated into
our daily lives. However, despite their utility, these AI systems are
vulnerable to a wide range of attacks such as adversarial, backdoor, data
poisoning, membership inference, model inversion, and model stealing attacks.
In particular, numerous attacks are designed to target a particular model or
system, yet their effects can spread to additional targets, referred to as
transferable attacks. Although considerable efforts have been directed toward
developing transferable attacks, a holistic understanding of the advancements
in transferable attacks remains elusive. In this paper, we comprehensively
explore learning-based attacks from the perspective of transferability,
particularly within the context of cyber-physical security. We delve into
different domains -- the image, text, graph, audio, and video domains -- to
highlight the ubiquitous and pervasive nature of transferable attacks. This
paper categorizes and reviews the architecture of existing attacks from various
viewpoints: data, process, model, and system. We further examine the
implications of transferable attacks in practical scenarios such as autonomous
driving, speech recognition, and large language models (LLMs). Additionally, we
outline the potential research directions to encourage efforts in exploring the
landscape of transferable attacks. This survey offers a holistic understanding
of the prevailing transferable attacks and their impacts across different
domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayar Lotfy, Anna Alperovich, Tommaso Giannantonio, Bjorn Barz, Xiaohan Zhang, Felix Holm, Nassir Navab, Felix Boehm, Carolin Schwamborn, Thomas K. Hoffmann, Patrick J. Schuler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting the boundary between tumor and healthy tissue during surgical
cancer resection poses a significant challenge. In recent years, Hyperspectral
Imaging (HSI) combined with Machine Learning (ML) has emerged as a promising
solution. However, due to the extensive information contained within the
spectral domain, most ML approaches primarily classify individual HSI
(super-)pixels, or tiles, without taking into account their spatial context. In
this paper, we propose an improved methodology that leverages the spatial
context of tiles for more robust and smoother segmentation. To address the
irregular shapes of tiles, we utilize Graph Neural Networks (GNNs) to propagate
context information across neighboring regions. The features for each tile
within the graph are extracted using a Convolutional Neural Network (CNN),
which is trained simultaneously with the subsequent GNN. Moreover, we
incorporate local image quality metrics into the loss function to enhance the
training procedure's robustness against low-quality regions in the training
images. We demonstrate the superiority of our proposed method using a clinical
ex vivo dataset consisting of 51 HSI images from 30 patients. Despite the
limited dataset, the GNN-based model significantly outperforms context-agnostic
approaches, accurately distinguishing between healthy and tumor tissues, even
in images from previously unseen patients. Furthermore, we show that our
carefully designed loss function, accounting for local image quality, results
in additional improvements. Our findings demonstrate that context-aware GNN
algorithms can robustly find tumor demarcations on HSI images, ultimately
contributing to better surgery success and patient outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal deep learning for mapping forest dominant height by fusing
  GEDI with earth observation data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Man Chen, Wenquan Dong, Hao Yu, Iain Woodhouse, Casey M. Ryan, Haoyu Liu, Selena Georgiou, Edward T. A. Mitchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of multisource remote sensing data and deep learning models
offers new possibilities for accurately mapping high spatial resolution forest
height. We found that GEDI relative heights (RH) metrics exhibited strong
correlation with the mean of the top 10 highest trees (dominant height)
measured in situ at the corresponding footprint locations. Consequently, we
proposed a novel deep learning framework termed the multi-modal attention
remote sensing network (MARSNet) to estimate forest dominant height by
extrapolating dominant height derived from GEDI, using Setinel-1 data, ALOS-2
PALSAR-2 data, Sentinel-2 optical data and ancillary data. MARSNet comprises
separate encoders for each remote sensing data modality to extract multi-scale
features, and a shared decoder to fuse the features and estimate height. Using
individual encoders for each remote sensing imagery avoids interference across
modalities and extracts distinct representations. To focus on the efficacious
information from each dataset, we reduced the prevalent spatial and band
redundancies in each remote sensing data by incorporating the extended spatial
and band reconstruction convolution modules in the encoders. MARSNet achieved
commendable performance in estimating dominant height, with an R2 of 0.62 and
RMSE of 2.82 m, outperforming the widely used random forest approach which
attained an R2 of 0.55 and RMSE of 3.05 m. Finally, we applied the trained
MARSNet model to generate wall-to-wall maps at 10 m resolution for Jilin,
China. Through independent validation using field measurements, MARSNet
demonstrated an R2 of 0.58 and RMSE of 3.76 m, compared to 0.41 and 4.37 m for
the random forest baseline. Our research demonstrates the effectiveness of a
multimodal deep learning approach fusing GEDI with SAR and passive optical
imagery for enhancing the accuracy of high resolution dominant height
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical cross-sensor color constancy using a dual-mapping strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuwei Yue, Minchen Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) have been widely used for illumination
estimation, which is time-consuming and requires sensor-specific data
collection. Our proposed method uses a dual-mapping strategy and only requires
a simple white point from a test sensor under a D65 condition. This allows us
to derive a mapping matrix, enabling the reconstructions of image data and
illuminants. In the second mapping phase, we transform the re-constructed image
data into sparse features, which are then optimized with a lightweight
multi-layer perceptron (MLP) model using the re-constructed illuminants as
ground truths. This approach effectively reduces sensor discrepancies and
delivers performance on par with leading cross-sensor methods. It only requires
a small amount of memory (~0.003 MB), and takes ~1 hour training on an
RTX3070Ti GPU. More importantly, the method can be implemented very fast, with
~0.3 ms and ~1 ms on a GPU or CPU respectively, and is not sensitive to the
input image resolution. Therefore, it offers a practical solution to the great
challenges of data recollection that is faced by the industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Good Feature Extractor Is All You Need for Weakly Supervised Learning
  in Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Wölflein, Dyke Ferber, Asier Rabasco Meneghetti, Omar S. M. El Nahhas, Daniel Truhn, Zunamys I. Carrero, David J. Harrison, Ognjen Arandjelović, Jakob N. Kather
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is revolutionising pathology, offering novel opportunities in
disease prognosis and personalised treatment. Historically, stain normalisation
has been a crucial preprocessing step in computational pathology pipelines, and
persists into the deep learning era. Yet, with the emergence of feature
extractors trained using self-supervised learning (SSL) on diverse pathology
datasets, we call this practice into question. In an empirical evaluation of
publicly available feature extractors, we find that omitting stain
normalisation and image augmentations does not compromise downstream
performance, while incurring substantial savings in memory and compute.
Further, we show that the top-performing feature extractors are remarkably
robust to variations in stain and augmentations like rotation in their latent
space. Contrary to previous patch-level benchmarking studies, our approach
emphasises clinical relevance by focusing on slide-level prediction tasks in a
weakly supervised setting with external validation cohorts. This work
represents the most comprehensive robustness evaluation of public pathology SSL
feature extractors to date, involving more than 6,000 training runs across nine
tasks, five datasets, three downstream architectures, and various preprocessing
setups. Our findings stand to streamline digital pathology workflows by
minimising preprocessing needs and informing the selection of feature
extractors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Contact NIR PPG Sensing through Large Sequence Signal Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Hanley, Dara Golden, Robyn Maxwell, Ashkan Parsi, Joseph Lemley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-Contact sensing is an emerging technology with applications across many
industries from driver monitoring in vehicles to patient monitoring in
healthcare. Current state-of-the-art implementations focus on RGB video, but
this struggles in varying/noisy light conditions and is almost completely
unfeasible in the dark. Near Infra-Red (NIR) video, however, does not suffer
from these constraints. This paper aims to demonstrate the effectiveness of an
alternative Convolution Attention Network (CAN) architecture, to regress
photoplethysmography (PPG) signal from a sequence of NIR frames. A combination
of two publicly available datasets, which is split into train and test sets, is
used for training the CAN. This combined dataset is augmented to reduce
overfitting to the 'normal' 60 - 80 bpm heart rate range by providing the full
range of heart rates along with corresponding videos for each subject. This
CAN, when implemented over video cropped to the subject's head, achieved a Mean
Average Error (MAE) of just 0.99 bpm, proving its effectiveness on NIR video
and the architecture's feasibility to regress an accurate signal output.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, 3 tables, Irish Machine Vision and Image
  Processing Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-Scale Car Parts (LSCP) <span class="highlight-title">Dataset</span> for Lightweight Fine-Grained
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Jie, Zhong Yilin, Cao Qianqian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automotive related datasets have previously been used for training autonomous
driving systems or vehicle classification tasks. However, there is a lack of
datasets in the field of automotive AI for car parts detection, and most
available datasets are limited in size and scope, struggling to cover diverse
scenarios. To address this gap, this paper presents a large-scale and
fine-grained automotive dataset consisting of 84,162 images for detecting 12
different types of car parts. This dataset was collected from natural cameras
and online websites which covers various car brands, scenarios, and shooting
angles. To alleviate the burden of manual annotation, we propose a novel
semi-supervised auto-labeling method that leverages state-of-the-art
pre-trained detectors. Moreover, we study the limitations of the Grounding DINO
approach for zero-shot labeling. Finally, we evaluate the effectiveness of our
proposed dataset through fine-grained car parts detection by training several
lightweight YOLO-series detectors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdvGen: Physical Adversarial Attack on Face Presentation Attack
  Detection Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Amrit Patnaik, Shivali Chansoriya, Anil K. Jain, Anoop M. Namboodiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the risk level of adversarial images is essential for safely
deploying face authentication models in the real world. Popular approaches for
physical-world attacks, such as print or replay attacks, suffer from some
limitations, like including physical and geometrical artifacts. Recently,
adversarial attacks have gained attraction, which try to digitally deceive the
learning strategy of a recognition system using slight modifications to the
captured image. While most previous research assumes that the adversarial image
could be digitally fed into the authentication systems, this is not always the
case for systems deployed in the real world. This paper demonstrates the
vulnerability of face authentication systems to adversarial images in physical
world scenarios. We propose AdvGen, an automated Generative Adversarial
Network, to simulate print and replay attacks and generate adversarial images
that can fool state-of-the-art PADs in a physical domain attack setting. Using
this attack strategy, the attack success rate goes up to 82.01%. We test AdvGen
extensively on four datasets and ten state-of-the-art PADs. We also demonstrate
the effectiveness of our attack by conducting experiments in a realistic,
physical environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, Accepted to the International Joint Conference
  on Biometrics (IJCB 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuzzy Information Seeded Region Growing for Automated Lesions After
  Stroke Segmentation in MR Brain Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Pascual González
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of medical imaging, precise segmentation of stroke lesions from
brain MRI images stands as a critical challenge with significant implications
for patient diagnosis and treatment. Addressing this, our study introduces an
innovative approach using a Fuzzy Information Seeded Region Growing (FISRG)
algorithm. Designed to effectively delineate the complex and irregular
boundaries of stroke lesions, the FISRG algorithm combines fuzzy logic with
Seeded Region Growing (SRG) techniques, aiming to enhance segmentation
accuracy.
  The research involved three experiments to optimize the FISRG algorithm's
performance, each focusing on different parameters to improve the accuracy of
stroke lesion segmentation. The highest Dice score achieved in these
experiments was 94.2\%, indicating a high degree of similarity between the
algorithm's output and the expert-validated ground truth. Notably, the best
average Dice score, amounting to 88.1\%, was recorded in the third experiment,
highlighting the efficacy of the algorithm in consistently segmenting stroke
lesions across various slices.
  Our findings reveal the FISRG algorithm's strengths in handling the
heterogeneity of stroke lesions. However, challenges remain in areas of abrupt
lesion topology changes and in distinguishing lesions from similar intensity
brain regions. The results underscore the potential of the FISRG algorithm in
contributing significantly to advancements in medical imaging analysis for
stroke diagnosis and treatment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 14 figures. Associated code and data available at:
  https://github.com/Mawio02/FISRG-for-Automated-Lesion-After-Stroke-Segmentation-in-MRI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse4D v3: Advancing End-to-End 3D Detection and Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuewu Lin, Zixiang Pei, Tianwei Lin, Lichao Huang, Zhizhong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving perception systems, 3D detection and tracking are the
two fundamental tasks. This paper delves deeper into this field, building upon
the Sparse4D framework. We introduce two auxiliary training tasks (Temporal
Instance Denoising and Quality Estimation) and propose decoupled attention to
make structural improvements, leading to significant enhancements in detection
performance. Additionally, we extend the detector into a tracker using a
straightforward approach that assigns instance ID during inference, further
highlighting the advantages of query-based algorithms. Extensive experiments
conducted on the nuScenes benchmark validate the effectiveness of the proposed
improvements. With ResNet50 as the backbone, we witnessed enhancements of
3.0\%, 2.2\%, and 7.6\% in mAP, NDS, and AMOTA, achieving 46.9\%, 56.1\%, and
49.0\%, respectively. Our best model achieved 71.9\% NDS and 67.7\% AMOTA on
the nuScenes test set. Code will be released at
\url{https://github.com/linxuewu/Sparse4D}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Importance of Large Objects in CNN Based Object Detection
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Ben Saad, Gabriele Facciolo, Axel Davy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection models, a prominent class of machine learning algorithms,
aim to identify and precisely locate objects in images or videos. However, this
task might yield uneven performances sometimes caused by the objects sizes and
the quality of the images and labels used for training. In this paper, we
highlight the importance of large objects in learning features that are
critical for all sizes. Given these findings, we propose to introduce a
weighting term into the training loss. This term is a function of the object
area size. We show that giving more weight to large objects leads to improved
detection scores across all object sizes and so an overall improvement in
Object Detectors performances (+2 p.p. of mAP on small objects, +2 p.p. on
medium and +4 p.p. on large on COCO val 2017 with InternImage-T). Additional
experiments and ablation studies with different models and on a different
dataset further confirm the robustness of our findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Yan, Delin Qu, Dong Wang, Dan Xu, Zhigang Wang, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D
Gaussian representation in the Simultaneous Localization and Mapping (SLAM)
system. It facilitates a better balance between efficiency and accuracy.
Compared to recent SLAM methods employing neural implicit representations, our
method utilizes a real-time differentiable splatting rendering pipeline that
offers significant speedup to map optimization and RGB-D re-rendering.
Specifically, we propose an adaptive expansion strategy that adds new or
deletes noisy 3D Gaussian in order to efficiently reconstruct new observed
scene geometry and improve the mapping of previously observed areas. This
strategy is essential to extend 3D Gaussian representation to reconstruct the
whole scene rather than synthesize a static object in existing methods.
Moreover, in the pose tracking process, an effective coarse-to-fine technique
is designed to select reliable 3D Gaussian representations to optimize camera
pose, resulting in runtime reduction and robust estimation. Our method achieves
competitive performance compared with existing state-of-the-art real-time
methods on the Replica, TUM-RGBD datasets. The source code will be released
upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cut-and-Paste: Subject-Driven Video Editing with Attention Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Zuo, Zhao Zhang, Yan Luo, Yang Zhao, Haijun Zhang, Yi Yang, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel framework termed Cut-and-Paste for real-word
semantic video editing under the guidance of text prompt and additional
reference image. While the text-driven video editing has demonstrated
remarkable ability to generate highly diverse videos following given text
prompts, the fine-grained semantic edits are hard to control by plain textual
prompt only in terms of object details and edited region, and cumbersome long
text descriptions are usually needed for the task. We therefore investigate
subject-driven video editing for more precise control of both edited regions
and background preservation, and fine-grained semantic generation. We achieve
this goal by introducing an reference image as supplementary input to the
text-driven video editing, which avoids racking your brain to come up with a
cumbersome text prompt describing the detailed appearance of the object. To
limit the editing area, we refer to a method of cross attention control in
image editing and successfully extend it to video editing by fusing the
attention map of adjacent frames, which strikes a balance between maintaining
video background and spatio-temporal consistency. Compared with current
methods, the whole process of our method is like ``cut" the source object to be
edited and then ``paste" the target object provided by reference image. We
demonstrate that our method performs favorably over prior arts for video
editing under the guidance of text prompt and extra reference image, as
measured by both quantitative and subjective evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clarity Chat<span class="highlight-title">GPT</span>: An Interactive and Adaptive Processing System for Image
  Restoration and Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyan Wei, Zhao Zhang, Jiahuan Ren, Xiaogang Xu, Richang Hong, Yi Yang, Shuicheng Yan, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generalization capability of existing image restoration and enhancement
(IRE) methods is constrained by the limited pre-trained datasets, making it
difficult to handle agnostic inputs such as different degradation levels and
scenarios beyond their design scopes. Moreover, they are not equipped with
interactive mechanisms to consider user preferences or feedback, and their
end-to-end settings cannot provide users with more choices. Faced with the
above-mentioned IRE method's limited performance and insufficient
interactivity, we try to solve it from the engineering and system framework
levels. Specifically, we propose Clarity ChatGPT-a transformative system that
combines the conversational intelligence of ChatGPT with multiple IRE methods.
Clarity ChatGPT can automatically detect image degradation types and select
appropriate IRE methods to restore images, or iteratively generate satisfactory
results based on user feedback. Its innovative features include a CLIP-powered
detector for accurate degradation classification, no-reference image quality
evaluation for performance evaluation, region-specific processing for precise
enhancements, and advanced fusion techniques for optimal restoration results.
Clarity ChatGPT marks a significant advancement in integrating language and
vision, enhancing image-text interactions, and providing a robust,
high-performance IRE solution. Our case studies demonstrate that Clarity
ChatGPT effectively improves the generalization and interaction capabilities in
the IRE, and also fills the gap in the low-level domain of the existing
vision-language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segment Together: A Versatile Paradigm for Semi-Supervised Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingjie Zeng, Yutong Xie, Zilin Lu, Mengkang Lu, Yicheng Wu, Yong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotation scarcity has become a major obstacle for training powerful
deep-learning models for medical image segmentation, restricting their
deployment in clinical scenarios. To address it, semi-supervised learning by
exploiting abundant unlabeled data is highly desirable to boost the model
training. However, most existing works still focus on limited medical tasks and
underestimate the potential of learning across diverse tasks and multiple
datasets. Therefore, in this paper, we introduce a \textbf{Ver}satile
\textbf{Semi}-supervised framework (VerSemi) to point out a new perspective
that integrates various tasks into a unified model with a broad label space, to
exploit more unlabeled data for semi-supervised medical image segmentation.
Specifically, we introduce a dynamic task-prompted design to segment various
targets from different datasets. Next, this unified model is used to identify
the foreground regions from all labeled data, to capture cross-dataset
semantics. Particularly, we create a synthetic task with a cutmix strategy to
augment foreground targets within the expanded label space. To effectively
utilize unlabeled data, we introduce a consistency constraint. This involves
aligning aggregated predictions from various tasks with those from the
synthetic task, further guiding the model in accurately segmenting foreground
regions during training. We evaluated our VerSemi model on four public
benchmarking datasets. Extensive experiments demonstrated that VerSemi can
consistently outperform the second-best method by a large margin (e.g., an
average 2.69\% Dice gain on four datasets), setting new SOTA performance for
semi-supervised medical image segmentation. The code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViP-Mixer: A Convolutional Mixer for Video Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zheng, Ziang Peng, Yuan Cao, Hongming Shan, Junping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video prediction aims to predict future frames from a video's previous
content. Existing methods mainly process video data where the time dimension
mingles with the space and channel dimensions from three distinct angles: as a
sequence of individual frames, as a 3D volume in spatiotemporal coordinates, or
as a stacked image where frames are treated as separate channels. Most of them
generally focus on one of these perspectives and may fail to fully exploit the
relationships across different dimensions. To address this issue, this paper
introduces a convolutional mixer for video prediction, termed ViP-Mixer, to
model the spatiotemporal evolution in the latent space of an autoencoder. The
ViP-Mixers are stacked sequentially and interleave feature mixing at three
levels: frames, channels, and locations. Extensive experiments demonstrate that
our proposed method achieves new state-of-the-art prediction performance on
three benchmark video datasets covering both synthetic and real-world
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMP-Swin: Multi-Scale Patch Message Passing Swin <span class="highlight-title">Transformer</span> for Retinal
  Disease Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihan Yang, Zhiming Cheng, Tengjin Weng, Shucheng He, Yaqi Wang, Xin Ye, Shuai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retinal disease is one of the primary causes of visual impairment, and early
diagnosis is essential for preventing further deterioration. Nowadays, many
works have explored Transformers for diagnosing diseases due to their strong
visual representation capabilities. However, retinal diseases exhibit milder
forms and often present with overlapping signs, which pose great difficulties
for accurate multi-class classification. Therefore, we propose a new framework
named Multi-Scale Patch Message Passing Swin Transformer for multi-class
retinal disease classification. Specifically, we design a Patch Message Passing
(PMP) module based on the Message Passing mechanism to establish global
interaction for pathological semantic features and to exploit the subtle
differences further between different diseases. Moreover, considering the
various scale of pathological features we integrate multiple PMP modules for
different patch sizes. For evaluation, we have constructed a new dataset, named
OPTOS dataset, consisting of 1,033 high-resolution fundus images photographed
by Optos camera and conducted comprehensive experiments to validate the
efficacy of our proposed method. And the results on both the public dataset and
our dataset demonstrate that our method achieves remarkable performance
compared to state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniSeg3D: Omniversal 3D Segmentation via Hierarchical Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Ying, Yixuan Yin, Jinzhi Zhang, Fan Wang, Tao Yu, Ruqi Huang, Lu Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Towards holistic understanding of 3D scenes, a general 3D segmentation method
is needed that can segment diverse objects without restrictions on object
quantity or categories, while also reflecting the inherent hierarchical
structure. To achieve this, we propose OmniSeg3D, an omniversal segmentation
method aims for segmenting anything in 3D all at once. The key insight is to
lift multi-view inconsistent 2D segmentations into a consistent 3D feature
field through a hierarchical contrastive learning framework, which is
accomplished by two steps. Firstly, we design a novel hierarchical
representation based on category-agnostic 2D segmentations to model the
multi-level relationship among pixels. Secondly, image features rendered from
the 3D feature field are clustered at different levels, which can be further
drawn closer or pushed apart according to the hierarchical relationship between
different levels. In tackling the challenges posed by inconsistent 2D
segmentations, this framework yields a global consistent 3D feature field,
which further enables hierarchical segmentation, multi-object selection, and
global discretization. Extensive experiments demonstrate the effectiveness of
our method on high-quality 3D segmentation and accurate hierarchical structure
understanding. A graphical user interface further facilitates flexible
interaction for omniversal 3D segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Spatio-Temporal Context for Temporally Consistent Robust 3D
  Human Motion Recovery from Monocular Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sushovan Chanda, Amogh Tiwari, Lokender Tiwari, Brojeshwar Bhowmick, Avinash Sharma, Hrishav Barua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering temporally consistent 3D human body pose, shape and motion from a
monocular video is a challenging task due to (self-)occlusions, poor lighting
conditions, complex articulated body poses, depth ambiguity, and limited
availability of annotated data. Further, doing a simple perframe estimation is
insufficient as it leads to jittery and implausible results. In this paper, we
propose a novel method for temporally consistent motion estimation from a
monocular video. Instead of using generic ResNet-like features, our method uses
a body-aware feature representation and an independent per-frame pose and
camera initialization over a temporal window followed by a novel
spatio-temporal feature aggregation by using a combination of self-similarity
and self-attention over the body-aware features and the perframe
initialization. Together, they yield enhanced spatiotemporal context for every
frame by considering remaining past and future frames. These features are used
to predict the pose and shape parameters of the human body model, which are
further refined using an LSTM. Experimental results on the publicly available
benchmark data show that our method attains significantly lower acceleration
error and outperforms the existing state-of-the-art methods over all key
quantitative evaluation metrics, including complex scenarios like partial
occlusion, complex poses and even relatively low illumination.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MGCT: Mutual-Guided Cross-Modality <span class="highlight-title">Transformer</span> for Survival Outcome
  Prediction using Integrative Histopathology-Genomic Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxin Liu, Yunzan Liu, Hui Cui, Chunquan Li, Jiquan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapidly emerging field of deep learning-based computational pathology has
shown promising results in utilizing whole slide images (WSIs) to objectively
prognosticate cancer patients. However, most prognostic methods are currently
limited to either histopathology or genomics alone, which inevitably reduces
their potential to accurately predict patient prognosis. Whereas integrating
WSIs and genomic features presents three main challenges: (1) the enormous
heterogeneity of gigapixel WSIs which can reach sizes as large as
150,000x150,000 pixels; (2) the absence of a spatially corresponding
relationship between histopathology images and genomic molecular data; and (3)
the existing early, late, and intermediate multimodal feature fusion strategies
struggle to capture the explicit interactions between WSIs and genomics. To
ameliorate these issues, we propose the Mutual-Guided Cross-Modality
Transformer (MGCT), a weakly-supervised, attention-based multimodal learning
framework that can combine histology features and genomic features to model the
genotype-phenotype interactions within the tumor microenvironment. To validate
the effectiveness of MGCT, we conduct experiments using nearly 3,600 gigapixel
WSIs across five different cancer types sourced from The Cancer Genome Atlas
(TCGA). Extensive experimental results consistently emphasize that MGCT
outperforms the state-of-the-art (SOTA) methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, accepted by 2023 IEEE International Conference on
  Bioinformatics and Biomedicine (BIBM 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Double-Condensing Attention Condenser: Leveraging Attention in Deep
  Learning to Detect Skin Cancer from Skin Lesion Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-en Amy Tai, Elizabeth Janes, Chris Czarnecki, Alexander Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is the most common type of cancer in the United States and is
estimated to affect one in five Americans. Recent advances have demonstrated
strong performance on skin cancer detection, as exemplified by state of the art
performance in the SIIM-ISIC Melanoma Classification Challenge; however these
solutions leverage ensembles of complex deep neural architectures requiring
immense storage and compute costs, and therefore may not be tractable. A recent
movement for TinyML applications is integrating Double-Condensing Attention
Condensers (DC-AC) into a self-attention neural network backbone architecture
to allow for faster and more efficient computation. This paper explores
leveraging an efficient self-attention structure to detect skin cancer in skin
lesion images and introduces a deep neural network design with DC-AC customized
for skin cancer detection from skin lesion images. The final model is publicly
available as a part of a global open-source initiative dedicated to
accelerating advancement in machine learning to aid clinicians in the fight
against cancer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cancer-Net PCa-Data: An Open-Source Benchmark <span class="highlight-title">Dataset</span> for Prostate
  Cancer Clinical Decision Support using Synthetic Correlated Diffusion Imaging
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hayden Gunraj, Chi-en Amy Tai, Alexander Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent introduction of synthetic correlated diffusion (CDI$^s$) imaging
has demonstrated significant potential in the realm of clinical decision
support for prostate cancer (PCa). CDI$^s$ is a new form of magnetic resonance
imaging (MRI) designed to characterize tissue characteristics through the joint
correlation of diffusion signal attenuation across different Brownian motion
sensitivities. Despite the performance improvement, the CDI$^s$ data for PCa
has not been previously made publicly available. In our commitment to advance
research efforts for PCa, we introduce Cancer-Net PCa-Data, an open-source
benchmark dataset of volumetric CDI$^s$ imaging data of PCa patients.
Cancer-Net PCa-Data consists of CDI$^s$ volumetric images from a patient cohort
of 200 patient cases, along with full annotations (gland masks, tumor masks,
and PCa diagnosis for each tumor). We also analyze the demographic and label
region diversity of Cancer-Net PCa-Data for potential biases. Cancer-Net
PCa-Data is the first-ever public dataset of CDI$^s$ imaging data for PCa, and
is a part of the global open-source initiative dedicated to advancement in
machine learning and imaging research to aid clinicians in the global fight
against cancer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CastDet: Toward Open Vocabulary Aerial Object Detection with
  CLIP-Activated Student-Teacher Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Weiwei Guo, Dunyun He, Jiaqi Zhou, Yuze Gao, Wenxian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection in aerial images is a pivotal task for various earth
observation applications, whereas current algorithms learn to detect only a
pre-defined set of object categories demanding sufficient bounding-box
annotated training samples and fail to detect novel object categories. In this
paper, we consider open-vocabulary object detection (OVD) in aerial images that
enables the characterization of new objects beyond training categories on the
earth surface without annotating training images for these new categories. The
performance of OVD depends on the quality of class-agnostic region proposals
and pseudo-labels that can generalize well to novel object categories. To
simultaneously generate high-quality proposals and pseudo-labels, we propose
CastDet, a CLIP-activated student-teacher open-vocabulary object Detection
framework. Our end-to-end framework within the student-teacher mechanism
employs the CLIP model as an extra omniscient teacher of rich knowledge into
the student-teacher self-learning process. By doing so, our approach boosts
novel object proposals and classification. Furthermore, we design a dynamic
label queue technique to maintain high-quality pseudo labels during batch
training and mitigate label imbalance. We conduct extensive experiments on
multiple existing aerial object detection datasets, which are set up for the
OVD task. Experimental results demonstrate our CastDet achieving superior
open-vocabulary detection performance, e.g., reaching 40.0 HM (Harmonic Mean),
which outperforms previous methods Detic/ViLD by 26.9/21.1 on the VisDroneZSD
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Muqeet, Kyuchul Lee, Bumsoo Kim, Yohan Hong, Hyungrae Lee, Woonggon Kim, Kwang Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video face re-aging deals with altering the apparent age of a person to the
target age in videos. This problem is challenging due to the lack of paired
video datasets maintaining temporal consistency in identity and age. Most
re-aging methods process each image individually without considering the
temporal consistency of videos. While some existing works address the issue of
temporal coherence through video facial attribute manipulation in latent space,
they often fail to deliver satisfactory performance in age transformation. To
tackle the issues, we propose (1) a novel synthetic video dataset that features
subjects across a diverse range of age groups; (2) a baseline architecture
designed to validate the effectiveness of our proposed dataset, and (3) the
development of three novel metrics tailored explicitly for evaluating the
temporal consistency of video re-aging techniques. Our comprehensive
experiments on public datasets, such as VFHQ and CelebV-HQ, show that our
method outperforms the existing approaches in terms of both age transformation
and temporal consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reti-Diff: Illumination Degradation Image Restoration with Retinex-based
  Latent Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunming He, Chengyu Fang, Yulun Zhang, Kai Li, Longxiang Tang, Chenyu You, Fengyang Xiao, Zhenhua Guo, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Illumination degradation image restoration (IDIR) techniques aim to improve
the visibility of degraded images and mitigate the adverse effects of
deteriorated illumination. Among these algorithms, diffusion model (DM)-based
methods have shown promising performance but are often burdened by heavy
computational demands and pixel misalignment issues when predicting the
image-level distribution. To tackle these problems, we propose to leverage DM
within a compact latent space to generate concise guidance priors and introduce
a novel solution called Reti-Diff for the IDIR task. Reti-Diff comprises two
key components: the Retinex-based latent DM (RLDM) and the Retinex-guided
transformer (RGformer). To ensure detailed reconstruction and illumination
correction, RLDM is empowered to acquire Retinex knowledge and extract
reflectance and illumination priors. These priors are subsequently utilized by
RGformer to guide the decomposition of image features into their respective
reflectance and illumination components. Following this, RGformer further
enhances and consolidates the decomposed features, resulting in the production
of refined images with consistent content and robustness to handle complex
degradation scenarios. Extensive experiments show that Reti-Diff outperforms
existing methods on three IDIR tasks, as well as downstream applications. Code
will be available at \url{https://github.com/ChunmingHe/Reti-Diff}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Realistic Counterfactuals for Retinal Fundus and OCT Images
  using Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Indu Ilanchezian, Valentyn Boreiko, Laura Kühlewein, Ziwei Huang, Murat Seçkin Ayhan, Matthias Hein, Lisa Koch, Philipp Berens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual reasoning is often used in a clinical setting to explain
decisions or weigh alternatives. Therefore, for imaging based modalities such
as ophthalmology, it would be beneficial to be able to create counterfactual
images, illustrating the answer to the question: "If the subject had had
diabetic retinopathy, how would the fundus image have looked?" Here, we
demonstrate that using a diffusion model in combination with an adversarially
robust classifier trained on retinal disease classification tasks enables
generation of highly realistic counterfactuals of retinal fundus images and
optical coherence tomorgraphy (OCT) B-scans. Ideally, these classifiers encode
the salient features indicative for each disease class and can steer the
diffusion model to show realistic disease signs or remove disease-related
lesions in a realistic way. Importantly, in a user study, domain experts found
the counterfactuals generated using our method significantly more realistic
than counterfactuals generated from a previous method, and even
indistiguishable from realistic images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-Preserved Point-based Human Avatar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixiang Lin, Jianke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enable realistic experience in AR/VR and digital entertainment, we present
the first point-based human avatar model that embodies the entirety expressive
range of digital humans. We employ two MLPs to model pose-dependent deformation
and linear skinning (LBS) weights. The representation of appearance relies on a
decoder and the features that attached to each point. In contrast to
alternative implicit approaches, the oriented points representation not only
provides a more intuitive way to model human avatar animation but also
significantly reduces both training and inference time. Moreover, we propose a
novel method to transfer semantic information from the SMPL-X model to the
points, which enables to better understand human body movements. By leveraging
the semantic information of points, we can facilitate virtual try-on and human
avatar composition through exchanging the points of same category across
different subjects. Experimental results demonstrate the efficacy of our
presented method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CurriculumLoc: Enhancing Cross-Domain Geolocalization through
  Multi-Stage Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boni Hu, Lin Chen, Runjian Chen, Shuhui Bu, Pengcheng Han, Haowei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual geolocalization is a cost-effective and scalable task that involves
matching one or more query images, taken at some unknown location, to a set of
geo-tagged reference images. Existing methods, devoted to semantic features
representation, evolving towards robustness to a wide variety between query and
reference, including illumination and viewpoint changes, as well as scale and
seasonal variations. However, practical visual geolocalization approaches need
to be robust in appearance changing and extreme viewpoint variation conditions,
while providing accurate global location estimates. Therefore, inspired by
curriculum design, human learn general knowledge first and then delve into
professional expertise. We first recognize semantic scene and then measure
geometric structure. Our approach, termed CurriculumLoc, involves a delicate
design of multi-stage refinement pipeline and a novel keypoint detection and
description with global semantic awareness and local geometric verification. We
rerank candidates and solve a particular cross-domain perspective-n-point (PnP)
problem based on these keypoints and corresponding descriptors, position
refinement occurs incrementally. The extensive experimental results on our
collected dataset, TerraTrack and a benchmark dataset, ALTO, demonstrate that
our approach results in the aforementioned desirable characteristics of a
practical visual geolocalization solution. Additionally, we achieve new high
recall@1 scores of 62.6% and 94.5% on ALTO, with two different distances
metrics, respectively. Dataset, code and trained models are publicly available
on https://github.com/npupilab/CurriculumLoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-In-Single-Out Network for Video Frame Interpolation without
  Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemin Lee, Minseok Seo, Sangwoo Lee, Hyobin Park, Dong-Geol Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In general, deep learning-based video frame interpolation (VFI) methods have
predominantly focused on estimating motion vectors between two input frames and
warping them to the target time. While this approach has shown impressive
performance for linear motion between two input frames, it exhibits limitations
when dealing with occlusions and nonlinear movements. Recently, generative
models have been applied to VFI to address these issues. However, as VFI is not
a task focused on generating plausible images, but rather on predicting
accurate intermediate frames between two given frames, performance limitations
still persist. In this paper, we propose a multi-in-single-out (MISO) based VFI
method that does not rely on motion vector estimation, allowing it to
effectively model occlusions and nonlinear motion. Additionally, we introduce a
novel motion perceptual loss that enables MISO-VFI to better capture the
spatio-temporal correlations within the video frames. Our MISO-VFI method
achieves state-of-the-art results on VFI benchmarks Vimeo90K, Middlebury, and
UCF101, with a significant performance gap compared to existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Equilibrium Diffusion Restoration with Parallel Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiezhang Cao, Yue Shi, Kai Zhang, Yulun Zhang, Radu Timofte, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based image restoration (IR) methods aim to use diffusion models to
recover high-quality (HQ) images from degraded images and achieve promising
performance. Due to the inherent property of diffusion models, most of these
methods need long serial sampling chains to restore HQ images step-by-step. As
a result, it leads to expensive sampling time and high computation costs.
Moreover, such long sampling chains hinder understanding the relationship
between the restoration results and the inputs since it is hard to compute the
gradients in the whole chains. In this work, we aim to rethink the
diffusion-based IR models through a different perspective, i.e., a deep
equilibrium (DEQ) fixed point system. Specifically, we derive an analytical
solution by modeling the entire sampling chain in diffusion-based IR models as
a joint multivariate fixed point system. With the help of the analytical
solution, we are able to conduct single-image sampling in a parallel way and
restore HQ images without training. Furthermore, we compute fast gradients in
DEQ and found that initialization optimization can boost performance and
control the generation direction. Extensive experiments on benchmarks
demonstrate the effectiveness of our proposed method on typical IR tasks and
real-world settings. The code and models will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting urban tree cover from incomplete point labels and limited
  background information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Zhang, Ankit Kariryaa, Venkanna Babu Guthula, Christian Igel, Stefan Oehmcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trees inside cities are important for the urban microclimate, contributing
positively to the physical and mental health of the urban dwellers. Despite
their importance, often only limited information about city trees is available.
Therefore in this paper, we propose a method for mapping urban trees in
high-resolution aerial imagery using limited datasets and deep learning. Deep
learning has become best-practice for this task, however, existing approaches
rely on large and accurately labelled training datasets, which can be difficult
and expensive to obtain. However, often noisy and incomplete data may be
available that can be combined and utilized to solve more difficult tasks than
those datasets were intended for. This paper studies how to combine accurate
point labels of urban trees along streets with crowd-sourced annotations from
an open geographic database to delineate city trees in remote sensing images, a
task which is challenging even for humans. To that end, we perform semantic
segmentation of very high resolution aerial imagery using a fully convolutional
neural network. The main challenge is that our segmentation maps are sparsely
annotated and incomplete. Small areas around the point labels of the street
trees coming from official and crowd-sourced data are marked as foreground
class. Crowd-sourced annotations of streets, buildings, etc. define the
background class. Since the tree data is incomplete, we introduce a masking to
avoid class confusion. Our experiments in Hamburg, Germany, showed that the
system is able to produce tree cover maps, not limited to trees along streets,
without providing tree delineations. We evaluated the method on manually
labelled trees and show that performance drastically deteriorates if the open
geographic database is not used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Urban Renewal: An Automated Approach to Generating Historical
  Arcade Facades with Stable Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyuan Kuang, Jiaxin Zhang, Yiying Huang, Yunqin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban renewal and transformation processes necessitate the preservation of
the historical urban fabric, particularly in districts known for their
architectural and historical significance. These regions, with their diverse
architectural styles, have traditionally required extensive preliminary
research, often leading to subjective results. However, the advent of machine
learning models has opened up new avenues for generating building facade
images. Despite this, creating high-quality images for historical district
renovations remains challenging, due to the complexity and diversity inherent
in such districts. In response to these challenges, our study introduces a new
methodology for automatically generating images of historical arcade facades,
utilizing Stable Diffusion models conditioned on textual descriptions. By
classifying and tagging a variety of arcade styles, we have constructed several
realistic arcade facade image datasets. We trained multiple low-rank adaptation
(LoRA) models to control the stylistic aspects of the generated images,
supplemented by ControlNet models for improved precision and authenticity. Our
approach has demonstrated high levels of precision, authenticity, and diversity
in the generated images, showing promising potential for real-world urban
renewal projects. This new methodology offers a more efficient and accurate
alternative to conventional design processes in urban renewal, bypassing issues
of unconvincing image details, lack of precision, and limited stylistic
variety. Future research could focus on integrating this two-dimensional image
generation with three-dimensional modeling techniques, providing a more
comprehensive solution for renovating architectural facades in historical
districts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HABITS OF THE ANTHROPOCENE - Proceedings of the 43rd ACADIA
  Conference - Volume II: Proceedings book one, University of Colorado Denver,
  Denver, Colorado, USA, 26-28 October 2023, pp. 616-625, CUMINCAD, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AKConv: Convolutional Kernel with Arbitrary Sampled Shapes and Arbitrary
  Number of Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Yingze Song, Tingting Song, Degang Yang, Yichen Ye, Jie Zhou, Liming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks based on convolutional operations have achieved remarkable
results in the field of deep learning, but there are two inherent flaws in
standard convolutional operations. On the one hand, the convolution operation
be confined to a local window and cannot capture information from other
locations, and its sampled shapes is fixed. On the other hand, the size of the
convolutional kernel is fixed to k $\times$ k, which is a fixed square shape,
and the number of parameters tends to grow squarely with size. It is obvious
that the shape and size of targets are various in different datasets and at
different locations. Convolutional kernels with fixed sample shapes and squares
do not adapt well to changing targets. In response to the above questions, the
Alterable Kernel Convolution (AKConv) is explored in this work, which gives the
convolution kernel an arbitrary number of parameters and arbitrary sampled
shapes to provide richer options for the trade-off between network overhead and
performance. In AKConv, we define initial positions for convolutional kernels
of arbitrary size by means of a new coordinate generation algorithm. To adapt
to changes for targets, we introduce offsets to adjust the shape of the samples
at each position. Moreover, we explore the effect of the neural network by
using the AKConv with the same size and different initial sampled shapes.
AKConv completes the process of efficient feature extraction by irregular
convolutional operations and brings more exploration options for convolutional
sampling shapes. Object detection experiments on representative datasets
COCO2017, VOC 7+12 and VisDrone-DET2021 fully demonstrate the advantages of
AKConv. AKConv can be used as a plug-and-play convolutional operation to
replace convolutional operations to improve network performance. The code for
the relevant tasks can be found at https://github.com/CV-ZhangXin/AKConv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeaDSC: A video-based unsupervised method for dynamic scene change
  detection in unmanned surface vehicles <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linh Trinh, Ali Anwar, Siegfried Mercelis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been an upsurge in the research on maritime vision, where
a lot of works are influenced by the application of computer vision for
Unmanned Surface Vehicles (USVs). Various sensor modalities such as camera,
radar, and lidar have been used to perform tasks such as object detection,
segmentation, object tracking, and motion planning. A large subset of this
research is focused on the video analysis, since most of the current vessel
fleets contain the camera's onboard for various surveillance tasks. Due to the
vast abundance of the video data, video scene change detection is an initial
and crucial stage for scene understanding of USVs. This paper outlines our
approach to detect dynamic scene changes in USVs. To the best of our
understanding, this work represents the first investigation of scene change
detection in the maritime vision application. Our objective is to identify
significant changes in the dynamic scenes of maritime video data, particularly
those scenes that exhibit a high degree of resemblance. In our system for
dynamic scene change detection, we propose completely unsupervised learning
method. In contrast to earlier studies, we utilize a modified cutting-edge
generative picture model called VQ-VAE-2 to train on multiple marine datasets,
aiming to enhance the feature extraction. Next, we introduce our innovative
similarity scoring technique for directly calculating the level of similarity
in a sequence of consecutive frames by utilizing grid calculation on retrieved
features. The experiments were conducted using a nautical video dataset called
RoboWhaler to showcase the efficient performance of our technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A 3D Multi-Style Cross-Modality Segmentation Framework for Segmenting
  Vestibular Schwannoma and Cochlea 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhou Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The crossMoDA2023 challenge aims to segment the vestibular schwannoma
(sub-divided into intra- and extra-meatal components) and cochlea regions of
unlabeled hrT2 scans by leveraging labeled ceT1 scans. In this work, we
proposed a 3D multi-style cross-modality segmentation framework for the
crossMoDA2023 challenge, including the multi-style translation and
self-training segmentation phases. Considering heterogeneous distributions and
various image sizes in multi-institutional scans, we first utilize the min-max
normalization, voxel size resampling, and center cropping to obtain fixed-size
sub-volumes from ceT1 and hrT2 scans for training. Then, we perform the
multi-style image translation phase to overcome the intensity distribution
discrepancy between unpaired multi-modal scans. Specifically, we design three
different translation networks with 2D or 2.5D inputs to generate multi-style
and realistic target-like volumes from labeled ceT1 volumes. Finally, we
perform the self-training volumetric segmentation phase in the target domain,
which employs the nnU-Net framework and iterative self-training method using
pseudo-labels for training accurate segmentation models in the unlabeled target
domain. On the crossMoDA2023 validation dataset, our method produces promising
results and achieves the mean DSC values of 72.78% and 80.64% and ASSD values
of 5.85 mm and 0.25 mm for VS tumor and cochlea regions, respectively.
Moreover, for intra- and extra-meatal regions, our method achieves the DSC
values of 59.77% and 77.14%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report of cmda2023 challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupled DETR For Few-shot Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Shangguan, Lian Huai, Tong Liu, Xingqun Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot object detection (FSOD), an efficient method for addressing the
severe data-hungry problem, has been extensively discussed. Current works have
significantly advanced the problem in terms of model and data. However, the
overall performance of most FSOD methods still does not fulfill the desired
accuracy. In this paper we improve the FSOD model to address the severe issue
of sample imbalance and weak feature propagation. To alleviate modeling bias
from data-sufficient base classes, we examine the effect of decoupling the
parameters for classes with sufficient data and classes with few samples in
various ways. We design a base-novel categories decoupled DETR (DeDETR) for
FSOD. We also explore various types of skip connection between the encoder and
decoder for DETR. Besides, we notice that the best outputs could come from the
intermediate layer of the decoder instead of the last layer; therefore, we
build a unified decoder module that could dynamically fuse the decoder layers
as the output feature. We evaluate our model on commonly used datasets such as
PASCAL VOC and MSCOCO. Our results indicate that our proposed module could
achieve stable improvements of 5% to 10% in both fine-tuning and meta-learning
paradigms and has outperformed the highest score in recent works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORE-MM: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotian Han, Quanzeng You, Yongfei Liu, Wentao Chen, Huangjie Zheng, Khalil Mrini, Xudong Lin, Yiqi Wang, Bohan Zhai, Jianbo Yuan, Heng Wang, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) are increasingly prominent in the
field of artificial intelligence. These models not only excel in traditional
vision-language tasks but also demonstrate impressive performance in
contemporary multi-modal benchmarks. Although many of these benchmarks attempt
to holistically evaluate MLLMs, they typically concentrate on basic reasoning
tasks, often yielding only simple yes/no or multi-choice responses. These
methods naturally lead to confusion and difficulties in conclusively
determining the reasoning capabilities of MLLMs. To mitigate this issue, we
manually curate a benchmark dataset specifically designed for MLLMs, with a
focus on complex reasoning tasks. Our benchmark comprises three key reasoning
categories: deductive, abductive, and analogical reasoning. The queries in our
dataset are intentionally constructed to engage the reasoning capabilities of
MLLMs in the process of generating answers. For a fair comparison across
various MLLMs, we incorporate intermediate reasoning steps into our evaluation
criteria. In instances where an MLLM is unable to produce a definitive answer,
its reasoning ability is evaluated by requesting intermediate reasoning steps.
If these steps align with our manual annotations, appropriate scores are
assigned. This evaluation scheme resembles methods commonly used in human
assessments, such as exams or assignments, and represents what we consider a
more effective assessment technique compared with existing benchmarks. We
evaluate a selection of representative MLLMs using this rigorously developed
open-ended multi-step elaborate reasoning benchmark, designed to challenge and
accurately measure their reasoning capabilities. The code and data will be
released at https://core-mm.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does complimentary information from multispectral imaging improve face
  presentation attack detection? <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narayan Vetrekar, Raghavendra Ramachandra, Sushma Venkatesh, Jyoti D. Pawar, R. S. Gad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Presentation Attack Detection (PAD) has been extensively studied,
particularly in the visible spectrum. With the advancement of sensing
technology beyond the visible range, multispectral imaging has gained
significant attention in this direction. We present PAD based on multispectral
images constructed for eight different presentation artifacts resulted from
three different artifact species. In this work, we introduce Face Presentation
Attack Multispectral (FPAMS) database to demonstrate the significance of
employing multispectral imaging. The goal of this work is to study
complementary information that can be combined in two different ways (image
fusion and score fusion) from multispectral imaging to improve the face PAD.
The experimental evaluation results present an extensive qualitative analysis
of 61650 sample multispectral images collected for bonafide and artifacts. The
PAD based on the score fusion and image fusion method presents superior
performance, demonstrating the significance of employing multispectral imaging
to detect presentation artifacts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in International IEEE Applied Sensing Conference (IEEE
  APSCON) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NePF: Neural Photon Field for Single-Stage Inverse Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuen-Yue Tsui, Qin Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel single-stage framework, Neural Photon Field (NePF), to
address the ill-posed inverse rendering from multi-view images. Contrary to
previous methods that recover the geometry, material, and illumination in
multiple stages and extract the properties from various multi-layer perceptrons
across different neural fields, we question such complexities and introduce our
method - a single-stage framework that uniformly recovers all properties. NePF
achieves this unification by fully utilizing the physical implication behind
the weight function of neural implicit surfaces and the view-dependent
radiance. Moreover, we introduce an innovative coordinate-based illumination
model for rapid volume physically-based rendering. To regularize this
illumination, we implement the subsurface scattering model for diffuse
estimation. We evaluate our method on both real and synthetic datasets. The
results demonstrate the superiority of our approach in recovering high-fidelity
geometry and visual-plausible material attributes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unearthing Common Inconsistency for Generalisable Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beilin Chu, Xuan Xu, Weike You, Linna Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfake has emerged for several years, yet efficient detection techniques
could generalize over different manipulation methods require further research.
While current image-level detection method fails to generalize to unseen
domains, owing to the domain-shift phenomenon brought by CNN's strong inductive
bias towards Deepfake texture, video-level one shows its potential to have both
generalization across multiple domains and robustness to compression. We argue
that although distinct face manipulation tools have different inherent bias,
they all disrupt the consistency between frames, which is a natural
characteristic shared by authentic videos. Inspired by this, we proposed a
detection approach by capturing frame inconsistency that broadly exists in
different forgery techniques, termed unearthing-common-inconsistency (UCI).
Concretely, the UCI network based on self-supervised contrastive learning can
better distinguish temporal consistency between real and fake videos from
multiple domains. We introduced a temporally-preserved module method to
introduce spatial noise perturbations, directing the model's attention towards
temporal information. Subsequently, leveraging a multi-view cross-correlation
learning module, we extensively learn the disparities in temporal
representations between genuine and fake samples. Extensive experiments
demonstrate the generalization ability of our method on unseen Deepfake
domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures and 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event Camera Data Dense <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Yang, Liyuan Pan, Liu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a self-supervised learning framework designed for
pre-training neural networks tailored to dense prediction tasks using event
camera data. Our approach utilizes solely event data for training.
  Transferring achievements from dense RGB pre-training directly to event
camera data yields subpar performance. This is attributed to the spatial
sparsity inherent in an event image (converted from event data), where many
pixels do not contain information. To mitigate this sparsity issue, we encode
an event image into event patch features, automatically mine contextual
similarity relationships among patches, group the patch features into
distinctive contexts, and enforce context-to-context similarities to learn
discriminative event features.
  For training our framework, we curate a synthetic event camera dataset
featuring diverse scene and motion patterns. Transfer learning performance on
downstream dense prediction tasks illustrates the superiority of our method
over state-of-the-art approaches. Notably, our single model secured the top
position in the challenging DSEC-Flow benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Category Discovery in Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyuan Peng, Qijian Tian, Jianqing Xu, Yizhang Jin, Xuequan Lu, Xin Tan, Yuan Xie, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores a novel setting called Generalized Category Discovery in
Semantic Segmentation (GCDSS), aiming to segment unlabeled images given prior
knowledge from a labeled set of base classes. The unlabeled images contain
pixels of the base class or novel class. In contrast to Novel Category
Discovery in Semantic Segmentation (NCDSS), there is no prerequisite for prior
knowledge mandating the existence of at least one novel class in each unlabeled
image. Besides, we broaden the segmentation scope beyond foreground objects to
include the entire image. Existing NCDSS methods rely on the aforementioned
priors, making them challenging to truly apply in real-world situations. We
propose a straightforward yet effective framework that reinterprets the GCDSS
challenge as a task of mask classification. Additionally, we construct a
baseline method and introduce the Neighborhood Relations-Guided Mask Clustering
Algorithm (NeRG-MaskCA) for mask categorization to address the fragmentation in
semantic representation. A benchmark dataset, Cityscapes-GCD, derived from the
Cityscapes dataset, is established to evaluate the GCDSS framework. Our method
demonstrates the feasibility of the GCDSS problem and the potential for
discovering and segmenting novel object classes in unlabeled images. We employ
the generated pseudo-labels from our approach as ground truth to supervise the
training of other models, thereby enabling them with the ability to segment
novel classes. It paves the way for further research in generalized category
discovery, broadening the horizons of semantic segmentation and its
applications. For details, please visit https://github.com/JethroPeng/GCDSS
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Liver Tumor Prediction with Advanced Attention Mechanisms Integrated
  into a Depth-Based Variant Search Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        P. Kalaiselvi, S. Anusuya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent days, Deep Learning (DL) techniques have become an emerging
transformation in the field of machine learning, artificial intelligence,
computer vision, and so on. Subsequently, researchers and industries have been
highly endorsed in the medical field, predicting and controlling diverse
diseases at specific intervals. Liver tumor prediction is a vital chore in
analyzing and treating liver diseases. This paper proposes a novel approach for
predicting liver tumors using Convolutional Neural Networks (CNN) and a
depth-based variant search algorithm with advanced attention mechanisms
(CNN-DS-AM). The proposed work aims to improve accuracy and robustness in
diagnosing and treating liver diseases. The anticipated model is assessed on a
Computed Tomography (CT) scan dataset containing both benign and malignant
liver tumors. The proposed approach achieved high accuracy in predicting liver
tumors, outperforming other state-of-the-art methods. Additionally, advanced
attention mechanisms were incorporated into the CNN model to enable the
identification and highlighting of regions of the CT scans most relevant to
predicting liver tumors. The results suggest that incorporating attention
mechanisms and a depth-based variant search algorithm into the CNN model is a
promising approach for improving the accuracy and robustness of liver tumor
prediction. It can assist radiologists in their diagnosis and treatment
planning. The proposed system achieved a high accuracy of 95.5% in predicting
liver tumors, outperforming other state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing through the Mask: Multi-task Generative Mask Decoupling Face
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Wang, Sufang Zhang, Jianteng Peng, Xinyi Wang, Yandong Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The outbreak of COVID-19 pandemic make people wear masks more frequently than
ever. Current general face recognition system suffers from serious performance
degradation,when encountering occluded scenes. The potential reason is that
face features are corrupted by occlusions on key facial regions. To tackle this
problem, previous works either extract identity-related embeddings on feature
level by additional mask prediction, or restore the occluded facial part by
generative models. However, the former lacks visual results for model
interpretation, while the latter suffers from artifacts which may affect
downstream recognition. Therefore, this paper proposes a Multi-task gEnerative
mask dEcoupling face Recognition (MEER) network to jointly handle these two
tasks, which can learn occlusionirrelevant and identity-related representation
while achieving unmasked face synthesis. We first present a novel mask
decoupling module to disentangle mask and identity information, which makes the
network obtain purer identity features from visible facial components. Then, an
unmasked face is restored by a joint-training strategy, which will be further
used to refine the recognition network with an id-preserving loss. Experiments
on masked face recognition under realistic and synthetic occlusions benchmarks
demonstrate that the MEER can outperform the state-ofthe-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What's left can't be right -- The remaining positional incompetence of
  contrastive vision-language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Hoehing, Ellen Rushe, Anthony Ventresque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive vision-language models like CLIP have been found to lack spatial
understanding capabilities. In this paper we discuss the possible causes of
this phenomenon by analysing both datasets and embedding space. By focusing on
simple left-right positional relations, we show that this behaviour is entirely
predictable, even with large-scale datasets, demonstrate that these relations
can be taught using synthetic data and show that this approach can generalise
well to natural images - improving the performance on left-right relations on
Visual Genome Relations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HandSight: DeCAF & Improved Fisher Vectors to Classify Clothing Color
  and Texture with a Finger-Mounted Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander J. Medeiros, Lee Stearns, Jon E. Froehlich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate the use of DeCAF and Improved Fisher Vector image features to
classify clothing texture. The issue of choosing clothes is a problem for the
blind every day. This work attempts to solve the issue with a finger-mounted
camera and state-of-the-art classification algorithms. To evaluate our
solution, we collected 520 close-up images across 29 pieces of clothing. We
contribute (1) the HCTD, an image dataset taken with a NanEyeGS camera, a
camera small enough to be mounted on the finger, and (2) evaluations of
state-of-the-art recognition algorithms applied to our dataset - achieving an
accuracy >95%. Throughout the paper, we will discuss previous work, evaluate
the current work, and finally, suggest the project's future direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nepotistically Trained Generative-AI Models Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matyas Bohacek, Hany Farid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trained on massive amounts of human-generated content, AI (artificial
intelligence) image synthesis is capable of reproducing semantically coherent
images that match the visual appearance of its training data. We show that when
retrained on even small amounts of their own creation, these generative-AI
models produce highly distorted images. We also show that this distortion
extends beyond the text prompts used in retraining, and that once poisoned, the
models struggle to fully heal even after retraining on only real images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Xie, Zeshun Zong, Yuxin Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PhysGaussian, a new method that seamlessly integrates physically
grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel
motion synthesis. Employing a custom Material Point Method (MPM), our approach
enriches 3D Gaussian kernels with physically meaningful kinematic deformation
and mechanical stress attributes, all evolved in line with continuum mechanics
principles. A defining characteristic of our method is the seamless integration
between physical simulation and visual rendering: both components utilize the
same 3D Gaussian kernels as their discrete representations. This negates the
necessity for triangle/tetrahedron meshing, marching cubes, "cage meshes," or
any other geometry embedding, highlighting the principle of "what you see is
what you simulate (WS$^2$)." Our method demonstrates exceptional versatility
across a wide variety of materials--including elastic entities, metals,
non-Newtonian fluids, and granular materials--showcasing its strong
capabilities in creating diverse visual content with novel viewpoints and
movements. Our project page is at: https://xpandora.github.io/PhysGaussian/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffAvatar: Simulation-Ready Garment Optimization with Differentiable
  Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Li, Hsiao-yu Chen, Egor Larionov, Nikolaos Sarafianos, Wojciech Matusik, Tuur Stuyck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The realism of digital avatars is crucial in enabling telepresence
applications with self-expression and customization. A key aspect of this
realism originates from the physical accuracy of both a true-to-life body shape
and clothing. While physical simulations can produce high-quality, realistic
motions for clothed humans, they require precise estimation of body shape and
high-quality garment assets with associated physical parameters for cloth
simulations. However, manually creating these assets and calibrating their
parameters is labor-intensive and requires specialized expertise. To address
this gap, we propose DiffAvatar, a novel approach that performs body and
garment co-optimization using differentiable simulation. By integrating
physical simulation into the optimization loop and accounting for the complex
nonlinear behavior of cloth and its intricate interaction with the body, our
framework recovers body and garment geometry and extracts important material
parameters in a physically plausible way. Our experiments demonstrate that our
approach generates realistic clothing and body shape that can be easily used in
downstream applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Structure and Appearance in ViT Feature Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narek Tumanyan, Omer Bar-Tal, Shir Amir, Shai Bagon, Tali Dekel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for semantically transferring the visual appearance of
one natural image to another. Specifically, our goal is to generate an image in
which objects in a source structure image are "painted" with the visual
appearance of their semantically related objects in a target appearance image.
To integrate semantic information into our framework, our key idea is to
leverage a pre-trained and fixed Vision Transformer (ViT) model. Specifically,
we derive novel disentangled representations of structure and appearance
extracted from deep ViT features. We then establish an objective function that
splices the desired structure and appearance representations, interweaving them
together in the space of ViT features. Based on our objective function, we
propose two frameworks of semantic appearance transfer -- "Splice", which works
by training a generator on a single and arbitrary pair of structure-appearance
images, and "SpliceNet", a feed-forward real-time appearance transfer model
trained on a dataset of images from a specific domain. Our frameworks do not
involve adversarial training, nor do they require any additional input
information such as semantic segmentation or correspondences. We demonstrate
high-resolution results on a variety of in-the-wild image pairs, under
significant variations in the number of objects, pose, and appearance. Code and
supplementary material are available in our project page: splice-vit.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Transactions on Graphics. arXiv admin note:
  substantial text overlap with arXiv:2201.00424</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LABELMAKER: Automatic Semantic Label Generation from RGB-D Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvan Weder, Hermann Blum, Francis Engelmann, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic annotations are indispensable to train or evaluate perception
models, yet very costly to acquire. This work introduces a fully automated
2D/3D labeling framework that, without any human intervention, can generate
labels for RGB-D scans at equal (or better) level of accuracy than comparable
manually annotated datasets such as ScanNet. Our approach is based on an
ensemble of state-of-the-art segmentation models and 3D lifting through neural
rendering. We demonstrate the effectiveness of our LabelMaker pipeline by
generating significantly better labels for the ScanNet datasets and
automatically labelling the previously unlabeled ARKitScenes dataset. Code and
models are available at https://labelmaker.org
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChemScraper: Graphics Extraction, Molecular Diagram Parsing, and
  Annotated Data Generation for PDF Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Kumar Shah, Bryan Manrique Amador, Abhisek Dey, Ming Creekmore, Blake Ocampo, Scott Denmark, Richard Zanibbi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing visual parsers for molecule diagrams translate pixel-based raster
images such as PNGs to chemical structure representations (e.g., SMILES).
However, PDFs created by word processors including \LaTeX{} and Word provide
explicit locations and shapes for characters, lines, and polygons. We
%introduce a method to extract symbols from born-digital PDF molecule images
and then apply simple graph transformations to capture both visual and chemical
structure in editable ChemDraw files (CDXML). Our fast ( PDF $\rightarrow$
visual graph $\rightarrow$ chemical graph ) pipeline does not require GPUs,
Optical Character Recognition (OCR) or vectorization. We evaluate on standard
benchmarks using SMILES strings, along with a novel evaluation that provides
graph-based metrics and error compilation using LgEval. The geometric
information in born-digital PDFs produces a highly accurate parser, motivating
generating training data for visual parsers that recognize from raster images,
with extracted graphics, visual structure, and chemical structure as
annotations. To do this we render SMILES strings in Indigo, parse molecule
structure, and then validate recognized structure to select correct files.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages without references, 10 figures, 3 Tables, submitted to
  International Journal on Document Analysis and Recognition (IJDAR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Modeling Based Automatic Video Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Hong Huang, Chao-Han Huck Yang, Pin-Yu Chen, Min-Hung Chen, Marcel Worring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of video summarization is to shorten videos automatically while
retaining the key information necessary to convey the overall story. Video
summarization methods mainly rely on visual factors, such as visual
consecutiveness and diversity, which may not be sufficient to fully understand
the content of the video. There are other non-visual factors, such as
interestingness, representativeness, and storyline consistency that should also
be considered for generating high-quality video summaries. Current methods do
not adequately take into account these non-visual factors, resulting in
suboptimal performance. In this work, a new approach to video summarization is
proposed based on insights gained from how humans create ground truth video
summaries. The method utilizes a conditional modeling perspective and
introduces multiple meaningful random variables and joint distributions to
characterize the key components of video summarization. Helper distributions
are employed to improve the training of the model. A conditional attention
module is designed to mitigate potential performance degradation in the
presence of multi-modal input. The proposed video summarization method
incorporates the above innovative design choices that aim to narrow the gap
between human-generated and machine-generated video summaries. Extensive
experiments show that the proposed approach outperforms existing methods and
achieves state-of-the-art performance on commonly used video summarization
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  arXiv admin note: substantial text overlap with arXiv:2305.00455</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-aware 3D Eye Gaze from Weak and Few-shot Supervisions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola Popovic, Dimitrios Christodoulou, Danda Pani Paudel, Xi Wang, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of predicting 3D eye gaze from eye images can be performed either by
(a) end-to-end learning for image-to-gaze mapping or by (b) fitting a 3D eye
model onto images. The former case requires 3D gaze labels, while the latter
requires eye semantics or landmarks to facilitate the model fitting. Although
obtaining eye semantics and landmarks is relatively easy, fitting an accurate
3D eye model on them remains to be very challenging due to its ill-posed nature
in general. On the other hand, obtaining large-scale 3D gaze data is cumbersome
due to the required hardware setups and computational demands. In this work, we
propose to predict 3D eye gaze from weak supervision of eye semantic
segmentation masks and direct supervision of a few 3D gaze vectors. The
proposed method combines the best of both worlds by leveraging large amounts of
weak annotations--which are easy to obtain, and only a few 3D gaze
vectors--which alleviate the difficulty of fitting 3D eye models on the
semantic segmentation of eye images. Thus, the eye gaze vectors, used in the
model fitting, are directly supervised using the few-shot gaze labels.
Additionally, we propose a transformer-based network architecture, that serves
as a solid baseline for our improvements. Our experiments in diverse settings
illustrate the significant benefits of the proposed method, achieving about 5
degrees lower angular gaze error over the baseline, when only 0.05% 3D
annotations of the training images are used. The source code is available at
https://github.com/dimitris-christodoulou57/Model-aware_3D_Eye_Gaze.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISMAR2023 as a poster paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Estimation in Contrast-Enhanced MR Image Translation with
  Multi-Axis Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivo M. Baltruschat, Parvaneh Janbakhshi, Melanie Dohmen, Matthias Lenga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, deep learning has been applied to a wide range of medical
imaging and image processing tasks. In this work, we focus on the estimation of
epistemic uncertainty for 3D medical image-to-image translation. We propose a
novel model uncertainty quantification method, Multi-Axis Fusion (MAF), which
relies on the integration of complementary information derived from multiple
views on volumetric image data. The proposed approach is applied to the task of
synthesizing contrast enhanced T1-weighted images based on native T1, T2 and
T2-FLAIR scans. The quantitative findings indicate a strong correlation
($\rho_{\text healthy} = 0.89$) between the mean absolute image synthetization
error and the mean uncertainty score for our MAF method. Hence, we consider MAF
as a promising approach to solve the highly relevant task of detecting
synthetization failures at inference time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Robots to Build Simulations of Themselves 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Hu, Jiong Lin, Hod Lipson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation enables robots to plan and estimate the outcomes of prospective
actions without the need to physically execute them. We introduce a
self-supervised learning framework to enable robots model and predict their
morphology, kinematics and motor control using only brief raw video data,
eliminating the need for extensive real-world data collection and kinematic
priors. By observing their own movements, akin to humans watching their
reflection in a mirror, robots learn an ability to simulate themselves and
predict their spatial motion for various tasks. Our results demonstrate that
this self-learned simulation not only enables accurate motion planning but also
allows the robot to detect abnormalities and recover from damage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applications of Large Scale Foundation Models for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Huang, Yue Chen, Zhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,
autonomous driving has been the most active field of AI applications. Recently
powered by large language models (LLMs), chat systems, such as chatGPT and
PaLM, emerge and rapidly become a promising direction to achieve artificial
general intelligence (AGI) in natural language processing (NLP). There comes a
natural thinking that we could employ these abilities to reformulate autonomous
driving. By combining LLM with foundation models, it is possible to utilize the
human knowledge, commonsense and reasoning to rebuild autonomous driving
systems from the current long-tailed AI dilemma. In this paper, we investigate
the techniques of foundation models and LLMs applied for autonomous driving,
categorized as simulation, world model, data annotation and planning or E2E
solutions etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fingerspelling PoseNet: Enhancing Fingerspelling Translation with
  Pose-Based <span class="highlight-title">Transformer</span> Models <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pooya Fayyazsanavi, Negar Nejatishahidin, Jana Kosecka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the task of American Sign Language fingerspelling translation
using videos in the wild. We exploit advances in more accurate hand pose
estimation and propose a novel architecture that leverages the transformer
based encoder-decoder model enabling seamless contextual word translation. The
translation model is augmented by a novel loss term that accurately predicts
the length of the finger-spelled word, benefiting both training and inference.
We also propose a novel two-stage inference approach that re-ranks the
hypotheses using the language model capabilities of the decoder. Through
extensive experiments, we demonstrate that our proposed method outperforms the
state-of-the-art models on ChicagoFSWild and ChicagoFSWild+ achieving more than
10% relative improvement in performance. Our findings highlight the
effectiveness of our approach and its potential to advance fingerspelling
recognition in sign language translation. Code is also available at
https://github.com/pooyafayyaz/Fingerspelling-PoseNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixing-Denoising Generalizable Occupancy Networks <span class="chip">3DV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amine Ouasfi, Adnane Boukhayma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While current state-of-the-art generalizable implicit neural shape models
rely on the inductive bias of convolutions, it is still not entirely clear how
properties emerging from such biases are compatible with the task of 3D
reconstruction from point cloud. We explore an alternative approach to
generalizability in this context. We relax the intrinsic model bias (i.e. using
MLPs to encode local features as opposed to convolutions) and constrain the
hypothesis space instead with an auxiliary regularization related to the
reconstruction task, i.e. denoising. The resulting model is the first only-MLP
locally conditioned implicit shape reconstruction from point cloud network with
fast feed forward inference. Point cloud borne features and denoising offsets
are predicted from an exclusively MLP-made network in a single forward pass. A
decoder predicts occupancy probabilities for queries anywhere in space by
pooling nearby features from the point cloud order-invariantly, guided by
denoised relative positional encoding. We outperform the state-of-the-art
convolutional method while using half the number of model parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3DV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, David Bau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method to create interpretable concept sliders that enable
precise control over attributes in image generations from diffusion models. Our
approach identifies a low-rank parameter direction corresponding to one concept
while minimizing interference with other attributes. A slider is created using
a small set of prompts or sample images; thus slider directions can be created
for either textual or visual concepts. Concept Sliders are plug-and-play: they
can be composed efficiently and continuously modulated, enabling precise
control over image generation. In quantitative experiments comparing to
previous editing techniques, our sliders exhibit stronger targeted edits with
lower interference. We showcase sliders for weather, age, styles, and
expressions, as well as slider compositions. We show how sliders can transfer
latents from StyleGAN for intuitive editing of visual concepts for which
textual description is difficult. We also find that our method can help address
persistent quality issues in Stable Diffusion XL including repair of object
deformations and fixing distorted hands. Our code, data, and trained sliders
are available at https://sliders.baulab.info/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAS: A Deformable Attention to Capture Salient Information in CNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farzad Salajegheh, Nader Asadi, Soroush Saryazdi, Sudhir Mudur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) excel in local spatial pattern
recognition. For many vision tasks, such as object recognition and
segmentation, salient information is also present outside CNN's kernel
boundaries. However, CNNs struggle in capturing such relevant information due
to their confined receptive fields. Self-attention can improve a model's access
to global information but increases computational overhead. We present a fast
and simple fully convolutional method called DAS that helps focus attention on
relevant information. It uses deformable convolutions for the location of
pertinent image regions and separable convolutions for efficiency. DAS plugs
into existing CNNs and propagates relevant information using a gating
mechanism. Compared to the O(n^2) computational complexity of transformer-style
attention, DAS is O(n). Our claim is that DAS's ability to pay increased
attention to relevant features results in performance improvements when added
to popular CNNs for Image Classification and Object Detection. For example, DAS
yields an improvement on Stanford Dogs (4.47%), ImageNet (1.91%), and COCO AP
(3.3%) with base ResNet50 backbone. This outperforms other CNN attention
mechanisms while using similar or less FLOPs. Our code will be publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Dual-Stream Neural Network Explains the Functional Segregation of
  Dorsal and Ventral Visual Pathways in Human Brains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13849v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13849v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minkyu Choi, Kuan Han, Xiaokai Wang, Yizhen Zhang, Zhongming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human visual system uses two parallel pathways for spatial processing and
object recognition. In contrast, computer vision systems tend to use a single
feedforward pathway, rendering them less robust, adaptive, or efficient than
human vision. To bridge this gap, we developed a dual-stream vision model
inspired by the human eyes and brain. At the input level, the model samples two
complementary visual patterns to mimic how the human eyes use magnocellular and
parvocellular retinal ganglion cells to separate retinal inputs to the brain.
At the backend, the model processes the separate input patterns through two
branches of convolutional neural networks (CNN) to mimic how the human brain
uses the dorsal and ventral cortical pathways for parallel visual processing.
The first branch (WhereCNN) samples a global view to learn spatial attention
and control eye movements. The second branch (WhatCNN) samples a local view to
represent the object around the fixation. Over time, the two branches interact
recurrently to build a scene representation from moving fixations. We compared
this model with the human brains processing the same movie and evaluated their
functional alignment by linear transformation. The WhereCNN and WhatCNN
branches were found to differentially match the dorsal and ventral pathways of
the visual cortex, respectively, primarily due to their different learning
objectives. These model-based results lead us to speculate that the distinct
responses and representations of the ventral and dorsal streams are more
influenced by their distinct goals in visual attention and object recognition
than by their specific bias or selectivity in retinal inputs. This dual-stream
model takes a further step in brain-inspired computer vision, enabling parallel
neural networks to actively explore and understand the visual surroundings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SWAT: Spatial Structure Within and Among Tokens <span class="chip">IJCAI23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13677v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13677v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumara Kahatapitiya, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling visual data as tokens (i.e., image patches) using attention
mechanisms, feed-forward networks or convolutions has been highly effective in
recent years. Such methods usually have a common pipeline: a tokenization
method, followed by a set of layers/blocks for information mixing, both within
and among tokens. When image patches are converted into tokens, they are often
flattened, discarding the spatial structure within each patch. As a result, any
processing that follows (eg: multi-head self-attention) may fail to recover
and/or benefit from such information. In this paper, we argue that models can
have significant gains when spatial structure is preserved during tokenization,
and is explicitly used during the mixing stage. We propose two key
contributions: (1) Structure-aware Tokenization and, (2) Structure-aware
Mixing, both of which can be combined with existing models with minimal effort.
We introduce a family of models (SWAT), showing improvements over the likes of
DeiT, MLP-Mixer and Swin Transformer, across multiple benchmarks including
ImageNet classification and ADE20K segmentation. Our code is available at
https://github.com/kkahatapitiya/SWAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to be published at IJCAI23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balancing stability and plasticity in continual learning: the
  readout-decomposition of activation change (RDAC) framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04741v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04741v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Anthes, Sushrut Thorat, Peter König, Tim C. Kietzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) algorithms strive to acquire new knowledge while
preserving prior information. However, this stability-plasticity trade-off
remains a central challenge. This paper introduces a framework that dissects
this trade-off, offering valuable insights into CL algorithms. The
Readout-Decomposition of Activation Change (RDAC) framework first addresses the
stability-plasticity dilemma and its relation to catastrophic forgetting. It
relates learning-induced activation changes in the range of prior readouts to
the degree of stability and changes in the null space to the degree of
plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the
framework clarifies the stability-plasticity trade-offs of the popular
regularization algorithms Synaptic intelligence (SI), Elastic-weight
consolidation (EWC), and learning without Forgetting (LwF), and replay-based
algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay
preserved stability and plasticity, while SI, EWC, and LwF traded off
plasticity for stability. The inability of the regularization algorithms to
maintain plasticity was linked to them restricting the change of activations in
the null space of the prior readout. Additionally, for one-hidden-layer linear
neural networks, we derived a gradient decomposition algorithm to restrict
activation change only in the range of the prior readouts, to maintain high
stability while not further sacrificing plasticity. Results demonstrate that
the algorithm maintained stability without significant plasticity loss. The
RDAC framework informs the behavior of existing CL algorithms and paves the way
for novel CL approaches. Finally, it sheds light on the connection between
learning-induced activation/representation changes and the stability-plasticity
dilemma, also offering insights into representational drift in biological
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, Revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preserving Patient Privacy in MRI Scans: A Comprehensive Approach with
  3D Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Alexander Van der Goten, Kevin Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MRI scans provide valuable medical information, however they also contain
sensitive and personally identifiable information (PII) that needs to be
protected. Whereas MRI metadata is easily sanitized, MRI image data is a
privacy risk because it contains information to render highly-realistic 3D
visualizations of a patient's head, enabling malicious actors to possibly
identify the subject by cross-referencing a database. Data anonymization and
de-identification is concerned with ensuring the privacy and confidentiality of
individuals' personal information. Traditional MRI de-identification methods
remove privacy-sensitive parts (e.g. eyes, nose etc.) from a given scan. This
comes at the expense of introducing a domain shift that can throw off
downstream analyses. Recently, a GAN-based approach was proposed to de-identify
a patient's scan by remodeling it (\eg changing the face) rather than by
removing parts. In this work, we propose CP-MAE, a model that de-identifies the
face using masked autoencoders and that outperforms all previous approaches in
terms of downstream task performance as well as de-identification. With our
method we are able to synthesize scans of resolution up to $256^3$ (previously
$128^3$) which constitutes an eight-fold increase in the number of voxels.
Using our construction we were able to design a system that exhibits a highly
robust training stage, making it easy to fit the network on novel data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynthEnsemble: A Fusion of CNN, Vision <span class="highlight-title">Transformer</span>, and Hybrid Models
  for Multi-Label Chest X-Ray Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. M. Nabil Ashraf, Md. Adyelullahil Mamun, Hasnat Md. Abdullah, Md. Golam Rabiul Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-rays are widely used to diagnose thoracic diseases, but the lack of
detailed information about these abnormalities makes it challenging to develop
accurate automated diagnosis systems, which is crucial for early detection and
effective treatment. To address this challenge, we employed deep learning
techniques to identify patterns in chest X-rays that correspond to different
diseases. We conducted experiments on the "ChestX-ray14" dataset using various
pre-trained CNNs, transformers, hybrid(CNN+Transformer) models and classical
models. The best individual model was the CoAtNet, which achieved an area under
the receiver operating characteristic curve (AUROC) of 84.2%. By combining the
predictions of all trained models using a weighted average ensemble where the
weight of each model was determined using differential evolution, we further
improved the AUROC to 85.4%, outperforming other state-of-the-art methods in
this field. Our findings demonstrate the potential of deep learning techniques,
particularly ensemble deep learning, for improving the accuracy of automatic
diagnosis of thoracic diseases from chest X-rays.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in International Conference on Computer and Information
  Technology (ICCIT) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Motion Tracking by Registering an Articulated Surface to 3-D
  Points and Normals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.04514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.04514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radu Horaud, Matti Niskanen, Guillaume Dewaele, Edmond Boyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of human motion tracking by registering a surface to
3-D data. We propose a method that iteratively computes two things: Maximum
likelihood estimates for both the kinematic and free-motion parameters of a
kinematic human-body representation, as well as probabilities that the data are
assigned either to a body part, or to an outlier cluster. We introduce a new
metric between observed points and normals on one side, and a parameterized
surface on the other side, the latter being defined as a blending over a set of
ellipsoids. We claim that this metric is well suited when one deals with either
visual-hull or visual-shape observations. We illustrate the method by tracking
human motions using sparse visual-shape data (3-D surface points and normals)
gathered from imperfect silhouettes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Matching with Scale Adjustment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.05582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.05582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yves Dufournaud, Cordelia Schmid, Radu Horaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we address the problem of matching two images with two
different resolutions: a high-resolution image and a low-resolution one. The
difference in resolution between the two images is not known and without loss
of generality one of the images is assumed to be the high-resolution one. On
the premise that changes in resolution act as a smoothing equivalent to changes
in scale, a scale-space representation of the high-resolution image is
produced. Hence the one-to-one classical image matching paradigm becomes
one-to-many because the low-resolution image is compared with all the
scale-space representations of the high-resolution one. Key to the success of
such a process is the proper representation of the features to be matched in
scale-space. We show how to represent and extract interest points at variable
scales and we devise a method allowing the comparison of two images at two
different resolutions. The method comprises the use of photometric- and
rotation-invariant descriptors, a geometric model mapping the high-resolution
image onto a low-resolution image region, and an image matching strategy based
on local constraints and on the robust estimation of this geometric model.
Extensive experiments show that our matching method can be used for scale
changes up to a factor of 6.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D SA-UNet: 3D Spatial Attention UNet with 3D ASPP for White Matter
  Hyperintensities Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08402v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08402v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changlu Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  White Matter Hyperintensity (WMH) is an imaging feature related to various
diseases such as dementia and stroke. Accurately segmenting WMH using computer
technology is crucial for early disease diagnosis. However, this task remains
challenging due to the small lesions with low contrast and high discontinuity
in the images, which contain limited contextual and spatial information. To
address this challenge, we propose a deep learning model called 3D Spatial
Attention U-Net (3D SA-UNet) for automatic WMH segmentation using only Fluid
Attenuation Inversion Recovery (FLAIR) scans. The 3D SA-UNet introduces a 3D
Spatial Attention Module that highlights important lesion features, such as
WMH, while suppressing unimportant regions. Additionally, to capture features
at different scales, we extend the Atrous Spatial Pyramid Pooling (ASPP) module
to a 3D version, enhancing the segmentation performance of the network. We
evaluate our method on publicly available dataset and demonstrate the
effectiveness of 3D spatial attention module and 3D ASPP in WMH segmentation.
Through experimental results, it has been demonstrated that our proposed 3D
SA-UNet model achieves higher accuracy compared to other state-of-the-art 3D
convolutional neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scattering Vision <span class="highlight-title">Transformer</span>: Spectral Mixing Matters <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badri N. Patro, Vijay Srinivas Agneeswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers have gained significant attention and achieved
state-of-the-art performance in various computer vision tasks, including image
classification, instance segmentation, and object detection. However,
challenges remain in addressing attention complexity and effectively capturing
fine-grained information within images. Existing solutions often resort to
down-sampling operations, such as pooling, to reduce computational cost.
Unfortunately, such operations are non-invertible and can result in information
loss. In this paper, we present a novel approach called Scattering Vision
Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally
scattering network that enables the capture of intricate image details. SVT
overcomes the invertibility issue associated with down-sampling operations by
separating low-frequency and high-frequency components. Furthermore, SVT
introduces a unique spectral gating network utilizing Einstein multiplication
for token and channel mixing, effectively reducing complexity. We show that SVT
achieves state-of-the-art performance on the ImageNet dataset with a
significant reduction in a number of parameters and FLOPS. SVT shows 2\%
improvement over LiTv2 and iFormer. SVT-H-S reaches 84.2\% top-1 accuracy,
while SVT-H-B reaches 85.2\% (state-of-art for base versions) and SVT-H-L
reaches 85.7\% (again state-of-art for large versions). SVT also shows
comparable results in other vision tasks such as instance segmentation. SVT
also outperforms other transformers in transfer learning on standard datasets
such as CIFAR10, CIFAR100, Oxford Flower, and Stanford Car datasets. The
project page is available on this
webpage.\url{https://badripatro.github.io/svt/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted @NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Sun, Qian Bao, Wu Liu, Tao Mei, Michael J. Black
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although the estimation of 3D human pose and shape (HPS) is rapidly
progressing, current methods still cannot reliably estimate moving humans in
global coordinates, which is critical for many applications. This is
particularly challenging when the camera is also moving, entangling human and
camera motion. To address these issues, we adopt a novel 5D representation
(space, time, and identity) that enables end-to-end reasoning about people in
scenes. Our method, called TRACE, introduces several novel architectural
components. Most importantly, it uses two new "maps" to reason about the 3D
trajectory of people over time in camera, and world, coordinates. An additional
memory unit enables persistent tracking of people even during long occlusions.
TRACE is the first one-stage method to jointly recover and track 3D humans in
global coordinates from dynamic cameras. By training it end-to-end, and using
full image information, TRACE achieves state-of-the-art performance on tracking
and HPS benchmarks. The code and dataset are released for research purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://www.yusun.work/TRACE/TRACE.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Examples Are Not Real Features <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Li, Yifei Wang, Yiwen Guo, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existence of adversarial examples has been a mystery for years and
attracted much interest. A well-known theory by \citet{ilyas2019adversarial}
explains adversarial vulnerability from a data perspective by showing that one
can extract non-robust features from adversarial examples and these features
alone are useful for classification. However, the explanation remains quite
counter-intuitive since non-robust features are mostly noise features to
humans. In this paper, we re-examine the theory from a larger context by
incorporating multiple learning paradigms. Notably, we find that contrary to
their good usefulness under supervised learning, non-robust features attain
poor usefulness when transferred to other self-supervised learning paradigms,
such as contrastive learning, masked image modeling, and diffusion models. It
reveals that non-robust features are not really as useful as robust or natural
features that enjoy good transferability between these paradigms. Meanwhile,
for robustness, we also show that naturally trained encoders from robust
features are largely non-robust under AutoAttack. Our cross-paradigm
examination suggests that the non-robust features are not really useful but
more like paradigm-wise shortcuts, and robust features alone might be
insufficient to attain reliable model robustness. Code is available at
\url{https://github.com/PKU-ML/AdvNotRealFeatures}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Women Wearing Lipstick: Measuring the Bias Between an Object and Its
  Related Gender <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Sabir, Lluís Padró
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the impact of objects on gender bias in image
captioning systems. Our results show that only gender-specific objects have a
strong gender bias (e.g., women-lipstick). In addition, we propose a visual
semantic-based gender score that measures the degree of bias and can be used as
a plug-in for any image captioning system. Our experiments demonstrate the
utility of the gender score, since we observe that our score can measure the
bias relation between a caption and its related gender; therefore, our score
can be used as an additional metric to the existing Object Gender Co-Occ
approach. Code and data are publicly available at
\url{https://github.com/ahmedssabir/GenderScore}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TokenFlow: Consistent Diffusion Features for Consistent Video Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10373v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10373v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Geyer, Omer Bar-Tal, Shai Bagon, Tali Dekel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generative AI revolution has recently expanded to videos. Nevertheless,
current state-of-the-art video models are still lagging behind image models in
terms of visual quality and user control over the generated content. In this
work, we present a framework that harnesses the power of a text-to-image
diffusion model for the task of text-driven video editing. Specifically, given
a source video and a target text-prompt, our method generates a high-quality
video that adheres to the target text, while preserving the spatial layout and
motion of the input video. Our method is based on a key observation that
consistency in the edited video can be obtained by enforcing consistency in the
diffusion feature space. We achieve this by explicitly propagating diffusion
features based on inter-frame correspondences, readily available in the model.
Thus, our framework does not require any training or fine-tuning, and can work
in conjunction with any off-the-shelf text-to-image editing method. We
demonstrate state-of-the-art editing results on a variety of real-world videos.
Webpage: https://diffusion-tokenflow.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Hierarchical Regional <span class="highlight-title">Transformer</span>-based Multiple Instance
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josef Cersovsky, Sadegh Mohammadi, Dagmar Kainmueller, Johannes Hoehne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classification of gigapixel histopathology images with deep multiple
instance learning models has become a critical task in digital pathology and
precision medicine. In this work, we propose a Transformer-based multiple
instance learning approach that replaces the traditional learned attention
mechanism with a regional, Vision Transformer inspired self-attention
mechanism. We present a method that fuses regional patch information to derive
slide-level predictions and show how this regional aggregation can be stacked
to hierarchically process features on different distance levels. To increase
predictive accuracy, especially for datasets with small, local morphological
features, we introduce a method to focus the image processing on high attention
regions during inference. Our approach is able to significantly improve
performance over the baseline on two histopathology datasets and points towards
promising directions for further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, LaTeX; header update after published, fixed typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint covariance property under geometric image transformations for
  spatio-temporal receptive fields according to the generalized Gaussian
  derivative model for visual receptive fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Lindeberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The influence of natural image transformations on receptive field responses
is crucial for modelling visual operations in computer vision and biological
vision. In this regard, covariance properties with respect to geometric image
transformations in the earliest layers of the visual hierarchy are essential
for expressing robust image operations and for formulating invariant visual
operations at higher levels. This paper defines and proves a joint covariance
property under compositions of spatial scaling transformations, spatial affine
transformations, Galilean transformations and temporal scaling transformations,
which makes it possible to characterize how different types of image
transformations interact with each other. Specifically, the derived relations
show how the receptive field parameters need to be transformed, in order to
match the output from spatio-temporal receptive fields with the underlying
spatio-temporal image transformations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Environment-Aware Affordance for 3D Articulated Object
  Manipulation under Occlusions <span class="chip">NeurIPS
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07510v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07510v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Cheng, Ruihai Wu, Yan Shen, Chuanruo Ning, Guanqi Zhan, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceiving and manipulating 3D articulated objects in diverse environments is
essential for home-assistant robots. Recent studies have shown that point-level
affordance provides actionable priors for downstream manipulation tasks.
However, existing works primarily focus on single-object scenarios with
homogeneous agents, overlooking the realistic constraints imposed by the
environment and the agent's morphology, e.g., occlusions and physical
limitations. In this paper, we propose an environment-aware affordance
framework that incorporates both object-level actionable priors and environment
constraints. Unlike object-centric affordance approaches, learning
environment-aware affordance faces the challenge of combinatorial explosion due
to the complexity of various occlusions, characterized by their quantities,
geometries, positions and poses. To address this and enhance data efficiency,
we introduce a novel contrastive affordance learning framework capable of
training on scenes containing a single occluder and generalizing to scenes with
complex occluder combinations. Experiments demonstrate the effectiveness of our
proposed approach in learning affordance considering environment constraints.
Project page at https://chengkaiacademycity.github.io/EnvAwareAfford/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In 37th Conference on Neural Information Processing Systems (NeurIPS
  2023). Website at https://chengkaiacademycity.github.io/EnvAwareAfford/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the ability of existing large-scale text-to-image (T2I) models to
generate high-quality images from detailed textual descriptions, they often
lack the ability to precisely edit the generated or real images. In this paper,
we propose a novel image editing method, DragonDiffusion, enabling Drag-style
manipulation on Diffusion models. Specifically, we construct classifier
guidance based on the strong correspondence of intermediate features in the
diffusion model. It can transform the editing signals into gradients via
feature correspondence loss to modify the intermediate representation of the
diffusion model. Based on this guidance strategy, we also build a multi-scale
guidance to consider both semantic and geometric alignment. Moreover, a
cross-branch self-attention is added to maintain the consistency between the
original image and the editing result. Our method, through an efficient design,
achieves various editing modes for the generated or real images, such as object
moving, object resizing, object appearance replacement, and content dragging.
It is worth noting that all editing and content preservation signals come from
the image itself, and the model does not require fine-tuning or additional
modules. Our source code will be available at
https://github.com/MC-E/DragonDiffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Framework for 3D Point Cloud Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojia Lin, Yongdong Luo, Xiawu Zheng, Lijiang Li, Fei Chao, Taisong Jin, Donghao Luo, Yan Wang, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to its precise spatial referencing, 3D point cloud visual grounding is
essential for deep understanding and dynamic interaction in 3D environments,
encompassing 3D Referring Expression Comprehension (3DREC) and Segmentation
(3DRES). We argue that 3DREC and 3DRES should be unified in one framework,
which is also a natural progression in the community. To explain, 3DREC help
3DRES locate the referent, while 3DRES also facilitate 3DREC via more
fine-grained language-visual alignment. To achieve this, this paper takes the
initiative step to integrate 3DREC and 3DRES into a unified framework, termed
3D Referring Transformer (3DRefTR). Its key idea is to build upon a mature
3DREC model and leverage ready query embeddings and visual tokens from the
3DREC model to construct a dedicated mask branch. Specially, we propose
Superpoint Mask Branch, which serves a dual purpose: i) By harnessing on the
inherent association between the superpoints and point cloud, it eliminates the
heavy computational overhead on the high-resolution visual features for
upsampling; ii) By leveraging the heterogeneous CPU-GPU parallelism, while the
GPU is occupied generating visual and language tokens, the CPU concurrently
produces superpoints, equivalently accomplishing the upsampling computation.
This elaborate design enables 3DRefTR to achieve both well-performing 3DRES and
3DREC capacities with only a 6% additional latency compared to the original
3DREC model. Empirical evaluations affirm the superiority of 3DRefTR.
Specifically, on the ScanRefer dataset, 3DRefTR surpasses the state-of-the-art
3DRES method by 12.43% in mIoU and improves upon the SOTA 3DREC method by 0.6%
Acc@0.25IoU. The codes and models will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating the Generalization in Deep Neural Networks via Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.00851v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.00851v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhao, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalization is the key capability for deep neural networks (DNNs).
However, it is challenging to give a reliable measure of the generalization
ability of a DNN via only its nature. In this paper, we propose a novel method
for estimating the generalization gap based on network sparsity. In our method,
two key quantities are proposed first. They have close relationship with the
generalization ability and can be calculated directly from the training results
alone. Then a simple linear model involving two key quantities are constructed
to give accurate estimation of the generalization gap. By training DNNs with a
wide range of generalization gap on popular datasets, we show that our key
quantities and linear model could be efficient tools for estimating the
generalization gap of DNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Role Taxonomy of Units in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.00789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.00789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhao, Hao Zhang, Xiuyuan Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying the role of network units in deep neural networks (DNNs) is
critical in many aspects including giving understandings on the mechanisms of
DNNs and building basic connections between deep learning and neuroscience.
However, there remains unclear on which roles the units in DNNs with different
generalization ability could present. To this end, we give role taxonomy of
units in DNNs via introducing the retrieval-of-function test, where units are
categorized into four types in terms of their functional preference on
separately the training set and testing set. We show that ratios of the four
categories are highly associated with the generalization ability of DNNs from
two distinct perspectives, based on which we give signs of DNNs with well
generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking the Backward Propagation for Adversarial Transferability <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosen Wang, Kangheng Tong, Kun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer-based attacks generate adversarial examples on the surrogate model,
which can mislead other black-box models without access, making it promising to
attack real-world applications. Recently, several works have been proposed to
boost adversarial transferability, in which the surrogate model is usually
overlooked. In this work, we identify that non-linear layers (e.g., ReLU,
max-pooling, etc.) truncate the gradient during backward propagation, making
the gradient w.r.t. input image imprecise to the loss function. We hypothesize
and empirically validate that such truncation undermines the transferability of
adversarial examples. Based on these findings, we propose a novel method called
Backward Propagation Attack (BPA) to increase the relevance between the
gradient w.r.t. input image and loss function so as to generate adversarial
examples with higher transferability. Specifically, BPA adopts a non-monotonic
function as the derivative of ReLU and incorporates softmax with temperature to
smooth the derivative of max-pooling, thereby mitigating the information loss
during the backward propagation of gradients. Empirical results on the ImageNet
dataset demonstrate that not only does our method substantially boost the
adversarial transferability, but it is also general to existing transfer-based
attacks. Code is available at https://github.com/Trustworthy-AI-Group/RPA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explicit3D: Graph Network with Spatial Inference for Single Image 3D
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Liu, Wenming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indoor 3D object detection is an essential task in single image scene
understanding, impacting spatial cognition fundamentally in visual reasoning.
Existing works on 3D object detection from a single image either pursue this
goal through independent predictions of each object or implicitly reason over
all possible objects, failing to harness relational geometric information
between objects. To address this problem, we propose a dynamic sparse graph
pipeline named Explicit3D based on object geometry and semantics features.
Taking the efficiency into consideration, we further define a relatedness score
and design a novel dynamic pruning algorithm followed by a cluster sampling
method for sparse scene graph generation and updating. Furthermore, our
Explicit3D introduces homogeneous matrices and defines new relative loss and
corner loss to model the spatial difference between target pairs explicitly.
Instead of using ground-truth labels as direct supervision, our relative and
corner loss are derived from the homogeneous transformation, which renders the
model to learn the geometric consistency between objects. The experimental
results on the SUN RGB-D dataset demonstrate that our Explicit3D achieves
better performance balance than the-state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Transfer in Latent Space (DTLS) Wins on Image Super-Resolution --
  a Non-Denoising Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02358v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02358v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun-Chuen Hui, Wan-Chi Siu, Ngai-Fong Law
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large scale image super-resolution is a challenging computer vision task,
since vast information is missing in a highly degraded image, say for example
forscale x16 super-resolution. Diffusion models are used successfully in recent
years in extreme super-resolution applications, in which Gaussian noise is used
as a means to form a latent photo-realistic space, and acts as a link between
the space of latent vectors and the latent photo-realistic space. There are
quite a few sophisticated mathematical derivations on mapping the statistics of
Gaussian noises making Diffusion Models successful. In this paper we propose a
simple approach which gets away from using Gaussian noise but adopts some basic
structures of diffusion models for efficient image super-resolution.
Essentially, we propose a DNN to perform domain transfer between neighbor
domains, which can learn the differences in statistical properties to
facilitate gradual interpolation with results of reasonable quality. Further
quality improvement is achieved by conditioning the domain transfer with
reference to the input LR image. Experimental results show that our method
outperforms not only state-of-the-art large scale super resolution models, but
also the current diffusion models for image super-resolution. The approach can
readily be extended to other image-to-image tasks, such as image enlightening,
inpainting, denoising, etc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Arbitrary Shaped Clustering through Correlated Gaussian Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ole Christian Eidheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is no convincing evidence that backpropagation is a biologically
plausible mechanism, and further studies of alternative learning methods are
needed. A novel online clustering algorithm is presented that can produce
arbitrary shaped clusters from inputs in an unsupervised manner, and requires
no prior knowledge of the number of clusters in the input data. This is
achieved by finding correlated outputs from functions that capture commonly
occurring input patterns. The algorithm can be deemed more biologically
plausible than model optimization through backpropagation, although practical
applicability may require additional research. However, the method yields
satisfactory results on several toy datasets on a noteworthy range of
hyperparameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrected uniform distribution range; removed "average" from last
  sentence in section 4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaze Estimation on Spresense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Ruegg, Pietro Bonazzi, Andrea Ronco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaze estimation is a valuable technology with numerous applications in fields
such as human-computer interaction, virtual reality, and medicine. This report
presents the implementation of a gaze estimation system using the Sony
Spresense microcontroller board and explores its performance in latency,
MAC/cycle, and power consumption. The report also provides insights into the
system's architecture, including the gaze estimation model used. Additionally,
a demonstration of the system is presented, showcasing its functionality and
performance. Our lightweight model TinyTrackerS is a mere 169Kb in size, using
85.8k parameters and runs on the Spresense platform at 3 FPS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TinyTracker: Ultra-Fast and Ultra-Low-Power Edge Vision In-Sensor for
  Gaze Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07813v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07813v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Bonazzi, Thomas Ruegg, Sizhen Bian, Yawei Li, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent edge vision tasks encounter the critical challenge of ensuring
power and latency efficiency due to the typically heavy computational load they
impose on edge platforms.This work leverages one of the first "AI in sensor"
vision platforms, IMX500 by Sony, to achieve ultra-fast and ultra-low-power
end-to-end edge vision applications. We evaluate the IMX500 and compare it to
other edge platforms, such as the Google Coral Dev Micro and Sony Spresense, by
exploring gaze estimation as a case study. We propose TinyTracker, a highly
efficient, fully quantized model for 2D gaze estimation designed to maximize
the performance of the edge vision systems considered in this study.
TinyTracker achieves a 41x size reduction (600Kb) compared to iTracker [1]
without significant loss in gaze estimation accuracy (maximum of 0.16 cm when
fully quantized). TinyTracker's deployment on the Sony IMX500 vision sensor
results in end-to-end latency of around 19ms. The camera takes around 17.9ms to
read, process and transmit the pixels to the accelerator. The inference time of
the network is 0.86ms with an additional 0.24 ms for retrieving the results
from the sensor. The overall energy consumption of the end-to-end system is 4.9
mJ, including 0.06 mJ for inference. The end-to-end study shows that IMX500 is
1.7x faster than CoralMicro (19ms vs 34.4ms) and 7x more power efficient (4.9mJ
VS 34.2mJ)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SparseTrack: Multi-Object Tracking by Performing Scene Decomposition
  based on Pseudo-Depth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelin Liu, Xinggang Wang, Cheng Wang, Wenyu Liu, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring robust and efficient association methods has always been an
important issue in multiple-object tracking (MOT). Although existing tracking
methods have achieved impressive performance, congestion and frequent
occlusions still pose challenging problems in multi-object tracking. We reveal
that performing sparse decomposition on dense scenes is a crucial step to
enhance the performance of associating occluded targets. To this end, we
propose a pseudo-depth estimation method for obtaining the relative depth of
targets from 2D images. Secondly, we design a depth cascading matching (DCM)
algorithm, which can use the obtained depth information to convert a dense
target set into multiple sparse target subsets and perform data association on
these sparse target subsets in order from near to far. By integrating the
pseudo-depth method and the DCM strategy into the data association process, we
propose a new tracker, called SparseTrack. SparseTrack provides a new
perspective for solving the challenging crowded scene MOT problem. Only using
IoU matching, SparseTrack achieves comparable performance with the
state-of-the-art (SOTA) methods on the MOT17 and MOT20 benchmarks. Code and
models are publicly available at \url{https://github.com/hustvl/SparseTrack}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with
  Uncertainty <span class="chip">ACM MM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02722v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02722v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhang, Weihua Chen, Yichen Lu, Tao Huang, Xiuyu Sun, Jian Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation is an effective paradigm for boosting the performance
of pocket-size model, especially when multiple teacher models are available,
the student would break the upper limit again. However, it is not economical to
train diverse teacher models for the disposable distillation. In this paper, we
introduce a new concept dubbed Avatars for distillation, which are the
inference ensemble models derived from the teacher. Concretely, (1) For each
iteration of distillation training, various Avatars are generated by a
perturbation transformation. We validate that Avatars own higher upper limit of
working capacity and teaching ability, aiding the student model in learning
diverse and receptive knowledge perspectives from the teacher model. (2) During
the distillation, we propose an uncertainty-aware factor from the variance of
statistical differences between the vanilla teacher and Avatars, to adjust
Avatars' contribution on knowledge transfer adaptively. Avatar Knowledge
Distillation AKD is fundamentally different from existing methods and refines
with the innovative view of unequal training. Comprehensive experiments
demonstrate the effectiveness of our Avatars mechanism, which polishes up the
state-of-the-art distillation methods for dense prediction without more extra
computational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object
Detection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation,
respectively. Code is available at https://github.com/Gumpest/AvatarKD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accurate and Efficient Stereo Matching via Attention Concatenation
  Volume 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12699v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12699v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gangwei Xu, Yun Wang, Junda Cheng, Jinhui Tang, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereo matching is a fundamental building block for many vision and robotics
applications. An informative and concise cost volume representation is vital
for stereo matching of high accuracy and efficiency. In this paper, we present
a novel cost volume construction method, named attention concatenation volume
(ACV), which generates attention weights from correlation clues to suppress
redundant information and enhance matching-related information in the
concatenation volume. The ACV can be seamlessly embedded into most stereo
matching networks, the resulting networks can use a more lightweight
aggregation network and meanwhile achieve higher accuracy. We further design a
fast version of ACV to enable real-time performance, named Fast-ACV, which
generates high likelihood disparity hypotheses and the corresponding attention
weights from low-resolution correlation clues to significantly reduce
computational and memory cost and meanwhile maintain a satisfactory accuracy.
The core idea of our Fast-ACV is volume attention propagation (VAP) which can
automatically select accurate correlation values from an upsampled correlation
volume and propagate these accurate values to the surroundings pixels with
ambiguous correlation clues. Furthermore, we design a highly accurate network
ACVNet and a real-time network Fast-ACVNet based on our ACV and Fast-ACV
respectively, which achieve the state-of-the-art performance on several
benchmarks (i.e., our ACVNet ranks the 2nd on KITTI 2015 and Scene Flow, and
the 3rd on KITTI 2012 and ETH3D among all the published methods; our
Fast-ACVNet outperforms almost all state-of-the-art real-time methods on Scene
Flow, KITTI 2012 and 2015 and meanwhile has better generalization ability)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TPAMI 2023. arXiv admin note: substantial text overlap
  with arXiv:2203.02146</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Robust Representation in Adversarial Training: Alignment and
  Exclusion Criteria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuoyan Zhou, Nannan Wang, Decheng Liu, Dawei Zhou, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are vulnerable to adversarial noise. Adversarial
Training (AT) has been demonstrated to be the most effective defense strategy
to protect neural networks from being fooled. However, we find AT omits to
learning robust features, resulting in poor performance of adversarial
robustness. To address this issue, we highlight two criteria of robust
representation: (1) Exclusion: \emph{the feature of examples keeps away from
that of other classes}; (2) Alignment: \emph{the feature of natural and
corresponding adversarial examples is close to each other}. These motivate us
to propose a generic framework of AT to gain robust representation, by the
asymmetric negative contrast and reverse attention. Specifically, we design an
asymmetric negative contrast based on predicted probabilities, to push away
examples of different classes in the feature space. Moreover, we propose to
weight feature by parameters of the linear classifier as the reverse attention,
to obtain class-aware feature and pull close the feature of the same class.
Empirical evaluations on three benchmark datasets show our methods greatly
advance the robustness of AT and achieve state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, Submitted to TIFS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image
  Enhancement for Gastrointestinal Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Triet M. Thai, Anh T. Vo, Hao K. Tieu, Linh N. P. Bui, Thien T. B. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, artificial intelligence has played an important role in
medicine and disease diagnosis, with many applications to be mentioned, one of
which is Medical Visual Question Answering (MedVQA). By combining computer
vision and natural language processing, MedVQA systems can assist experts in
extracting relevant information from medical image based on a given question
and providing precise diagnostic answers. The ImageCLEFmed-MEDVQA-GI-2023
challenge carried out visual question answering task in the gastrointestinal
domain, which includes gastroscopy and colonoscopy images. Our team approached
Task 1 of the challenge by proposing a multimodal learning method with image
enhancement to improve the VQA performance on gastrointestinal images. The
multimodal architecture is set up with BERT encoder and different pre-trained
vision models based on convolutional neural network (CNN) and Transformer
architecture for features extraction from question and endoscopy image. The
result of this study highlights the dominance of Transformer-based vision
models over the CNNs and demonstrates the effectiveness of the image
enhancement process, with six out of the eight vision models achieving better
F1-Score. Our best method, which takes advantages of BERT+BEiT fusion and image
enhancement, achieves up to 87.25% accuracy and 91.85% F1-Score on the
development test set, while also producing good result on the private test set
with accuracy of 82.01%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ImageCLEF2023 published version:
  https://ceur-ws.org/Vol-3497/paper-129.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Battle of the Backbones: A Large-Scale Comparison of <span class="highlight-title">Pretrain</span>ed Models
  across Computer Vision Tasks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Micah Goldblum, Hossein Souri, Renkun Ni, Manli Shu, Viraj Prabhu, Gowthami Somepalli, Prithvijit Chattopadhyay, Mark Ibrahim, Adrien Bardes, Judy Hoffman, Rama Chellappa, Andrew Gordon Wilson, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network based computer vision systems are typically built on a
backbone, a pretrained or randomly initialized feature extractor. Several years
ago, the default option was an ImageNet-trained convolutional neural network.
However, the recent past has seen the emergence of countless backbones
pretrained using various algorithms and datasets. While this abundance of
choice has led to performance increases for a range of systems, it is difficult
for practitioners to make informed decisions about which backbone to choose.
Battle of the Backbones (BoB) makes this choice easier by benchmarking a
diverse suite of pretrained models, including vision-language models, those
trained via self-supervised learning, and the Stable Diffusion backbone, across
a diverse set of computer vision tasks ranging from classification to object
detection to OOD generalization and more. Furthermore, BoB sheds light on
promising directions for the research community to advance computer vision by
illuminating strengths and weakness of existing approaches through a
comprehensive analysis conducted on more than 1500 training runs. While vision
transformers (ViTs) and self-supervised learning (SSL) are increasingly
popular, we find that convolutional neural networks pretrained in a supervised
fashion on large training sets still perform best on most tasks among the
models we consider. Moreover, in apples-to-apples comparisons on the same
architectures and similarly sized pretraining datasets, we find that SSL
backbones are highly competitive, indicating that future works should perform
SSL pretraining with advanced architectures and larger pretraining datasets. We
release the raw results of our experiments along with code that allows
researchers to put their own backbones through the gauntlet here:
https://github.com/hsouri/Battle-of-the-Backbones
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transcript to Video: Efficient Clip Sequencing from Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.11851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.11851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xiong, Fabian Caba Heilbron, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Among numerous videos shared on the web, well-edited ones always attract more
attention. However, it is difficult for inexperienced users to make well-edited
videos because it requires professional expertise and immense manual labor. To
meet the demands for non-experts, we present Transcript-to-Video -- a
weakly-supervised framework that uses texts as input to automatically create
video sequences from an extensive collection of shots. Specifically, we propose
a Content Retrieval Module and a Temporal Coherent Module to learn
visual-language representations and model shot sequencing styles, respectively.
For fast inference, we introduce an efficient search strategy for real-time
video clip sequencing. Quantitative results and user studies demonstrate
empirically that the proposed learning framework can retrieve content-relevant
shots while creating plausible video sequences in terms of style. Besides, the
run-time performance analysis shows that our framework can support real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report; Demo and project page at
  http://www.xiongyu.me/projects/transcript2video/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LymphoML: An interpretable artificial intelligence-based method
  identifies morphologic features that correlate with lymphoma subtype 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09574v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09574v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Shankar, Xiaoli Yang, Vrishab Krishna, Brent Tan, Oscar Silva, Rebecca Rojansky, Andrew Ng, Fabiola Valvert, Edward Briercheck, David Weinstock, Yasodha Natkunam, Sebastian Fernandez-Pol, Pranav Rajpurkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML's interpretable models, developed on a
limited volume of H&E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proceedings of the 3rd Machine Learning for Health
  symposium, Proceedings of Machine Learning Research (PMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Manifold-Aware Self-Training for Unsupervised Domain Adaptation on
  Regressing 6D Object Pose <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Zhang, Jiehong Lin, Ke Chen, Zelin Xu, Yaowei Wang, Kui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain gap between synthetic and real data in visual regression (e.g. 6D pose
estimation) is bridged in this paper via global feature alignment and local
refinement on the coarse classification of discretized anchor classes in target
space, which imposes a piece-wise target manifold regularization into
domain-invariant representation learning. Specifically, our method incorporates
an explicit self-supervised manifold regularization, revealing consistent
cumulative target dependency across domains, to a self-training scheme (e.g.
the popular Self-Paced Self-Training) to encourage more discriminative
transferable representations of regression tasks. Moreover, learning unified
implicit neural functions to estimate relative direction and distance of
targets to their nearest class bins aims to refine target classification
predictions, which can gain robust performance against inconsistent feature
scaling sensitive to UDA regressors. Experiment results on three public
benchmarks of the challenging 6D pose estimation task can verify the
effectiveness of our method, consistently achieving superior performance to the
state-of-the-art for UDA on 6D pose estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniMOS: A Universal Framework For Multi-Organ Segmentation Over
  Label-Constrained <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Li, Sheng Shao, Junyi Qu, Shuchao Pang, Mehmet A. Orgun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models for medical images can help physicians diagnose and
manage diseases. However, due to the fact that medical image annotation
requires a great deal of manpower and expertise, as well as the fact that
clinical departments perform image annotation based on task orientation, there
is the problem of having fewer medical image annotation data with more
unlabeled data and having many datasets that annotate only a single organ. In
this paper, we present UniMOS, the first universal framework for achieving the
utilization of fully and partially labeled images as well as unlabeled images.
Specifically, we construct a Multi-Organ Segmentation (MOS) module over
fully/partially labeled data as the basenet and designed a new target adaptive
loss. Furthermore, we incorporate a semi-supervised training module that
combines consistent regularization and pseudolabeling techniques on unlabeled
data, which significantly improves the segmentation of unlabeled data.
Experiments show that the framework exhibits excellent performance in several
medical image segmentation tasks compared to other advanced methods, and also
significantly improves data utilization and reduces annotation cost. Code and
models are available at: https://github.com/lw8807001/UniMOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by BIBM2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin Mehta, Mohammad Rastegari, Oncel Tuzel, Hadi Pouransari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of publicly available vision foundation models (VFMs), such as
CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed
with distinct capabilities stemming from their pre-training objectives. For
instance, CLIP excels in semantic understanding, while SAM specializes in
spatial understanding for segmentation. In this work, we introduce a simple
recipe to efficiently merge VFMs into a unified model that absorbs their
expertise. Our method integrates techniques of multi-task learning, continual
learning, and distillation. Further, it demands significantly less
computational cost compared to traditional multi-task training from scratch,
and it only needs a small fraction of the pre-training datasets that were
initially used to train individual models. By applying our method to SAM and
CLIP, we obtain SAM-CLIP: a unified model that combines the capabilities of SAM
and CLIP into a single vision transformer. Compared with deploying SAM and CLIP
independently, our merged model, SAM-CLIP, reduces storage and compute costs
for inference, making it well-suited for edge device applications. We show that
SAM-CLIP not only retains the foundational strengths of SAM and CLIP, but also
introduces synergistic functionalities, notably in zero-shot semantic
segmentation, where SAM-CLIP establishes new state-of-the-art results on 5
benchmarks. It outperforms previous models that are specifically designed for
this task by a large margin, including +6.8% and +5.9% mean IoU improvement on
Pascal-VOC and COCO-Stuff datasets, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diagonal Hierarchical Consistency Learning for Semi-supervised Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heejoon Koo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation, which is essential for many clinical
applications, has achieved almost human-level performance via data-driven deep
learning techniques. Nevertheless, its performance is predicated upon the
costly process of manually annotating a vast amount of medical images. To this
end, we propose a novel framework for robust semi-supervised medical image
segmentation using diagonal hierarchical consistency learning (DiHC-Net).
First, it is composed of multiple sub-models with identical multi-scale
architecture but with distinct sub-layers, such as up-sampling and
normalisation layers. Second, along with mutual consistency, a novel diagonal
hierarchical consistency is enforced between one model's intermediate and final
prediction and other models' soft pseudo labels in a diagonal hierarchical
fashion. Experimental results verify the efficacy of our simple framework,
outperforming all previous approaches on public Left Atrium (LA) dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, and 2 tables. Corrected typos and errors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding AI-Generated Faces in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonzalo J. Aniano Porcile, Jack Gindi, Shivansh Mundra, James R. Verbus, Hany Farid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-based image generation has continued to rapidly improve, producing
increasingly more realistic images with fewer obvious visual flaws.
AI-generated images are being used to create fake online profiles which in turn
are being used for spam, fraud, and disinformation campaigns. As the general
problem of detecting any type of manipulated or synthesized content is
receiving increasing attention, here we focus on a more narrow task of
distinguishing a real face from an AI-generated face. This is particularly
applicable when tackling inauthentic online accounts with a fake user profile
photo. We show that by focusing on only faces, a more resilient and
general-purpose artifact can be detected that allows for the detection of
AI-generated faces from a variety of GAN- and diffusion-based synthesis
engines, and across image resolutions (as low as 128 x 128 pixels) and
qualities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Removed anonymization of the LinkedIn platform</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn the Time to Learn: Replay Scheduling in Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcus Klasson, Hedvig Kjellström, Cheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Replay methods are known to be successful at mitigating catastrophic
forgetting in continual learning scenarios despite having limited access to
historical data. However, storing historical data is cheap in many real-world
settings, yet replaying all historical data is often prohibited due to
processing time constraints. In such settings, we propose that continual
learning systems should learn the time to learn and schedule which tasks to
replay at different time steps. We first demonstrate the benefits of our
proposal by using Monte Carlo tree search to find a proper replay schedule, and
show that the found replay schedules can outperform fixed scheduling policies
when combined with various replay methods in different continual learning
settings. Additionally, we propose a framework for learning replay scheduling
policies with reinforcement learning. We show that the learned policies can
generalize better in new continual learning scenarios compared to equally
replaying all seen tasks, without added computational cost. Our study reveals
the importance of learning the time to learn in continual learning, which
brings current research closer to real-world needs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR (2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Machine Learning in Image-Based and Clinical Biomedicine:
  <span class="highlight-title">Survey</span> and Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02332v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02332v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisa Warner, Joonsang Lee, William Hsu, Tanveer Syeda-Mahmood, Charles Kahn, Olivier Gevaert, Arvind Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) applications in medical artificial intelligence (AI)
systems have shifted from traditional and statistical methods to increasing
application of deep learning models. This survey navigates the current
landscape of multimodal ML, focusing on its profound impact on medical image
analysis and clinical decision support systems. Emphasizing challenges and
innovations in addressing multimodal representation, fusion, translation,
alignment, and co-learning, the paper explores the transformative potential of
multimodal models for clinical predictions. It also questions practical
implementation of such models, bringing attention to the dynamics between
decision support systems and healthcare providers. Despite advancements,
challenges such as data biases and the scarcity of "big data" in many
biomedical domains persist. We conclude with a discussion on effective
innovation and collaborative efforts to further the miss
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Helmet Violation Detection in AI City Challenge 2023 with
  Genetic Algorithm-Enhanced YOLOv5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elham Soltanikazemi, Ashwin Dhakal, Bijaya Kumar Hatuwal, Imad Eddine Toubal, Armstrong Aboah, Kannappan Palaniappan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research focuses on real-time surveillance systems as a means for
tackling the issue of non-compliance with helmet regulations, a practice that
considerably amplifies the risk for motorcycle drivers or riders. Despite the
well-established advantages of helmet usage, achieving widespread compliance
remains challenging due to diverse contributing factors. To effectively address
this concern, real-time monitoring and enforcement of helmet laws have been
proposed as a plausible solution. However, previous attempts at real-time
helmet violation detection have been hindered by their limited ability to
operate in real-time. To overcome this limitation, the current paper introduces
a novel real-time helmet violation detection system that utilizes the YOLOv5
single-stage object detection model. This model is trained on the 2023 NVIDIA
AI City Challenge 2023 Track 5 dataset. The optimal hyperparameters for
training the model are determined using genetic algorithms. Additionally, data
augmentation and various sampling techniques are implemented to enhance the
model's performance. The efficacy of the models is evaluated using precision,
recall, and mean Average Precision (mAP) metrics. The results demonstrate
impressive precision, recall, and mAP scores of 0.848, 0.599, and 0.641,
respectively for the training data. Furthermore, the model achieves notable mAP
score of 0.6667 for the test datasets, leading to a commendable 4th place rank
in the public leaderboard. This innovative approach represents a notable
breakthrough in the field and holds immense potential to substantially enhance
motorcycle safety. By enabling real-time monitoring and enforcement
capabilities, this system has the capacity to contribute towards increased
compliance with helmet laws, thereby effectively reducing the risks faced by
motorcycle riders and passengers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CartiMorph: a framework for automated knee articular cartilage
  morphometrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01981v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01981v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongcheng Yao, Junru Zhong, Liping Zhang, Sheheryar Khan, Weitian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CartiMorph, a framework for automated knee articular cartilage
morphometrics. It takes an image as input and generates quantitative metrics
for cartilage subregions, including the percentage of full-thickness cartilage
loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the
power of deep learning models for hierarchical image feature representation.
Deep learning models were trained and validated for tissue segmentation,
template construction, and template-to-image registration. We established
methods for surface-normal-based cartilage thickness mapping, FCL estimation,
and rule-based cartilage parcellation. Our cartilage thickness map showed less
error in thin and peripheral regions. We evaluated the effectiveness of the
adopted segmentation model by comparing the quantitative metrics obtained from
model segmentation and those from manual segmentation. The root-mean-squared
deviation of the FCL measurements was less than 8%, and strong correlations
were observed for the mean thickness (Pearson's correlation coefficient $\rho
\in [0.82,0.97]$), surface area ($\rho \in [0.82,0.98]$) and volume ($\rho \in
[0.89,0.98]$) measurements. We compared our FCL measurements with those from a
previous study and found that our measurements deviated less from the ground
truths. We observed superior performance of the proposed rule-based cartilage
parcellation method compared with the atlas-based approach. CartiMorph has the
potential to promote imaging biomarkers discovery for knee osteoarthritis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint is an proofread version of a paper published in Medical
  Image Analysis (2023), which can be found at
  https://doi.org/10.1016/j.media.2023.103035</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Variational Embedding Collaborative Filtering <span class="chip">PAKDD2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narges Sadat Fazeli Dehkordi, Hadi Zare, Parham Moradi, Mahdi Jalili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The customization of recommended content to users holds significant
importance in enhancing user experiences across a wide spectrum of applications
such as e-commerce, music, and shopping. Graph-based methods have achieved
considerable performance by capturing user-item interactions. However, these
methods tend to utilize randomly constructed embeddings in the dataset used for
training the recommender, which lacks any user preferences. Here, we propose
the concept of variational embeddings as a means of pre-training the
recommender system to improve the feature propagation through the layers of
graph convolutional networks (GCNs). The graph variational embedding
collaborative filtering (GVECF) is introduced as a novel framework to
incorporate representations learned through a variational graph auto-encoder
which are embedded into a GCN-based collaborative filtering. This approach
effectively transforms latent high-order user-item interactions into more
trainable vectors, ultimately resulting in better performance in terms of
recall and normalized discounted cumulative gain(NDCG) metrics. The experiments
conducted on benchmark datasets demonstrate that our proposed method achieves
up to 13.78% improvement in the recall over the test data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for PAKDD2024 conference,12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control in Hybrid Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Rüdel, Jochen L. Leidner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customer data typically is held in database systems, which can be seen as
rule-based knowledge base, whereas businesses increasingly want to benefit from
the capabilities of large, pre-trained language models.
  In this technical report, we describe a case study of how a commercial rule
engine and an integrated neural chatbot may be integrated, and what level of
control that particular integration mode leads to. We also discuss alternative
ways (including past ways realized in other systems) how researchers strive to
maintain control and avoid what has recently been called model "hallucination".
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Text Retrieval with Progressive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Wu, Yulei Qin, Enwei Zhang, Zihan Xu, Yuting Gao, Ke Li, Xing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmentation has become an effective solution to empower large
language models (LLMs) with external and verified knowledge sources from the
database, which overcomes the limitations and hallucinations of LLMs in
handling up-to-date and domain-specific information. However, existing
embedding models for text retrieval usually have three non-negligible
limitations. First, the number and diversity of samples in a batch are too
restricted to supervise the modeling of textual nuances at scale. Second, the
high proportional noise are detrimental to the semantic correctness and
consistency of embeddings. Third, the equal treatment to easy and difficult
samples would cause sub-optimum convergence of embeddings with poorer
generalization. In this paper, we propose the PEG, a progressively learned
embeddings for robust text retrieval. Specifically, we increase the training
in-batch negative samples to 80,000, and for each query, we extracted five hard
negatives. Concurrently, we incorporated a progressive learning mechanism,
enabling the model to dynamically modulate its attention to the samples
throughout the entire training process. Additionally, PEG is trained on more
than 100 million data, encompassing a wide range of domains (e.g., finance,
medicine, and tourism) and covering various tasks (e.g., question-answering,
machine reading comprehension, and similarity matching). Extensive experiments
conducted on C-MTEB and DuReader demonstrate that PEG surpasses
state-of-the-art embeddings in retrieving true positives, highlighting its
significant potential for applications in LLMs. Our model is publicly available
at https://huggingface.co/TownsWu/PEG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Modeling Based Automatic Video Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Hong Huang, Chao-Han Huck Yang, Pin-Yu Chen, Min-Hung Chen, Marcel Worring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of video summarization is to shorten videos automatically while
retaining the key information necessary to convey the overall story. Video
summarization methods mainly rely on visual factors, such as visual
consecutiveness and diversity, which may not be sufficient to fully understand
the content of the video. There are other non-visual factors, such as
interestingness, representativeness, and storyline consistency that should also
be considered for generating high-quality video summaries. Current methods do
not adequately take into account these non-visual factors, resulting in
suboptimal performance. In this work, a new approach to video summarization is
proposed based on insights gained from how humans create ground truth video
summaries. The method utilizes a conditional modeling perspective and
introduces multiple meaningful random variables and joint distributions to
characterize the key components of video summarization. Helper distributions
are employed to improve the training of the model. A conditional attention
module is designed to mitigate potential performance degradation in the
presence of multi-modal input. The proposed video summarization method
incorporates the above innovative design choices that aim to narrow the gap
between human-generated and machine-generated video summaries. Extensive
experiments show that the proposed approach outperforms existing methods and
achieves state-of-the-art performance on commonly used video summarization
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  arXiv admin note: substantial text overlap with arXiv:2305.00455</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-view Graph Convolution for Participant Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolong Liu, Liangwei Yang, Chen Wang, Mingdai Yang, Zhiwei Liu, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social networks have become essential for people's lives. The proliferation
of web services further expands social networks at an unprecedented scale,
leading to immeasurable commercial value for online platforms. Recently, the
group buying (GB) business mode is prevalent and also becoming more popular in
E-commerce. GB explicitly forms groups of users with similar interests to
secure better discounts from the merchants, often operating within social
networks. It is a novel way to further unlock the commercial value by
explicitly utilizing the online social network in E-commerce. Participant
recommendation, a fundamental problem emerging together with GB, aims to find
the participants for a launched group buying process with an initiator and a
target item to increase the GB success rate. This paper proposes Multi-View
Graph Convolution for Participant Recommendation (MVPRec) to tackle this
problem. To differentiate the roles of users (Initiator/Participant) within the
GB process, we explicitly reconstruct historical GB data into initiator-view
and participant-view graphs. Together with the social graph, we obtain a
multi-view user representation with graph encoders. Then MVPRec fuses the GB
and social representation with an attention module to obtain the user
representation and learns a matching score with the initiator's social friends
via a multi-head attention mechanism. Social friends with the Top-k matching
score are recommended for the corresponding GB process. Experiments on three
datasets justify the effectiveness of MVPRec in the emerging participant
recommendation problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 2023 IEEE International Conference on Big Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inverse Learning with Extremely Sparse Feedback for Recommendation <span class="chip">WSDM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanyu Lin, Chen Gao, Yu Zheng, Yinfeng Li, Jianxin Chang, Yanan Niu, Yang Song, Kun Gai, Zhiheng Li, Depeng Jin, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern personalized recommendation services often rely on user feedback,
either explicit or implicit, to improve the quality of services. Explicit
feedback refers to behaviors like ratings, while implicit feedback refers to
behaviors like user clicks. However, in the scenario of full-screen video
viewing experiences like Tiktok and Reels, the click action is absent,
resulting in unclear feedback from users, hence introducing noises in modeling
training. Existing approaches on de-noising recommendation mainly focus on
positive instances while ignoring the noise in a large amount of sampled
negative feedback. In this paper, we propose a meta-learning method to annotate
the unlabeled data from loss and gradient perspectives, which considers the
noises in both positive and negative instances. Specifically, we first propose
an Inverse Dual Loss (IDL) to boost the true label learning and prevent the
false label learning. Then we further propose an Inverse Gradient (IG) method
to explore the correct updating gradient and adjust the updating based on
meta-learning. Finally, we conduct extensive experiments on both benchmark and
industrial datasets where our proposed method can significantly improve AUC by
9.25% against state-of-the-art methods. Further analysis verifies the proposed
inverse learning framework is model-agnostic and can improve a variety of
recommendation backbones. The source code, along with the best hyper-parameter
settings, is available at this link:
https://github.com/Guanyu-Lin/InverseLearning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WSDM 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">122</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hourglass Tokenizer for Efficient <span class="highlight-title">Transformer</span>-Based 3D Human Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Our source code will be open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient
  Language Model Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Guo, Philip Greengard, Eric P. Xing, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a simple approach for memory-efficient adaptation of pretrained
language models. Our approach uses an iterative algorithm to decompose each
pretrained matrix into a high-precision low-rank component and a
memory-efficient quantized component. During finetuning, the quantized
component remains fixed and only the low-rank component is updated. We present
an integer linear programming formulation of the quantization component which
enables dynamic configuration of quantization parameters (e.g., bit-width,
block size) for each matrix given an overall target memory budget. We further
explore a data-aware version of the algorithm which uses an approximation of
the Fisher information matrix to weight the reconstruction objective during
matrix decomposition. Experiments on adapting RoBERTa and LLaMA-2 (7B and 70B)
demonstrate that our low-rank plus quantized matrix decomposition approach
(LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and moreover enables
more aggressive quantization. For example, on the OpenAssistant benchmark
LQ-LoRA is able to learn a 2.5-bit LLaMA-2 model that is competitive with a
model finetuned with 4-bit QLoRA. When finetuned on a language modeling
calibration dataset, LQ-LoRA can also be used for model compression; in this
setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when
including the low-rank components and requires 27GB of GPU memory) is
competitive with the original model in full precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk-averse Batch Active Inverse Reward Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panagiotis Liampas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing a perfect reward function that depicts all the aspects of the
intended behavior is almost impossible, especially generalizing it outside of
the training environments. Active Inverse Reward Design (AIRD) proposed the use
of a series of queries, comparing possible reward functions in a single
training environment. This allows the human to give information to the agent
about suboptimal behaviors, in order to compute a probability distribution over
the intended reward function. However, it ignores the possibility of unknown
features appearing in real-world environments, and the safety measures needed
until the agent completely learns the reward function. I improved this method
and created Risk-averse Batch Active Inverse Reward Design (RBAIRD), which
constructs batches, sets of environments the agent encounters when being used
in the real world, processes them sequentially, and, for a predetermined number
of iterations, asks queries that the human needs to answer for each environment
of the batch. After this process is completed in one batch, the probabilities
have been improved and are transferred to the next batch. This makes it capable
of adapting to real-world scenarios and learning how to treat unknown features
it encounters for the first time. I also integrated a risk-averse planner,
similar to that of Inverse Reward Design (IRD), which samples a set of reward
functions from the probability distribution and computes a trajectory that
takes the most certain rewards possible. This ensures safety while the agent is
still learning the reward function, and enables the use of this approach in
situations where cautiousness is vital. RBAIRD outperformed the previous
approaches in terms of efficiency, accuracy, and action certainty, demonstrated
quick adaptability to new, unknown features, and can be more widely used for
the alignment of crucial, powerful AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BrainWash: A Poisoning Attack to Forget in Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Abbasi, Parsa Nooralinejad, Hamed Pirsiavash, Soheil Kolouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning has gained substantial attention within the deep learning
community, offering promising solutions to the challenging problem of
sequential learning. Yet, a largely unexplored facet of this paradigm is its
susceptibility to adversarial attacks, especially with the aim of inducing
forgetting. In this paper, we introduce "BrainWash," a novel data poisoning
method tailored to impose forgetting on a continual learner. By adding the
BrainWash noise to a variety of baselines, we demonstrate how a trained
continual learner can be induced to forget its previously learned tasks
catastrophically, even when using these continual learning baselines. An
important feature of our approach is that the attacker requires no access to
previous tasks' data and is armed merely with the model's current parameters
and the data belonging to the most recent task. Our extensive experiments
highlight the efficacy of BrainWash, showcasing degradation in performance
across various regularization-based continual learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Lip Segmentation Techniques in Computer Vision: A Comparative
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro B. S. Masur, Francisco Braulio Oliveira, Lucas Moreira Medino, Emanuel Huber, Milene Haraguchi Padilha, Cassio de Alcantara, Renata Sellaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lip segmentation is crucial in computer vision, especially for lip reading.
Despite extensive face segmentation research, lip segmentation has received
limited attention. The aim of this study is to compare state-of-the-art lip
segmentation models using a standardized setting and a publicly available
dataset. Five techniques, namely EHANet, Mask2Former, BiSeNet V2, PIDNet, and
STDC1, are qualitatively selected based on their reported performance,
inference time, code availability, recency, and popularity. The CelebAMask-HQ
dataset, comprising manually annotated face images, is used to fairly assess
the lip segmentation performance of the selected models. Inference experiments
are conducted on a Raspberry Pi4 to emulate limited computational resources.
The results show that Mask2Former and EHANet have the best performances in
terms of mIoU score. BiSeNet V2 demonstrate competitive performance, while
PIDNet excels in recall but has lower precision. Most models present inference
time ranging from 1000 to around 3000 milliseconds on a Raspberry Pi4, with
PIDNet having the lowest mean inference time. This study provides a
comprehensive evaluation of lip segmentation models, highlighting their
performance and inference times. The findings contribute to the development of
lightweight techniques and establish benchmarks for future advances in lip
segmentation, especially in IoT and edge computing scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine-Learned Atomic Cluster Expansion Potentials for Fast and
  Quantum-Accurate Thermal Simulations of Wurtzite AlN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guang Yang, Yuan-Bin Liu, Lei Yang, Bing-Yang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using the atomic cluster expansion (ACE) framework, we develop a machine
learning interatomic potential for fast and accurately modelling the phonon
transport properties of wurtzite aluminum nitride. The predictive power of the
ACE potential against density functional theory (DFT) is demonstrated across a
broad range of properties of w-AlN, including ground-state lattice parameters,
specific heat capacity, coefficients of thermal expansion, bulk modulus, and
harmonic phonon dispersions. Validation of lattice thermal conductivity is
further carried out by comparing the ACE-predicted values to the DFT
calculations and experiments, exhibiting the overall capability of our ACE
potential in sufficiently describing anharmonic phonon interactions. As a
practical application, we perform a lattice dynamics analysis using the
potential to unravel the effects of biaxial strains on thermal conductivity and
phonon properties of w-AlN, which is identified as a significant tuning factor
for near-junction thermal design of w-AlN-based electronics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Previous Facial Action Units Knowledge for Emotion
  Recognition on Faces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro B. S. Masur, Willams Costa, Lucas S. Figueredo, Veronica Teichrieb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People naturally understand emotions, thus permitting a machine to do the
same could open new paths for human-computer interaction. Facial expressions
can be very useful for emotion recognition techniques, as these are the biggest
transmitters of non-verbal cues capable of being correlated with emotions.
Several techniques are based on Convolutional Neural Networks (CNNs) to extract
information in a machine learning process. However, simple CNNs are not always
sufficient to locate points of interest on the face that can be correlated with
emotions. In this work, we intend to expand the capacity of emotion recognition
techniques by proposing the usage of Facial Action Units (AUs) recognition
techniques to recognize emotions. This recognition will be based on the Facial
Action Coding System (FACS) and computed by a machine learning system. In
particular, our method expands over EmotiRAM, an approach for multi-cue emotion
recognition, in which we improve over their facial encoding module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Supervision Levels Trade-Offs for Infrared-Based People
  Counting <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Latortue, Moetez Kdayem, Fidel A Guerrero Peña, Eric Granger, Marco Pedersoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection models are commonly used for people counting (and
localization) in many applications but require a dataset with costly bounding
box annotations for training. Given the importance of privacy in people
counting, these models rely more and more on infrared images, making the task
even harder. In this paper, we explore how weaker levels of supervision can
affect the performance of deep person counting architectures for image
classification and point-level localization. Our experiments indicate that
counting people using a CNN Image-Level model achieves competitive results with
YOLO detectors and point-level models, yet provides a higher frame rate and a
similar amount of model parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Training Distributions with Scalable Online Bilevel
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Grangier, Pierre Ablin, Awni Hannun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large neural networks pretrained on web-scale corpora are central to modern
machine learning. In this paradigm, the distribution of the large,
heterogeneous pretraining data rarely matches that of the application domain.
This work considers modifying the pretraining distribution in the case where
one has a small sample of data reflecting the targeted test conditions. We
propose an algorithm motivated by a recent formulation of this setting as an
online, bilevel optimization problem. With scalability in mind, our algorithm
prioritizes computing gradients at training points which are likely to most
improve the loss on the targeted distribution. Empirically, we show that in
some cases this approach is beneficial over existing strategies from the domain
adaptation literature but may not succeed in other cases. We propose a simple
test to evaluate when our approach can be expected to work well and point
towards further research to address current limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Efficient CVaR RL in Low-rank MDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulai Zhao, Wenhao Zhan, Xiaoyan Hu, Ho-fung Leung, Farzan Farnia, Wen Sun, Jason D. Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study risk-sensitive Reinforcement Learning (RL), where we aim to maximize
the Conditional Value at Risk (CVaR) with a fixed risk tolerance $\tau$. Prior
theoretical work studying risk-sensitive RL focuses on the tabular Markov
Decision Processes (MDPs) setting. To extend CVaR RL to settings where state
space is large, function approximation must be deployed. We study CVaR RL in
low-rank MDPs with nonlinear function approximation. Low-rank MDPs assume the
underlying transition kernel admits a low-rank decomposition, but unlike prior
linear models, low-rank MDPs do not assume the feature or state-action
representation is known. We propose a novel Upper Confidence Bound (UCB)
bonus-driven algorithm to carefully balance the interplay between exploration,
exploitation, and representation learning in CVaR RL. We prove that our
algorithm achieves a sample complexity of $\tilde{O}\left(\frac{H^7 A^2
d^4}{\tau^2 \epsilon^2}\right)$ to yield an $\epsilon$-optimal CVaR, where $H$
is the length of each episode, $A$ is the capacity of action space, and $d$ is
the dimension of representations. Computational-wise, we design a novel
discretized Least-Squares Value Iteration (LSVI) algorithm for the CVaR
objective as the planning oracle and show that we can find the near-optimal
policy in a polynomial running time with a Maximum Likelihood Estimation
oracle. To our knowledge, this is the first provably efficient CVaR RL
algorithm in low-rank MDPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first three authors contribute equally and are ordered randomly</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Can AutoML Do For Continual Learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Kilickaya, Joaquin Vanschoren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This position paper outlines the potential of AutoML for incremental
(continual) learning to encourage more research in this direction. Incremental
learning involves incorporating new data from a stream of tasks and
distributions to learn enhanced deep representations and adapt better to new
tasks. However, a significant limitation of incremental learners is that most
current techniques freeze the backbone architecture, hyperparameters, and the
order & structure of the learning tasks throughout the learning and adaptation
process. We strongly believe that AutoML offers promising solutions to address
these limitations, enabling incremental learning to adapt to more diverse
real-world tasks. Therefore, instead of directly proposing a new method, this
paper takes a step back by posing the question: "What can AutoML do for
incremental learning?" We outline three key areas of research that can
contribute to making incremental learners more dynamic, highlighting concrete
opportunities to apply AutoML methods in novel ways as well as entirely new
challenges for AutoML research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NNG-Mix: Improving Semi-supervised Anomaly Detection with Pseudo-anomaly
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dong, Gaëtan Frusque, Yue Zhao, Eleni Chatzi, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection (AD) is essential in identifying rare and often critical
events in complex systems, finding applications in fields such as network
intrusion detection, financial fraud detection, and fault detection in
infrastructure and industrial systems. While AD is typically treated as an
unsupervised learning task due to the high cost of label annotation, it is more
practical to assume access to a small set of labeled anomaly samples from
domain experts, as is the case for semi-supervised anomaly detection.
Semi-supervised and supervised approaches can leverage such labeled data,
resulting in improved performance. In this paper, rather than proposing a new
semi-supervised or supervised approach for AD, we introduce a novel algorithm
for generating additional pseudo-anomalies on the basis of the limited labeled
anomalies and a large volume of unlabeled data. This serves as an augmentation
to facilitate the detection of new anomalies. Our proposed algorithm, named
Nearest Neighbor Gaussian Mixup (NNG-Mix), efficiently integrates information
from both labeled and unlabeled data to generate pseudo-anomalies. We compare
the performance of this novel algorithm with commonly applied augmentation
techniques, such as Mixup and Cutout. We evaluate NNG-Mix by training various
existing semi-supervised and supervised anomaly detection algorithms on the
original training data along with the generated pseudo-anomalies. Through
extensive experiments on 57 benchmark datasets in ADBench, reflecting different
data types, we demonstrate that NNG-Mix outperforms other data augmentation
methods. It yields significant performance improvements compared to the
baselines trained exclusively on the original training data. Notably, NNG-Mix
yields up to 16.4%, 8.8%, and 8.0% improvements on Classical, CV, and NLP
datasets in ADBench. Our source code will be available at
https://github.com/donghao51/NNG-Mix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correlated Attention in <span class="highlight-title">Transformer</span>s for Multivariate Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang Minh Nguyen, Lam M. Nguyen, Subhro Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series (MTS) analysis prevails in real-world applications
such as finance, climate science and healthcare. The various self-attention
mechanisms, the backbone of the state-of-the-art Transformer-based models,
efficiently discover the temporal dependencies, yet cannot well capture the
intricate cross-correlation between different features of MTS data, which
inherently stems from complex dynamical systems in practice. To this end, we
propose a novel correlated attention mechanism, which not only efficiently
captures feature-wise dependencies, but can also be seamlessly integrated
within the encoder blocks of existing well-known Transformers to gain
efficiency improvement. In particular, correlated attention operates across
feature channels to compute cross-covariance matrices between queries and keys
with different lag values, and selectively aggregate representations at the
sub-series level. This architecture facilitates automated discovery and
representation learning of not only instantaneous but also lagged
cross-correlations, while inherently capturing time series auto-correlation.
When combined with prevalent Transformer baselines, correlated attention
mechanism constitutes a better alternative for encoder-only architectures,
which are suitable for a wide range of tasks including imputation, anomaly
detection and classification. Extensive experiments on the aforementioned tasks
consistently underscore the advantages of correlated attention mechanism in
enhancing base Transformer models, and demonstrate our state-of-the-art results
in imputation, anomaly detection and classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimation of entropy-regularized optimal transport maps between
  non-compactly supported measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Werenski, James M. Murphy, Shuchin Aeron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of estimating entropy-regularized optimal
transport (EOT) maps with squared-Euclidean cost between source and target
measures that are subGaussian. In the case that the target measure is compactly
supported or strongly log-concave, we show that for a recently proposed
in-sample estimator, the expected squared $L^2$-error decays at least as fast
as $O(n^{-1/3})$ where $n$ is the sample size. For the general subGaussian case
we show that the expected $L^1$-error decays at least as fast as $O(n^{-1/6})$,
and in both cases we have polynomial dependence on the regularization
parameter. While these results are suboptimal compared to known results in the
case of compactness of both the source and target measures (squared $L^2$-error
converging at a rate $O(n^{-1})$) and for when the source is subGaussian while
the target is compactly supported (squared $L^2$-error converging at a rate
$O(n^{-1/2})$), their importance lie in eliminating the compact support
requirements. The proof technique makes use of a bias-variance decomposition
where the variance is controlled using standard concentration of measure
results and the bias is handled by T1-transport inequalities along with sample
complexity results in estimation of EOT cost under subGaussian assumptions. Our
experimental results point to a looseness in controlling the variance terms and
we conclude by posing several open problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ovarian Cancer Data Analysis using Deep Learning: A Systematic <span class="highlight-title">Review</span>
  from the Perspectives of Key Features of Data Analysis and AI Assurance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muta Tah Hira, Mohammad A. Razzaque, Mosharraf Sarker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background and objectives: By extracting this information, Machine or Deep
Learning (ML/DL)-based autonomous data analysis tools can assist clinicians and
cancer researchers in discovering patterns and relationships from complex data
sets. Many DL-based analyses on ovarian cancer (OC) data have recently been
published. These analyses are highly diverse in various aspects of cancer
(e.g., subdomain(s) and cancer type they address) and data analysis features.
However, a comprehensive understanding of these analyses in terms of these
features and AI assurance (AIA) is currently lacking. This systematic review
aims to fill this gap by examining the existing literature and identifying
important aspects of OC data analysis using DL, explicitly focusing on the key
features and AI assurance perspectives. Methods: The PRISMA framework was used
to conduct comprehensive searches in three journal databases. Only studies
published between 2015 and 2023 in peer-reviewed journals were included in the
analysis. Results: In the review, a total of 96 DL-driven analyses were
examined. The findings reveal several important insights regarding DL-driven
ovarian cancer data analysis: - Most studies 71% (68 out of 96) focused on
detection and diagnosis, while no study addressed the prediction and prevention
of OC. - The analyses were predominantly based on samples from a non-diverse
population (75% (72/96 studies)), limited to a geographic location or country.
- Only a small proportion of studies (only 33% (32/96)) performed integrated
analyses, most of which used homogeneous data (clinical or omics). - Notably, a
mere 8.3% (8/96) of the studies validated their models using external and
diverse data sets, highlighting the need for enhanced model validation, and -
The inclusion of AIA in cancer data analysis is in a very early stage; only
2.1% (2/96) explicitly addressed AIA through explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Calibration of Market Simulations using Neural Density Estimators
  and Embedding Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namid R. Stillman, Rory Baggott, Justin Lyon, Jianfei Zhang, Dingqiu Zhu, Tao Chen, Perukrishnen Vytelingum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to construct a realistic simulator of financial exchanges,
including reproducing the dynamics of the limit order book, can give insight
into many counterfactual scenarios, such as a flash crash, a margin call, or
changes in macroeconomic outlook. In recent years, agent-based models have been
developed that reproduce many features of an exchange, as summarised by a set
of stylised facts and statistics. However, the ability to calibrate simulators
to a specific period of trading remains an open challenge. In this work, we
develop a novel approach to the calibration of market simulators by leveraging
recent advances in deep learning, specifically using neural density estimators
and embedding networks. We demonstrate that our approach is able to correctly
identify high probability parameter sets, both when applied to synthetic and
historical data, and without reliance on manually selected or weighted
ensembles of stylised facts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4th ACM International Conference on AI in Finance (ICAIF 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Certification of Distributional Individual Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Wicker, Vihari Piratia, Adrian Weller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing formal guarantees of algorithmic fairness is of paramount
importance to socially responsible deployment of machine learning algorithms.
In this work, we study formal guarantees, i.e., certificates, for individual
fairness (IF) of neural networks. We start by introducing a novel convex
approximation of IF constraints that exponentially decreases the computational
cost of providing formal guarantees of local individual fairness. We highlight
that prior methods are constrained by their focus on global IF certification
and can therefore only scale to models with a few dozen hidden neurons, thus
limiting their practical impact. We propose to certify distributional
individual fairness which ensures that for a given empirical distribution and
all distributions within a $\gamma$-Wasserstein ball, the neural network has
guaranteed individually fair predictions. Leveraging developments in
quasi-convex optimization, we provide novel and efficient certified bounds on
distributional individual fairness and show that our method allows us to
certify and regularize neural networks that are several orders of magnitude
larger than those considered by prior works. Moreover, we study real-world
distribution shifts and find our bounds to be a scalable, practical, and sound
source of IF guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 Pages, Neural Information Processing Systems 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning: Applications and the Road Forward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eli Verwimp, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L. Hayes, Eyke Hüllermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu, Andreas S. Tolias, Joost van de Weijer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars, Gido M. van de Ven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning is a sub-field of machine learning, which aims to allow
machine learning models to continuously learn on new data, by accumulating
knowledge without forgetting what was learned in the past. In this work, we
take a step back, and ask: "Why should one care about continual learning in the
first place?". We set the stage by surveying recent continual learning papers
published at three major machine learning conferences, and show that
memory-constrained settings dominate the field. Then, we discuss five open
problems in machine learning, and even though they seem unrelated to continual
learning at first sight, we show that continual learning will inevitably be
part of their solution. These problems are model-editing, personalization,
on-device learning, faster (re-)training and reinforcement learning. Finally,
by comparing the desiderata from these unsolved problems and the current
assumptions in continual learning, we highlight and discuss four future
directions for continual learning research. We hope that this work offers an
interesting perspective on the future of continual learning, while displaying
its potential value and the paths we have to pursue in order to make it
successful. This work is the result of the many discussions the authors had at
the Dagstuhl seminar on Deep Continual Learning, in March 2023.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Surface-to-Air Missile Engagement Zone Prediction Using
  Simulation and Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao P. A. Dantas, Diego Geraldo, Felipe L. L. Medeiros, Marcos R. O. A. Maximo, Takashi Yoneyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface-to-Air Missiles (SAMs) are crucial in modern air defense systems. A
critical aspect of their effectiveness is the Engagement Zone (EZ), the spatial
region within which a SAM can effectively engage and neutralize a target.
Notably, the EZ is intrinsically related to the missile's maximum range; it
defines the furthest distance at which a missile can intercept a target. The
accurate computation of this EZ is essential but challenging due to the dynamic
and complex factors involved, which often lead to high computational costs and
extended processing times when using conventional simulation methods. In light
of these challenges, our study investigates the potential of machine learning
techniques, proposing an approach that integrates machine learning with a
custom-designed simulation tool to train supervised algorithms. We leverage a
comprehensive dataset of pre-computed SAM EZ simulations, enabling our model to
accurately predict the SAM EZ for new input parameters. It accelerates SAM EZ
simulations, enhances air defense strategic planning, and provides real-time
insights, improving SAM system performance. The study also includes a
comparative analysis of machine learning algorithms, illuminating their
capabilities and performance metrics and suggesting areas for future research,
highlighting the transformative potential of machine learning in SAM EZ
simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs as Visual Explainers: Advancing Image Classification with Evolving
  Visual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhao Han, Le Zhuo, Yue Liao, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) offer a promising paradigm for image
classification by comparing the similarity between images and class embeddings.
A critical challenge lies in crafting precise textual representations for class
names. While previous studies have leveraged recent advancements in large
language models (LLMs) to enhance these descriptors, their outputs often suffer
from ambiguity and inaccuracy. We identify two primary causes: 1) The prevalent
reliance on textual interactions with LLMs, leading to a mismatch between the
generated text and the visual content in VLMs' latent space - a phenomenon we
term the "explain without seeing" dilemma. 2) The oversight of the inter-class
relationships, resulting in descriptors that fail to differentiate similar
classes effectively. To address these issues, we propose a novel image
classification framework combining VLMs with LLMs, named Iterative Optimization
with Visual Feedback. In particular, our method develops an LLM-based agent,
employing an evolutionary optimization strategy to refine class descriptors.
Crucially, we incorporate visual feedback from VLM classification metrics,
thereby guiding the optimization process with concrete visual data. Our method
leads to improving accuracy on a wide range of image classification benchmarks,
with 3.47\% average gains over state-of-the-art methods. We also highlight the
resulting descriptions serve as explainable and robust features that can
consistently improve the performance across various backbone models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring and Mitigating Biases in Motor Insurance Pricing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mulah Moriah, Franck Vermet, Arthur Charpentier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The non-life insurance sector operates within a highly competitive and
tightly regulated framework, confronting a pivotal juncture in the formulation
of pricing strategies. Insurers are compelled to harness a range of statistical
methodologies and available data to construct optimal pricing structures that
align with the overarching corporate strategy while accommodating the dynamics
of market competition. Given the fundamental societal role played by insurance,
premium rates are subject to rigorous scrutiny by regulatory authorities. These
rates must conform to principles of transparency, explainability, and ethical
considerations. Consequently, the act of pricing transcends mere statistical
calculations and carries the weight of strategic and societal factors. These
multifaceted concerns may drive insurers to establish equitable premiums,
taking into account various variables. For instance, regulations mandate the
provision of equitable premiums, considering factors such as policyholder
gender or mutualist group dynamics in accordance with respective corporate
strategies. Age-based premium fairness is also mandated. In certain insurance
domains, variables such as the presence of serious illnesses or disabilities
are emerging as new dimensions for evaluating fairness. Regardless of the
motivating factor prompting an insurer to adopt fairer pricing strategies for a
specific variable, the insurer must possess the capability to define, measure,
and ultimately mitigate any ethical biases inherent in its pricing practices
while upholding standards of consistency and performance. This study seeks to
provide a comprehensive set of tools for these endeavors and assess their
effectiveness through practical application in the context of automobile
insurance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMES: A Differentiable Embedding Space Selection Framework for Latent
  Graph Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Lu, Haitz Sáez de Ocáriz Borde, Pietro Liò
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, although data entities may possess inherent
relationships, the specific graph illustrating their connections might not be
directly accessible. Latent graph inference addresses this issue by enabling
Graph Neural Networks (GNNs) to operate on point cloud data, dynamically
learning the necessary graph structure. These graphs are often derived from a
latent embedding space, which can be modeled using Euclidean, hyperbolic,
spherical, or product spaces. However, currently, there is no principled
differentiable method for determining the optimal embedding space. In this
work, we introduce the Attentional Multi-Embedding Selection (AMES) framework,
a differentiable method for selecting the best embedding space for latent graph
inference through backpropagation, considering a downstream task. Our framework
consistently achieves comparable or superior results compared to previous
methods for latent graph inference across five benchmark datasets. Importantly,
our approach eliminates the need for conducting multiple experiments to
identify the optimal embedding space. Furthermore, we explore interpretability
techniques that track the gradient contributions of different latent graphs,
shedding light on how our attention-based, fully differentiable approach learns
to choose the appropriate latent space. In line with previous works, our
experiments emphasize the advantages of hyperbolic spaces in enhancing
performance. More importantly, our interpretability framework provides a
general approach for quantitatively comparing embedding spaces across different
tasks based on their contributions, a dimension that has been overlooked in
previous literature on latent graph inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Neural Networks for Tiny Machine Learning: A Comprehensive
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Tri Lê, Pierre Wolinski, Julyan Arbel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Tiny Machine Learning (TinyML) has gained significant attention
due to its potential to enable intelligent applications on resource-constrained
devices. This review provides an in-depth analysis of the advancements in
efficient neural networks and the deployment of deep learning models on
ultra-low power microcontrollers (MCUs) for TinyML applications. It begins by
introducing neural networks and discussing their architectures and resource
requirements. It then explores MEMS-based applications on ultra-low power MCUs,
highlighting their potential for enabling TinyML on resource-constrained
devices. The core of the review centres on efficient neural networks for
TinyML. It covers techniques such as model compression, quantization, and
low-rank factorization, which optimize neural network architectures for minimal
resource utilization on MCUs. The paper then delves into the deployment of deep
learning models on ultra-low power MCUs, addressing challenges such as limited
computational capabilities and memory resources. Techniques like model pruning,
hardware acceleration, and algorithm-architecture co-design are discussed as
strategies to enable efficient deployment. Lastly, the review provides an
overview of current limitations in the field, including the trade-off between
model complexity and resource constraints. Overall, this review paper presents
a comprehensive analysis of efficient neural networks and deployment strategies
for TinyML on ultra-low-power MCUs. It identifies future research directions
for unlocking the full potential of TinyML applications on resource-constrained
devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 9 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Faces (MTF) Data Set: A Legally and Ethically Compliant
  Collection of Face Images for Various Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rami Haffar, David Sánchez, Josep Domingo-Ferrer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human facial data hold tremendous potential to address a variety of
classification problems, including face recognition, age estimation, gender
identification, emotion analysis, and race classification. However, recent
privacy regulations, such as the EU General Data Protection Regulation and
others, have restricted the ways in which human images may be collected and
used for research. As a result, several previously published data sets
containing human faces have been removed from the internet due to inadequate
data collection methods that failed to meet privacy regulations. Data sets
consisting of synthetic data have been proposed as an alternative, but they
fall short of accurately representing the real data distribution. On the other
hand, most available data sets are labeled for just a single task, which limits
their applicability. To address these issues, we present the Multi-Task Faces
(MTF) image data set, a meticulously curated collection of face images designed
for various classification tasks, including face recognition, as well as race,
gender, and age classification. The MTF data set has been ethically gathered by
leveraging publicly available images of celebrities and strictly adhering to
copyright regulations. In this paper, we present this data set and provide
detailed descriptions of the followed data collection and processing
procedures. Furthermore, we evaluate the performance of five deep learning (DL)
models on the MTF data set across the aforementioned classification tasks.
Additionally, we compare the performance of DL models over the processed MTF
data and over raw data crawled from the internet. The reported results
constitute a baseline for further research employing these data. The MTF data
set can be accessed through the following link (please cite the present paper
if you use the data set): https://github.com/RamiHaf/MTF_data_set
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 2 figures, 9 Tables,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forward Gradients for Data-Driven CFD Wall Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Hückelheim, Tadbhagya Kumar, Krishnan Raghavan, Pinaki Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational Fluid Dynamics (CFD) is used in the design and optimization of
gas turbines and many other industrial/ scientific applications. However, the
practical use is often limited by the high computational cost, and the accurate
resolution of near-wall flow is a significant contributor to this cost. Machine
learning (ML) and other data-driven methods can complement existing wall
models. Nevertheless, training these models is bottlenecked by the large
computational effort and memory footprint demanded by back-propagation. Recent
work has presented alternatives for computing gradients of neural networks
where a separate forward and backward sweep is not needed and storage of
intermediate results between sweeps is not required because an unbiased
estimator for the gradient is computed in a single forward sweep. In this
paper, we discuss the application of this approach for training a subgrid wall
model that could potentially be used as a surrogate in wall-bounded flow CFD
simulations to reduce the computational overhead while preserving predictive
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training robust and generalizable quantum models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Berberich, Daniel Fink, Daniel Pranjić, Christian Tutschku, Christian Holm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial robustness and generalization are both crucial properties of
reliable machine learning models. In this paper, we study these properties in
the context of quantum machine learning based on Lipschitz bounds. We derive
tailored, parameter-dependent Lipschitz bounds for quantum models with
trainable encoding, showing that the norm of the data encoding has a crucial
impact on the robustness against perturbations in the input data. Further, we
derive a bound on the generalization error which explicitly depends on the
parameters of the data encoding. Our theoretical findings give rise to a
practical strategy for training robust and generalizable quantum models by
regularizing the Lipschitz bound in the cost. Further, we show that, for fixed
and non-trainable encodings as frequently employed in quantum machine learning,
the Lipschitz bound cannot be influenced by tuning the parameters. Thus,
trainable encodings are crucial for systematically adapting robustness and
generalization during training. With numerical results, we demonstrate that,
indeed, Lipschitz bound regularization leads to substantially more robust and
generalizable quantum models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Establishing Central Sensitization Inventory Cut-off Values in patients
  with Chronic Low Back Pain by Unsupervised Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoping Zheng, Claudine JC Lamoth, Hans Timmerman, Ebert Otten, Michiel F Reneman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Assumed Central Sensitization is involved in the development and
maintenance of chronic low back pain (CLBP). The Central Sensitization
Inventory (CSI) was developed to evaluate the presence of HACS, with a cut-off
value of 40/100 based on patients with chronic pain. However, various factors
including pain conditions (e.g., CLBP), and gender may influence this cut-off
value. For chronic pain condition such as CLBP, unsupervised clustering
approaches can take these factors into consideration and automatically learn
the HACS-related patterns. Therefore, this study aimed to determine the cut-off
values for a Dutch-speaking population with CLBP, considering the total group
and stratified by gender based on unsupervised machine learning. In this study,
questionnaire data covering pain, physical, and psychological aspects were
collected from patients with CLBP and aged-matched pain-free adults (referred
to as healthy controls, HC). Four clustering approaches were applied to
identify HACS-related clusters based on the questionnaire data and gender. The
clustering performance was assessed using internal and external indicators.
Subsequently, receiver operating characteristic analysis was conducted on the
best clustering results to determine the optimal cut-off values. The study
included 151 subjects, consisting of 63 HCs and 88 patients with CLBP.
Hierarchical clustering yielded the best results, identifying three clusters:
healthy group, CLBP with low HACS level, and CLBP with high HACS level groups.
Based on the low HACS levels group (including HC and CLBP with low HACS level)
and high HACS level group, the cut-off value for the overall groups were 35, 34
for females, and 35 for. The findings suggest that the optimal cut-off values
for CLBP is 35. The gender-related cut-off values should be interpreted with
caution due to the unbalanced gender distribution in the sample.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 5 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning complete intersection Calabi-Yau manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harold Erbin, Riccardo Finotello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We review advancements in deep learning techniques for complete intersection
Calabi-Yau (CICY) 3- and 4-folds, with the aim of understanding better how to
handle algebraic topological data with machine learning. We first discuss
methodological aspects and data analysis, before describing neural networks
architectures. Then, we describe the state-of-the art accuracy in predicting
Hodge numbers. We include new results on extrapolating predictions from low to
high Hodge numbers, and conversely.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages; match version published in "Machine Learning in Pure
  Mathematics and Theoretical Physics" (edited by Y.-H. He, World Scientific
  Press)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High Probability Guarantees for Random Reshuffling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengxu Yu, Xiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the stochastic gradient method with random reshuffling
($\mathsf{RR}$) for tackling smooth nonconvex optimization problems.
$\mathsf{RR}$ finds broad applications in practice, notably in training neural
networks. In this work, we first investigate the concentration property of
$\mathsf{RR}$'s sampling procedure and establish a new high probability sample
complexity guarantee for driving the gradient (without expectation) below
$\varepsilon$, which effectively characterizes the efficiency of a single
$\mathsf{RR}$ execution. Our derived complexity matches the best existing
in-expectation one up to a logarithmic term while imposing no additional
assumptions nor changing $\mathsf{RR}$'s updating rule. Furthermore, by
leveraging our derived high probability descent property and bound on the
stochastic error, we propose a simple and computable stopping criterion for
$\mathsf{RR}$ (denoted as $\mathsf{RR}$-$\mathsf{sc}$). This criterion is
guaranteed to be triggered after a finite number of iterations, and then
$\mathsf{RR}$-$\mathsf{sc}$ returns an iterate with its gradient below
$\varepsilon$ with high probability. Moreover, building on the proposed
stopping criterion, we design a perturbed random reshuffling method
($\mathsf{p}$-$\mathsf{RR}$) that involves an additional randomized
perturbation procedure near stationary points. We derive that
$\mathsf{p}$-$\mathsf{RR}$ provably escapes strict saddle points and
efficiently returns a second-order stationary point with high probability,
without making any sub-Gaussian tail-type assumptions on the stochastic
gradient errors. Finally, we conduct numerical experiments on neural network
training to support our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kandinsky Conformal Prediction: Efficient Calibration of Image
  Segmentation Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joren Brunekreef, Eric Marcus, Ray Sheombarsing, Jan-Jakob Sonke, Jonas Teuwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation algorithms can be understood as a collection of pixel
classifiers, for which the outcomes of nearby pixels are correlated. Classifier
models can be calibrated using Inductive Conformal Prediction, but this
requires holding back a sufficiently large calibration dataset for computing
the distribution of non-conformity scores of the model's predictions. If one
only requires only marginal calibration on the image level, this calibration
set consists of all individual pixels in the images available for calibration.
However, if the goal is to attain proper calibration for each individual pixel
classifier, the calibration set consists of individual images. In a scenario
where data are scarce (such as the medical domain), it may not always be
possible to set aside sufficiently many images for this pixel-level
calibration. The method we propose, dubbed ``Kandinsky calibration'', makes use
of the spatial structure present in the distribution of natural images to
simultaneously calibrate the classifiers of ``similar'' pixels. This can be
seen as an intermediate approach between marginal (imagewise) and conditional
(pixelwise) calibration, where non-conformity scores are aggregated over
similar image regions, thereby making more efficient use of the images
available for calibration. We run experiments on segmentation algorithms
trained and calibrated on subsets of the public MS-COCO and Medical Decathlon
datasets, demonstrating that Kandinsky calibration method can significantly
improve the coverage. When compared to both pixelwise and imagewise calibration
on little data, the Kandinsky method achieves much lower coverage errors,
indicating the data efficiency of the Kandinsky calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ System 2 Attention (is something you might need too) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Weston, Sainbayar Sukhbaatar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft attention in Transformer-based Large Language Models (LLMs) is
susceptible to incorporating irrelevant information from the context into its
latent representations, which adversely affects next token generations. To help
rectify these issues, we introduce System 2 Attention (S2A), which leverages
the ability of LLMs to reason in natural language and follow instructions in
order to decide what to attend to. S2A regenerates the input context to only
include the relevant portions, before attending to the regenerated context to
elicit the final response. In experiments, S2A outperforms standard
attention-based LLMs on three tasks containing opinion or irrelevant
information, QA, math word problems and longform generation, where S2A
increases factuality and objectivity, and decreases sycophancy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Multispectral Segmentation with Representations Generated by
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dilith Jayakody, Thanuja Ambegoda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of multispectral image segmentation (segmentation of images with
numerous channels/bands, each capturing a specific range of wavelengths of
electromagnetic radiation) has been previously explored in contexts with large
amounts of labeled data. However, these models tend not to generalize well to
datasets of smaller size. In this paper, we propose a novel approach for
improving few-shot segmentation performance on multispectral images using
reinforcement learning to generate representations. These representations are
generated in the form of mathematical expressions between channels and are
tailored to the specific class being segmented. Our methodology involves
training an agent to identify the most informative expressions, updating the
dataset using these expressions, and then using the updated dataset to perform
segmentation. Due to the limited length of the expressions, the model receives
useful representations without any added risk of overfitting. We evaluate the
effectiveness of our approach on several multispectral datasets and demonstrate
its effectiveness in boosting the performance of segmentation algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero redundancy distributed learning with differential privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Bu, Justin Chiu, Ruixuan Liu, Sheng Zha, George Karypis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning using large models have achieved great success in a wide range
of domains. However, training these models on billions of parameters is very
challenging in terms of the training speed, memory cost, and communication
efficiency, especially under the privacy-preserving regime with differential
privacy (DP). On the one hand, DP optimization has comparable efficiency to the
standard non-private optimization on a single GPU, but on multiple GPUs,
existing DP distributed learning (such as pipeline parallel) has suffered from
significantly worse efficiency. On the other hand, the Zero Redundancy
Optimizer (ZeRO) is a state-of-the-art solution to the standard distributed
learning, exhibiting excellent training efficiency on large models, but to work
compatibly with DP is technically complicated. In this work, we develop a new
systematic solution, DP-ZeRO, (I) to scale up the trainable DP model size, e.g.
to GPT-100B, (II) to obtain the same computation and communication efficiency
as the standard ZeRO, and (III) to enable mixed-precision DP training. Our
DP-ZeRO, like the standard ZeRO, has the potential to train models with
arbitrary size and is evaluated on the world's largest DP models in terms of
the number of trainable parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-View Graph Consistency Learning for Invariant Graph
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Chen, Zhiming Li, Hua Mao, Wai Lok Woo, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph representation learning is fundamental for analyzing graph-structured
data. Exploring invariant graph representations remains a challenge for most
existing graph representation learning methods. In this paper, we propose a
cross-view graph consistency learning (CGCL) method that learns invariant graph
representations for link prediction. First, two complementary augmented views
are derived from an incomplete graph structure through a bidirectional graph
structure augmentation scheme. This augmentation scheme mitigates the potential
information loss that is commonly associated with various data augmentation
techniques involving raw graph data, such as edge perturbation, node removal,
and attribute masking. Second, we propose a CGCL model that can learn invariant
graph representations. A cross-view training scheme is proposed to train the
proposed CGCL model. This scheme attempts to maximize the consistency
information between one augmented view and the graph structure reconstructed
from the other augmented view. Furthermore, we offer a comprehensive
theoretical CGCL analysis. This paper empirically and experimentally
demonstrates the effectiveness of the proposed CGCL method, achieving
competitive results on graph datasets in comparisons with several
state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized super-resolution 4D Flow MRI -- using ensemble learning to
  extend across the cardiovascular system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Ericsson, Adam Hjalmarsson, Muhammad Usman Akbar, Edward Ferdian, Mia Bonini, Brandon Hardy, Jonas Schollenberger, Maria Aristova, Patrick Winter, Nicholas Burris, Alexander Fyrdahl, Andreas Sigfridsson, Susanne Schnell, C. Alberto Figueroa, David Nordsletten, Alistair A. Young, David Marlevi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive
measurement technique capable of quantifying blood flow across the
cardiovascular system. While practical use is limited by spatial resolution and
image noise, incorporation of trained super-resolution (SR) networks has
potential to enhance image quality post-scan. However, these efforts have
predominantly been restricted to narrowly defined cardiovascular domains, with
limited exploration of how SR performance extends across the cardiovascular
system; a task aggravated by contrasting hemodynamic conditions apparent across
the cardiovasculature. The aim of our study was to explore the generalizability
of SR 4D Flow MRI using a combination of heterogeneous training sets and
dedicated ensemble learning. With synthetic training data generated across
three disparate domains (cardiac, aortic, cerebrovascular), varying
convolutional base and ensemble learners were evaluated as a function of domain
and architecture, quantifying performance on both in-silico and acquired
in-vivo data from the same three domains. Results show that both bagging and
stacking ensembling enhance SR performance across domains, accurately
predicting high-resolution velocities from low-resolution input data in-silico.
Likewise, optimized networks successfully recover native resolution velocities
from downsampled in-vivo data, as well as show qualitative potential in
generating denoised SR-images from clinical level input data. In conclusion,
our work presents a viable approach for generalized SR 4D Flow MRI, with
ensemble learning extending utility across various clinical areas of interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LogLead -- Fast and Integrated Log Loader, Enhancer, and Anomaly
  Detector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mika Mäntylä, Yuqing Wang, Jesse Nyyssölä
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces LogLead, a tool designed for efficient log analysis.
LogLead combines three essential steps in log processing: loading, enhancing,
and anomaly detection. The tool leverages Polars, a high-speed DataFrame
library. We currently have 7 Loaders out of which 4 is for public data sets
(HDFS, Hadoop, BGL, and Thunderbird). We have multiple enhancers with three
parsers (Drain, Spell, LenMa), Bert embedding creation and other log
representation techniques like bag-of-words. LogLead integrates to 5 supervised
and 4 unsupervised machine learning algorithms for anomaly detection from
SKLearn. By integrating diverse datasets, log representation methods and
anomaly detectors, LogLead facilitates comprehensive benchmarking in log
analysis research. We demonstrate that log loading from raw file to dataframe
is over 10x faster with LogLead is compared to past solutions. We demonstrate
roughly 2x improvement in Drain parsing speed by off-loading log message
normalization to LogLead. We demonstrate a brief benchmarking on HDFS
suggesting that log representations beyond bag-of-words provide limited
benefits. Screencast demonstrating the tool: https://youtu.be/8stdbtTfJVo
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Operator Learning for Continuous Spatial-Temporal Model with A Hybrid
  Optimization Scheme 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanqi Chen, Jin-Long Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial differential equations are often used in the spatial-temporal
modeling of complex dynamical systems in many engineering applications. In this
work, we build on the recent progress of operator learning and present a
data-driven modeling framework that is continuous in both space and time. A key
feature of the proposed model is the resolution-invariance with respect to both
spatial and temporal discretizations. To improve the long-term performance of
the calibrated model, we further propose a hybrid optimization scheme that
leverages both gradient-based and derivative-free optimization methods and
efficiently trains on both short-term time series and long-term statistics. We
investigate the performance of the spatial-temporal continuous learning
framework with three numerical examples, including the viscous Burgers'
equation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation.
The results confirm the resolution-invariance of the proposed modeling
framework and also demonstrate stable long-term simulations with only
short-term time series data. In addition, we show that the proposed model can
better predict long-term statistics via the hybrid optimization scheme with a
combined use of short-term and long-term data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate Linear Programming and Decentralized Policy Improvement in
  Cooperative Multi-agent Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshmi Mandal, Chandrashekar Lakshminarayanan, Shalabh Bhatnagar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider a `cooperative' multi-agent Markov decision process
(MDP) involving m greater than 1 agents, where all agents are aware of the
system model. At each decision epoch, all the m agents cooperatively select
actions in order to maximize a common long-term objective. Since the number of
actions grows exponentially in the number of agents, policy improvement is
computationally expensive. Recent works have proposed using decentralized
policy improvement in which each agent assumes that the decisions of the other
agents are fixed and it improves its decisions unilaterally. Yet, in these
works, exact values are computed. In our work, for cooperative multi-agent
finite and infinite horizon discounted MDPs, we propose suitable approximate
policy iteration algorithms, wherein we use approximate linear programming to
compute the approximate value function and use decentralized policy
improvement. Thus our algorithms can handle both large number of states as well
as multiple agents. We provide theoretical guarantees for our algorithms and
also demonstrate the performance of our algorithms on some numerical examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayar Lotfy, Anna Alperovich, Tommaso Giannantonio, Bjorn Barz, Xiaohan Zhang, Felix Holm, Nassir Navab, Felix Boehm, Carolin Schwamborn, Thomas K. Hoffmann, Patrick J. Schuler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting the boundary between tumor and healthy tissue during surgical
cancer resection poses a significant challenge. In recent years, Hyperspectral
Imaging (HSI) combined with Machine Learning (ML) has emerged as a promising
solution. However, due to the extensive information contained within the
spectral domain, most ML approaches primarily classify individual HSI
(super-)pixels, or tiles, without taking into account their spatial context. In
this paper, we propose an improved methodology that leverages the spatial
context of tiles for more robust and smoother segmentation. To address the
irregular shapes of tiles, we utilize Graph Neural Networks (GNNs) to propagate
context information across neighboring regions. The features for each tile
within the graph are extracted using a Convolutional Neural Network (CNN),
which is trained simultaneously with the subsequent GNN. Moreover, we
incorporate local image quality metrics into the loss function to enhance the
training procedure's robustness against low-quality regions in the training
images. We demonstrate the superiority of our proposed method using a clinical
ex vivo dataset consisting of 51 HSI images from 30 patients. Despite the
limited dataset, the GNN-based model significantly outperforms context-agnostic
approaches, accurately distinguishing between healthy and tumor tissues, even
in images from previously unseen patients. Furthermore, we show that our
carefully designed loss function, accounting for local image quality, results
in additional improvements. Our findings demonstrate that context-aware GNN
algorithms can robustly find tumor demarcations on HSI images, ultimately
contributing to better surgery success and patient outcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal deep learning for mapping forest dominant height by fusing
  GEDI with earth observation data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Man Chen, Wenquan Dong, Hao Yu, Iain Woodhouse, Casey M. Ryan, Haoyu Liu, Selena Georgiou, Edward T. A. Mitchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of multisource remote sensing data and deep learning models
offers new possibilities for accurately mapping high spatial resolution forest
height. We found that GEDI relative heights (RH) metrics exhibited strong
correlation with the mean of the top 10 highest trees (dominant height)
measured in situ at the corresponding footprint locations. Consequently, we
proposed a novel deep learning framework termed the multi-modal attention
remote sensing network (MARSNet) to estimate forest dominant height by
extrapolating dominant height derived from GEDI, using Setinel-1 data, ALOS-2
PALSAR-2 data, Sentinel-2 optical data and ancillary data. MARSNet comprises
separate encoders for each remote sensing data modality to extract multi-scale
features, and a shared decoder to fuse the features and estimate height. Using
individual encoders for each remote sensing imagery avoids interference across
modalities and extracts distinct representations. To focus on the efficacious
information from each dataset, we reduced the prevalent spatial and band
redundancies in each remote sensing data by incorporating the extended spatial
and band reconstruction convolution modules in the encoders. MARSNet achieved
commendable performance in estimating dominant height, with an R2 of 0.62 and
RMSE of 2.82 m, outperforming the widely used random forest approach which
attained an R2 of 0.55 and RMSE of 3.05 m. Finally, we applied the trained
MARSNet model to generate wall-to-wall maps at 10 m resolution for Jilin,
China. Through independent validation using field measurements, MARSNet
demonstrated an R2 of 0.58 and RMSE of 3.76 m, compared to 0.41 and 4.37 m for
the random forest baseline. Our research demonstrates the effectiveness of a
multimodal deep learning approach fusing GEDI with SAR and passive optical
imagery for enhancing the accuracy of high resolution dominant height
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Good Feature Extractor Is All You Need for Weakly Supervised Learning
  in Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Wölflein, Dyke Ferber, Asier Rabasco Meneghetti, Omar S. M. El Nahhas, Daniel Truhn, Zunamys I. Carrero, David J. Harrison, Ognjen Arandjelović, Jakob N. Kather
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is revolutionising pathology, offering novel opportunities in
disease prognosis and personalised treatment. Historically, stain normalisation
has been a crucial preprocessing step in computational pathology pipelines, and
persists into the deep learning era. Yet, with the emergence of feature
extractors trained using self-supervised learning (SSL) on diverse pathology
datasets, we call this practice into question. In an empirical evaluation of
publicly available feature extractors, we find that omitting stain
normalisation and image augmentations does not compromise downstream
performance, while incurring substantial savings in memory and compute.
Further, we show that the top-performing feature extractors are remarkably
robust to variations in stain and augmentations like rotation in their latent
space. Contrary to previous patch-level benchmarking studies, our approach
emphasises clinical relevance by focusing on slide-level prediction tasks in a
weakly supervised setting with external validation cohorts. This work
represents the most comprehensive robustness evaluation of public pathology SSL
feature extractors to date, involving more than 6,000 training runs across nine
tasks, five datasets, three downstream architectures, and various preprocessing
setups. Our findings stand to streamline digital pathology workflows by
minimising preprocessing needs and informing the selection of feature
extractors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MUVO: A Multimodal Generative World Model for Autonomous Driving with
  Geometric Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Bogdoll, Yitian Yang, J. Marius Zöllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning unsupervised world models for autonomous driving has the potential
to improve the reasoning capabilities of today's systems dramatically. However,
most work neglects the physical attributes of the world and focuses on sensor
data alone. We propose MUVO, a MUltimodal World Model with Geometric VOxel
Representations to address this challenge. We utilize raw camera and lidar data
to learn a sensor-agnostic geometric representation of the world, which can
directly be used by downstream tasks, such as planning. We demonstrate
multimodal future predictions and show that our geometric representation
improves the prediction quality of both camera images and lidar point clouds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Unseen Potential of Graph Learning through MLPs: Effective
  Graph Learners Using Propagation-Embracing MLPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong-Min Shin, Won-Yong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies attempted to utilize multilayer perceptrons (MLPs) to solve
semi-supervised node classification on graphs, by training a student MLP by
knowledge distillation (KD) from a teacher graph neural network (GNN). While
previous studies have focused mostly on training the student MLP by matching
the output probability distributions between the teacher and student models
during KD, it has not been systematically studied how to inject the structural
information in an explicit and interpretable manner. Inspired by GNNs that
separate feature transformation $T$ and propagation $\Pi$, we re-frame the KD
process as enabling the student MLP to explicitly learn both $T$ and $\Pi$.
Although this can be achieved by applying the inverse propagation $\Pi^{-1}$
before distillation from the teacher GNN, it still comes with a high
computational cost from large matrix multiplications during training. To solve
this problem, we propose Propagate & Distill (P&D), which propagates the output
of the teacher GNN before KD and can be interpreted as an approximate process
of the inverse propagation $\Pi^{-1}$. Through comprehensive evaluations using
real-world benchmark datasets, we demonstrate the effectiveness of P&D by
showing further performance boost of the student MLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 5 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing behavioral impact on mobility prediction networks through
  causal interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Hong, Yanan Xin, Simon Dirmeier, Fernando Perez-Cruz, Martin Raubal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are increasingly utilized in mobility prediction tasks,
yet their intricate internal workings pose challenges for interpretability,
especially in comprehending how various aspects of mobility behavior affect
predictions. In this study, we introduce a causal intervention framework to
assess the impact of mobility-related factors on neural networks designed for
next location prediction -- a task focusing on predicting the immediate next
location of an individual. To achieve this, we employ individual mobility
models to generate synthetic location visit sequences and control behavior
dynamics by intervening in their data generation process. We evaluate the
interventional location sequences using mobility metrics and input them into
well-trained networks to analyze performance variations. The results
demonstrate the effectiveness in producing location sequences with distinct
mobility behaviors, thus facilitating the simulation of diverse spatial and
temporal changes. These changes result in performance fluctuations in next
location prediction networks, revealing impacts of critical mobility behavior
factors, including sequential patterns in location transitions, proclivity for
exploring new locations, and preferences in location choices at population and
individual levels. The gained insights hold significant value for the
real-world application of mobility prediction networks, and the framework is
expected to promote the use of causal inference for enhancing the
interpretability and robustness of neural networks in mobility applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Uncertainty Estimates To Improve Classifier Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gundeep Arora, Srujana Merugu, Anoop Saladi, Rajeev Rastogi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary classification involves predicting the label of an instance based on
whether the model score for the positive class exceeds a threshold chosen based
on the application requirements (e.g., maximizing recall for a precision
bound). However, model scores are often not aligned with the true positivity
rate. This is especially true when the training involves a differential
sampling across classes or there is distributional drift between train and test
settings. In this paper, we provide theoretical analysis and empirical evidence
of the dependence of model score estimation bias on both uncertainty and score
itself. Further, we formulate the decision boundary selection in terms of both
model score and uncertainty, prove that it is NP-hard, and present algorithms
based on dynamic programming and isotonic regression. Evaluation of the
proposed algorithms on three real-world datasets yield 25%-40% gain in recall
at high precision bounds over the traditional approach of using model score
alone, highlighting the benefits of leveraging uncertainty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can we infer the presence of Differential Privacy in Deep Learning
  models' weights? Towards more secure Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Jiménez-López,  Daniel,  Rodríguez-Barroso,  Nuria,  Luzón, M. Victoria,  Herrera,  Francisco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential Privacy (DP) is a key property to protect data and models from
integrity attacks. In the Deep Learning (DL) field, it is commonly implemented
through the Differentially Private Stochastic Gradient Descent (DP-SGD).
However, when a model is shared or released, there is no way to check whether
it is differentially private, that is, it required to trust the model provider.
This situation poses a problem when data privacy is mandatory, specially with
current data regulations, as the presence of DP can not be certificated
consistently by any third party. Thus, we face the challenge of determining
whether a DL model has been trained with DP, according to the title question:
Can we infer the presence of Differential Privacy in Deep Learning models'
weights? Since the DP-SGD significantly changes the training process of a DL
model, we hypothesize that DP leaves an imprint in the weights of a DL model,
which can be used to predict whether a model has been trained with DP
regardless of its architecture and the training dataset. In this paper, we
propose to employ the imprint in model weights of using DP to infer the
presence of DP training in a DL model. To substantiate our hypothesis, we
developed an experimental methodology based on two datasets of weights of DL
models, each with models with and without DP training and a meta-classifier to
infer whether DP was used in the training process of a DL model, by accessing
its weights. We accomplish both, the removal of the requirement of a trusted
model provider and a strong foundation for this interesting line of research.
Thus, our contribution is an additional layer of security on top of the strict
private requirements of DP training in DL models, towards to DL models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Low-rank Adaptation of <span class="highlight-title">Pre-train</span>ed Language Models <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained large language models in a parameter-efficient manner
is widely studied for its effectiveness and efficiency. The popular method of
low-rank adaptation (LoRA) offers a notable approach, hypothesizing that the
adaptation process is intrinsically low-dimensional. Although LoRA has
demonstrated commendable performance, it is implemented with a fixed and
unalterable intrinsic rank that might not always be the ideal choice.
Recognizing the need for more flexible adaptation, we extend the methodology of
LoRA to an innovative approach we call sparse low-rank adaptation (SoRA) that
enables dynamic adjustments to the intrinsic rank during the adaptation
process. We achieve this through the incorporation of a gate unit optimized
with proximal gradient method in the training stage, controlling the
cardinality of rank under the sparsity of the gate. In the subsequent inference
stage, we eliminate the parameter blocks corresponding to the zeroed-out ranks,
to reduce each SoRA module back to a concise yet rank-optimal LoRA. Our
approach strengthens the representation power of LoRA by initializing it with a
higher rank, while efficiently taming a temporarily increased number of
parameters via updating in a sparse way. We further introduce a sparsifying
scheduler for SoRA, aiming to examine the impact of the number of non-zero
parameters on the model's memorization and generalization. Our experimental
results demonstrate that SoRA can outperform other baselines even with 70%
retained parameters and 70% training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Power of Self-Attention for Shipping Cost Prediction: The
  Rate Card <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        P Aditya Sreekar, Sahil Verma, Varun Madhavan, Abhishek Persad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amazon ships billions of packages to its customers annually within the United
States. Shipping cost of these packages are used on the day of shipping (day 0)
to estimate profitability of sales. Downstream systems utilize these days 0
profitability estimates to make financial decisions, such as pricing strategies
and delisting loss-making products. However, obtaining accurate shipping cost
estimates on day 0 is complex for reasons like delay in carrier invoicing or
fixed cost components getting recorded at monthly cadence. Inaccurate shipping
cost estimates can lead to bad decision, such as pricing items too low or high,
or promoting the wrong product to the customers. Current solutions for
estimating shipping costs on day 0 rely on tree-based models that require
extensive manual engineering efforts. In this study, we propose a novel
architecture called the Rate Card Transformer (RCT) that uses self-attention to
encode all package shipping information such as package attributes, carrier
information and route plan. Unlike other transformer-based tabular models, RCT
has the ability to encode a variable list of one-to-many relations of a
shipment, allowing it to capture more information about a shipment. For
example, RCT can encode properties of all products in a package. Our results
demonstrate that cost predictions made by the RCT have 28.82% less error
compared to tree-based GBDT model. Moreover, the RCT outperforms the
state-of-the-art transformer-based tabular model, FTTransformer, by 6.08%. We
also illustrate that the RCT learns a generalized manifold of the rate card
that can improve the performance of tree-based models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unraveling the Control Engineer's Craft with Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Braghadeesh Lakshminarayanan, Federico Dettù, Cristian R. Rojas, Simone Formentin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many industrial processes require suitable controllers to meet their
performance requirements. More often, a sophisticated digital twin is
available, which is a highly complex model that is a virtual representation of
a given physical process, whose parameters may not be properly tuned to capture
the variations in the physical process. In this paper, we present a sim2real,
direct data-driven controller tuning approach, where the digital twin is used
to generate input-output data and suitable controllers for several
perturbations in its parameters. State-of-the art neural-network architectures
are then used to learn the controller tuning rule that maps input-output data
onto the controller parameters, based on artificially generated data from
perturbed versions of the digital twin. In this way, as far as we are aware, we
tackle for the first time the problem of re-calibrating the controller by
meta-learning the tuning rule directly from data, thus practically replacing
the control engineer with a machine learning model. The benefits of this
methodology are illustrated via numerical simulations for several choices of
neural-network architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Realistic Counterfactuals for Retinal Fundus and OCT Images
  using Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Indu Ilanchezian, Valentyn Boreiko, Laura Kühlewein, Ziwei Huang, Murat Seçkin Ayhan, Matthias Hein, Lisa Koch, Philipp Berens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual reasoning is often used in a clinical setting to explain
decisions or weigh alternatives. Therefore, for imaging based modalities such
as ophthalmology, it would be beneficial to be able to create counterfactual
images, illustrating the answer to the question: "If the subject had had
diabetic retinopathy, how would the fundus image have looked?" Here, we
demonstrate that using a diffusion model in combination with an adversarially
robust classifier trained on retinal disease classification tasks enables
generation of highly realistic counterfactuals of retinal fundus images and
optical coherence tomorgraphy (OCT) B-scans. Ideally, these classifiers encode
the salient features indicative for each disease class and can steer the
diffusion model to show realistic disease signs or remove disease-related
lesions in a realistic way. Importantly, in a user study, domain experts found
the counterfactuals generated using our method significantly more realistic
than counterfactuals generated from a previous method, and even
indistiguishable from realistic images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating LLM Priors into Tabular Learners <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Zhu, Siniša Stanivuk, Andrija Petrovic, Mladen Nikolic, Pietro Lio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method to integrate Large Language Models (LLMs) and traditional
tabular data classification techniques, addressing LLMs challenges like data
serialization sensitivity and biases. We introduce two strategies utilizing
LLMs for ranking categorical variables and generating priors on correlations
between continuous variables and targets, enhancing performance in few-shot
scenarios. We focus on Logistic Regression, introducing MonotonicLR that
employs a non-linear monotonic function for mapping ordinals to cardinals while
preserving LLM-determined orders. Validation against baseline models reveals
the superior performance of our approach, especially in low-data scenarios,
while remaining interpretable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Table Representation Learning Workshop at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A novel <span class="highlight-title">transformer</span>-based approach for soil temperature prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammet Mucahit Enes Yurtsever, Ayhan Kucukmanisa, Zeynep Hilal Kilimci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soil temperature is one of the most significant parameters that plays a
crucial role in glacier energy, dynamics of mass balance, processes of surface
hydrological, coaction of glacier-atmosphere, nutrient cycling, ecological
stability, the management of soil, water, and field crop. In this work, we
introduce a novel approach using transformer models for the purpose of
forecasting soil temperature prediction. To the best of our knowledge, the
usage of transformer models in this work is the very first attempt to predict
soil temperature. Experiments are carried out using six different FLUXNET
stations by modeling them with five different transformer models, namely,
Vanilla Transformer, Informer, Autoformer, Reformer, and ETSformer. To
demonstrate the effectiveness of the proposed model, experiment results are
compared with both deep learning approaches and literature studies. Experiment
results show that the utilization of transformer models ensures a significant
contribution to the literature, thence determining the new state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing multivariate normality by testing independence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Povilas Daniušis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a simple multivariate normality test based on Kac-Bernstein's
characterization, which can be conducted by utilising existing statistical
independence tests for sums and differences of data samples. We also perform
its empirical investigation, which reveals that for high-dimensional data, the
proposed approach may be more efficient than the alternative ones. The
accompanying code repository is provided at \url{https://shorturl.at/rtuy5}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep-Genetic Algorithm (Deep-GA) Approach for High-Dimensional
  Nonlinear Parabolic Partial Differential Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Endah Rokhmati Merdika Putri, Muhammad Luthfi Shahab, Mohammad Iqbal, Imam Mukhlash, Amirul Hakam, Lutfi Mardianto, Hadi Susanto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new method, called a deep-genetic algorithm (deep-GA), to
accelerate the performance of the so-called deep-BSDE method, which is a deep
learning algorithm to solve high dimensional partial differential equations
through their corresponding backward stochastic differential equations (BSDEs).
Recognizing the sensitivity of the solver to the initial guess selection, we
embed a genetic algorithm (GA) into the solver to optimize the selection. We
aim to achieve faster convergence for the nonlinear PDEs on a broader interval
than deep-BSDE. Our proposed method is applied to two nonlinear parabolic PDEs,
i.e., the Black-Scholes (BS) equation with default risk and the
Hamilton-Jacobi-Bellman (HJB) equation. We compare the results of our method
with those of the deep-BSDE and show that our method provides comparable
accuracy with significantly improved computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Computers and Mathematics with
  Applications, 19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Replay-enhanced Continual Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiantian Zhang, Kevin Zehua Shen, Zichuan Lin, Bo Yuan, Xueqian Wang, Xiu Li, Deheng Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Replaying past experiences has proven to be a highly effective approach for
averting catastrophic forgetting in supervised continual learning. However,
some crucial factors are still largely ignored, making it vulnerable to serious
failure, when used as a solution to forgetting in continual reinforcement
learning, even in the context of perfect memory where all data of previous
tasks are accessible in the current task. On the one hand, since most
reinforcement learning algorithms are not invariant to the reward scale, the
previously well-learned tasks (with high rewards) may appear to be more salient
to the current learning process than the current task (with small initial
rewards). This causes the agent to concentrate on those salient tasks at the
expense of generality on the current task. On the other hand, offline learning
on replayed tasks while learning a new task may induce a distributional shift
between the dataset and the learned policy on old tasks, resulting in
forgetting. In this paper, we introduce RECALL, a replay-enhanced method that
greatly improves the plasticity of existing replay-based methods on new tasks
while effectively avoiding the recurrence of catastrophic forgetting in
continual reinforcement learning. RECALL leverages adaptive normalization on
approximate targets and policy distillation on old tasks to enhance generality
and stability, respectively. Extensive experiments on the Continual World
benchmark show that RECALL performs significantly better than purely perfect
memory replay, and achieves comparable or better overall performance against
state-of-the-art continual learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Transactions on Machine Learning Research 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring <span class="highlight-title">Prompt</span>ing Large Language Models as Explainable Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ghazaleh Mahmoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the IUST NLP Lab submission to the Prompting Large
Language Models as Explainable Metrics Shared Task at the Eval4NLP 2023
Workshop on Evaluation & Comparison of NLP Systems. We have proposed a
zero-shot prompt-based strategy for explainable evaluation of the summarization
task using Large Language Models (LLMs). The conducted experiments demonstrate
the promising potential of LLMs as evaluation metrics in Natural Language
Processing (NLP), particularly in the field of summarization. Both few-shot and
zero-shot approaches are employed in these experiments. The performance of our
best provided prompts achieved a Kendall correlation of 0.477 with human
evaluations in the text summarization task on the test data. Code and results
are publicly available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Eval4NLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Variation in Subpopulation Susceptibility to Poisoning
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evan Rose, Fnu Suya, David Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is susceptible to poisoning attacks, in which an attacker
controls a small fraction of the training data and chooses that data with the
goal of inducing some behavior unintended by the model developer in the trained
model. We consider a realistic setting in which the adversary with the ability
to insert a limited number of data points attempts to control the model's
behavior on a specific subpopulation. Inspired by previous observations on
disparate effectiveness of random label-flipping attacks on different
subpopulations, we investigate the properties that can impact the effectiveness
of state-of-the-art poisoning attacks against different subpopulations. For a
family of 2-dimensional synthetic datasets, we empirically find that dataset
separability plays a dominant role in subpopulation vulnerability for less
separable datasets. However, well-separated datasets exhibit more dependence on
individual subpopulation properties. We further discover that a crucial
subpopulation property is captured by the difference in loss on the clean
dataset between the clean model and a target model that misclassifies the
subpopulation, and a subpopulation is much easier to attack if the loss
difference is small. This property also generalizes to high-dimensional
benchmark datasets. For the Adult benchmark dataset, we show that we can find
semantically-meaningful subpopulation properties that are related to the
susceptibilities of a selected group of subpopulations. The results in this
paper are accompanied by a fully interactive web-based visualization of
subpopulation poisoning attacks found at
https://uvasrg.github.io/visualizing-poisoning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADAPTER-RL: Adaptation of Any Agent using Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhao Jin, Greg Slabaugh, Simon Lucas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) agents frequently face challenges in
adapting to tasks outside their training distribution, including issues with
over-fitting, catastrophic forgetting and sample inefficiency. Although the
application of adapters has proven effective in supervised learning contexts
such as natural language processing and computer vision, their potential within
the DRL domain remains largely unexplored. This paper delves into the
integration of adapters in reinforcement learning, presenting an innovative
adaptation strategy that demonstrates enhanced training efficiency and
improvement of the base-agent, experimentally in the nanoRTS environment, a
real-time strategy (RTS) game simulation. Our proposed universal approach is
not only compatible with pre-trained neural networks but also with rule-based
agents, offering a means to integrate human expertise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Hyperparameter $ε$ for Adaptive Stochastic Optimizers
  through Gradient Histograms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Silva, Paul Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizers are essential components for successfully training deep neural
network models. In order to achieve the best performance from such models,
designers need to carefully choose the optimizer hyperparameters. However, this
can be a computationally expensive and time-consuming process. Although it is
known that all optimizer hyperparameters must be tuned for maximum performance,
there is still a lack of clarity regarding the individual influence of minor
priority hyperparameters, including the safeguard factor $\epsilon$ and
momentum factor $\beta$, in leading adaptive optimizers (specifically, those
based on the Adam optimizers). In this manuscript, we introduce a new framework
based on gradient histograms to analyze and justify important attributes of
adaptive optimizers, such as their optimal performance and the relationships
and dependencies among hyperparameters. Furthermore, we propose a novel
gradient histogram-based algorithm that automatically estimates a reduced and
accurate search space for the safeguard hyperparameter $\epsilon$, where the
optimal value can be easily found.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Liver Tumor Prediction with Advanced Attention Mechanisms Integrated
  into a Depth-Based Variant Search Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        P. Kalaiselvi, S. Anusuya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent days, Deep Learning (DL) techniques have become an emerging
transformation in the field of machine learning, artificial intelligence,
computer vision, and so on. Subsequently, researchers and industries have been
highly endorsed in the medical field, predicting and controlling diverse
diseases at specific intervals. Liver tumor prediction is a vital chore in
analyzing and treating liver diseases. This paper proposes a novel approach for
predicting liver tumors using Convolutional Neural Networks (CNN) and a
depth-based variant search algorithm with advanced attention mechanisms
(CNN-DS-AM). The proposed work aims to improve accuracy and robustness in
diagnosing and treating liver diseases. The anticipated model is assessed on a
Computed Tomography (CT) scan dataset containing both benign and malignant
liver tumors. The proposed approach achieved high accuracy in predicting liver
tumors, outperforming other state-of-the-art methods. Additionally, advanced
attention mechanisms were incorporated into the CNN model to enable the
identification and highlighting of regions of the CT scans most relevant to
predicting liver tumors. The results suggest that incorporating attention
mechanisms and a depth-based variant search algorithm into the CNN model is a
promising approach for improving the accuracy and robustness of liver tumor
prediction. It can assist radiologists in their diagnosis and treatment
planning. The proposed system achieved a high accuracy of 95.5% in predicting
liver tumors, outperforming other state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-teacher Distillation for Multilingual Spelling Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingfen Zhang, Xuan Guo, Sravan Bodapati, Christopher Potts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate spelling correction is a critical step in modern search interfaces,
especially in an era of mobile devices and speech-to-text interfaces. For
services that are deployed around the world, this poses a significant challenge
for multilingual NLP: spelling errors need to be caught and corrected in all
languages, and even in queries that use multiple languages. In this paper, we
tackle this challenge using multi-teacher distillation. On our approach, a
monolingual teacher model is trained for each language/locale, and these
individual models are distilled into a single multilingual student model
intended to serve all languages/locales. In experiments using open-source data
as well as user data from a worldwide search service, we show that this leads
to highly effective spelling correction models that can meet the tight latency
requirements of deployed services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token-Level Adversarial <span class="highlight-title">Prompt</span> Detection Based on Perplexity Measures
  and Contextual Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengmian Hu, Gang Wu, Saayan Mitra, Ruiyi Zhang, Tong Sun, Heng Huang, Vishy Swaminathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLM) have emerged as pivotal tools in
various applications. However, these models are susceptible to adversarial
prompt attacks, where attackers can carefully curate input strings that lead to
undesirable outputs. The inherent vulnerability of LLMs stems from their
input-output mechanisms, especially when presented with intensely
out-of-distribution (OOD) inputs. This paper proposes a token-level detection
method to identify adversarial prompts, leveraging the LLM's capability to
predict the next token's probability. We measure the degree of the model's
perplexity and incorporate neighboring token information to encourage the
detection of contiguous adversarial prompt sequences. As a result, we propose
two methods: one that identifies each token as either being part of an
adversarial prompt or not, and another that estimates the probability of each
token being part of an adversarial prompt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiLoRA: Democratizing LoRA for Better Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Wang, Yu Lin, Xiaodong Zeng, Guannan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LoRA achieves remarkable resource efficiency and comparable performance when
adapting LLMs for specific tasks. Since ChatGPT demonstrated superior
performance on various tasks, there has been a growing desire to adapt one
model for all tasks. However, the explicit low-rank of LoRA limits the
adaptation performance in complex multi-task scenarios. LoRA is dominated by a
small number of top singular vectors while fine-tuning decomposes into a set of
less important unitary transforms. In this paper, we propose MultiLoRA for
better multi-task adaptation by reducing the dominance of top singular vectors
observed in LoRA. MultiLoRA scales LoRA modules horizontally and change
parameter initialization of adaptation matrices to reduce parameter dependency,
thus yields more balanced unitary subspaces. We unprecedentedly construct
specialized training data by mixing datasets of instruction follow, natural
language understanding, world knowledge, to cover semantically and
syntactically different samples. With only 2.5% of additional parameters,
MultiLoRA outperforms single LoRA counterparts and fine-tuning on multiple
benchmarks and model scales. Further investigation into weight update matrices
of MultiLoRA exhibits reduced dependency on top singular vectors and more
democratic unitary transform contributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretability in Machine Learning: on the Interplay with
  Explainability, Predictive Performances and Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Leblanc, Pascal Germain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability has recently gained attention in the field of machine
learning, for it is crucial when it comes to high-stakes decisions or
troubleshooting. This abstract concept is hard to grasp and has been
associated, over time, with many labels and preconceived ideas. In this
position paper, in order to clarify some misunderstandings regarding
interpretability, we discuss its relationship with significant concepts in
machine learning: explainability, predictive performances, and machine learning
models. For instance, we challenge the idea that interpretability and
explainability are substitutes to one another, or that a fixed degree of
interpretability can be associated with a given machine learning model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An NMF-Based Building Block for Interpretable Neural Networks With
  Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian K. Vogel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing learning methods often struggle to balance interpretability and
predictive performance. While models like nearest neighbors and non-negative
matrix factorization (NMF) offer high interpretability, their predictive
performance on supervised learning tasks is often limited. In contrast, neural
networks based on the multi-layer perceptron (MLP) support the modular
construction of expressive architectures and tend to have better recognition
accuracy but are often regarded as black boxes in terms of interpretability.
Our approach aims to strike a better balance between these two aspects through
the use of a building block based on NMF that incorporates supervised neural
network training methods to achieve high predictive performance while retaining
the desirable interpretability properties of NMF. We evaluate our Predictive
Factorized Coupling (PFC) block on small datasets and show that it achieves
competitive predictive performance with MLPs while also offering improved
interpretability. We demonstrate the benefits of this approach in various
scenarios, such as continual learning, training on non-i.i.d. data, and
knowledge removal after training. Additionally, we show examples of using the
PFC block to build more expressive architectures, including a fully-connected
residual network as well as a factorized recurrent neural network (RNN) that
performs competitively with vanilla RNNs while providing improved
interpretability. The PFC block uses an iterative inference algorithm that
converges to a fixed point, making it possible to trade off accuracy vs
computation after training but also currently preventing its use as a general
MLP replacement in some scenarios such as training on very large datasets. We
provide source code at https://github.com/bkvogel/pfc
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Center Study on the Adaptability of a Shared Foundation Model
  for Electronic Health Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Lawrence Guo, Jason Fries, Ethan Steinberg, Scott Lanyon Fleming, Keith Morse, Catherine Aftandilian, Jose Posada, Nigam Shah, Lillian Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models hold promise for transforming AI in healthcare by providing
modular components that are easily adaptable to downstream healthcare tasks,
making AI development more scalable and cost-effective. Structured EHR
foundation models, trained on coded medical records from millions of patients,
demonstrated benefits including increased performance with fewer training
labels, and improved robustness to distribution shifts. However, questions
remain on the feasibility of sharing these models across different hospitals
and their performance for local task adaptation. This multi-center study
examined the adaptability of a recently released structured EHR foundation
model ($FM_{SM}$), trained on longitudinal medical record data from 2.57M
Stanford Medicine patients. Experiments were conducted using EHR data at The
Hospital for Sick Children and MIMIC-IV. We assessed both adaptability via
continued pretraining on local data, and task adaptability compared to
baselines of training models from scratch at each site, including a local
foundation model. We evaluated the performance of these models on 8 clinical
prediction tasks. In both datasets, adapting the off-the-shelf $FM_{SM}$
matched the performance of GBM models locally trained on all data while
providing a 13% improvement in settings with few task-specific training labels.
With continued pretraining on local data, label efficiency substantially
improved, such that $FM_{SM}$ required fewer than 1% of training examples to
match the fully trained GBM's performance. Continued pretraining was also 60 to
90% more sample-efficient than training local foundation models from scratch.
Our findings show that adapting shared EHR foundation models across hospitals
provides improved prediction performance at less cost, underscoring the utility
of base foundation models as modular components to streamline the development
of healthcare AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 3 figures, 2 tables, 16 appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Interpolation Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gao, Jian Huang, Yuling Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian denoising has emerged as a powerful principle for constructing
simulation-free continuous normalizing flows for generative modeling. Despite
their empirical successes, theoretical properties of these flows and the
regularizing effect of Gaussian denoising have remained largely unexplored. In
this work, we aim to address this gap by investigating the well-posedness of
simulation-free continuous normalizing flows built on Gaussian denoising.
Through a unified framework termed Gaussian interpolation flow, we establish
the Lipschitz regularity of the flow velocity field, the existence and
uniqueness of the flow, and the Lipschitz continuity of the flow map and the
time-reversed flow map for several rich classes of target distributions. This
analysis also sheds light on the auto-encoding and cycle-consistency properties
of Gaussian interpolation flows. Additionally, we delve into the stability of
these flows in source distributions and perturbations of the velocity field,
using the quadratic Wasserstein distance as a metric. Our findings offer
valuable insights into the learning techniques employed in Gaussian
interpolation flows for generative modeling, providing a solid theoretical
foundation for end-to-end error analyses of learning GIFs with empirical
observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSGNN: Conquering Noisy Node labels via Dynamic Class-wise Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Zhen Tan, Kai Shu, Zongsheng Cao, Yu Kong, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have emerged as a powerful tool for
representation learning on graphs, but they often suffer from overfitting and
label noise issues, especially when the data is scarce or imbalanced. Different
from the paradigm of previous methods that rely on single-node confidence, in
this paper, we introduce a novel Class-wise Selection for Graph Neural
Networks, dubbed CSGNN, which employs a neighbor-aggregated latent space to
adaptively select reliable nodes across different classes. Specifically, 1) to
tackle the class imbalance issue, we introduce a dynamic class-wise selection
mechanism, leveraging the clustering technique to identify clean nodes based on
the neighbor-aggregated confidences. In this way, our approach can avoid the
pitfalls of biased sampling which is common with global threshold techniques.
2) To alleviate the problem of noisy labels, built on the concept of the
memorization effect, CSGNN prioritizes learning from clean nodes before noisy
ones, thereby iteratively enhancing model performance while mitigating label
noise. Through extensive experiments, we demonstrate that CSGNN outperforms
state-of-the-art methods in terms of both effectiveness and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Post-Market Monitoring Framework for Machine Learning-based
  Medical Devices: A case study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Feng, Adarsh Subbaswamy, Alexej Gossmann, Harvineet Singh, Berkman Sahiner, Mi-Ok Kim, Gene Pennello, Nicholas Petrick, Romain Pirracchio, Fan Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  After a machine learning (ML)-based system is deployed in clinical practice,
performance monitoring is important to ensure the safety and effectiveness of
the algorithm over time. The goal of this work is to highlight the complexity
of designing a monitoring strategy and the need for a systematic framework that
compares the multitude of monitoring options. One of the main decisions is
choosing between using real-world (observational) versus interventional data.
Although the former is the most convenient source of monitoring data, it
exhibits well-known biases, such as confounding, selection, and missingness. In
fact, when the ML algorithm interacts with its environment, the algorithm
itself may be a primary source of bias. On the other hand, a carefully designed
interventional study that randomizes individuals can explicitly eliminate such
biases, but the ethics, feasibility, and cost of such an approach must be
carefully considered. Beyond the decision of the data source, monitoring
strategies vary in the performance criteria they track, the interpretability of
the test statistics, the strength of their assumptions, and their speed at
detecting performance decay. As a first step towards developing a framework
that compares the various monitoring options, we consider a case study of an
ML-based risk prediction algorithm for postoperative nausea and vomiting
(PONV). Bringing together tools from causal inference and statistical process
control, we walk through the basic steps of defining candidate monitoring
criteria, describing potential sources of bias and the causal model, and
specifying and comparing candidate monitoring procedures. We hypothesize that
these steps can be applied more generally, as causal inference can address
other sources of biases as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Ended Instructable Embodied Agents with Memory-Augmented Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Sarch, Yue Wu, Michael J. Tarr, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained and frozen large language models (LLMs) can effectively map
simple scene rearrangement instructions to programs over a robot's visuomotor
functions through appropriate few-shot example prompting. To parse open-domain
natural language and adapt to a user's idiosyncratic procedures, not known
during prompt engineering time, fixed prompts fall short. In this paper, we
introduce HELPER, an embodied agent equipped with an external memory of
language-program pairs that parses free-form human-robot dialogue into action
programs through retrieval-augmented LLM prompting: relevant memories are
retrieved based on the current dialogue, instruction, correction, or VLM
description, and used as in-context prompt examples for LLM querying. The
memory is expanded during deployment to include pairs of user's language and
action plans, to assist future inferences and personalize them to the user's
language and routines. HELPER sets a new state-of-the-art in the TEACh
benchmark in both Execution from Dialog History (EDH) and Trajectory from
Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for
TfD. Our models, code, and video results can be found in our project's website:
https://helper-agent-llm.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page with code & videos: https://helper-agent-llm.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ERUDITE: Human-in-the-Loop IoT for an Adaptive Personalized Learning
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04292v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04292v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mojtaba Taherisadr, Mohammad Abdullah Al Faruque, Salma Elmalaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to the rapid growth in wearable technologies and recent advancement in
machine learning and signal processing, monitoring complex human contexts
becomes feasible, paving the way to develop human-in-the-loop IoT systems that
naturally evolve to adapt to the human and environment state autonomously.
Nevertheless, a central challenge in designing many of these IoT systems arises
from the requirement to infer the human mental state, such as intention,
stress, cognition load, or learning ability. While different human contexts can
be inferred from the fusion of different sensor modalities that can correlate
to a particular mental state, the human brain provides a richer sensor modality
that gives us more insights into the required human context. This paper
proposes ERUDITE, a human-in-the-loop IoT system for the learning environment
that exploits recent wearable neurotechnology to decode brain signals. Through
insights from concept learning theory, ERUDITE can infer the human state of
learning and understand when human learning increases or declines. By
quantifying human learning as an input sensory signal, ERUDITE can provide
adequate personalized feedback to humans in a learning environment to enhance
their learning experience. ERUDITE is evaluated across $15$ participants and
showed that by using the brain signals as a sensor modality to infer the human
learning state and providing personalized adaptation to the learning
environment, the participants' learning performance increased on average by
$26\%$. Furthermore, we showed that ERUDITE can be deployed on an edge-based
prototype to evaluate its practicality and scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>It is under review in the IEEE IoT journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Task Embeddings for Teamwork Adaptation in Multi-Agent
  Reinforcement Learning <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Schäfer, Filippos Christianos, Amos Storkey, Stefano V. Albrecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Successful deployment of multi-agent reinforcement learning often requires
agents to adapt their behaviour. In this work, we discuss the problem of
teamwork adaptation in which a team of agents needs to adapt their policies to
solve novel tasks with limited fine-tuning. Motivated by the intuition that
agents need to be able to identify and distinguish tasks in order to adapt
their behaviour to the current task, we propose to learn multi-agent task
embeddings (MATE). These task embeddings are trained using an encoder-decoder
architecture optimised for reconstruction of the transition and reward
functions which uniquely identify tasks. We show that a team of agents is able
to adapt to novel tasks when provided with task embeddings. We propose three
MATE training paradigms: independent MATE, centralised MATE, and mixed MATE
which vary in the information used for the task encoding. We show that the
embeddings learned by MATE identify tasks and provide useful information which
agents leverage during adaptation to novel tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at the Seventh Workshop on Generalization in Planning
  at the NeurIPS 2023 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-Path Learning for Multi-relational Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Ferrini, Antonio Longa, Andrea Passerini, Manfred Jaeger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing multi-relational graph neural networks use one of two strategies for
identifying informative relations: either they reduce this problem to low-level
weight learning, or they rely on handcrafted chains of relational dependencies,
called meta-paths. However, the former approach faces challenges in the
presence of many relations (e.g., knowledge graphs), while the latter requires
substantial domain expertise to identify relevant meta-paths. In this work we
propose a novel approach to learn meta-paths and meta-path GNNs that are highly
accurate based on a small number of informative meta-paths. Key element of our
approach is a scoring function for measuring the potential informativeness of a
relation in the incremental construction of the meta-path. Our experimental
evaluation shows that the approach manages to correctly identify relevant
meta-paths even with a large number of relations, and substantially outperforms
existing multi-relational GNNs on synthetic and real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infinite Width Graph Neural Networks for Node Regression/ Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08176v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08176v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunus Cobanoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work analyzes Graph Neural Networks, a generalization of Fully-Connected
Deep Neural Nets on Graph structured data, when their width, that is the number
of nodes in each fullyconnected layer is increasing to infinity. Infinite Width
Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels,
both Machine Learning Frameworks with long traditions and extensive theoretical
foundations. Gaussian Processes and Kernels have much less hyperparameters then
Neural Networks and can be used for uncertainty estimation, making them more
user friendly for applications. This works extends the increasing amount of
research connecting Gaussian Processes and Kernels to Neural Networks. The
Kernel and Gaussian Process closed forms are derived for a variety of
architectures, namely the standard Graph Neural Network, the Graph Neural
Network with Skip-Concatenate Connections and the Graph Attention Neural
Network. All architectures are evaluated on a variety of datasets on the task
of transductive Node Regression and Classification. Additionally, a Spectral
Sparsification method known as Effective Resistance is used to improve runtime
and memory requirements. Extending the setting to inductive graph learning
tasks (Graph Regression/ Classification) is straightforward and is briefly
discussed in 3.5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 Pages, 2 Figures (with subfigures), multiple tables, v2: made
  table of contents fit to one page and added derivatives on GAT*NTK and GAT*GP
  in A.4, v3: shorten parts of introduction and fixed typos, added numberings
  to equations and discussion section, v4: fix two missing citations on page 10</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Let the Flows Tell: Solving Graph Combinatorial Optimization Problems
  with GFlowNets <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17010v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17010v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron Courville, <span class="highlight-author">Yoshua Bengio</span>, Ling Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial optimization (CO) problems are often NP-hard and thus out of
reach for exact algorithms, making them a tempting domain to apply machine
learning methods. The highly structured constraints in these problems can
hinder either optimization or sampling directly in the solution space. On the
other hand, GFlowNets have recently emerged as a powerful machinery to
efficiently sample from composite unnormalized densities sequentially and have
the potential to amortize such solution-searching processes in CO, as well as
generate diverse solution candidates. In this paper, we design Markov decision
processes (MDPs) for different combinatorial problems and propose to train
conditional GFlowNets to sample from the solution space. Efficient training
techniques are also developed to benefit long-range credit assignment. Through
extensive experiments on a variety of different CO tasks with synthetic and
realistic data, we demonstrate that GFlowNet policies can efficiently find
high-quality solutions. Our implementation is open-sourced at
https://github.com/zdhNarsil/GFlowNet-CombOpt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023 as spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sustainable Concrete via Bayesian Optimization <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18288v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18288v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Ament, Andrew Witte, Nishant Garg, Julius Kusuma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eight percent of global carbon dioxide emissions can be attributed to the
production of cement, the main component of concrete, which is also the
dominant source of CO2 emissions in the construction of data centers. The
discovery of lower-carbon concrete formulae is therefore of high significance
for sustainability. However, experimenting with new concrete formulae is time
consuming and labor intensive, as one usually has to wait to record the
concrete's 28-day compressive strength, a quantity whose measurement can by its
definition not be accelerated. This provides an opportunity for experimental
design methodology like Bayesian Optimization (BO) to accelerate the search for
strong and sustainable concrete formulae. Herein, we 1) propose modeling steps
that make concrete strength amenable to be predicted accurately by a Gaussian
process model with relatively few measurements, 2) formulate the search for
sustainable concrete as a multi-objective optimization problem, and 3) leverage
the proposed model to carry out multi-objective BO with real-world strength
measurements of the algorithmically proposed mixes. Our experimental results
show improved trade-offs between the mixtures' global warming potential (GWP)
and their associated compressive strengths, compared to mixes based on current
industry practices. Our methods are open-sourced at
github.com/facebookresearch/SustainableConcrete.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 Workshop on Adaptive Experimental Design and Active
  Learning in the Real World</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balancing stability and plasticity in continual learning: the
  readout-decomposition of activation change (RDAC) framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04741v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04741v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Anthes, Sushrut Thorat, Peter König, Tim C. Kietzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) algorithms strive to acquire new knowledge while
preserving prior information. However, this stability-plasticity trade-off
remains a central challenge. This paper introduces a framework that dissects
this trade-off, offering valuable insights into CL algorithms. The
Readout-Decomposition of Activation Change (RDAC) framework first addresses the
stability-plasticity dilemma and its relation to catastrophic forgetting. It
relates learning-induced activation changes in the range of prior readouts to
the degree of stability and changes in the null space to the degree of
plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the
framework clarifies the stability-plasticity trade-offs of the popular
regularization algorithms Synaptic intelligence (SI), Elastic-weight
consolidation (EWC), and learning without Forgetting (LwF), and replay-based
algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay
preserved stability and plasticity, while SI, EWC, and LwF traded off
plasticity for stability. The inability of the regularization algorithms to
maintain plasticity was linked to them restricting the change of activations in
the null space of the prior readout. Additionally, for one-hidden-layer linear
neural networks, we derived a gradient decomposition algorithm to restrict
activation change only in the range of the prior readouts, to maintain high
stability while not further sacrificing plasticity. Results demonstrate that
the algorithm maintained stability without significant plasticity loss. The
RDAC framework informs the behavior of existing CL algorithms and paves the way
for novel CL approaches. Finally, it sheds light on the connection between
learning-induced activation/representation changes and the stability-plasticity
dilemma, also offering insights into representational drift in biological
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, Revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JaxMARL: Multi-Agent RL Environments in JAX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10090v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10090v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Gardar Ingvarsson, Timon Willi, Akbir Khan, Christian Schroeder de Witt, Alexandra Souly, Saptarashmi Bandyopadhyay, Mikayel Samvelyan, Minqi Jiang, Robert Tjarko Lange, Shimon Whiteson, Bruno Lacerda, Nick Hawes, Tim Rocktaschel, Chris Lu, Jakob Nicolaus Foerster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarks play an important role in the development of machine learning
algorithms. For example, research in reinforcement learning (RL) has been
heavily influenced by available environments and benchmarks. However, RL
environments are traditionally run on the CPU, limiting their scalability with
typical academic compute. Recent advancements in JAX have enabled the wider use
of hardware acceleration to overcome these computational hurdles, enabling
massively parallel RL training pipelines and environments. This is particularly
useful for multi-agent reinforcement learning (MARL) research. First of all,
multiple agents must be considered at each environment step, adding
computational burden, and secondly, the sample complexity is increased due to
non-stationarity, decentralised partial observability, or other MARL
challenges. In this paper, we present JaxMARL, the first open-source code base
that combines ease-of-use with GPU enabled efficiency, and supports a large
number of commonly used MARL environments as well as popular baseline
algorithms. When considering wall clock time, our experiments show that per-run
our JAX-based training pipeline is up to 12500x faster than existing
approaches. This enables efficient and thorough evaluations, with the potential
to alleviate the evaluation crisis of the field. We also introduce and
benchmark SMAX, a vectorised, simplified version of the popular StarCraft
Multi-Agent Challenge, which removes the need to run the StarCraft II game
engine. This not only enables GPU acceleration, but also provides a more
flexible MARL environment, unlocking the potential for self-play,
meta-learning, and other future applications in MARL. We provide code at
https://github.com/flairox/jaxmarl.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward Teaching for Federated Multi-armed Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02441v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02441v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengshuai Shi, Wei Xiong, Cong Shen, Jing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the existing federated multi-armed bandits (FMAB) designs are based
on the presumption that clients will implement the specified design to
collaborate with the server. In reality, however, it may not be possible to
modify the clients' existing protocols. To address this challenge, this work
focuses on clients who always maximize their individual cumulative rewards, and
introduces a novel idea of ``reward teaching'', where the server guides the
clients towards global optimality through implicit local reward adjustments.
Under this framework, the server faces two tightly coupled tasks of bandit
learning and target teaching, whose combination is non-trivial and challenging.
A phased approach, called Teaching-After-Learning (TAL), is first designed to
encourage and discourage clients' explorations separately. General performance
analyses of TAL are established when the clients' strategies satisfy certain
mild requirements. With novel technical approaches developed to analyze the
warm-start behaviors of bandit algorithms, particularized guarantees of TAL
with clients running UCB or epsilon-greedy strategies are then obtained. These
results demonstrate that TAL achieves logarithmic regrets while only incurring
logarithmic adjustment costs, which is order-optimal w.r.t. a natural lower
bound. As a further extension, the Teaching-While-Learning (TWL) algorithm is
developed with the idea of successive arm elimination to break the non-adaptive
phase separation in TAL. Rigorous analyses demonstrate that when facing clients
with UCB1, TWL outperforms TAL in terms of the dependencies on sub-optimality
gaps thanks to its adaptive design. Experimental results demonstrate the
effectiveness and generality of the proposed algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20049v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20049v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Künzli, Florian Grötschla, Joël Mathys, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulating fluid dynamics is crucial for the design and development process,
ranging from simple valves to complex turbomachinery. Accurately solving the
underlying physical equations is computationally expensive. Therefore,
learning-based solvers that model interactions on meshes have gained interest
due to their promising speed-ups. However, it is unknown to what extent these
models truly understand the underlying physical principles and can generalize
rather than interpolate. Generalization is a key requirement for a
general-purpose fluid simulator, which should adapt to different topologies,
resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to
test the $\textit{generalization}$ of learned graph-based fluid simulators.
SURF comprises individual datasets and provides specific performance and
generalization metrics for evaluating and comparing different models. We
empirically demonstrate the applicability of SURF by thoroughly investigating
the two state-of-the-art graph-based models, yielding new insights into their
generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LoG 2023, Learning on Graphs Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Transportable Causal Network Model Based on Observational
  Healthcare Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Bernasconi, Alessio Zanga, Peter J. F. Lucas, Marco Scutari, Fabio Stella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last decades, many prognostic models based on artificial
intelligence techniques have been used to provide detailed predictions in
healthcare. Unfortunately, the real-world observational data used to train and
validate these models are almost always affected by biases that can strongly
impact the outcomes validity: two examples are values missing not-at-random and
selection bias. Addressing them is a key element in achieving transportability
and in studying the causal relationships that are critical in clinical decision
making, going beyond simpler statistical approaches based on probabilistic
association.
  In this context, we propose a novel approach that combines selection
diagrams, missingness graphs, causal discovery and prior knowledge into a
single graphical model to estimate the cardiovascular risk of adolescent and
young females who survived breast cancer. We learn this model from data
comprising two different cohorts of patients. The resulting causal network
model is validated by expert clinicians in terms of risk assessment, accuracy
and explainability, and provides a prognostic model that outperforms competing
machine learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structural Node Embeddings with Homomorphism Counts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hinrikus Wolf, Luca Oeljeklaus, Pascal Kühner, Martin Grohe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph homomorphism counts, first explored by Lov\'asz in 1967, have recently
garnered interest as a powerful tool in graph-based machine learning. Grohe
(PODS 2020) proposed the theoretical foundations for using homomorphism counts
in machine learning on graph level as well as node level tasks. By their very
nature, these capture local structural information, which enables the creation
of robust structural embeddings. While a first approach for graph level tasks
has been made by Nguyen and Maehara (ICML 2020), we experimentally show the
effectiveness of homomorphism count based node embeddings. Enriched with node
labels, node weights, and edge weights, these offer an interpretable
representation of graph data, allowing for enhanced explainability of machine
learning models.
  We propose a theoretical framework for isomorphism-invariant homomorphism
count based embeddings which lend themselves to a wide variety of downstream
tasks. Our approach capitalises on the efficient computability of graph
homomorphism counts for bounded treewidth graph classes, rendering it a
practical solution for real-world applications. We demonstrate their
expressivity through experiments on benchmark datasets. Although our results do
not match the accuracy of state-of-the-art neural architectures, they are
comparable to other advanced graph learning models. Remarkably, our approach
demarcates itself by ensuring explainability for each individual feature. By
integrating interpretable machine learning algorithms like SVMs or Random
Forests, we establish a seamless, end-to-end explainable pipeline. Our study
contributes to the advancement of graph-based techniques that offer both
performance and interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynthEnsemble: A Fusion of CNN, Vision <span class="highlight-title">Transformer</span>, and Hybrid Models
  for Multi-Label Chest X-Ray Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. M. Nabil Ashraf, Md. Adyelullahil Mamun, Hasnat Md. Abdullah, Md. Golam Rabiul Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-rays are widely used to diagnose thoracic diseases, but the lack of
detailed information about these abnormalities makes it challenging to develop
accurate automated diagnosis systems, which is crucial for early detection and
effective treatment. To address this challenge, we employed deep learning
techniques to identify patterns in chest X-rays that correspond to different
diseases. We conducted experiments on the "ChestX-ray14" dataset using various
pre-trained CNNs, transformers, hybrid(CNN+Transformer) models and classical
models. The best individual model was the CoAtNet, which achieved an area under
the receiver operating characteristic curve (AUROC) of 84.2%. By combining the
predictions of all trained models using a weighted average ensemble where the
weight of each model was determined using differential evolution, we further
improved the AUROC to 85.4%, outperforming other state-of-the-art methods in
this field. Our findings demonstrate the potential of deep learning techniques,
particularly ensemble deep learning, for improving the accuracy of automatic
diagnosis of thoracic diseases from chest X-rays.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in International Conference on Computer and Information
  Technology (ICCIT) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lag-Llama: Towards Foundation Models for Time Series Forecasting <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Biloš, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, Sahil Garg, Alexandre Drouin, Nicolas Chapados, Yuriy Nevmyvaka, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming to build foundation models for time-series forecasting and study their
scaling behavior, we present here our work-in-progress on Lag-Llama, a
general-purpose univariate probabilistic time-series forecasting model trained
on a large collection of time-series data. The model shows good zero-shot
prediction capabilities on unseen "out-of-distribution" time-series datasets,
outperforming supervised baselines. We use smoothly broken power-laws to fit
and predict model scaling behavior. The open source code is made available at
https://github.com/kashif/pytorch-transformer-ts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary Draft. Accepted at NeurIPS 2023 R0-FoMo Workshop. Full
  paper coming soon with comprehensive results and open-source model
  checkpoints</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Channel and Gradient-Importance Aware Device Scheduling for Over-the-Air
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchang Sun, Zehong lin, Yuyi Mao, Shi Jin, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a popular privacy-preserving distributed training
scheme, where multiple devices collaborate to train machine learning models by
uploading local model updates. To improve communication efficiency,
over-the-air computation (AirComp) has been applied to FL, which leverages
analog modulation to harness the superposition property of radio waves such
that numerous devices can upload their model updates concurrently for
aggregation. However, the uplink channel noise incurs considerable model
aggregation distortion, which is critically determined by the device scheduling
and compromises the learned model performance. In this paper, we propose a
probabilistic device scheduling framework for over-the-air FL, named PO-FL, to
mitigate the negative impact of channel noise, where each device is scheduled
according to a certain probability and its model update is reweighted using
this probability in aggregation. We prove the unbiasedness of this aggregation
scheme and demonstrate the convergence of PO-FL on both convex and non-convex
loss functions. Our convergence bounds unveil that the device scheduling
affects the learning performance through the communication distortion and
global update variance. Based on the convergence analysis, we further develop a
channel and gradient-importance aware algorithm to optimize the device
scheduling probabilities in PO-FL. Extensive simulation results show that the
proposed PO-FL framework with channel and gradient-importance awareness
achieves faster convergence and produces better models than baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15363v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15363v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL
task. However, the absence of a systematical benchmark inhibits the development
of designing effective, efficient and economic LLM-based Text-to-SQL solutions.
To address this challenge, in this paper, we first conduct a systematical and
extensive comparison over existing prompt engineering methods, including
question representation, example selection and example organization, and with
these experimental results, we elaborate their pros and cons. Based on these
findings, we propose a new integrated solution, named DAIL-SQL, which refreshes
the Spider leaderboard with 86.6% execution accuracy and sets a new bar. To
explore the potential of open-source LLM, we investigate them in various
scenarios, and further enhance their performance with supervised fine-tuning.
Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well
as the advantages and disadvantages of the supervised fine-tuning.
Additionally, towards an efficient and economic LLM-based Text-to-SQL solution,
we emphasize the token efficiency in prompt engineering and compare the prior
studies under this metric. We hope that our work provides a deeper
understanding of Text-to-SQL with LLMs, and inspires further investigations and
broad applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We have released code on https://github.com/BeachWang/DAIL-SQL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Variational Autoencoder for Heterogeneous Temporal and Longitudinal
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.09369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.09369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mine Öğretir, Siddharth Ramchandran, Dimitrios Papatheodorou, Harri Lähdesmäki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The variational autoencoder (VAE) is a popular deep latent variable model
used to analyse high-dimensional datasets by learning a low-dimensional latent
representation of the data. It simultaneously learns a generative model and an
inference network to perform approximate posterior inference. Recently proposed
extensions to VAEs that can handle temporal and longitudinal data have
applications in healthcare, behavioural modelling, and predictive maintenance.
However, these extensions do not account for heterogeneous data (i.e., data
comprising of continuous and discrete attributes), which is common in many
real-life applications. In this work, we propose the heterogeneous longitudinal
VAE (HL-VAE) that extends the existing temporal and longitudinal VAEs to
heterogeneous data. HL-VAE provides efficient inference for high-dimensional
datasets and includes likelihood models for continuous, count, categorical, and
ordinal data while accounting for missing observations. We demonstrate our
model's efficacy through simulated as well as clinical datasets, and show that
our proposed model achieves competitive performance in missing value imputation
and predictive accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HEALNet -- Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Hemker, Nikola Simidjievski, Mateja Jamnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Technological advances in medical data collection such as high-resolution
histopathology and high-throughput genomic sequencing have contributed to the
rising requirement for multi-modal biomedical modelling, specifically for
image, tabular, and graph data. Most multi-modal deep learning approaches use
modality-specific architectures that are trained separately and cannot capture
the crucial cross-modal information that motivates the integration of different
data sources. This paper presents the Hybrid Early-fusion Attention Learning
Network (HEALNet): a flexible multi-modal fusion architecture, which a)
preserves modality-specific structural information, b) captures the cross-modal
interactions and structural information in a shared latent space, c) can
effectively handle missing modalities during training and inference, and d)
enables intuitive model inspection by learning on the raw data input instead of
opaque embeddings. We conduct multi-modal survival analysis on Whole Slide
Images and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas
(TCGA). HEALNet achieves state-of-the-art performance, substantially improving
over both uni-modal and recent multi-modal baselines, whilst being robust in
scenarios with missing modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages body, 5 pages appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient learning of nonlinear prediction models with time-series
  privileged information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07067v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07067v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastian Jung, Fredrik D Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In domains where sample sizes are limited, efficient learning algorithms are
critical. Learning using privileged information (LuPI) offers increased sample
efficiency by allowing prediction models access to auxiliary information at
training time which is unavailable when the models are used. In recent work, it
was shown that for prediction in linear-Gaussian dynamical systems, a LuPI
learner with access to intermediate time series data is never worse and often
better in expectation than any unbiased classical learner. We provide new
insights into this analysis and generalize it to nonlinear prediction tasks in
latent dynamical systems, extending theoretical guarantees to the case where
the map connecting latent variables and observations is known up to a linear
transform. In addition, we propose algorithms based on random features and
representation learning for the case when this map is unknown. A suite of
empirical results confirm theoretical findings and show the potential of using
privileged time-series information in nonlinear prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ timeXplain -- A Framework for Explaining the Predictions of Time Series
  Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.07606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.07606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Mujkanovic, Vanja Doskoč, Martin Schirneck, Patrick Schäfer, Tobias Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern time series classifiers display impressive predictive capabilities,
yet their decision-making processes mostly remain black boxes to the user. At
the same time, model-agnostic explainers, such as the recently proposed SHAP,
promise to make the predictions of machine learning models interpretable,
provided there are well-designed domain mappings. We bring both worlds together
in our timeXplain framework, extending the reach of explainable artificial
intelligence to time series classification and value prediction. We present
novel domain mappings for the time domain, frequency domain, and time series
statistics and analyze their explicative power as well as their limits. We
employ a novel evaluation metric to experimentally compare timeXplain to
several model-specific explanation approaches for state-of-the-art time series
classifiers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages; published code, added combined time slice and frequency band
  mapping, added quantitative evaluation and comparison to model-specific
  explainers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-performance deep spiking neural networks with 0.3 spikes per neuron 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ana Stanojevic, Stanisław Woźniak, Guillaume Bellec, Giovanni Cherubini, Angeliki Pantazi, Wulfram Gerstner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communication by rare, binary spikes is a key factor for the energy
efficiency of biological brains. However, it is harder to train
biologically-inspired spiking neural networks (SNNs) than artificial neural
networks (ANNs). This is puzzling given that theoretical results provide exact
mapping algorithms from ANNs to SNNs with time-to-first-spike (TTFS) coding. In
this paper we analyze in theory and simulation the learning dynamics of
TTFS-networks and identify a specific instance of the vanishing-or-exploding
gradient problem. While two choices of SNN mappings solve this problem at
initialization, only the one with a constant slope of the neuron membrane
potential at threshold guarantees the equivalence of the training trajectory
between SNNs and ANNs with rectified linear units. We demonstrate that training
deep SNN models achieves the exact same performance as that of ANNs, surpassing
previous SNNs on image classification datasets such as MNIST/Fashion-MNIST,
CIFAR10/CIFAR100 and PLACES365. Our SNN accomplishes high-performance
classification with less than 0.3 spikes per neuron, lending itself for an
energy-efficient implementation. We show that fine-tuning SNNs with our robust
gradient descent algorithm enables their optimization for hardware
implementations with low latency and resilience to noise and quantization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Handling Overlapping Asymmetric <span class="highlight-title">Dataset</span>s -- A Twice Penalized P-Spline
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew McTeer, Robin Henderson, Quentin M Anstee, Paolo Missier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overlapping asymmetric datasets are common in data science and pose questions
of how they can be incorporated together into a predictive analysis. In
healthcare datasets there is often a small amount of information that is
available for a larger number of patients such as an electronic health record,
however a small number of patients may have had extensive further testing.
Common solutions such as missing imputation can often be unwise if the smaller
cohort is significantly different in scale to the larger sample, therefore the
aim of this research is to develop a new method which can model the smaller
cohort against a particular response, whilst considering the larger cohort
also. Motivated by non-parametric models, and specifically flexible smoothing
techniques via generalized additive models, we model a twice penalized P-Spline
approximation method to firstly prevent over/under-fitting of the smaller
cohort and secondly to consider the larger cohort. This second penalty is
created through discrepancies in the marginal value of covariates that exist in
both the smaller and larger cohorts. Through data simulations, parameter
tunings and model adaptations to consider a continuous and binary response, we
find our twice penalized approach offers an enhanced fit over a linear B-Spline
and once penalized P-Spline approximation. Applying to a real-life dataset
relating to a person's risk of developing Non-Alcoholic Steatohepatitis, we see
an improved model fit performance of over 65%. Areas for future work within
this space include adapting our method to not require dimensionality reduction
and also consider parametric modelling methods. However, to our knowledge this
is the first work to propose additional marginal penalties in a flexible
regression of which we can report a vastly improved model fit that is able to
consider asymmetric datasets, without the need for missing data imputation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 17 figures, 8 tables, 34 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A novel approach to measuring patent claim scope based on probabilities
  obtained from (large) language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sébastien Ragot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes to measure the scope of a patent claim as the reciprocal
of the self-information contained in this claim. A probability of occurrence of
the claim is obtained from a language model and this probability is used to
compute the self-information. Grounded in information theory, this approach is
based on the assumption that an unlikely concept is more informative than a
usual concept, insofar as it is more surprising. In turn, the more surprising
the information required to defined the claim, the narrower its scope. Five
language models are considered, ranging from simplest models (each word or
character is assigned an identical probability) to intermediate models (using
average word or character frequencies), to a large language model (GPT2).
Interestingly, the scope resulting from the simplest language models is
proportional to the reciprocal of the number of words or characters involved in
the claim, a metric already used in previous works. Application is made to
multiple series of patent claims directed to distinct inventions, where each
series consists of claims devised to have a gradually decreasing scope. The
performance of the language models is assessed with respect to several ad hoc
tests. The more sophisticated the model, the better the results. I.e., the GPT2
probability model outperforms models based on word and character frequencies,
which themselves outdo the simplest models based on word or character counts.
Still, the character count appears to be a more reliable indicator than the
word count.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages, 8 tables, 6 figures. Substantial changes made to version 2:
  New section 4.1 added (including a new table); Minor normalization issue
  corrected in values listed in Appendix B; Content of former appendix C now
  moved to Section 3; and new Appendix C added. Minor changes made to version 3
  (style, typos, language)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi Time Scale World Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaisakh Shaj, Saleh Gholam Zadeh, Ozan Demir, Luiz Ricardo Douat, Gerhard Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent agents use internal world models to reason and make predictions
about different courses of their actions at many scales. Devising learning
paradigms and architectures that allow machines to learn world models that
operate at multiple levels of temporal abstractions while dealing with complex
uncertainty predictions is a major technical hurdle. In this work, we propose a
probabilistic formalism to learn multi-time scale world models which we call
the Multi Time Scale State Space (MTS3) model. Our model uses a computationally
efficient inference scheme on multiple time scales for highly accurate
long-horizon predictions and uncertainty estimates over several seconds into
the future. Our experiments, which focus on action conditional long horizon
future predictions, show that MTS3 outperforms recent methods on several system
identification benchmarks including complex simulated and real-world dynamical
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as spotlight at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scattering Vision <span class="highlight-title">Transformer</span>: Spectral Mixing Matters <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badri N. Patro, Vijay Srinivas Agneeswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers have gained significant attention and achieved
state-of-the-art performance in various computer vision tasks, including image
classification, instance segmentation, and object detection. However,
challenges remain in addressing attention complexity and effectively capturing
fine-grained information within images. Existing solutions often resort to
down-sampling operations, such as pooling, to reduce computational cost.
Unfortunately, such operations are non-invertible and can result in information
loss. In this paper, we present a novel approach called Scattering Vision
Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally
scattering network that enables the capture of intricate image details. SVT
overcomes the invertibility issue associated with down-sampling operations by
separating low-frequency and high-frequency components. Furthermore, SVT
introduces a unique spectral gating network utilizing Einstein multiplication
for token and channel mixing, effectively reducing complexity. We show that SVT
achieves state-of-the-art performance on the ImageNet dataset with a
significant reduction in a number of parameters and FLOPS. SVT shows 2\%
improvement over LiTv2 and iFormer. SVT-H-S reaches 84.2\% top-1 accuracy,
while SVT-H-B reaches 85.2\% (state-of-art for base versions) and SVT-H-L
reaches 85.7\% (again state-of-art for large versions). SVT also shows
comparable results in other vision tasks such as instance segmentation. SVT
also outperforms other transformers in transfer learning on standard datasets
such as CIFAR10, CIFAR100, Oxford Flower, and Stanford Car datasets. The
project page is available on this
webpage.\url{https://badripatro.github.io/svt/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted @NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Minder, Florian Grötschla, Joël Mathys, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce an extension to the CLRS algorithmic learning benchmark,
prioritizing scalability and the utilization of sparse representations. Many
algorithms in CLRS require global memory or information exchange, mirrored in
its execution model, which constructs fully connected (not sparse) graphs based
on the underlying problem. Despite CLRS's aim of assessing how effectively
learned algorithms can generalize to larger instances, the existing execution
model becomes a significant constraint due to its demanding memory requirements
and runtime (hard to scale). However, many important algorithms do not demand a
fully connected graph; these algorithms, primarily distributed in nature, align
closely with the message-passing paradigm employed by Graph Neural Networks.
Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark
specifically with scalability and sparseness in mind. Our approach includes
adapted algorithms from the original CLRS benchmark and introduces new problems
from distributed and randomized algorithms. Moreover, we perform a thorough
empirical evaluation of our benchmark. Code is publicly available at
https://github.com/jkminder/SALSA-CLRS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>(Extended Abstract) Presented at the Second Learning on Graphs
  Conference (LoG 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Examples Are Not Real Features <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Li, Yifei Wang, Yiwen Guo, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existence of adversarial examples has been a mystery for years and
attracted much interest. A well-known theory by \citet{ilyas2019adversarial}
explains adversarial vulnerability from a data perspective by showing that one
can extract non-robust features from adversarial examples and these features
alone are useful for classification. However, the explanation remains quite
counter-intuitive since non-robust features are mostly noise features to
humans. In this paper, we re-examine the theory from a larger context by
incorporating multiple learning paradigms. Notably, we find that contrary to
their good usefulness under supervised learning, non-robust features attain
poor usefulness when transferred to other self-supervised learning paradigms,
such as contrastive learning, masked image modeling, and diffusion models. It
reveals that non-robust features are not really as useful as robust or natural
features that enjoy good transferability between these paradigms. Meanwhile,
for robustness, we also show that naturally trained encoders from robust
features are largely non-robust under AutoAttack. Our cross-paradigm
examination suggests that the non-robust features are not really useful but
more like paradigm-wise shortcuts, and robust features alone might be
insufficient to attain reliable model robustness. Code is available at
\url{https://github.com/PKU-ML/AdvNotRealFeatures}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A benchmark of categorical encoders for binary classification <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09191v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09191v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Matteucci, Vadim Arzamasov, Klemens Boehm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Categorical encoders transform categorical features into numerical
representations that are indispensable for a wide range of machine learning
models. Existing encoder benchmark studies lack generalizability because of
their limited choice of (1) encoders, (2) experimental factors, and (3)
datasets. Additionally, inconsistencies arise from the adoption of varying
aggregation strategies. This paper is the most comprehensive benchmark of
categorical encoders to date, including an extensive evaluation of 32
configurations of encoders from diverse families, with 36 combinations of
experimental factors, and on 50 datasets. The study shows the profound
influence of dataset selection, experimental factors, and aggregation
strategies on the benchmark's conclusions -- aspects disregarded in previous
encoder benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the 37th Conference on Neural Information
  Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Augmented Machine Learning with Applications in Autonomous
  Driving: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.04712v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.04712v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Wörmann, Daniel Bogdoll, Christian Brunner, Etienne Bührle, Han Chen, Evaristus Fuh Chuo, Kostadin Cvejoski, Ludger van Elst, Philip Gottschall, Stefan Griesche, Christian Hellert, Christian Hesels, Sebastian Houben, Tim Joseph, Niklas Keil, Johann Kelsch, Mert Keser, Hendrik Königshof, Erwin Kraft, Leonie Kreuser, Kevin Krone, Tobias Latka, Denny Mattern, Stefan Matthes, Franz Motzkus, Mohsin Munir, Moritz Nekolla, Adrian Paschke, Stefan Pilar von Pilchau, Maximilian Alexander Pintz, Tianming Qiu, Faraz Qureishi, Syed Tahseen Raza Rizvi, Jörg Reichardt, Laura von Rueden, Alexander Sagel, Diogo Sasdelli, Tobias Scholl, Gerhard Schunk, Gesina Schwalbe, Hao Shen, Youssef Shoeb, Hendrik Stapelbroek, Vera Stehr, Gurucharan Srinivas, Anh Tuan Tran, Abhishek Vivekanandan, Ya Wang, Florian Wasserrab, Tino Werner, Christian Wirth, Stefan Zwicklbauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of representative datasets is an essential prerequisite for
many successful artificial intelligence and machine learning models. However,
in real life applications these models often encounter scenarios that are
inadequately represented in the data used for training. There are various
reasons for the absence of sufficient data, ranging from time and cost
constraints to ethical considerations. As a consequence, the reliable usage of
these models, especially in safety-critical applications, is still a tremendous
challenge. Leveraging additional, already existing sources of knowledge is key
to overcome the limitations of purely data-driven approaches. Knowledge
augmented machine learning approaches offer the possibility of compensating for
deficiencies, errors, or ambiguities in the data, thus increasing the
generalization capability of the applied models. Even more, predictions that
conform with knowledge are crucial for making trustworthy and safe decisions
even in underrepresented scenarios. This work provides an overview of existing
techniques and methods in the literature that combine data-driven models with
existing knowledge. The identified approaches are structured according to the
categories knowledge integration, extraction and conformity. In particular, we
address the application of the presented methods in the field of autonomous
driving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>111 pages, Added section on Run-time Network Verification</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attribution Patching Outperforms Automated Circuit Discovery <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaquib Syed, Can Rager, Arthur Conmy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated interpretability research has recently attracted attention as a
potential research direction that could scale explanations of neural network
behavior to large models. Existing automated circuit discovery work applies
activation patching to identify subnetworks responsible for solving specific
tasks (circuits). In this work, we show that a simple method based on
attribution patching outperforms all existing methods while requiring just two
forward passes and a backward pass. We apply a linear approximation to
activation patching to estimate the importance of each edge in the
computational subgraph. Using this approximation, we prune the least important
edges of the network. We survey the performance and limitations of this method,
finding that averaged over all tasks our method has greater AUC from circuit
recovery than other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 main paper pages, 6 additional pages. NeurIPS 2023 ATTRIB Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Linear Subspace Identification: A Machine Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03197v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03197v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loris Di Natale, Muhammad Zakwan, Bratislav Svetozarevic, Philipp Heer, Giancarlo Ferrari Trecate, Colin N. Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) and linear System Identification (SI) have been
historically developed independently. In this paper, we leverage
well-established ML tools - especially the automatic differentiation framework
- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space
SI methods using backpropagation. SIMBa relies on a novel
Linear-Matrix-Inequality-based free parametrization of Schur matrices to ensure
the stability of the identified model.
  We show how SIMBa generally outperforms traditional linear state-space SI
methods, and sometimes significantly, although at the price of a higher
computational burden. This performance gap is particularly remarkable compared
to other SI methods with stability guarantees, where the gain is frequently
above 25% in our investigations, hinting at SIMBa's ability to simultaneously
achieve state-of-the-art fitting performance and enforce stability.
Interestingly, these observations hold for a wide variety of input-output
systems and on both simulated and real-world data, showcasing the flexibility
of the proposed approach. We postulate that this new SI paradigm presents a
great extension potential to identify structured nonlinear models from data,
and we hence open-source SIMBa on https://github.com/Cemempamoi/simba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ECC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relationship between Batch Size and Number of Steps Needed for Nonconvex
  Optimization of Stochastic Gradient Descent using Armijo Line Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.13831v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.13831v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Tsukada, Hideaki Iiduka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic gradient descent (SGD) is the simplest deep learning optimizer
with which to train deep neural networks. While SGD can use various learning
rates, such as constant or diminishing rates, the previous numerical results
showed that SGD performs better than other deep learning optimizers using when
it uses learning rates given by line search methods. In this paper, we perform
a convergence analysis on SGD with a learning rate given by an Armijo line
search for nonconvex optimization. The analysis indicates that the upper bound
of the expectation of the squared norm of the full gradient becomes small when
the number of steps and the batch size are large. Next, we show that, for SGD
with the Armijo-line-search learning rate, the number of steps needed for
nonconvex optimization is a monotone decreasing convex function of the batch
size; that is, the number of steps needed for nonconvex optimization decreases
as the batch size increases. Furthermore, we show that the stochastic
first-order oracle (SFO) complexity, which is the stochastic gradient
computation cost, is a convex function of the batch size; that is, there exists
a critical batch size that minimizes the SFO complexity. Finally, we provide
numerical results that support our theoretical results. The numerical results
indicate that the number of steps needed for training deep neural networks
decreases as the batch size increases and that there exist the critical batch
sizes that can be estimated from the theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Hierarchical Regional <span class="highlight-title">Transformer</span>-based Multiple Instance
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josef Cersovsky, Sadegh Mohammadi, Dagmar Kainmueller, Johannes Hoehne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classification of gigapixel histopathology images with deep multiple
instance learning models has become a critical task in digital pathology and
precision medicine. In this work, we propose a Transformer-based multiple
instance learning approach that replaces the traditional learned attention
mechanism with a regional, Vision Transformer inspired self-attention
mechanism. We present a method that fuses regional patch information to derive
slide-level predictions and show how this regional aggregation can be stacked
to hierarchically process features on different distance levels. To increase
predictive accuracy, especially for datasets with small, local morphological
features, we introduce a method to focus the image processing on high attention
regions during inference. Our approach is able to significantly improve
performance over the baseline on two histopathology datasets and points towards
promising directions for further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, LaTeX; header update after published, fixed typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Role Taxonomy of Units in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.00789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.00789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhao, Hao Zhang, Xiuyuan Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying the role of network units in deep neural networks (DNNs) is
critical in many aspects including giving understandings on the mechanisms of
DNNs and building basic connections between deep learning and neuroscience.
However, there remains unclear on which roles the units in DNNs with different
generalization ability could present. To this end, we give role taxonomy of
units in DNNs via introducing the retrieval-of-function test, where units are
categorized into four types in terms of their functional preference on
separately the training set and testing set. We show that ratios of the four
categories are highly associated with the generalization ability of DNNs from
two distinct perspectives, based on which we give signs of DNNs with well
generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Algebra <span class="highlight-title">Transformer</span> <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18415v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18415v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johann Brehmer, Pim de Haan, Sönke Behrends, Taco Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Problems involving geometric data arise in physics, chemistry, robotics,
computer vision, and many other fields. Such data can take numerous forms, for
instance points, direction vectors, translations, or rotations, but to date
there is no single architecture that can be applied to such a wide variety of
geometric types while respecting their symmetries. In this paper we introduce
the Geometric Algebra Transformer (GATr), a general-purpose architecture for
geometric data. GATr represents inputs, outputs, and hidden states in the
projective geometric (or Clifford) algebra, which offers an efficient
16-dimensional vector-space representation of common geometric objects as well
as operators acting on them. GATr is equivariant with respect to E(3), the
symmetry group of 3D Euclidean space. As a Transformer, GATr is versatile,
efficient, and scalable. We demonstrate GATr in problems from n-body modeling
to wall-shear-stress estimation on large arterial meshes to robotic motion
planning. GATr consistently outperforms both non-geometric and equivariant
baselines in terms of error, data efficiency, and scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2023, implementation available at
  https://github.com/qualcomm-ai-research/geometric-algebra-transformer . v3:
  matches camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Arbitrary Shaped Clustering through Correlated Gaussian Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ole Christian Eidheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is no convincing evidence that backpropagation is a biologically
plausible mechanism, and further studies of alternative learning methods are
needed. A novel online clustering algorithm is presented that can produce
arbitrary shaped clusters from inputs in an unsupervised manner, and requires
no prior knowledge of the number of clusters in the input data. This is
achieved by finding correlated outputs from functions that capture commonly
occurring input patterns. The algorithm can be deemed more biologically
plausible than model optimization through backpropagation, although practical
applicability may require additional research. However, the method yields
satisfactory results on several toy datasets on a noteworthy range of
hyperparameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrected uniform distribution range; removed "average" from last
  sentence in section 4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ oneDNN Graph Compiler: A Hybrid Approach for High-Performance Deep
  Learning Compilation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhui Li, Zhennan Qin, Yijie Mei, Jingze Cui, Yunfei Song, Ciyong Chen, Yifei Zhang, Longsheng Du, Xianhang Cheng, Baihui Jin, Yan Zhang, Igor Safonov, Jason Ye, Eric Lin, Dan Lavery
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of deep learning models and hardware support for
dense computing, the deep learning workload characteristics changed
significantly from a few hot spots on compute-intensive operations to a broad
range of operations scattered across the models. Accelerating a few
compute-intensive operations using the expert-tuned implementation of
primitives does not fully exploit the performance potential of AI hardware.
Various efforts have been made to compile a full deep neural network (DNN)
graph. One of the biggest challenges is to achieve high-performance tensor
compilation by generating expert level performance code for the dense
compute-intensive operations and applying compilation optimization at the scope
of DNN computation graph across multiple compute-intensive operations.
  We present oneDNN Graph Compiler, a tensor compiler that employs a hybrid
approach of using techniques from both compiler optimization and expert-tuned
kernels for high performance code generation of the deep neural network graph.
oneDNN Graph Compiler addresses unique optimization challenges in the deep
learning domain, such as low-precision computation, aggressive fusion of graph
operations, optimization for static tensor shapes and memory layout, constant
weight optimization, and memory buffer reuse. Experimental results demonstrate
significant performance gains over existing tensor compiler and primitives
library for performance-critical DNN computation graphs and end-to-end models
on Intel Xeon Scalable Processors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages excluding reference, 9 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Robust Representation in Adversarial Training: Alignment and
  Exclusion Criteria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuoyan Zhou, Nannan Wang, Decheng Liu, Dawei Zhou, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are vulnerable to adversarial noise. Adversarial
Training (AT) has been demonstrated to be the most effective defense strategy
to protect neural networks from being fooled. However, we find AT omits to
learning robust features, resulting in poor performance of adversarial
robustness. To address this issue, we highlight two criteria of robust
representation: (1) Exclusion: \emph{the feature of examples keeps away from
that of other classes}; (2) Alignment: \emph{the feature of natural and
corresponding adversarial examples is close to each other}. These motivate us
to propose a generic framework of AT to gain robust representation, by the
asymmetric negative contrast and reverse attention. Specifically, we design an
asymmetric negative contrast based on predicted probabilities, to push away
examples of different classes in the feature space. Moreover, we propose to
weight feature by parameters of the linear classifier as the reverse attention,
to obtain class-aware feature and pull close the feature of the same class.
Empirical evaluations on three benchmark datasets show our methods greatly
advance the robustness of AT and achieve state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, Submitted to TIFS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Single-Pass Contrastive Learning Can Work for Both Homophilic and
  Heterophilic Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10890v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10890v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Wang, Jieyu Zhang, Qi Zhu, Wei Huang, Kenji Kawaguchi, Xiaokui Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing graph contrastive learning (GCL) techniques typically require two
forward passes for a single instance to construct the contrastive loss, which
is effective for capturing the low-frequency signals of node features. Such a
dual-pass design has shown empirical success on homophilic graphs, but its
effectiveness on heterophilic graphs, where directly connected nodes typically
have different labels, is unknown. In addition, existing GCL approaches fail to
provide strong performance guarantees. Coupled with the unpredictability of GCL
approaches on heterophilic graphs, their applicability in real-world contexts
is limited. Then, a natural question arises: Can we design a GCL method that
works for both homophilic and heterophilic graphs with a performance guarantee?
To answer this question, we theoretically study the concentration property of
features obtained by neighborhood aggregation on homophilic and heterophilic
graphs, introduce the single-pass augmentation-free graph contrastive learning
loss based on the property, and provide performance guarantees for the
minimizer of the loss on downstream tasks. As a direct consequence of our
analysis, we implement the Single-Pass Graph Contrastive Learning method
(SP-GCL). Empirically, on 14 benchmark datasets with varying degrees of
homophily, the features learned by the SP-GCL can match or outperform existing
strong baselines with significantly less computational overhead, which
demonstrates the usefulness of our findings in real-world cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted for publication by the Transactions on
  Machine Learning Research. OpenReview at:
  https://openreview.net/forum?id=244KePn09i</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Attention-based Deep Reinforcement Learning for solving the
  Chinese Postman Problem with Load-dependent costs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Truong Son Hy, Cong Dao Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Deep reinforcement learning (DRL) models have shown promising
results in solving routing problems. However, most DRL solvers are commonly
proposed to solve node routing problems, such as the Traveling Salesman Problem
(TSP). Meanwhile, there has been limited research on applying neural methods to
arc routing problems, such as the Chinese Postman Problem (CPP), since they
often feature irregular and complex solution spaces compared to TSP. To fill
these gaps, this paper proposes a novel DRL framework to address the CPP with
load-dependent costs (CPP-LC) (Corberan et al., 2018), which is a complex arc
routing problem with load constraints. The novelty of our method is two-fold.
First, we formulate the CPP-LC as a Markov Decision Process (MDP) sequential
model. Subsequently, we introduce an autoregressive model based on DRL, namely
Arc-DRL, consisting of an encoder and decoder to address the CPP-LC challenge
effectively. Such a framework allows the DRL model to work efficiently and
scalably to arc routing problems. Furthermore, we propose a new bio-inspired
meta-heuristic solution based on Evolutionary Algorithm (EA) for CPP-LC.
Extensive experiments show that Arc-DRL outperforms existing meta-heuristic
methods such as Iterative Local Search (ILS) and Variable Neighborhood Search
(VNS) proposed by (Corberan et al., 2018) on large benchmark datasets for
CPP-LC regarding both solution quality and running time; while the EA gives the
best solution quality with much more running time. We release our C++
implementations for metaheuristics such as EA, ILS and VNS along with the code
for data generation and our generated data at
https://github.com/HySonLab/Chinese_Postman_Problem
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Model-Augmented Behavioral Cloning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13335v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13335v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsiang-Chun Wang, Shang-Fu Chen, Ming-Hao Hsu, Chun-Mao Lai, Shao-Hua Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning addresses the challenge of learning by observing an
expert's demonstrations without access to reward signals from environments.
Most existing imitation learning methods that do not require interacting with
environments either model the expert distribution as the conditional
probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s,
a). Despite its simplicity, modeling the conditional probability with BC
usually struggles with generalization. While modeling the joint probability can
lead to improved generalization performance, the inference procedure is often
time-consuming and the model can suffer from manifold overfitting. This work
proposes an imitation learning framework that benefits from modeling both the
conditional and joint probability of the expert distribution. Our proposed
diffusion model-augmented behavioral cloning (DBC) employs a diffusion model
trained to model expert behaviors and learns a policy to optimize both the BC
loss (conditional) and our proposed diffusion model loss (joint). DBC
outperforms baselines in various continuous control tasks in navigation, robot
arm manipulation, dexterous manipulation, and locomotion. We design additional
experiments to verify the limitations of modeling either the conditional
probability or the joint probability of the expert distribution as well as
compare different generative models. Ablation studies justify the effectiveness
of our design choices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Source Bias for Fairer Weak Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changho Shin, Sonia Cromp, Dyah Adila, Frederic Sala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weak supervision enables efficient development of training sets by reducing
the need for ground truth labels. However, the techniques that make weak
supervision attractive -- such as integrating any source of signal to estimate
unknown labels -- also entail the danger that the produced pseudolabels are
highly biased. Surprisingly, given everyday use and the potential for increased
bias, weak supervision has not been studied from the point of view of fairness.
We begin such a study, starting with the observation that even when a fair
model can be built from a dataset with access to ground-truth labels, the
corresponding dataset labeled via weak supervision can be arbitrarily unfair.
To address this, we propose and empirically validate a model for source
unfairness in weak supervision, then introduce a simple counterfactual
fairness-based technique that can mitigate these biases. Theoretically, we show
that it is possible for our approach to simultaneously improve both accuracy
and fairness -- in contrast to standard fairness approaches that suffer from
tradeoffs. Empirically, we show that our technique improves accuracy on weak
supervision baselines by as much as 32\% while reducing demographic parity gap
by 82.5\%. A simple extension of our method aimed at maximizing performance
produces state-of-the-art performance in five out of ten datasets in the WRENCH
benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StyleTTS: A Style-Based Generative Model for Natural and Diverse
  Text-to-Speech Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Aaron Li, Cong Han, Nima Mesgarani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Speech (TTS) has recently seen great progress in synthesizing
high-quality speech owing to the rapid development of parallel TTS systems, but
producing speech with naturalistic prosodic variations, speaking styles and
emotional tones remains challenging. Moreover, since duration and speech are
generated separately, parallel TTS models still have problems finding the best
monotonic alignments that are crucial for naturalistic speech synthesis. Here,
we propose StyleTTS, a style-based generative model for parallel TTS that can
synthesize diverse speech with natural prosody from a reference speech
utterance. With novel Transferable Monotonic Aligner (TMA) and
duration-invariant data augmentation schemes, our method significantly
outperforms state-of-the-art models on both single and multi-speaker datasets
in subjective tests of speech naturalness and speaker similarity. Through
self-supervised learning of the speaking styles, our model can synthesize
speech with the same prosodic and emotional tone as any given reference speech
without the need for explicitly labeling these categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion
  and Adversarial Training with Large Speech Language Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Aaron Li, Cong Han, Vinay S. Raghavan, Gavin Mischler, Nima Mesgarani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that
leverages style diffusion and adversarial training with large speech language
models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its
predecessor by modeling styles as a latent random variable through diffusion
models to generate the most suitable style for the text without requiring
reference speech, achieving efficient latent diffusion while benefiting from
the diverse speech synthesis offered by diffusion models. Furthermore, we
employ large pre-trained SLMs, such as WavLM, as discriminators with our novel
differentiable duration modeling for end-to-end training, resulting in improved
speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker
LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by
native English speakers. Moreover, when trained on the LibriTTS dataset, our
model outperforms previous publicly available models for zero-shot speaker
adaptation. This work achieves the first human-level TTS on both single and
multispeaker datasets, showcasing the potential of style diffusion and
adversarial training with large SLMs. The audio demos and source code are
available at https://styletts2.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Bayesian Optimization via Learning Correlated Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20258v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20258v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghun Lee, Jaewon Chu, Sihyeon Kim, Juyeon Ko, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimization is a powerful method for optimizing black-box functions
with limited function evaluations. Recent works have shown that optimization in
a latent space through deep generative models such as variational autoencoders
leads to effective and efficient Bayesian optimization for structured or
discrete data. However, as the optimization does not take place in the input
space, it leads to an inherent gap that results in potentially suboptimal
solutions. To alleviate the discrepancy, we propose Correlated latent space
Bayesian Optimization (CoBO), which focuses on learning correlated latent
spaces characterized by a strong correlation between the distances in the
latent space and the distances within the objective function. Specifically, our
method introduces Lipschitz regularization, loss weighting, and trust region
recoordination to minimize the inherent gap around the promising areas. We
demonstrate the effectiveness of our approach on several optimization tasks in
discrete data, such as molecule design and arithmetic expression fitting, and
achieve high performance within a small budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Practical Robust Reinforcement Learning: Practical Uncertainty Set
  and Double-Agent Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06657v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06657v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ukjo Hwang, Songnam Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust reinforcement learning (RRL) aims at seeking a robust policy to
optimize the worst case performance over an uncertainty set of Markov decision
processes (MDPs). This set contains some perturbed MDPs from a nominal MDP
(N-MDP) that generate samples for training, which reflects some potential
mismatches between training (i.e., N-MDP) and true environments. In this paper
we present an elaborated uncertainty set by excluding some implausible MDPs
from the existing sets. Under this uncertainty set, we develop a sample-based
RRL algorithm (named ARQ-Learning) for tabular setting and characterize its
finite-time error bound. Also, it is proved that ARQ-Learning converges as fast
as the standard Q-Learning and robust Q-Learning while ensuring better
robustness. We introduce an additional pessimistic agent which can tackle the
major bottleneck for the extension of ARQ-Learning into the cases with larger
or continuous state spaces. Incorporating this idea into RL algorithms, we
propose double-agent algorithms for model-free RRL. Via experiments, we
demonstrate the effectiveness of the proposed algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Battle of the Backbones: A Large-Scale Comparison of <span class="highlight-title">Pretrain</span>ed Models
  across Computer Vision Tasks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Micah Goldblum, Hossein Souri, Renkun Ni, Manli Shu, Viraj Prabhu, Gowthami Somepalli, Prithvijit Chattopadhyay, Mark Ibrahim, Adrien Bardes, Judy Hoffman, Rama Chellappa, Andrew Gordon Wilson, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network based computer vision systems are typically built on a
backbone, a pretrained or randomly initialized feature extractor. Several years
ago, the default option was an ImageNet-trained convolutional neural network.
However, the recent past has seen the emergence of countless backbones
pretrained using various algorithms and datasets. While this abundance of
choice has led to performance increases for a range of systems, it is difficult
for practitioners to make informed decisions about which backbone to choose.
Battle of the Backbones (BoB) makes this choice easier by benchmarking a
diverse suite of pretrained models, including vision-language models, those
trained via self-supervised learning, and the Stable Diffusion backbone, across
a diverse set of computer vision tasks ranging from classification to object
detection to OOD generalization and more. Furthermore, BoB sheds light on
promising directions for the research community to advance computer vision by
illuminating strengths and weakness of existing approaches through a
comprehensive analysis conducted on more than 1500 training runs. While vision
transformers (ViTs) and self-supervised learning (SSL) are increasingly
popular, we find that convolutional neural networks pretrained in a supervised
fashion on large training sets still perform best on most tasks among the
models we consider. Moreover, in apples-to-apples comparisons on the same
architectures and similarly sized pretraining datasets, we find that SSL
backbones are highly competitive, indicating that future works should perform
SSL pretraining with advanced architectures and larger pretraining datasets. We
release the raw results of our experiments along with code that allows
researchers to put their own backbones through the gauntlet here:
https://github.com/hsouri/Battle-of-the-Backbones
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding emergence in data: causal emergence inspired dynamics learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingzhe Yang, Zhipeng Wang, Kaiwei Liu, Yingqi Rong, Bing Yuan, Jiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modelling complex dynamical systems in a data-driven manner is challenging
due to the presence of emergent behaviors and properties that cannot be
directly captured by micro-level observational data. Therefore, it is crucial
to develop a model that can effectively capture emergent dynamics at the
macro-level and quantify emergence based on the available data. Drawing
inspiration from the theory of causal emergence, this paper introduces a
machine learning framework aimed at learning macro-dynamics within an emergent
latent space. The framework achieves this by maximizing the effective
information (EI) to obtain a macro-dynamics model with stronger causal effects.
Experimental results on both simulated and real data demonstrate the
effectiveness of the proposed framework. Not only does it successfully capture
emergent patterns, but it also learns the coarse-graining strategy and
quantifies the degree of causal emergence in the data. Furthermore, experiments
conducted on environments different from the training dataset highlight the
superior generalization ability of our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LymphoML: An interpretable artificial intelligence-based method
  identifies morphologic features that correlate with lymphoma subtype 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09574v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09574v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Shankar, Xiaoli Yang, Vrishab Krishna, Brent Tan, Oscar Silva, Rebecca Rojansky, Andrew Ng, Fabiola Valvert, Edward Briercheck, David Weinstock, Yasodha Natkunam, Sebastian Fernandez-Pol, Pranav Rajpurkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML's interpretable models, developed on a
limited volume of H&E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proceedings of the 3rd Machine Learning for Health
  symposium, Proceedings of Machine Learning Research (PMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniMOS: A Universal Framework For Multi-Organ Segmentation Over
  Label-Constrained <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Li, Sheng Shao, Junyi Qu, Shuchao Pang, Mehmet A. Orgun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models for medical images can help physicians diagnose and
manage diseases. However, due to the fact that medical image annotation
requires a great deal of manpower and expertise, as well as the fact that
clinical departments perform image annotation based on task orientation, there
is the problem of having fewer medical image annotation data with more
unlabeled data and having many datasets that annotate only a single organ. In
this paper, we present UniMOS, the first universal framework for achieving the
utilization of fully and partially labeled images as well as unlabeled images.
Specifically, we construct a Multi-Organ Segmentation (MOS) module over
fully/partially labeled data as the basenet and designed a new target adaptive
loss. Furthermore, we incorporate a semi-supervised training module that
combines consistent regularization and pseudolabeling techniques on unlabeled
data, which significantly improves the segmentation of unlabeled data.
Experiments show that the framework exhibits excellent performance in several
medical image segmentation tasks compared to other advanced methods, and also
significantly improves data utilization and reduces annotation cost. Code and
models are available at: https://github.com/lw8807001/UniMOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by BIBM2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Landmark Attention: Random-Access Infinite Context Length for
  <span class="highlight-title">Transformer</span>s <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirkeivan Mohtashami, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Transformers have shown remarkable success in natural language
processing, their attention mechanism's large memory requirements have limited
their ability to handle longer contexts. Prior approaches, such as recurrent
memory or retrieval-based augmentation, have either compromised the
random-access flexibility of attention (i.e., the capability to select any
token in the entire context) or relied on separate mechanisms for relevant
context retrieval, which may not be compatible with the model's attention. In
this paper, we present a novel approach that allows access to the complete
context while retaining random-access flexibility, closely resembling running
attention on the entire context. Our method uses a landmark token to represent
each block of the input and trains the attention to use it for selecting
relevant blocks, enabling retrieval of blocks directly through the attention
mechanism instead of by relying on a separate mechanism. Our approach
seamlessly integrates with specialized data structures and the system's memory
hierarchy, enabling processing of arbitrarily long context lengths. We
demonstrate that our method can obtain comparable performance with
Transformer-XL while significantly reducing the number of retrieved tokens in
each step. Finally, we show that fine-tuning LLaMA 7B with our method
successfully extends its context length capacity to over 32k tokens, allowing
for inference at the context lengths of GPT-4. We release the implementation of
landmark attention and the code to reproduce our experiments at
https://github.com/epfml/landmark-attention/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at NeurIPS 2023 - 37th Conference on
  Neural Information Processing Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin Mehta, Mohammad Rastegari, Oncel Tuzel, Hadi Pouransari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of publicly available vision foundation models (VFMs), such as
CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed
with distinct capabilities stemming from their pre-training objectives. For
instance, CLIP excels in semantic understanding, while SAM specializes in
spatial understanding for segmentation. In this work, we introduce a simple
recipe to efficiently merge VFMs into a unified model that absorbs their
expertise. Our method integrates techniques of multi-task learning, continual
learning, and distillation. Further, it demands significantly less
computational cost compared to traditional multi-task training from scratch,
and it only needs a small fraction of the pre-training datasets that were
initially used to train individual models. By applying our method to SAM and
CLIP, we obtain SAM-CLIP: a unified model that combines the capabilities of SAM
and CLIP into a single vision transformer. Compared with deploying SAM and CLIP
independently, our merged model, SAM-CLIP, reduces storage and compute costs
for inference, making it well-suited for edge device applications. We show that
SAM-CLIP not only retains the foundational strengths of SAM and CLIP, but also
introduces synergistic functionalities, notably in zero-shot semantic
segmentation, where SAM-CLIP establishes new state-of-the-art results on 5
benchmarks. It outperforms previous models that are specifically designed for
this task by a large margin, including +6.8% and +5.9% mean IoU improvement on
Pascal-VOC and COCO-Stuff datasets, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Characterization of Emotion within Multimedia Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayo Samuel Banjo, Connice Trimmingham, Niloofar Yousefi, Nitin Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Technological advancement and its omnipresent connection have pushed humans
past the boundaries and limitations of a computer screen, physical state, or
geographical location. It has provided a depth of avenues that facilitate
human-computer interaction that was once inconceivable such as audio and body
language detection. Given the complex modularities of emotions, it becomes
vital to study human-computer interaction, as it is the commencement of a
thorough understanding of the emotional state of users and, in the context of
social networks, the producers of multimodal information. This study first
acknowledges the accuracy of classification found within multimodal emotion
detection systems compared to unimodal solutions. Second, it explores the
characterization of multimedia content produced based on their emotions and the
coherence of emotion in different modalities by utilizing deep learning models
to classify emotion across different modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, Published in International Conference on Computers and
  Computation (COMPUTE 2022), November 03-04, 2022, San Francisco, United
  States</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CityScope: Enhanced Localozation and Synchronizing AR for Dynamic Urban
  Weather Visualization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tzu Hsin Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CityScope uses augmented reality (AR) to change our interaction with weather
data. The main goal is to develop real-time 3D weather visualizations, with
Taiwan as the model. It displays live weather data from the Central Weather
Bureau (CWB), projected onto a physical representation of Taiwan's landscape. A
pivotal advancement in our project is the integration of AprilTag with plane
detection technology. This innovative combination significantly enhances the
precision of the virtual visualizations within the physical world. By
accurately aligning AR elements with real-world environments, CityScope
achieves a seamless and realistic amalgamation of weather data and the physical
terrain of Taiwan. This breakthrough in AR technology not only enhances the
accuracy of weather visualizations but also enriches user experience, offering
an immersive and interactive way to understand and engage with meteorological
information. CityScope stands as a testament to the potential of AR in
transforming data visualization and public engagement in meteorology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Modeling Based Automatic Video Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Hong Huang, Chao-Han Huck Yang, Pin-Yu Chen, Min-Hung Chen, Marcel Worring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of video summarization is to shorten videos automatically while
retaining the key information necessary to convey the overall story. Video
summarization methods mainly rely on visual factors, such as visual
consecutiveness and diversity, which may not be sufficient to fully understand
the content of the video. There are other non-visual factors, such as
interestingness, representativeness, and storyline consistency that should also
be considered for generating high-quality video summaries. Current methods do
not adequately take into account these non-visual factors, resulting in
suboptimal performance. In this work, a new approach to video summarization is
proposed based on insights gained from how humans create ground truth video
summaries. The method utilizes a conditional modeling perspective and
introduces multiple meaningful random variables and joint distributions to
characterize the key components of video summarization. Helper distributions
are employed to improve the training of the model. A conditional attention
module is designed to mitigate potential performance degradation in the
presence of multi-modal input. The proposed video summarization method
incorporates the above innovative design choices that aim to narrow the gap
between human-generated and machine-generated video summaries. Extensive
experiments show that the proposed approach outperforms existing methods and
achieves state-of-the-art performance on commonly used video summarization
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  arXiv admin note: substantial text overlap with arXiv:2305.00455</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Muqeet, Kyuchul Lee, Bumsoo Kim, Yohan Hong, Hyungrae Lee, Woonggon Kim, Kwang Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video face re-aging deals with altering the apparent age of a person to the
target age in videos. This problem is challenging due to the lack of paired
video datasets maintaining temporal consistency in identity and age. Most
re-aging methods process each image individually without considering the
temporal consistency of videos. While some existing works address the issue of
temporal coherence through video facial attribute manipulation in latent space,
they often fail to deliver satisfactory performance in age transformation. To
tackle the issues, we propose (1) a novel synthetic video dataset that features
subjects across a diverse range of age groups; (2) a baseline architecture
designed to validate the effectiveness of our proposed dataset, and (3) the
development of three novel metrics tailored explicitly for evaluating the
temporal consistency of video re-aging techniques. Our comprehensive
experiments on public datasets, such as VFHQ and CelebV-HQ, show that our
method outperforms the existing approaches in terms of both age transformation
and temporal consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-11-19T00:00:00Z">2023-11-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">23</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM aided semi-supervision for Extractive Dialog Summarization <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishant Mishra, Gaurav Sahu, Iacer Calixto, Ameen Abu-Hanna, Issam H. Laradji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating high-quality summaries for chat dialogs often requires large
labeled datasets. We propose a method to efficiently use unlabeled data for
extractive summarization of customer-agent dialogs. In our method, we frame
summarization as a question-answering problem and use state-of-the-art large
language models (LLMs) to generate pseudo-labels for a dialog. We then use
these pseudo-labels to fine-tune a chat summarization model, effectively
transferring knowledge from the large LLM into a smaller specialized model. We
demonstrate our method on the \tweetsumm dataset, and show that using 10\% of
the original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L,
whereas the current state-of-the-art trained on the entire training data set
obtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case
(i.e., ROUGE-L) we still effectively retain 94.7% of the performance while
using only 10% of the data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in EMNLP Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using
  Clustering and Information Theory Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasilii Gromov, Quynh Nhu Dang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of generative models like GPT-3, it is increasingly more
challenging to differentiate generated texts from human-written ones. There is
a large number of studies that have demonstrated good results in bot
identification. However, the majority of such works depend on supervised
learning methods that require labelled data and/or prior knowledge about the
bot-model architecture. In this work, we propose a bot identification algorithm
that is based on unsupervised learning techniques and does not depend on a
large amount of labelled data. By combining findings in semantic analysis by
clustering (crisp and fuzzy) and information techniques, we construct a robust
model that detects a generated text for different types of bot. We find that
the generated texts tend to be more chaotic while literary works are more
complex. We also demonstrate that the clustering of human texts results in
fuzzier clusters in comparison to the more compact and well-separated clusters
of bot-generated texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Pattern Recognition and Machine Intelligence 2023. 8
  pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis
  of COVID-19 Vaccines in India 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milind Gupta, Abhishek Kaushik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In March 2020, the World Health Organisation declared COVID-19 a global
pandemic as it spread to nearly every country. By mid-2021, India had
introduced three vaccines: Covishield, Covaxin, and Sputnik. To ensure
successful vaccination in a densely populated country like India, understanding
public sentiment was crucial. Social media, particularly Reddit with over 430
million users, played a vital role in disseminating information. This study
employs data mining techniques to analyze Reddit data and gauge Indian
sentiments towards COVID-19 vaccines. Using Python's Text Blob library,
comments are annotated to assess general sentiments. Results show that most
Reddit users in India expressed neutrality about vaccination, posing a
challenge for the Indian government's efforts to vaccinate a significant
portion of the population.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Security Risk Taxonomy for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Derner, Kristina Batistič, Jan Zahálka, Robert Babuška
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) permeate more and more applications, an
assessment of their associated security risks becomes increasingly necessary.
The potential for exploitation by malicious actors, ranging from disinformation
to data breaches and reputation damage, is substantial. This paper addresses a
gap in current research by focusing on the security risks posed by LLMs, which
extends beyond the widely covered ethical and societal implications. Our work
proposes a taxonomy of security risks along the user-model communication
pipeline, explicitly focusing on prompt-based attacks on LLMs. We categorize
the attacks by target and attack type within a prompt-based interaction scheme.
The taxonomy is reinforced with specific attack examples to showcase the
real-world impact of these risks. Through this taxonomy, we aim to inform the
development of robust and secure LLM applications, enhancing their safety and
trustworthiness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for
  Improving ASR Robustness in Spoken Language Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuxin Cheng, Bowen Cao, Qichen Ye, Zhihong Zhu, Hongxiang Li, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken language understanding (SLU) is a fundamental task in the
task-oriented dialogue systems. However, the inevitable errors from automatic
speech recognition (ASR) usually impair the understanding performance and lead
to error propagation. Although there are some attempts to address this problem
through contrastive learning, they (1) treat clean manual transcripts and ASR
transcripts equally without discrimination in fine-tuning; (2) neglect the fact
that the semantically similar pairs are still pushed away when applying
contrastive learning; (3) suffer from the problem of Kullback-Leibler (KL)
vanishing. In this paper, we propose Mutual Learning and Large-Margin
Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness
in SLU. Specifically, in fine-tuning, we apply mutual learning and train two
SLU models on the manual transcripts and the ASR transcripts, respectively,
aiming to iteratively share knowledge between these two models. We also
introduce a distance polarization regularizer to avoid pushing away the
intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing
schedule to mitigate KL vanishing issue. Experiments on three datasets show
that ML-LMCL outperforms existing models and achieves new state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Portuguese FAQ for Financial Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paulo Finardi, Wanderley M. Melo, Edgard D. Medeiros Neto, Alex F. Mansano, Pablo B. Costa, Vinicius F. Caridá
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scarcity of domain-specific data in the Portuguese financial domain has
disfavored the development of Natural Language Processing (NLP) applications.
To address this limitation, the present study advocates for the utilization of
synthetic data generated through data augmentation techniques. The
investigation focuses on the augmentation of a dataset sourced from the Central
Bank of Brazil FAQ, employing techniques that vary in semantic similarity.
Supervised and unsupervised tasks are conducted to evaluate the impact of
augmented data on both low and high semantic similarity scenarios.
Additionally, the resultant dataset will be publicly disseminated on the
Hugging Face Datasets platform, thereby enhancing accessibility and fostering
broader engagement within the NLP research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arie Cattan, Tom Hope, Doug Downey, Roy Bar-Haim, Lilach Eden, Yoav Kantor, Ido Dagan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various NLP tasks require a complex hierarchical structure over nodes, where
each node is a cluster of items. Examples include generating entailment graphs,
hierarchical cross-document coreference resolution, annotating event and
subevent relations, etc. To enable efficient annotation of such hierarchical
structures, we release CHAMP, an open source tool allowing to incrementally
construct both clusters and hierarchy simultaneously over any type of texts.
This incremental approach significantly reduces annotation time compared to the
common pairwise annotation approach and also guarantees maintaining
transitivity at the cluster and hierarchy levels. Furthermore, CHAMP includes a
consolidation mode, where an adjudicator can easily compare multiple cluster
hierarchy annotations and resolve disagreements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Cross-Attention Augmented Model for Event-Triggered Context-Aware
  Story Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tang, Tyler Loakman, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements, existing story generation systems continue to
encounter difficulties in effectively incorporating contextual and event
features, which greatly influence the quality of generated narratives. To
tackle these challenges, we introduce a novel neural generation model, EtriCA,
that enhances the relevance and coherence of generated stories by employing a
cross-attention mechanism to map context features onto event sequences through
residual mapping. This feature capturing mechanism enables our model to exploit
logical relationships between events more effectively during the story
generation process. To further enhance our proposed model, we employ a
post-training framework for knowledge enhancement (KeEtriCA) on a large-scale
book corpus. This allows EtriCA to adapt to a wider range of data samples. This
results in approximately 5\% improvement in automatic metrics and over 10\%
improvement in human evaluation. We conduct extensive experiments, including
comparisons with state-of-the-art (SOTA) baseline models, to evaluate the
performance of our framework on story generation. The experimental results,
encompassing both automated metrics and human assessments, demonstrate the
superiority of our model over existing state-of-the-art baselines. These
results underscore the effectiveness of our model in leveraging context and
event features to improve the quality of generated narratives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to CSL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Real-World Writing Assistance: A Chinese Character Checking
  Benchmark with Faked and Misspelled Characters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Zishan Xu, Shaoshen Chen, Haojing Huang, Yangning Li, Yong Jiang, Zhongli Li, Qingyu Zhou, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing assistance is an application closely related to human life and is
also a fundamental Natural Language Processing (NLP) research field. Its aim is
to improve the correctness and quality of input texts, with character checking
being crucial in detecting and correcting wrong characters. From the
perspective of the real world where handwriting occupies the vast majority,
characters that humans get wrong include faked characters (i.e., untrue
characters created due to writing errors) and misspelled characters (i.e., true
characters used incorrectly due to spelling errors). However, existing datasets
and related studies only focus on misspelled characters mainly caused by
phonological or visual confusion, thereby ignoring faked characters which are
more common and difficult. To break through this dilemma, we present
Visual-C$^3$, a human-annotated Visual Chinese Character Checking dataset with
faked and misspelled Chinese characters. To the best of our knowledge,
Visual-C$^3$ is the first real-world visual and the largest human-crafted
dataset for the Chinese character checking scenario. Additionally, we also
propose and evaluate novel baseline methods on Visual-C$^3$. Extensive
empirical results and analyses show that Visual-C$^3$ is high-quality yet
challenging. The Visual-C$^3$ dataset and the baseline methods will be publicly
available to facilitate further research in the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Large Language Models in Mental Health Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxiong Ji, Tianlin Zhang, Kailai Yang, Sophia Ananiadou, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become valuable assets in mental health,
showing promise in both classification tasks and counseling applications. This
paper offers a perspective on using LLMs in mental health applications. It
discusses the instability of generative models for prediction and the potential
for generating hallucinatory outputs, underscoring the need for ongoing audits
and evaluations to maintain their reliability and dependability. The paper also
distinguishes between the often interchangeable terms ``explainability'' and
``interpretability'', advocating for developing inherently interpretable
methods instead of relying on potentially hallucinated self-explanations
generated by LLMs. Despite the advancements in LLMs, human counselors'
empathetic understanding, nuanced interpretation, and contextual awareness
remain irreplaceable in the sensitive and complex realm of mental health
counseling. The use of LLMs should be approached with a judicious and
considerate mindset, viewing them as tools that complement human expertise
rather than seeking to replace it.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal ATE Mitigates Unintended Bias in Controlled Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Madhavan, Kahini Wadhawan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study attribute control in language models through the method of Causal
Average Treatment Effect (Causal ATE). Existing methods for the attribute
control task in Language Models (LMs) check for the co-occurrence of words in a
sentence with the attribute of interest, and control for them. However,
spurious correlation of the words with the attribute in the training dataset,
can cause models to hallucinate the presence of the attribute when presented
with the spurious correlate during inference. We show that the simple
perturbation-based method of Causal ATE removes this unintended effect.
Additionally, we offer a theoretical foundation for investigating Causal ATE in
the classification task, and prove that it reduces the number of false
positives -- thereby mitigating the issue of unintended bias. Specifically, we
ground it in the problem of toxicity mitigation, where a significant challenge
lies in the inadvertent bias that often emerges towards protected groups post
detoxification. We show that this unintended bias can be solved by the use of
the Causal ATE metric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPLAIN: Augmenting CybersecurityWarnings with Reasons and Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vera A. Kazakova, Jena D. Hwang, Bonnie J. Dorr, Yorick Wilks, J. Blake Gage, Alex Memory, Mark A. Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective cyber threat recognition and prevention demand comprehensible
forecasting systems, as prior approaches commonly offer limited and,
ultimately, unconvincing information. We introduce Simplified Plaintext
Language (SPLAIN), a natural language generator that converts warning data into
user-friendly cyber threat explanations. SPLAIN is designed to generate clear,
actionable outputs, incorporating hierarchically organized explanatory details
about input data and system functionality. Given the inputs of individual
sensor-induced forecasting signals and an overall warning from a fusion module,
SPLAIN queries each signal for information on contributing sensors and data
signals. This collected data is processed into a coherent English explanation,
encompassing forecasting, sensing, and data elements for user review. SPLAIN's
template-based approach ensures consistent warning structure and vocabulary.
SPLAIN's hierarchical output structure allows each threat and its components to
be expanded to reveal underlying explanations on demand. Our conclusions
emphasize the need for designers to specify the "how" and "why" behind cyber
warnings, advocate for simple structured templates in generating consistent
explanations, and recognize that direct causal links in Machine Learning
approaches may not always be identifiable, requiring some explanations to focus
on general methodologies, such as model and training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at FLAIRS-2019 as poster (see ancillary files)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unmasking and Improving Data Credibility: A Study with <span class="highlight-title">Dataset</span>s for
  Training Harmless Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaowei Zhu, Jialu Wang, Hao Cheng, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have shown promise in various tasks but can be affected by
undesired data during training, fine-tuning, or alignment. For example, if some
unsafe conversations are wrongly annotated as safe ones, the model fine-tuned
on these samples may be harmful. Therefore, the correctness of annotations,
i.e., the credibility of the dataset, is important. This study focuses on the
credibility of real-world datasets, including the popular benchmarks Jigsaw
Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that
can be used for training a harmless language model. Given the cost and
difficulty of cleaning these datasets by humans, we introduce a systematic
framework for evaluating the credibility of datasets, identifying label errors,
and evaluating the influence of noisy labels in the curated language data,
specifically focusing on unsafe comments and conversation classification. With
the framework, we find and fix an average of 6.16% label errors in 11 datasets
constructed from the above benchmarks. The data credibility and downstream
learning performance can be remarkably improved by directly fixing label
errors, indicating the significance of cleaning existing real-world datasets.
Open-source: https://github.com/Docta-ai/docta.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Large Language Models to Knowledge Graphs for Biomarker Discovery
  in Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Rezaul Karim, Lina Molinas Comet, Md Shajalal, Oya Deniz Beyan, Dietrich Rebholz-Schuhmann, Stefan Decker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain experts often rely on most recent knowledge for apprehending and
disseminating specific biological processes that help them design strategies
for developing prevention and therapeutic decision-making in various disease
scenarios. A challenging scenarios for artificial intelligence (AI) is using
biomedical data (e.g., texts, imaging, omics, and clinical) to provide
diagnosis and treatment recommendations for cancerous conditions.~Data and
knowledge about biomedical entities like cancer, drugs, genes, proteins, and
their mechanism is spread across structured (knowledge bases (KBs)) and
unstructured (e.g., scientific articles) sources. A large-scale knowledge graph
(KG) can be constructed by integrating and extracting facts about semantically
interrelated entities and relations. Such a KG not only allows exploration and
question answering (QA) but also enables domain experts to deduce new
knowledge. However, exploring and querying large-scale KGs is tedious for
non-domain users due to their lack of understanding of the data assets and
semantic technologies. In this paper, we develop a domain KG to leverage
cancer-specific biomarker discovery and interactive QA. For this, we
constructed a domain ontology called OncoNet Ontology (ONO), which enables
semantic reasoning for validating gene-disease (different types of cancer)
relations. The KG is further enriched by harmonizing the ONO, metadata,
controlled vocabularies, and biomedical concepts from scientific articles by
employing BioBERT- and SciBERT-based information extractors. Further, since the
biomedical domain is evolving, where new findings often replace old ones,
without having access to up-to-date scientific findings, there is a high chance
an AI system exhibits concept drift while providing diagnosis and treatment.
Therefore, we fine-tune the KG using large language models (LLMs) based on more
recent articles and KBs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2302.04737</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Chat<span class="highlight-title">GPT</span> a General-Purpose Natural Language Processing Task Solver? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06476v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06476v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spurred by advancements in scale, large language models (LLMs) have
demonstrated the ability to perform a variety of natural language processing
(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,
the debut of ChatGPT has drawn a great deal of attention from the natural
language processing (NLP) community due to the fact that it can generate
high-quality responses to human input and self-correct previous mistakes based
on subsequent conversations. However, it is not yet known whether ChatGPT can
serve as a generalist model that can perform many NLP tasks zero-shot. In this
work, we empirically analyze the zero-shot learning ability of ChatGPT by
evaluating it on 20 popular NLP datasets covering 7 representative task
categories. With extensive empirical studies, we demonstrate both the
effectiveness and limitations of the current version of ChatGPT. We find that
ChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,
arithmetic reasoning) while it still faces challenges when solving specific
tasks such as sequence tagging. We additionally provide in-depth analysis
through qualitative case studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Generation from Human Brain Activities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Ye, Qingyao Ai, Yiqun Liu, Min Zhang, Christina Lioma, Tuukka Ruotsalo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating human language through non-invasive brain-computer interfaces
(BCIs) has the potential to unlock many applications, such as serving disabled
patients and improving communication. Currently, however, generating language
via BCIs has been previously successful only within a classification setup for
selecting pre-generated sentence continuation candidates with the most likely
cortical semantic representation. Inspired by recent research that revealed
associations between the brain and the large computational language models, we
propose a generative language BCI that utilizes the capacity of a large
language model (LLM) jointly with a semantic brain decoder to directly generate
language from functional magnetic resonance imaging (fMRI) input. The proposed
model can generate coherent language sequences aligned with the semantic
content of visual or auditory language stimuli perceived, without prior
knowledge of any pre-generated candidates. We compare the language generated
from the presented model with a random control, pre-generated language
selection approach, and a standard LLM, which generates common coherent text
solely based on the next word likelihood according to statistical language
training data. The proposed model is found to generate language that is more
aligned with semantic stimulus in response to which brain input is sampled. Our
findings demonstrate the potential and feasibility of employing BCIs in direct
language generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Initialize: Can Meta Learning Improve Cross-task
  Generalization in <span class="highlight-title">Prompt</span> Tuning? <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08143v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08143v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwei Qin, Qian Li, Ruochen Zhao, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning (PT) which only tunes the embeddings of an additional sequence
of tokens per task, keeping the pre-trained language model (PLM) frozen, has
shown remarkable performance in few-shot learning. Despite this, PT has been
shown to rely heavily on good initialization of the prompt embeddings. In this
work, we study meta prompt tuning (MPT) to systematically explore how
meta-learning can help improve (if it can) cross-task generalization in PT
through learning to initialize the prompt embeddings from other relevant tasks.
We empirically analyze a representative set of meta learning algorithms in a
wide range of adaptation settings with different source/target task
configurations on a large set of few-shot tasks. With extensive experiments and
analysis, we demonstrate the effectiveness of MPT. We find the improvement to
be significant particularly on classification tasks. For other kinds of tasks
such as question answering, we observe that while MPT can outperform PT in most
cases, it does not always outperform multi-task learning. We further provide an
in-depth analysis from the perspective of task similarity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lifelong Sequence Generation with Dynamic Module Expansion and
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09886v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09886v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwei Qin, Chen Chen, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lifelong sequence generation (LSG), a problem in continual learning, aims to
continually train a model on a sequence of generation tasks to learn constantly
emerging new generation patterns while avoiding the forgetting of previous
knowledge. Existing LSG methods mainly focus on maintaining old knowledge while
paying little attention to knowledge transfer across tasks. In contrast, humans
can better learn new tasks by leveraging previously acquired knowledge from
similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic
Module Expansion and Adaptation (DMEA), which enables the model to dynamically
determine the architecture for acquiring new knowledge based on task
correlation and select the most similar previous tasks to facilitate adaptation
to new tasks. In addition, as the learning process can easily be biased towards
the current task which might cause more severe forgetting of previously learned
knowledge, we propose dynamic gradient scaling to balance the learning of the
current task and replayed tasks. With extensive experiments, we demonstrate
that DMEA can consistently outperform existing methods in different LSG
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Uncertainty Calibration of Aligned Language Models under
  the Multiple-Choice Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11732v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11732v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guande He, Peng Cui, Jianfei Chen, Wenbo Hu, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the significant progress made in practical applications of aligned
language models (LMs), they tend to be overconfident in output answers compared
to the corresponding pre-trained LMs. In this work, we systematically evaluate
the impact of the alignment process on logit-based uncertainty calibration of
LMs under the multiple-choice setting. We first conduct a thoughtful empirical
study on how aligned LMs differ in calibration from their pre-trained
counterparts. Experimental results reveal that there are two distinct
uncertainties in LMs under the multiple-choice setting, which are responsible
for the answer decision and the format preference of the LMs, respectively.
Then, we investigate the role of these two uncertainties on aligned LM's
calibration through fine-tuning in simple synthetic alignment schemes and
conclude that one reason for aligned LMs' overconfidence is the conflation of
these two types of uncertainty. Furthermore, we examine the utility of common
post-hoc calibration methods for aligned LMs and propose an easy-to-implement
and sample-efficient method to calibrate aligned LMs. We hope our findings
could provide insights into the design of more reliable alignment processes for
LMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Language Models for Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we systematically review the recent advancements in code
processing with language models, covering 50+ models, 30+ evaluation tasks,
150+ datasets, and 550 related works. We break down code processing models into
general language models represented by the GPT family and specialized models
that are specifically pretrained on code, often with tailored objectives. We
discuss the relations and differences between these models, and highlight the
historical transition of code modeling from statistical models and RNNs to
pretrained Transformers and LLMs, which is exactly the same course that had
been taken by NLP. We also discuss code-specific features such as AST, CFG, and
unit tests, along with their application in training code language models, and
identify key challenges and potential future directions in this domain. We keep
the survey open and updated on GitHub repository at
https://github.com/codefuse-ai/Awesome-Code-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Repo is available at https://github.com/codefuse-ai/Awesome-Code-LLM.
  V2 adds several new tasks, and collates dozens more benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IRFL: Image Recognition of Figurative Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ron Yosef, Yonatan Bitton, Dafna Shahaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Figures of speech such as metaphors, similes, and idioms are integral parts
of human communication. They are ubiquitous in many forms of discourse,
allowing people to convey complex, abstract ideas and evoke emotion. As
figurative forms are often conveyed through multiple modalities (e.g., both
text and images), understanding multimodal figurative language is an important
AI challenge, weaving together profound vision, language, commonsense and
cultural knowledge.
  In this work, we develop the Image Recognition of Figurative Language (IRFL)
dataset. We leverage human annotation and an automatic pipeline we created to
generate a multimodal dataset, and introduce two novel tasks as a benchmark for
multimodal figurative language understanding. We experimented with
state-of-the-art vision and language models and found that the best (22%)
performed substantially worse than humans (97%). We release our dataset,
benchmark, and code, in hopes of driving the development of models that can
better understand figurative language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Analogies: A Testbed for Generalizing AI Oversight to
  Hard-To-Measure Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Clymer, Garrett Baker, Rohan Subramani, Sam Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI systems become more intelligent and their behavior becomes more
challenging to assess, they may learn to game the flaws of human feedback
instead of genuinely striving to follow instructions; however, this risk can be
mitigated by controlling how LLMs generalize human feedback to situations where
it is unreliable. To better understand how reward models generalize, we craft
69 distribution shifts spanning 8 categories. We find that reward models do not
learn to evaluate `instruction-following' by default and instead favor personas
that resemble internet text. Techniques for interpreting reward models'
internal representations achieve better generalization than standard
fine-tuning, but still frequently fail to distinguish instruction-following
from conflated behaviors. We consolidate the 15 most challenging distribution
shifts into the GENeralization analogIES (GENIES) benchmark, which we hope will
enable progress toward controlling reward model generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/Joshuaclymer/GENIES Website:
  https://joshuaclymer.github.io/generalization-analogies-website/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How To Train Your (Compressed) Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ananya Harsh Jha, Tom Sherborne, Evan Pete Walsh, Dirk Groeneveld, Emma Strubell, Iz Beltagy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increase in the size of large language models (LLMs), we need
compression methods that can reduce the model size while preserving the
generality and zero-shot promptability of the model. This goal is more
ambitious than the typical compression setup, which reduces the model's size at
the expense of specializing it to a specific end-task. To study this, we
develop a task-agnostic compression pipeline with a large-scale evaluation
comprising language modeling perplexity and 12 zero-shot end-tasks. Our results
show that a simple layer-wise pruning followed by continued language model
pretraining matches or outperforms three existing state-of-the-art baselines
while being 1.5x more computationally efficient. However, unlike typical
task-specialized compression, our best-compressed model significantly
underperforms a similar-sized model trained from scratch. We posit the
half-sized pretrained model as an upper bound for task-agnostic compression and
call for future work to bridge this gap under a reasonable token budget. Our
findings highlight the inadequacy of existing compression methods for LLMs and
establish a requirement for new methods that preserve a model's generality and
zero-shot promptability under compression. We release our code and evaluation
setup to facilitate reproducibility and help iterate on method design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">35</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Defect Detection and Classification Method for Advanced IC
  Nodes by Using Slicing Aided Hyper Inference with Refinement Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vic De Ridder, Bappaditya Dey, Victor Blanco, Sandip Halder, Bartel Van Waeyenberge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In semiconductor manufacturing, lithography has often been the manufacturing
step defining the smallest possible pattern dimensions. In recent years,
progress has been made towards high-NA (Numerical Aperture) EUVL
(Extreme-Ultraviolet-Lithography) paradigm, which promises to advance pattern
shrinking (2 nm node and beyond). However, a significant increase in stochastic
defects and the complexity of defect detection becomes more pronounced with
high-NA. Present defect inspection techniques (both non-machine learning and
machine learning based), fail to achieve satisfactory performance at high-NA
dimensions. In this work, we investigate the use of the Slicing Aided Hyper
Inference (SAHI) framework for improving upon current techniques. Using SAHI,
inference is performed on size-increased slices of the SEM images. This leads
to the object detector's receptive field being more effective in capturing
small defect instances. First, the performance on previously investigated
semiconductor datasets is benchmarked across various configurations, and the
SAHI approach is demonstrated to substantially enhance the detection of small
defects, by approx. 2x. Afterwards, we also demonstrated application of SAHI
leads to flawless detection rates on a new test dataset, with scenarios not
encountered during training, whereas previous trained models failed. Finally,
we formulate an extension of SAHI that does not significantly reduce
true-positive predictions while eliminating false-positive predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, to be presented at International Conference on
  Machine Intelligence with Applications (ICMIA), with proceedings by AIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Appearance Codes using Joint Embedding Learning of Multiple Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Zhang, Evan Dogariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of appearance codes in recent work on generative modeling has enabled
novel view renders with variable appearance and illumination, such as day-time
and night-time renders of a scene. A major limitation of this technique is the
need to re-train new appearance codes for every scene on inference, so in this
work we address this problem proposing a framework that learns a joint
embedding space for the appearance and structure of the scene by enforcing a
contrastive loss constraint between different modalities. We apply our
framework to a simple Variational Auto-Encoder model on the RADIATE dataset
\cite{sheeny2021radiate} and qualitatively demonstrate that we can generate new
renders of night-time photos using day-time appearance codes without additional
optimization iterations. Additionally, we compare our model to a baseline VAE
that uses the standard per-image appearance code technique and show that our
approach achieves generations of similar quality without learning appearance
codes for any unseen images on inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LifeLearner: Hardware-Aware Meta Continual Learning System for Embedded
  Computing Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young D. Kwon, Jagmohan Chauhan, Hong Jia, Stylianos I. Venieris, Cecilia Mascolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning (CL) allows applications such as user personalization and
household robots to learn on the fly and adapt to context. This is an important
feature when context, actions, and users change. However, enabling CL on
resource-constrained embedded systems is challenging due to the limited labeled
data, memory, and computing capacity. In this paper, we propose LifeLearner, a
hardware-aware meta continual learning system that drastically optimizes system
resources (lower memory, latency, energy consumption) while ensuring high
accuracy. Specifically, we (1) exploit meta-learning and rehearsal strategies
to explicitly cope with data scarcity issues and ensure high accuracy, (2)
effectively combine lossless and lossy compression to significantly reduce the
resource requirements of CL and rehearsal samples, and (3) developed
hardware-aware system on embedded and IoT platforms considering the hardware
characteristics. As a result, LifeLearner achieves near-optimal CL performance,
falling short by only 2.8% on accuracy compared to an Oracle baseline. With
respect to the state-of-the-art (SOTA) Meta CL method, LifeLearner drastically
reduces the memory footprint (by 178.7x), end-to-end latency by 80.8-94.2%, and
energy consumption by 80.9-94.2%. In addition, we successfully deployed
LifeLearner on two edge devices and a microcontroller unit, thereby enabling
efficient CL on resource-constrained platforms where it would be impractical to
run SOTA methods and the far-reaching deployment of adaptable CL in a
ubiquitous manner. Code is available at
https://github.com/theyoungkwon/LifeLearner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at SenSys 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffSCI: Zero-Shot Snapshot Compressive Imaging via Iterative Spectral
  Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenghao Pan, Haijin Zeng, Jiezhang Cao, Kai Zhang, Yongyong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper endeavors to advance the precision of snapshot compressive imaging
(SCI) reconstruction for multispectral image (MSI). To achieve this, we
integrate the advantageous attributes of established SCI techniques and an
image generative model, propose a novel structured zero-shot diffusion model,
dubbed DiffSCI. DiffSCI leverages the structural insights from the deep prior
and optimization-based methodologies, complemented by the generative
capabilities offered by the contemporary denoising diffusion model.
Specifically, firstly, we employ a pre-trained diffusion model, which has been
trained on a substantial corpus of RGB images, as the generative denoiser
within the Plug-and-Play framework for the first time. This integration allows
for the successful completion of SCI reconstruction, especially in the case
that current methods struggle to address effectively. Secondly, we
systematically account for spectral band correlations and introduce a robust
methodology to mitigate wavelength mismatch, thus enabling seamless adaptation
of the RGB diffusion model to MSIs. Thirdly, an accelerated algorithm is
implemented to expedite the resolution of the data subproblem. This
augmentation not only accelerates the convergence rate but also elevates the
quality of the reconstruction process. We present extensive testing to show
that DiffSCI exhibits discernible performance enhancements over prevailing
self-supervised and zero-shot approaches, surpassing even supervised
transformer counterparts across both simulated and real datasets. Our code will
be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Emerging Applications of Diffusion Probabilistic Models in
  MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Fan, Hanxi Liao, Shiqi Huang, Yimin Luo, Huazhu Fu, Haikun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion probabilistic models (DPMs) which employ explicit likelihood
characterization and a gradual sampling process to synthesize data, have gained
increasing research interest. Despite their huge computational burdens due to
the large number of steps involved during sampling, DPMs are widely appreciated
in various medical imaging tasks for their high-quality and diversity of
generation. Magnetic resonance imaging (MRI) is an important medical imaging
modality with excellent soft tissue contrast and superb spatial resolution,
which possesses unique opportunities for diffusion models. Although there is a
recent surge of studies exploring DPMs in MRI, a survey paper of DPMs
specifically designed for MRI applications is still lacking. This review
article aims to help researchers in the MRI community to grasp the advances of
DPMs in different applications. We first introduce the theory of two dominant
kinds of DPMs, categorized according to whether the diffusion time step is
discrete or continuous, and then provide a comprehensive review of emerging
DPMs in MRI, including reconstruction, image generation, image translation,
segmentation, anomaly detection, and further research topics. Finally, we
discuss the general limitations as well as limitations specific to the MRI
tasks of DPMs and point out potential areas that are worth further exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inspecting Explainability of <span class="highlight-title">Transformer</span> Models with Additional
  Statistical Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang C. Nguyen, Haeil Lee, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer becomes more popular in the vision domain in recent years so
there is a need for finding an effective way to interpret the Transformer model
by visualizing it. In recent work, Chefer et al. can visualize the Transformer
on vision and multi-modal tasks effectively by combining attention layers to
show the importance of each image patch. However, when applying to other
variants of Transformer such as the Swin Transformer, this method can not focus
on the predicted object. Our method, by considering the statistics of tokens in
layer normalization layers, shows a great ability to interpret the
explainability of Swin Transformer and ViT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SOccDPT: Semi-Supervised 3D Semantic Occupancy from Dense Prediction
  <span class="highlight-title">Transformer</span>s trained under memory constraints <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Nalgunda Ganesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SOccDPT, a memory-efficient approach for 3D semantic occupancy
prediction from monocular image input using dense prediction transformers. To
address the limitations of existing methods trained on structured traffic
datasets, we train our model on unstructured datasets including the Indian
Driving Dataset and Bengaluru Driving Dataset. Our semi-supervised training
pipeline allows SOccDPT to learn from datasets with limited labels by reducing
the requirement for manual labelling by substituting it with pseudo-ground
truth labels to produce our Bengaluru Semantic Occupancy Dataset. This broader
training enhances our model's ability to handle unstructured traffic scenarios
effectively. To overcome memory limitations during training, we introduce
patch-wise training where we select a subset of parameters to train each epoch,
reducing memory usage during auto-grad graph construction. In the context of
unstructured traffic and memory-constrained training and inference, SOccDPT
outperforms existing disparity estimation approaches as shown by the RMSE score
of 9.1473, achieves a semantic segmentation IoU score of 46.02% and operates at
a competitive frequency of 69.47 Hz. We make our code and semantic occupancy
dataset public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the ICRA 2024 IEEE for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidential Uncertainty Quantification: A Variance-Based Perspective <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruxiao Duan, Brian Caffo, Harrison X. Bai, Haris I. Sair, Craig Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification of deep neural networks has become an active field
of research and plays a crucial role in various downstream tasks such as active
learning. Recent advances in evidential deep learning shed light on the direct
quantification of aleatoric and epistemic uncertainties with a single forward
pass of the model. Most traditional approaches adopt an entropy-based method to
derive evidential uncertainty in classification, quantifying uncertainty at the
sample level. However, the variance-based method that has been widely applied
in regression problems is seldom used in the classification setting. In this
work, we adapt the variance-based approach from regression to classification,
quantifying classification uncertainty at the class level. The variance
decomposition technique in regression is extended to class covariance
decomposition in classification based on the law of total covariance, and the
class correlation is also derived from the covariance. Experiments on
cross-domain datasets are conducted to illustrate that the variance-based
approach not only results in similar accuracy as the entropy-based one in
active domain adaptation but also brings information about class-wise
uncertainties as well as between-class correlations. The code is available at
https://github.com/KerryDRX/EvidentialADA. This alternative means of evidential
uncertainty quantification will give researchers more options when class
uncertainties and correlations are important in their applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scale-aware competition network for palmprint recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengrui Gao, Ziyuan Yang, Min Zhu, Andrew Beng Jin Teo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Palmprint biometrics garner heightened attention in palm-scanning payment and
social security due to their distinctive attributes. However, prevailing
methodologies singularly prioritize texture orientation, neglecting the
significant texture scale dimension. We design an innovative network for
concurrently extracting intra-scale and inter-scale features to redress this
limitation. This paper proposes a scale-aware competitive network (SAC-Net),
which includes the Inner-Scale Competition Module (ISCM) and the Across-Scale
Competition Module (ASCM) to capture texture characteristics related to
orientation and scale. ISCM efficiently integrates learnable Gabor filters and
a self-attention mechanism to extract rich orientation data and discern
textures with long-range discriminative properties. Subsequently, ASCM
leverages a competitive strategy across various scales to effectively
encapsulate the competitive texture scale elements. By synergizing ISCM and
ASCM, our method adeptly characterizes palmprint features. Rigorous
experimentation across three benchmark datasets unequivocally demonstrates our
proposed approach's exceptional recognition performance and resilience relative
to state-of-the-art alternatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoVideo: Motion-Aware Video Generation with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyun Liang, Yuchen Fan, Kai Zhang, Radu Timofte, Luc Van Gool, Rakesh Ranjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent years have witnessed great progress on using diffusion models
for video generation, most of them are simple extensions of image generation
frameworks, which fail to explicitly consider one of the key differences
between videos and images, i.e., motion. In this paper, we propose a novel
motion-aware video generation (MoVideo) framework that takes motion into
consideration from two aspects: video depth and optical flow. The former
regulates motion by per-frame object distances and spatial layouts, while the
later describes motion by cross-frame correspondences that help in preserving
fine details and improving temporal consistency. More specifically, given a key
frame that exists or generated from text prompts, we first design a diffusion
model with spatio-temporal modules to generate the video depth and the
corresponding optical flows. Then, the video is generated in the latent space
by another spatio-temporal diffusion model under the guidance of depth, optical
flow-based warped latent video and the calculated occlusion mask. Lastly, we
use optical flows again to align and refine different frames for better video
decoding from the latent space to the pixel space. In experiments, MoVideo
achieves state-of-the-art results in both text-to-video and image-to-video
generation, showing promising prompt consistency, frame consistency and visual
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project homepage: https://jingyunliang.github.io/MoVideo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoSAM: Fine-tuning SAM with Sparse and Dense Visual <span class="highlight-title">Prompt</span>ing for
  Automated Segmentation of Mobility Infrastructure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafi Ibn Sultan, Chengyin Li, Hui Zhu, Prashant Khanduri, Marco Brocanelli, Dongxiao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) has shown impressive performance when
applied to natural image segmentation. However, it struggles with geographical
images like aerial and satellite imagery, especially when segmenting mobility
infrastructure including roads, sidewalks, and crosswalks. This inferior
performance stems from the narrow features of these objects, their textures
blending into the surroundings, and interference from objects like trees,
buildings, vehicles, and pedestrians - all of which can disorient the model to
produce inaccurate segmentation maps. To address these challenges, we propose
Geographical SAM (GeoSAM), a novel SAM-based framework that implements a
fine-tuning strategy using the dense visual prompt from zero-shot learning, and
the sparse visual prompt from a pre-trained CNN segmentation model. The
proposed GeoSAM outperforms existing approaches for geographical image
segmentation, specifically by 20%, 14.29%, and 17.65% for road infrastructure,
pedestrian infrastructure, and on average, respectively, representing a
momentous leap in leveraging foundation models to segment mobility
infrastructure including both road and pedestrian infrastructure in
geographical images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discrete approximations of Gaussian smoothing and Gaussian derivatives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Lindeberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops an in-depth treatment concerning the problem of
approximating the Gaussian smoothing and Gaussian derivative computations in
scale-space theory for application on discrete data. With close connections to
previous axiomatic treatments of continuous and discrete scale-space theory, we
consider three main ways discretizing these scale-space operations in terms of
explicit discrete convolutions, based on either (i) sampling the Gaussian
kernels and the Gaussian derivative kernels, (ii) locally integrating the
Gaussian kernels and the Gaussian derivative kernels over each pixel support
region and (iii) basing the scale-space analysis on the discrete analogue of
the Gaussian kernel, and then computing derivative approximations by applying
small-support central difference operators to the spatially smoothed image
data.
  We study the properties of these three main discretization methods both
theoretically and experimentally, and characterize their performance by
quantitative measures, including the results they give rise to with respect to
the task of scale selection, investigated for four different use cases, and
with emphasis on the behaviour at fine scales. The results show that the
sampled Gaussian kernels and derivatives as well as the integrated Gaussian
kernels and derivatives perform very poorly at very fine scales. At very fine
scales, the discrete analogue of the Gaussian kernel with its corresponding
discrete derivative approximations performs substantially better. The sampled
Gaussian kernel and the sampled Gaussian derivatives do, on the other hand,
lead to numerically very good approximations of the corresponding continuous
results, when the scale parameter is sufficiently large, in the experiments
presented in the paper, when the scale parameter is greater than a value of
about 1, in units of the grid spacing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 34 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing rgb-d semantic segmentation through multi-modal interaction
  and pooling attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Zhang, Minghong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of RGB-D images involves understanding the appearance
and spatial relationships of objects within a scene, which requires careful
consideration of various factors. However, in indoor environments, the simple
input of RGB and depth images often results in a relatively limited acquisition
of semantic and spatial information, leading to suboptimal segmentation
outcomes. To address this, we propose the Multi-modal Interaction and Pooling
Attention Network (MIPANet), a novel approach designed to harness the
interactive synergy between RGB and depth modalities, optimizing the
utilization of complementary information. Specifically, we incorporate a
Multi-modal Interaction Fusion Module (MIM) into the deepest layers of the
network. This module is engineered to facilitate the fusion of RGB and depth
information, allowing for mutual enhancement and correction. Additionally, we
introduce a Pooling Attention Module (PAM) at various stages of the encoder.
This module serves to amplify the features extracted by the network and
integrates the module's output into the decoder in a targeted manner,
significantly improving semantic segmentation performance. Our experimental
results demonstrate that MIPANet outperforms existing methods on two indoor
scene datasets, NYUDv2 and SUN-RGBD, underscoring its effectiveness in
enhancing RGB-D semantic segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UMAAF: Unveiling Aesthetics via Multifarious Attributes of Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Li, Yitian Wan, Xingjiao Wu, Junjie Xu, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing prevalence of smartphones and websites, Image Aesthetic
Assessment (IAA) has become increasingly crucial. While the significance of
attributes in IAA is widely recognized, many attribute-based methods lack
consideration for the selection and utilization of aesthetic attributes. Our
initial step involves the acquisition of aesthetic attributes from both intra-
and inter-perspectives. Within the intra-perspective, we extract the direct
visual attributes of images, constituting the absolute attribute. In the
inter-perspective, our focus lies in modeling the relative score relationships
between images within the same sequence, forming the relative attribute. Then,
to better utilize image attributes in aesthetic assessment, we propose the
Unified Multi-attribute Aesthetic Assessment Framework (UMAAF) to model both
absolute and relative attributes of images. For absolute attributes, we
leverage multiple absolute-attribute perception modules and an
absolute-attribute interacting network. The absolute-attribute perception
modules are first pre-trained on several absolute-attribute learning tasks and
then used to extract corresponding absolute attribute features. The
absolute-attribute interacting network adaptively learns the weight of diverse
absolute-attribute features, effectively integrating them with generic
aesthetic features from various absolute-attribute perspectives and generating
the aesthetic prediction. To model the relative attribute of images, we
consider the relative ranking and relative distance relationships between
images in a Relative-Relation Loss function, which boosts the robustness of the
UMAAF. Furthermore, UMAAF achieves state-of-the-art performance on TAD66K and
AVA datasets, and multiple experiments demonstrate the effectiveness of each
module and the model's alignment with human preference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exchanging Dual Encoder-Decoder: A New Strategy for Change Detection
  with Semantic Guidance and Spatial Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijie Zhao, Xueliang Zhang, Pengfeng Xiao, Guangjun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change detection is a critical task in earth observation applications.
Recently, deep learning-based methods have shown promising performance and are
quickly adopted in change detection. However, the widely used multiple encoder
and single decoder (MESD) as well as dual encoder-decoder (DED) architectures
still struggle to effectively handle change detection well. The former has
problems of bitemporal feature interference in the feature-level fusion, while
the latter is inapplicable to intraclass change detection and multiview
building change detection. To solve these problems, we propose a new strategy
with an exchanging dual encoder-decoder structure for binary change detection
with semantic guidance and spatial localization. The proposed strategy solves
the problems of bitemporal feature inference in MESD by fusing bitemporal
features in the decision level and the inapplicability in DED by determining
changed areas using bitemporal semantic features. We build a binary change
detection model based on this strategy, and then validate and compare it with
18 state-of-the-art change detection methods on six datasets in three
scenarios, including intraclass change detection datasets (CDD, SYSU),
single-view building change detection datasets (WHU, LEVIR-CD, LEVIR-CD+) and a
multiview building change detection dataset (NJDS). The experimental results
demonstrate that our model achieves superior performance with high efficiency
and outperforms all benchmark methods with F1-scores of 97.77%, 83.07%, 94.86%,
92.33%, 91.39%, 74.35% on CDD, SYSU, WHU, LEVIR-CD, LEVIR- CD+, and NJDS
datasets, respectively. The code of this work will be available at
https://github.com/NJU-LHRS/official-SGSLN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pair-wise Layer Attention with Spatial Masking for Video Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Li, Chenhan Zhang, Zheng Yang, Xianghua Xu, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video prediction yields future frames by employing the historical frames and
has exhibited its great potential in many applications, e.g., meteorological
prediction, and autonomous driving. Previous works often decode the ultimate
high-level semantic features to future frames without texture details, which
deteriorates the prediction quality. Motivated by this, we develop a Pair-wise
Layer Attention (PLA) module to enhance the layer-wise semantic dependency of
the feature maps derived from the U-shape structure in Translator, by coupling
low-level visual cues and high-level features. Hence, the texture details of
predicted frames are enriched. Moreover, most existing methods capture the
spatiotemporal dynamics by Translator, but fail to sufficiently utilize the
spatial features of Encoder. This inspires us to design a Spatial Masking (SM)
module to mask partial encoding features during pretraining, which adds the
visibility of remaining feature pixels by Decoder. To this end, we present a
Pair-wise Layer Attention with Spatial Masking (PLA-SM) framework for video
prediction to capture the spatiotemporal dynamics, which reflect the motion
trend. Extensive experiments and rigorous ablation studies on five benchmarks
demonstrate the advantages of the proposed approach. The code is available at
GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval
  Score Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, Yingcong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in text-to-3D generation mark a significant milestone
in generative models, unlocking new possibilities for creating imaginative 3D
assets across various real-world scenarios. While recent advancements in
text-to-3D generation have shown promise, they often fall short in rendering
detailed and high-quality 3D models. This problem is especially prevalent as
many methods base themselves on Score Distillation Sampling (SDS). This paper
identifies a notable deficiency in SDS, that it brings inconsistent and
low-quality updating direction for the 3D model, causing the over-smoothing
effect. To address this, we propose a novel approach called Interval Score
Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes
interval-based score matching to counteract over-smoothing. Furthermore, we
incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline.
Extensive experiments show that our model largely outperforms the
state-of-the-art in quality and training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. Our code will
  be available at: https://github.com/EnVision-Research/LucidDreamer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transcending Forgery Specificity with Latent Space Augmentation for
  Generalizable Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Yan, Yuhao Luo, Siwei Lyu, Qingshan Liu, Baoyuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfake detection faces a critical generalization hurdle, with performance
deteriorating when there is a mismatch between the distributions of training
and testing data. A broadly received explanation is the tendency of these
detectors to be overfitted to forgery-specific artifacts, rather than learning
features that are widely applicable across various forgeries. To address this
issue, we propose a simple yet effective detector called LSDA
(\underline{L}atent \underline{S}pace \underline{D}ata
\underline{A}ugmentation), which is based on a heuristic idea: representations
with a wider variety of forgeries should be able to learn a more generalizable
decision boundary, thereby mitigating the overfitting of method-specific
features (see Figure. 1). Following this idea, we propose to enlarge the
forgery space by constructing and simulating variations within and across
forgery features in the latent space. This approach encompasses the acquisition
of enriched, domain-specific features and the facilitation of smoother
transitions between different forgery types, effectively bridging domain gaps.
Our approach culminates in refining a binary classifier that leverages the
distilled knowledge from the enhanced features, striving for a generalizable
deepfake detector. Comprehensive experiments show that our proposed method is
surprisingly effective and transcends state-of-the-art detectors across several
widely used benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization and Hallucination of Large Vision-Language Models through
  a Camouflaged Lens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lv Tang, Peng-Tao Jiang, Zhihao Shen, Hao Zhang, Jinwei Chen, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Model (LVLM) has seen burgeoning development and
increasing attention recently. In this paper, we propose a novel framework,
camo-perceptive vision-language framework (CPVLF), to explore whether LVLM can
generalize to the challenging camouflaged object detection (COD) scenario in a
training-free manner. During the process of generalization, we find that due to
hallucination issues within LVLM, it can erroneously perceive objects in
camouflaged scenes, producing counterfactual concepts. Moreover, as LVLM is not
specifically trained for the precise localization of camouflaged objects, it
exhibits a degree of uncertainty in accurately pinpointing these objects.
Therefore, we propose chain of visual perception, which enhances LVLM's
perception of camouflaged scenes from both linguistic and visual perspectives,
reducing the hallucination issue and improving its capability in accurately
locating camouflaged objects. We validate the effectiveness of CPVLF on three
widely used COD datasets, and the experiments show the potential of LVLM in the
COD task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Real-World Writing Assistance: A Chinese Character Checking
  Benchmark with Faked and Misspelled Characters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Zishan Xu, Shaoshen Chen, Haojing Huang, Yangning Li, Yong Jiang, Zhongli Li, Qingyu Zhou, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing assistance is an application closely related to human life and is
also a fundamental Natural Language Processing (NLP) research field. Its aim is
to improve the correctness and quality of input texts, with character checking
being crucial in detecting and correcting wrong characters. From the
perspective of the real world where handwriting occupies the vast majority,
characters that humans get wrong include faked characters (i.e., untrue
characters created due to writing errors) and misspelled characters (i.e., true
characters used incorrectly due to spelling errors). However, existing datasets
and related studies only focus on misspelled characters mainly caused by
phonological or visual confusion, thereby ignoring faked characters which are
more common and difficult. To break through this dilemma, we present
Visual-C$^3$, a human-annotated Visual Chinese Character Checking dataset with
faked and misspelled Chinese characters. To the best of our knowledge,
Visual-C$^3$ is the first real-world visual and the largest human-crafted
dataset for the Chinese character checking scenario. Additionally, we also
propose and evaluate novel baseline methods on Visual-C$^3$. Extensive
empirical results and analyses show that Visual-C$^3$ is high-quality yet
challenging. The Visual-C$^3$ dataset and the baseline methods will be publicly
available to facilitate further research in the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial <span class="highlight-title">Prompt</span> Tuning for Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Zhang, Xingjun Ma, Xin Wang, Lingyu Qiu, Jiaqi Wang, Yu-Gang Jiang, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of multimodal learning, pre-trained
Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable
capacities in bridging the gap between visual and language modalities. However,
these models remain vulnerable to adversarial attacks, particularly in the
image modality, presenting considerable security risks. This paper introduces
Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial
robustness of image encoders in VLMs. AdvPT innovatively leverages learnable
text prompts and aligns them with adversarial image embeddings, to address the
vulnerabilities inherent in VLMs without the need for extensive parameter
training or modification of the model architecture. We demonstrate that AdvPT
improves resistance against white-box and black-box adversarial attacks and
exhibits a synergistic effect when combined with existing
image-processing-based defense techniques, further boosting defensive
capabilities. Comprehensive experimental analyses provide insights into
adversarial prompt tuning, a novel paradigm devoted to improving resistance to
adversarial images through textual input modifications, paving the way for
future robust multimodal learning research. These findings open up new
possibilities for enhancing the security of VLMs. Our code will be available
upon publication of the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Radarize: Large-Scale Radar SLAM for Indoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emerson Sie, Xinyu Wu, Heyu Guo, Deepak Vasisht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Radarize, a self-contained SLAM pipeline for indoor environments
that uses only a low-cost commodity single-chip mmWave radar. Our radar-native
approach leverages phenomena unique to radio frequencies, such as doppler
shift-based odometry, to improve performance. We evaluate our method on a
large-scale dataset of 146 trajectories spanning 4 campus buildings, totaling
approximately 4680m of travel distance. Our results show that our method
outperforms state-of-the-art radar-based approaches by approximately 5x in
terms of odometry and 8x in terms of end-to-end SLAM, as measured by absolute
trajectory error (ATE), without the need additional sensors such as IMUs or
wheel odometry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Tracking and Image-Video Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peirong Liu, Rui Wang, Pengchuan Zhang, Omid Poursaeed, Yipin Zhou, Xuefei Cao, Sreya Dutta Roy, Ashish Shah, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objection detection (OD) has been one of the most fundamental tasks in
computer vision. Recent developments in deep learning have pushed the
performance of image OD to new heights by learning-based, data-driven
approaches. On the other hand, video OD remains less explored, mostly due to
much more expensive data annotation needs. At the same time, multi-object
tracking (MOT) which requires reasoning about track identities and
spatio-temporal trajectories, shares similar spirits with video OD. However,
most MOT datasets are class-specific (e.g., person-annotated only), which
constrains a model's flexibility to perform tracking on other objects. We
propose TrIVD (Tracking and Image-Video Detection), the first framework that
unifies image OD, video OD, and MOT within one end-to-end model. To handle the
discrepancies and semantic overlaps of category labels across datasets, TrIVD
formulates detection/tracking as grounding and reasons about object categories
via visual-text alignments. The unified formulation enables cross-dataset,
multi-task training, and thus equips TrIVD with the ability to leverage
frame-level features, video-level spatio-temporal relations, as well as track
identity associations. With such joint training, we can now extend the
knowledge from OD data, that comes with much richer object category
annotations, to MOT and achieve zero-shot tracking capability. Experiments
demonstrate that multi-task co-trained TrIVD outperforms single-task baselines
across all image/video OD and MOT tasks. We further set the first baseline on
the new task of zero-shot tracking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differential Motion Evolution for Fine-Grained Motion Deformation in
  Unsupervised Image Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04658v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04658v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peirong Liu, Rui Wang, Xuefei Cao, Yipin Zhou, Ashish Shah, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image animation is the task of transferring the motion of a driving video to
a given object in a source image. While great progress has recently been made
in unsupervised motion transfer, requiring no labeled data or domain priors,
many current unsupervised approaches still struggle to capture the motion
deformations when large motion/view discrepancies occur between the source and
driving domains. Under such conditions, there is simply not enough information
to capture the motion field properly. We introduce DiME (Differential Motion
Evolution), an end-to-end unsupervised motion transfer framework integrating
differential refinement for motion estimation. Key findings are twofold: (1) by
capturing the motion transfer with an ordinary differential equation (ODE), it
helps to regularize the motion field, and (2) by utilizing the source image
itself, we are able to inpaint occluded/missing regions arising from large
motion changes. Additionally, we also propose a natural extension to the ODE
idea, which is that DiME can easily leverage multiple different views of the
source object whenever they are available by modeling an ODE per view.
Extensive experiments across 9 benchmarks show DiME outperforms the
state-of-the-arts by a significant margin and generalizes much better to unseen
objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do We Really Need Dice? The Hidden Region-Size Biases of Segmentation
  Losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.08717v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.08717v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingyuan Liu, Jose Dolz, Adrian Galdran, Riadh Kobbi, Ismail Ben Ayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most segmentation losses are arguably variants of the Cross-Entropy (CE) or
Dice losses. On the surface, these two categories of losses seem unrelated, and
there is no clear consensus as to which category is a better choice, with
varying performances for each across different benchmarks and applications.
Furthermore, it is widely argued within the medical-imaging community that Dice
and CE are complementary, which has motivated the use of compound CE-Dice
losses. In this work, we provide a theoretical analysis, which shows that CE
and Dice share a much deeper connection than previously thought. First, we show
that, from a constrained-optimization perspective, they both decompose into two
components, i.e., a similar ground-truth matching term, which pushes the
predicted foreground regions towards the ground-truth, and a region-size
penalty term imposing different biases on the size of the predicted regions.
Then, we provide bound relationships and an information-theoretic analysis,
which uncover hidden region-size biases: Dice has an intrinsic bias towards
specific extremely imbalanced solutions, whereas CE implicitly encourages the
ground-truth region proportions. Our theoretical results explain the wide
experimental evidence in the medical-imaging literature, whereby Dice losses
bring improvements for imbalanced segmentation. Based on our theoretical
analysis, we propose a principled and simple solution, which enables to control
explicitly the region-size bias. The proposed method integrates CE with
explicit terms based on L1 or the KL divergence, which encourage segmenting
region proportions to match target class proportions, thereby mitigating class
imbalance but without losing generality. Comprehensive experiments and ablation
studies over different losses and applications validate our theoretical
analysis, as well as the effectiveness of explicit and simple region-size
terms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published at Medical Image Analysis(Volume 91, January 2024,
  103015). Code available at https://github.com/by-liu/SegLossBias</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RegBN: Batch Normalization of Multimodal Data with Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morteza Ghahremani, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a surge of interest in integrating
high-dimensional data captured by multisource sensors, driven by the impressive
success of neural networks in the integration of multimodal data. However, the
integration of heterogeneous multimodal data poses a significant challenge, as
confounding effects and dependencies among such heterogeneous data sources
introduce unwanted variability and bias, leading to suboptimal performance of
multimodal models. Therefore, it becomes crucial to normalize the low- or
high-level features extracted from data modalities before their fusion takes
place. This paper introduces a novel approach for the normalization of
multimodal data, called RegBN, that incorporates regularization. RegBN uses the
Frobenius norm as a regularizer term to address the side effects of confounders
and underlying dependencies among different data sources. The proposed method
generalizes well across multiple modalities and eliminates the need for
learnable parameters, simplifying training and inference. We validate the
effectiveness of RegBN on eight databases from five research areas,
encompassing diverse modalities such as language, audio, image, video, depth,
tabular, and 3D MRI. The proposed method demonstrates broad applicability
across different architectures such as multilayer perceptrons, convolutional
neural networks, and vision transformers, enabling effective normalization of
both low- and high-level features in multimodal neural networks. RegBN is
available at \url{https://github.com/mogvision/regbn}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SA2-Net: Scale-aware Attention Network for Microscopic Image
  Segmentation <span class="chip">BMVC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16661v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16661v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustansar Fiaz, Moein Heidari, Rao Muhammad Anwer, Hisham Cholakkal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microscopic image segmentation is a challenging task, wherein the objective
is to assign semantic labels to each pixel in a given microscopic image. While
convolutional neural networks (CNNs) form the foundation of many existing
frameworks, they often struggle to explicitly capture long-range dependencies.
Although transformers were initially devised to address this issue using
self-attention, it has been proven that both local and global features are
crucial for addressing diverse challenges in microscopic images, including
variations in shape, size, appearance, and target region density. In this
paper, we introduce SA2-Net, an attention-guided method that leverages
multi-scale feature learning to effectively handle diverse structures within
microscopic images. Specifically, we propose scale-aware attention (SA2) module
designed to capture inherent variations in scales and shapes of microscopic
regions, such as cells, for accurate segmentation. This module incorporates
local attention at each level of multi-stage features, as well as global
attention across multiple resolutions. Furthermore, we address the issue of
blurred region boundaries (e.g., cell boundaries) by introducing a novel
upsampling strategy called the Adaptive Up-Attention (AuA) module. This module
enhances the discriminative ability for improved localization of microscopic
regions using an explicit attention mechanism. Extensive experiments on five
challenging datasets demonstrate the benefits of our SA2-Net model. Our source
code is publicly available at \url{https://github.com/mustansarfiaz/SA2-Net}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BMVC 2023 accepted as oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Computer Vision Models through Adversarial Training:
  Unveiling the Robustness-Interpretability Connection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delyan Boychev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the perpetual increase of complexity of the state-of-the-art deep neural
networks, it becomes a more and more challenging task to maintain their
interpretability. Our work aims to evaluate the effects of adversarial training
utilized to produce robust models - less vulnerable to adversarial attacks. It
has been shown to make computer vision models more interpretable.
Interpretability is as essential as robustness when we deploy the models to the
real world. To prove the correlation between these two problems, we extensively
examine the models using local feature-importance methods (SHAP, Integrated
Gradients) and feature visualization techniques (Representation Inversion,
Class Specific Image Generation). Standard models, compared to robust are more
susceptible to adversarial attacks, and their learned representations are less
meaningful to humans. Conversely, these models focus on distinctive regions of
the images which support their predictions. Moreover, the features learned by
the robust model are closer to the real ones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 19 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoybeanNet: <span class="highlight-title">Transformer</span>-Based Convolutional Neural Network for Soybean
  Pod Counting from Unmanned Aerial Vehicle (UAV) Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajia Li, Raju Thada Magar, Dong Chen, Feng Lin, Dechun Wang, Xiang Yin, Weichao Zhuang, Zhaojian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soybeans are a critical source of food, protein and oil, and thus have
received extensive research aimed at enhancing their yield, refining
cultivation practices, and advancing soybean breeding techniques. Within this
context, soybean pod counting plays an essential role in understanding and
optimizing production. Despite recent advancements, the development of a robust
pod-counting algorithm capable of performing effectively in real-field
conditions remains a significant challenge This paper presents a pioneering
work of accurate soybean pod counting utilizing unmanned aerial vehicle (UAV)
images captured from actual soybean fields in Michigan, USA. Specifically, this
paper presents SoybeanNet, a novel point-based counting network that harnesses
powerful transformer backbones for simultaneous soybean pod counting and
localization with high accuracy. In addition, a new dataset of UAV-acquired
images for soybean pod counting was created and open-sourced, consisting of 113
drone images with more than 260k manually annotated soybean pods captured under
natural lighting conditions. Through comprehensive evaluations, SoybeanNet
demonstrated superior performance over five state-of-the-art approaches when
tested on the collected images. Remarkably, SoybeanNet achieved a counting
accuracy of $84.51\%$ when tested on the testing dataset, attesting to its
efficacy in real-world scenarios. The publication also provides both the source
code (\url{https://github.com/JiajiaLi04/Soybean-Pod-Counting-from-UAV-Images})
and the labeled soybean dataset
(\url{https://www.kaggle.com/datasets/jiajiali/uav-based-soybean-pod-images}),
offering a valuable resource for future research endeavors in soybean pod
counting and related fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting and Advancing Adversarial Training Through A Simple Baseline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we delve into the essential components of adversarial training
which is a pioneering defense technique against adversarial attacks. We
indicate that some factors such as the loss function, learning rate scheduler,
and data augmentation, which are independent of the model architecture, will
influence adversarial robustness and generalization. When these factors are
controlled for, we introduce a simple baseline approach, termed SimpleAT, that
performs competitively with recent methods and mitigates robust overfitting. We
conduct extensive experiments on CIFAR-10/100 and Tiny-ImageNet, which validate
the robustness of SimpleAT against state-of-the-art adversarial attackers such
as AutoAttack. Our results also demonstrate that SimpleAT exhibits good
performance in the presence of various image corruptions, such as those found
in the CIFAR-10-C. In addition, we empirically show that SimpleAT is capable of
reducing the variance in model predictions, which is considered the primary
contributor to robust overfitting. Our results also reveal the connections
between SimpleAT and many advanced state-of-the-art adversarial defense
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FS-Net: Full Scale Network and Adaptive Threshold for Improving
  Extraction of Micro-Retinal Vessel Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melaku N. Getahun, Oleg Y. Rogov, Dmitry V. Dylov, Andrey Somov, Ahmed Bouridane, Rifat Hamoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retinal vascular segmentation, is a widely researched subject in biomedical
image processing, aims to relieve ophthalmologists' workload when treating and
detecting retinal disorders. However, segmenting retinal vessels has its own
set of challenges, with prior techniques failing to generate adequate results
when segmenting branches and microvascular structures. The neural network
approaches used recently are characterized by the inability to keep local and
global properties together and the failure to capture tiny end vessels make it
challenging to attain the desired result. To reduce this retinal vessel
segmentation problem, we propose a full-scale micro-vessel extraction mechanism
based on an encoder-decoder neural network architecture, sigmoid smoothing, and
an adaptive threshold method. The network consists of of residual, encoder
booster, bottleneck enhancement, squeeze, and excitation building blocks. All
of these blocks together help to improve the feature extraction and prediction
of the segmentation map. The proposed solution has been evaluated using the
DRIVE, CHASE-DB1, and STARE datasets, and competitive results are obtained when
compared with previous studies. The AUC and accuracy on the DRIVE dataset are
0.9884 and 0.9702, respectively. On the CHASE-DB1 dataset, the scores are
0.9903 and 0.9755, respectively. On the STARE dataset, the scores are 0.9916
and 0.9750, respectively. The performance achieved is one step ahead of what
has been done in previous studies, and this results in a higher chance of
having this solution in real-life diagnostic centers that seek ophthalmologists
attention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Hyperspectral Pansharpening via Low-rank Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Rui, Xiangyong Cao, Li Pang, Zeyu Zhu, Zongsheng Yue, Deyu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral pansharpening is a process of merging a high-resolution
panchromatic (PAN) image and a low-resolution hyperspectral (LRHS) image to
create a single high-resolution hyperspectral (HRHS) image. Existing
Bayesian-based HS pansharpening methods require designing handcraft image prior
to characterize the image features, and deep learning-based HS pansharpening
methods usually require a large number of paired training data and suffer from
poor generalization ability. To address these issues, in this work, we propose
a low-rank diffusion model for hyperspectral pansharpening by simultaneously
leveraging the power of the pre-trained deep diffusion model and better
generalization ability of Bayesian methods. Specifically, we assume that the
HRHS image can be recovered from the product of two low-rank tensors, i.e., the
base tensor and the coefficient matrix. The base tensor lies on the image field
and has a low spectral dimension. Thus, we can conveniently utilize a
pre-trained remote sensing diffusion model to capture its image structures.
Additionally, we derive a simple yet quite effective way to pre-estimate the
coefficient matrix from the observed LRHS image, which preserves the spectral
information of the HRHS. Experimental results demonstrate that the proposed
method performs better than some popular traditional approaches and gains
better generalization ability than some DL-based methods. The code is released
in https://github.com/xyrui/PLRDiff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLIC: Multi-Reference Entropy Model for Learned Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07273v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07273v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jiang, Jiayu Yang, Yongqi Zhai, Peirong Ning, Feng Gao, Ronggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, learned image compression has achieved remarkable performance. The
entropy model, which estimates the distribution of the latent representation,
plays a crucial role in boosting rate-distortion performance. However, most
entropy models only capture correlations in one dimension, while the latent
representation contain channel-wise, local spatial, and global spatial
correlations. To tackle this issue, we propose the Multi-Reference Entropy
Model (MEM) and the advanced version, MEM$^+$. These models capture the
different types of correlations present in latent representation. Specifically,
We first divide the latent representation into slices. When decoding the
current slice, we use previously decoded slices as context and employ the
attention map of the previously decoded slice to predict global correlations in
the current slice. To capture local contexts, we introduce two enhanced
checkerboard context capturing techniques that avoids performance degradation.
Based on MEM and MEM$^+$, we propose image compression models MLIC and
MLIC$^+$. Extensive experimental evaluations demonstrate that our MLIC and
MLIC$^+$ models achieve state-of-the-art performance, reducing BD-rate by
$8.05\%$ and $11.39\%$ on the Kodak dataset compared to VTM-17.0 when measured
in PSNR. Our code is available at https://github.com/JiangWeibeta/MLIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACMMM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Multi-view Pedestrian Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyin Liu, Chao Zhu, Shiqi Ren, Xu-Cheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the prosperity of the video surveillance, multiple cameras have been
applied to accurately locate pedestrians in a specific area. However, previous
methods rely on the human-labeled annotations in every video frame and camera
view, leading to heavier burden than necessary camera calibration and
synchronization. Therefore, we propose in this paper an Unsupervised Multi-view
Pedestrian Detection approach (UMPD) to eliminate the need of annotations to
learn a multi-view pedestrian detector via 2D-3D mapping. 1) Firstly,
Semantic-aware Iterative Segmentation (SIS) is proposed to extract unsupervised
representations of multi-view images, which are converted into 2D pedestrian
masks as pseudo labels, via our proposed iterative PCA and zero-shot semantic
classes from vision-language models. 2) Secondly, we propose Geometry-aware
Volume-based Detector (GVD) to end-to-end encode multi-view 2D images into a 3D
volume to predict voxel-wise density and color via 2D-to-3D geometric
projection, trained by 3D-to-2D rendering losses with SIS pseudo labels. 3)
Thirdly, for better detection results, i.e., the 3D density projected on
Birds-Eye-View from GVD, we propose Vertical-aware BEV Regularization (VBR) to
constraint them to be vertical like the natural pedestrian poses. Extensive
experiments on popular multi-view pedestrian detection benchmarks Wildtrack,
Terrace, and MultiviewX, show that our proposed UMPD approach, as the first
fully-unsupervised method to our best knowledge, performs competitively to the
previous state-of-the-art supervised techniques. Code will be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IRFL: Image Recognition of Figurative Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ron Yosef, Yonatan Bitton, Dafna Shahaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Figures of speech such as metaphors, similes, and idioms are integral parts
of human communication. They are ubiquitous in many forms of discourse,
allowing people to convey complex, abstract ideas and evoke emotion. As
figurative forms are often conveyed through multiple modalities (e.g., both
text and images), understanding multimodal figurative language is an important
AI challenge, weaving together profound vision, language, commonsense and
cultural knowledge.
  In this work, we develop the Image Recognition of Figurative Language (IRFL)
dataset. We leverage human annotation and an automatic pipeline we created to
generate a multimodal dataset, and introduce two novel tasks as a benchmark for
multimodal figurative language understanding. We experimented with
state-of-the-art vision and language models and found that the best (22%)
performed substantially worse than humans (97%). We release our dataset,
benchmark, and code, in hopes of driving the development of models that can
better understand figurative language.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Law of Large Sequential Recommendation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaowei Zhang, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling of neural networks has recently shown great potential to improve the
model capacity in various fields. Specifically, model performance has a
power-law relationship with model size or data size, which provides important
guidance for the development of large-scale models. However, there is still
limited understanding on the scaling effect of user behavior models in
recommender systems, where the unique data characteristics (e.g. data scarcity
and sparsity) pose new challenges to explore the scaling effect in
recommendation tasks. In this work, we focus on investigating the scaling laws
in large sequential recommendation models. Specially, we consider a pure
ID-based task formulation, where the interaction history of a user is formatted
as a chronological sequence of item IDs. We don't incorporate any side
information (e.g. item text), because we would like to explore how scaling law
holds from the perspective of user behavior. With specially improved
strategies, we scale up the model size to 0.8B parameters, making it feasible
to explore the scaling effect in a diverse range of model sizes. As the major
findings, we empirically show that scaling law still holds for these trained
models, even in data-constrained scenarios. We then fit the curve for scaling
law, and successfully predict the test loss of the two largest tested model
scales. Furthermore, we examine the performance advantage of scaling effect on
five challenging recommendation tasks, considering the unique issues (e.g. cold
start, robustness, long-term preference) in recommender systems. We find that
scaling up the model size can greatly boost the performance on these
challenging tasks, which again verifies the benefits of large recommendation
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dependency Relationships-Enhanced Attentive Group Recommendation in HINs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juntao Zhang, Sheng Wang, Zhiyu Chen, Xiandi Yang, Zhiyong Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommending suitable items to a group of users, commonly referred to as the
group recommendation task, is becoming increasingly urgent with the development
of group activities. The challenges within the group recommendation task
involve aggregating the individual preferences of group members as the group's
preferences and facing serious sparsity problems due to the lack of
user/group-item interactions. To solve these problems, we propose a novel
approach called Dependency Relationships-Enhanced Attentive Group
Recommendation (DREAGR) for the recommendation task of occasional groups.
Specifically, we introduce the dependency relationship between items as side
information to enhance the user/group-item interaction and alleviate the
interaction sparsity problem. Then, we propose a Path-Aware Attention Embedding
(PAAE) method to model users' preferences on different types of paths. Next, we
design a gated fusion mechanism to fuse users' preferences into their
comprehensive preferences. Finally, we develop an attention aggregator that
aggregates users' preferences as the group's preferences for the group
recommendation task. We conducted experiments on two datasets to demonstrate
the superiority of DREAGR by comparing it with state-of-the-art group
recommender models. The experimental results show that DREAGR outperforms other
models, especially HR@N and NDCG@N (N=5, 10), where DREAGR has improved in the
range of 3.64% to 7.01% and 2.57% to 3.39% on both datasets, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, This paper has been submitted to IEEE
  Transactions on Knowledge and Data Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Interactive Query Generation Assistant using LLM-based <span class="highlight-title">Prompt</span>
  Modification and User Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh D. Dhole, Ramraj Chandradevan, Eugene Agichtein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While search is the predominant method of accessing information, formulating
effective queries remains a challenging task, especially for situations where
the users are not familiar with a domain, or searching for documents in other
languages, or looking for complex information such as events, which are not
easily expressible as queries. Providing example documents or passages of
interest, might be easier for a user, however, such query-by-example scenarios
are prone to concept drift, and are highly sensitive to the query generation
method. This demo illustrates complementary approaches of using LLMs
interactively, assisting and enabling the user to provide edits and feedback at
all stages of the query formulation process. The proposed Query Generation
Assistant is a novel search interface which supports automatic and interactive
query generation over a mono-linguial or multi-lingual document collection.
Specifically, the proposed assistive interface enables the users to refine the
queries generated by different LLMs, to provide feedback on the retrieved
documents or passages, and is able to incorporate the users' feedback as
prompts to generate more effective queries. The proposed interface is a
valuable experimental tool for exploring fine-tuning and prompting of LLMs for
query generation to qualitatively evaluate the effectiveness of retrieval and
ranking models, and for conducting Human-in-the-Loop (HITL) experiments for
complex search tasks where users struggle to formulate queries without such
assistance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Intelligence Advanced Research Projects Activity (IARPA) BETTER
  Research Program</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Better Query Classification with Multi-Expert Knowledge
  Condensation in JD Ads Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun-Peng Ning, Ming Pang, Zheng Fang, Xue Jiang, Xi-Wei Zhao, Chang-Ping Peng, Zhan-Gang Lin, Jing-He Hu, Jing-Ping Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search query classification, as an effective way to understand user intents,
is of great importance in real-world online ads systems. To ensure a lower
latency, a shallow model (e.g. FastText) is widely used for efficient online
inference. However, the representation ability of the FastText model is
insufficient, resulting in poor classification performance, especially on some
low-frequency queries and tailed categories. Using a deeper and more complex
model (e.g. BERT) is an effective solution, but it will cause a higher online
inference latency and more expensive computing costs. Thus, how to juggle both
inference efficiency and classification performance is obviously of great
practical importance. To overcome this challenge, in this paper, we propose
knowledge condensation (KC), a simple yet effective knowledge distillation
framework to boost the classification performance of the online FastText model
under strict low latency constraints. Specifically, we propose to train an
offline BERT model to retrieve more potentially relevant data. Benefiting from
its powerful semantic representation, more relevant labels not exposed in the
historical data will be added into the training set for better FastText model
training. Moreover, a novel distribution-diverse multi-expert learning strategy
is proposed to further improve the mining ability of relevant data. By training
multiple BERT models from different data distributions, it can respectively
perform better at high, middle, and low-frequency search queries. The model
ensemble from multi-distribution makes its retrieval ability more powerful. We
have deployed two versions of this framework in JD search, and both offline
experiments and online A/B testing from multiple datasets have validated the
effectiveness of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Category Trees for ID-Based Recommendation: Exploring the Power
  of Differentiable Vector Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16761v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16761v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijiong Liu, Jiaren Xiao, Lu Fan, Jieming Zhu, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Category information plays a crucial role in enhancing the quality and
personalization of recommender systems. Nevertheless, the availability of item
category information is not consistently present, particularly in the context
of ID-based recommendations. In this work, we propose a novel approach to
automatically learn and generate entity (i.e., user or item) category trees for
ID-based recommendation. Specifically, we devise a differentiable vector
quantization framework for automatic category tree generation, namely CAGE,
which enables the simultaneous learning and refinement of categorical code
representations and entity embeddings in an end-to-end manner, starting from
the randomly initialized states. With its high adaptability, CAGE can be easily
integrated into both sequential and non-sequential recommender systems. We
validate the effectiveness of CAGE on various recommendation tasks including
list completion, collaborative filtering, and click-through rate prediction,
across different recommendation models. We release the code and data for others
to reproduce the reported results.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">28</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Enhanced TinyML for Real-Time Detection of Ground Magnetic
  Anomalies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Talha Siddique, MD Shaad Mahmud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Space weather phenomena like geomagnetic disturbances (GMDs) and
geomagnetically induced currents (GICs) pose significant risks to critical
technological infrastructure. While traditional predictive models, grounded in
simulation, hold theoretical robustness, they grapple with challenges, notably
the assimilation of imprecise data and extensive computational complexities. In
recent years, Tiny Machine Learning (TinyML) has been adopted to develop
Machine Learning (ML)-enabled magnetometer systems for predicting real-time
terrestrial magnetic perturbations as a proxy measure for GIC. While TinyML
offers efficient, real-time data processing, its intrinsic limitations prevent
the utilization of robust methods with high computational needs. This paper
developed a physics-guided TinyML framework to address the above challenges.
This framework integrates physics-based regularization at the stages of model
training and compression, thereby augmenting the reliability of predictions.
The developed pruning scheme within the framework harnesses the inherent
physical characteristics of the domain, striking a balance between model size
and robustness. The study presents empirical results, drawing a comprehensive
comparison between the accuracy and reliability of the developed framework and
its traditional counterpart. Such a comparative analysis underscores the
prospective applicability of the developed framework in conceptualizing robust,
ML-enabled magnetometer systems for real-time space weather forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weight Norm Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Loshchilov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We note that decoupled weight decay regularization is a particular case of
weight norm control where the target norm of weights is set to 0. Any
optimization method (e.g., Adam) which uses decoupled weight decay
regularization (respectively, AdamW) can be viewed as a particular case of a
more general algorithm with weight norm control (respectively, AdamWN). We
argue that setting the target norm of weights to 0 can be suboptimal and other
target norm values can be considered. For instance, any training run where
AdamW achieves a particular norm of weights can be challenged by AdamWN
scheduled to achieve a comparable norm of weights. We discuss various
implications of introducing weight norm control instead of weight decay.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Duality of Bures and Shape Distances with Implications for Comparing
  Neural Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah E. Harvey, Brett W. Larsen, Alex H. Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A multitude of (dis)similarity measures between neural network
representations have been proposed, resulting in a fragmented research
landscape. Most of these measures fall into one of two categories.
  First, measures such as linear regression, canonical correlations analysis
(CCA), and shape distances, all learn explicit mappings between neural units to
quantify similarity while accounting for expected invariances. Second, measures
such as representational similarity analysis (RSA), centered kernel alignment
(CKA), and normalized Bures similarity (NBS) all quantify similarity in summary
statistics, such as stimulus-by-stimulus kernel matrices, which are already
invariant to expected symmetries. Here, we take steps towards unifying these
two broad categories of methods by observing that the cosine of the Riemannian
shape distance (from category 1) is equal to NBS (from category 2). We explore
how this connection leads to new interpretations of shape distances and NBS,
and draw contrasts of these measures with CKA, a popular similarity measure in
the deep learning literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Heavy Inner Product Identification Between Weights and Inputs in
  Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianke Qin, Saayan Mitra, Zhao Song, Yuanyuan Yang, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider a heavy inner product identification problem,
which generalizes the Light Bulb problem~(\cite{prr89}): Given two sets $A
\subset \{-1,+1\}^d$ and $B \subset \{-1,+1\}^d$ with $|A|=|B| = n$, if there
are exact $k$ pairs whose inner product passes a certain threshold, i.e.,
$\{(a_1, b_1), \cdots, (a_k, b_k)\} \subset A \times B$ such that $\forall i
\in [k], \langle a_i,b_i \rangle \geq \rho \cdot d$, for a threshold $\rho \in
(0,1)$, the goal is to identify those $k$ heavy inner products. We provide an
algorithm that runs in $O(n^{2 \omega / 3+ o(1)})$ time to find the $k$ inner
product pairs that surpass $\rho \cdot d$ threshold with high probability,
where $\omega$ is the current matrix multiplication exponent. By solving this
problem, our method speed up the training of neural networks with ReLU
activation function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE BigData 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor-Aware Energy Accounting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timur Babakol, Yu David Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of Artificial Intelligence (AI) applications supported
by deep learning (DL), the energy efficiency of these applications has an
increasingly large impact on sustainability. We introduce Smaragdine, a new
energy accounting system for tensor-based DL programs implemented with
TensorFlow. At the heart of Smaragdine is a novel white-box methodology of
energy accounting: Smaragdine is aware of the internal structure of the DL
program, which we call tensor-aware energy accounting. With Smaragdine, the
energy consumption of a DL program can be broken down into units aligned with
its logical hierarchical decomposition structure. We apply Smaragdine for
understanding the energy behavior of BERT, one of the most widely used language
models. Layer-by-layer and tensor-by-tensor, Smaragdine is capable of
identifying the highest energy/power-consuming components of BERT. Furthermore,
we conduct two case studies on how Smaragdine supports downstream toolchain
building, one on the comparative energy impact of hyperparameter tuning of
BERT, the other on the energy behavior evolution when BERT evolves to its next
generation, ALBERT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Reinforcement Learning for Wireless Network Optimization with
  Mixture <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Yang, Cong Shen, Jing Yang, Shu-ping Yeh, Jerry Sydir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development of reinforcement learning (RL) has boosted the
adoption of online RL for wireless radio resource management (RRM). However,
online RL algorithms require direct interactions with the environment, which
may be undesirable given the potential performance loss due to the unavoidable
exploration in RL. In this work, we first investigate the use of \emph{offline}
RL algorithms in solving the RRM problem. We evaluate several state-of-the-art
offline RL algorithms, including behavior constrained Q-learning (BCQ),
conservative Q-learning (CQL), and implicit Q-learning (IQL), for a specific
RRM problem that aims at maximizing a linear combination {of sum and}
5-percentile rates via user scheduling. We observe that the performance of
offline RL for the RRM problem depends critically on the behavior policy used
for data collection, and further propose a novel offline RL solution that
leverages heterogeneous datasets collected by different behavior policies. We
show that with a proper mixture of the datasets, offline RL can produce a
near-optimal RL policy even when all involved behavior policies are highly
suboptimal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is the camera ready version for Asilomar 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precision at the indistinguishability threshold: a method for evaluating
  classification algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David J. T. Sumpter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There exist a wide range of single number metrics for assessing performance
of classification algorithms, including AUC and the F1-score (Wikipedia lists
17 such metrics, with 27 different names). In this article, I propose a new
metric to answer the following question: when an algorithm is tuned so that it
can no longer distinguish labelled cats from real cats, how often does a
randomly chosen image that has been labelled as containing a cat actually
contain a cat? The steps to construct this metric are as follows. First, we set
a threshold score such that when the algorithm is shown two randomly-chosen
images -- one that has a score greater than the threshold (i.e. a picture
labelled as containing a cat) and another from those pictures that really does
contain a cat -- the probability that the image with the highest score is the
one chosen from the set of real cat images is 50\%. At this decision threshold,
the set of positively labelled images are indistinguishable from the set of
images which are positive. Then, as a second step, we measure performance by
asking how often a randomly chosen picture from those labelled as containing a
cat actually contains a cat. This metric can be thought of as {\it precision at
the indistinguishability threshold}. While this new metric doesn't address the
tradeoff between precision and recall inherent to all such metrics, I do show
why this method avoids pitfalls that can occur when using, for example AUC, and
it is better motivated than, for example, the F1-score.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LifeLearner: Hardware-Aware Meta Continual Learning System for Embedded
  Computing Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young D. Kwon, Jagmohan Chauhan, Hong Jia, Stylianos I. Venieris, Cecilia Mascolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning (CL) allows applications such as user personalization and
household robots to learn on the fly and adapt to context. This is an important
feature when context, actions, and users change. However, enabling CL on
resource-constrained embedded systems is challenging due to the limited labeled
data, memory, and computing capacity. In this paper, we propose LifeLearner, a
hardware-aware meta continual learning system that drastically optimizes system
resources (lower memory, latency, energy consumption) while ensuring high
accuracy. Specifically, we (1) exploit meta-learning and rehearsal strategies
to explicitly cope with data scarcity issues and ensure high accuracy, (2)
effectively combine lossless and lossy compression to significantly reduce the
resource requirements of CL and rehearsal samples, and (3) developed
hardware-aware system on embedded and IoT platforms considering the hardware
characteristics. As a result, LifeLearner achieves near-optimal CL performance,
falling short by only 2.8% on accuracy compared to an Oracle baseline. With
respect to the state-of-the-art (SOTA) Meta CL method, LifeLearner drastically
reduces the memory footprint (by 178.7x), end-to-end latency by 80.8-94.2%, and
energy consumption by 80.9-94.2%. In addition, we successfully deployed
LifeLearner on two edge devices and a microcontroller unit, thereby enabling
efficient CL on resource-constrained platforms where it would be impractical to
run SOTA methods and the far-reaching deployment of adaptable CL in a
ubiquitous manner. Code is available at
https://github.com/theyoungkwon/LifeLearner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at SenSys 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Security Risk Taxonomy for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Derner, Kristina Batistič, Jan Zahálka, Robert Babuška
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) permeate more and more applications, an
assessment of their associated security risks becomes increasingly necessary.
The potential for exploitation by malicious actors, ranging from disinformation
to data breaches and reputation damage, is substantial. This paper addresses a
gap in current research by focusing on the security risks posed by LLMs, which
extends beyond the widely covered ethical and societal implications. Our work
proposes a taxonomy of security risks along the user-model communication
pipeline, explicitly focusing on prompt-based attacks on LLMs. We categorize
the attacks by target and attack type within a prompt-based interaction scheme.
The taxonomy is reinforced with specific attack examples to showcase the
real-world impact of these risks. Through this taxonomy, we aim to inform the
development of robust and secure LLM applications, enhancing their safety and
trustworthiness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large <span class="highlight-title">Pre-train</span>ed time series models for cross-domain Time series
  analysis tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshavardhan Kamarthi, B. Aditya Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pre-trained models have been instrumental in significant advancements
in domains like language and vision making model training for individual
downstream tasks more efficient as well as provide superior performance.
However, tackling time-series analysis tasks usually involves designing and
training a separate model from scratch leveraging training data and domain
expertise specific to the task. We tackle a significant challenge for
pre-training a general time-series model from multiple heterogeneous
time-series dataset: providing semantically useful inputs to models for
modeling time series of different dynamics from different domains. We observe
that partitioning time-series into segments as inputs to sequential models
produces semantically better inputs and propose a novel model LPTM that
automatically identifies optimal dataset-specific segmentation strategy
leveraging self-supervised learning loss during pre-training. LPTM provides
performance similar to or better than domain-specific state-of-art model and is
significantly more data and compute efficient taking up to 40% less data as
well as 50% less training time to achieve state-of-art performance in a wide
range of time-series analysis tasks from multiple disparate domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 Figures, 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negotiated Representations for Machine Mearning Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuri Korhan, Samet Bayram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overfitting is a phenomenon that occurs when a machine learning model is
trained for too long and focused too much on the exact fitness of the training
samples to the provided training labels and cannot keep track of the predictive
rules that would be useful on the test data. This phenomenon is commonly
attributed to memorization of particular samples, memorization of the noise,
and forced fitness into a data set of limited samples by using a high number of
neurons. While it is true that the model encodes various peculiarities as the
training process continues, we argue that most of the overfitting occurs in the
process of reconciling sharply defined membership ratios. In this study, we
present an approach that increases the classification accuracy of machine
learning models by allowing the model to negotiate output representations of
the samples with previously determined class labels. By setting up a
negotiation between the models interpretation of the inputs and the provided
labels, we not only increased average classification accuracy but also
decreased the rate of overfitting without applying any other regularization
tricks. By implementing our negotiation paradigm approach to several low regime
machine learning problems by generating overfitting scenarios from publicly
available data sets such as CIFAR 10, CIFAR 100, and MNIST we have demonstrated
that the proposed paradigm has more capacity than its intended purpose. We are
sharing the experimental results and inviting the machine learning community to
explore the limits of the proposed paradigm. We also aim to incentive the
community to exploit the negotiation paradigm to overcome the learning related
challenges in other research fields such as continual learning. The Python code
of the experimental setup is uploaded to GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards interpretable-by-design deep learning algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Plamen Angelov, Dmitry Kangin, Ziyang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proposed framework named IDEAL (Interpretable-by-design DEep learning
ALgorithms) recasts the standard supervised classification problem into a
function of similarity to a set of prototypes derived from the training data,
while taking advantage of existing latent spaces of large neural networks
forming so-called Foundation Models (FM). This addresses the issue of
explainability (stage B) while retaining the benefits from the tremendous
achievements offered by DL models (e.g., visual transformers, ViT) pre-trained
on huge data sets such as IG-3.6B + ImageNet-1K or LVD-142M (stage A). We show
that one can turn such DL models into conceptually simpler,
explainable-through-prototypes ones.
  The key findings can be summarized as follows: (1) the proposed models are
interpretable through prototypes, mitigating the issue of confounded
interpretations, (2) the proposed IDEAL framework circumvents the issue of
catastrophic forgetting allowing efficient class-incremental learning, and (3)
the proposed IDEAL approach demonstrates that ViT architectures narrow the gap
between finetuned and non-finetuned models allowing for transfer learning in a
fraction of time \textbf{without} finetuning of the feature space on a target
dataset with iterative supervised methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing the speed-accuracy simulation trade-off for adaptive spiking
  neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Taylor, Andrew J King, Nicol S Harper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adaptive leaky integrate-and-fire (ALIF) model is fundamental within
computational neuroscience and has been instrumental in studying our brains
$\textit{in silico}$. Due to the sequential nature of simulating these neural
models, a commonly faced issue is the speed-accuracy trade-off: either
accurately simulate a neuron using a small discretisation time-step (DT), which
is slow, or more quickly simulate a neuron using a larger DT and incur a loss
in simulation accuracy. Here we provide a solution to this dilemma, by
algorithmically reinterpreting the ALIF model, reducing the sequential
simulation complexity and permitting a more efficient parallelisation on GPUs.
We computationally validate our implementation to obtain over a $50\times$
training speedup using small DTs on synthetic benchmarks. We also obtained a
comparable performance to the standard ALIF implementation on different
supervised classification tasks - yet in a fraction of the training time.
Lastly, we showcase how our model makes it possible to quickly and accurately
fit real electrophysiological recordings of cortical neurons, where very fine
sub-millisecond DTs are crucial for capturing exact spike timing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Hendawy, Jan Peters, Carlo D'Eramo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Task Reinforcement Learning (MTRL) tackles the long-standing problem of
endowing agents with skills that generalize across a variety of problems. To
this end, sharing representations plays a fundamental role in capturing both
unique and common characteristics of the tasks. Tasks may exhibit similarities
in terms of skills, objects, or physical properties while leveraging their
representations eases the achievement of a universal policy. Nevertheless, the
pursuit of learning a shared set of diverse representations is still an open
challenge. In this paper, we introduce a novel approach for representation
learning in MTRL that encapsulates common structures among the tasks using
orthogonal representations to promote diversity. Our method, named Mixture Of
Orthogonal Experts (MOORE), leverages a Gram-Schmidt process to shape a shared
subspace of representations generated by a mixture of experts. When
task-specific information is provided, MOORE generates relevant representations
from this shared subspace. We assess the effectiveness of our approach on two
MTRL benchmarks, namely MiniGrid and MetaWorld, showing that MOORE surpasses
related baselines and establishes a new state-of-the-art result on MetaWorld.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Locally Private Nonparametric Classification with Public Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Ma, Hanfang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate the problem of public data-assisted
non-interactive LDP (Local Differential Privacy) learning with a focus on
non-parametric classification. Under the posterior drift assumption, we for the
first time derive the mini-max optimal convergence rate with LDP constraint.
Then, we present a novel approach, the locally private classification tree,
which attains the mini-max optimal convergence rate. Furthermore, we design a
data-driven pruning procedure that avoids parameter tuning and produces a fast
converging estimator. Comprehensive experiments conducted on synthetic and real
datasets show the superior performance of our proposed method. Both our
theoretical and experimental findings demonstrate the effectiveness of public
data compared to private data, which leads to practical suggestions for
prioritizing non-private data collection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pretrain</span>ing for Heterogeneous Hypergraph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdalgader Abubaker, Takanori Maehara, Madhav Nimishakavi, Vassilis Plachouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, pretraining methods for the Graph Neural Networks (GNNs) have been
successful at learning effective representations from unlabeled graph data.
However, most of these methods rely on pairwise relations in the graph and do
not capture the underling higher-order relations between entities. Hypergraphs
are versatile and expressive structures that can effectively model higher-order
relationships among entities in the data. Despite the efforts to adapt GNNs to
hypergraphs (HyperGNN), there are currently no fully self-supervised
pretraining methods for HyperGNN on heterogeneous hypergraphs. In this paper,
we present SPHH, a novel self-supervised pretraining framework for
heterogeneous HyperGNNs. Our method is able to effectively capture higher-order
relations among entities in the data in a self-supervised manner. SPHH is
consist of two self-supervised pretraining tasks that aim to simultaneously
learn both local and global representations of the entities in the hypergraph
by using informative representations derived from the hypergraph structure.
Overall, our work presents a significant advancement in the field of
self-supervised pretraining of HyperGNNs, and has the potential to improve the
performance of various graph-based downstream tasks such as node classification
and link prediction tasks which are mapped to hypergraph configuration. Our
experiments on two real-world benchmarks using four different HyperGNN models
show that our proposed SPHH framework consistently outperforms state-of-the-art
baselines in various downstream tasks. The results demonstrate that SPHH is
able to improve the performance of various HyperGNN models in various
downstream tasks, regardless of their architecture or complexity, which
highlights the robustness of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidential Uncertainty Quantification: A Variance-Based Perspective <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruxiao Duan, Brian Caffo, Harrison X. Bai, Haris I. Sair, Craig Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification of deep neural networks has become an active field
of research and plays a crucial role in various downstream tasks such as active
learning. Recent advances in evidential deep learning shed light on the direct
quantification of aleatoric and epistemic uncertainties with a single forward
pass of the model. Most traditional approaches adopt an entropy-based method to
derive evidential uncertainty in classification, quantifying uncertainty at the
sample level. However, the variance-based method that has been widely applied
in regression problems is seldom used in the classification setting. In this
work, we adapt the variance-based approach from regression to classification,
quantifying classification uncertainty at the class level. The variance
decomposition technique in regression is extended to class covariance
decomposition in classification based on the law of total covariance, and the
class correlation is also derived from the covariance. Experiments on
cross-domain datasets are conducted to illustrate that the variance-based
approach not only results in similar accuracy as the entropy-based one in
active domain adaptation but also brings information about class-wise
uncertainties as well as between-class correlations. The code is available at
https://github.com/KerryDRX/EvidentialADA. This alternative means of evidential
uncertainty quantification will give researchers more options when class
uncertainties and correlations are important in their applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symmetry-invariant quantum machine learning force fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabel Nha Minh Le, Oriel Kiss, Julian Schuhmacher, Ivano Tavernelli, Francesco Tacchino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning techniques are essential tools to compute efficient, yet
accurate, force fields for atomistic simulations. This approach has recently
been extended to incorporate quantum computational methods, making use of
variational quantum learning models to predict potential energy surfaces and
atomic forces from ab initio training data. However, the trainability and
scalability of such models are still limited, due to both theoretical and
practical barriers. Inspired by recent developments in geometric classical and
quantum machine learning, here we design quantum neural networks that
explicitly incorporate, as a data-inspired prior, an extensive set of
physically relevant symmetries. We find that our invariant quantum learning
models outperform their more generic counterparts on individual molecules of
growing complexity. Furthermore, we study a water dimer as a minimal example of
a system with multiple components, showcasing the versatility of our proposed
approach and opening the way towards larger simulations. Our results suggest
that molecular force fields generation can significantly profit from leveraging
the framework of geometric quantum machine learning, and that chemical systems
represent, in fact, an interesting and rich playground for the development and
application of advanced quantum machine learning tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsically motivated graph exploration using network theories of
  human curiosity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04962v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04962v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhankar P. Patankar, Mathieu Ouellet, Juan Cervino, Alejandro Ribeiro, Kieran A. Murphy, Dani S. Bassett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intrinsically motivated exploration has proven useful for reinforcement
learning, even without additional extrinsic rewards. When the environment is
naturally represented as a graph, how to guide exploration best remains an open
question. In this work, we propose a novel approach for exploring
graph-structured data motivated by two theories of human curiosity: the
information gap theory and the compression progress theory. The theories view
curiosity as an intrinsic motivation to optimize for topological features of
subgraphs induced by nodes visited in the environment. We use these proposed
features as rewards for graph neural-network-based reinforcement learning. On
multiple classes of synthetically generated graphs, we find that trained agents
generalize to longer exploratory walks and larger environments than are seen
during training. Our method computes more efficiently than the greedy
evaluation of the relevant topological properties. The proposed intrinsic
motivations bear particular relevance for recommender systems. We demonstrate
that next-node recommendations considering curiosity are more predictive of
human choices than PageRank centrality in several real-world graph
environments, including MovieLens, Amazon Books, and Wikipedia.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures in main text, and 18 pages, 9 figures in
  supplement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reflection-Equivariant Diffusion for 3D Structure Determination from
  Isotopologue Rotational Spectra in Natural Abundance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11609v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11609v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin Cheng, Alston Lo, Santiago Miret, Brooks Pate, Alán Aspuru-Guzik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structure determination is necessary to identify unknown organic molecules,
such as those in natural products, forensic samples, the interstellar medium,
and laboratory syntheses. Rotational spectroscopy enables structure
determination by providing accurate 3D information about small organic
molecules via their moments of inertia. Using these moments, Kraitchman
analysis determines isotopic substitution coordinates, which are the unsigned
$|x|,|y|,|z|$ coordinates of all atoms with natural isotopic abundance,
including carbon, nitrogen, and oxygen. While unsigned substitution coordinates
can verify guesses of structures, the missing $+/-$ signs make it challenging
to determine the actual structure from the substitution coordinates alone. To
tackle this inverse problem, we develop KREED (Kraitchman
REflection-Equivariant Diffusion), a generative diffusion model that infers a
molecule's complete 3D structure from its molecular formula, moments of
inertia, and unsigned substitution coordinates of heavy atoms. KREED's top-1
predictions identify the correct 3D structure with >98% accuracy on the QM9 and
GEOM datasets when provided with substitution coordinates of all heavy atoms
with natural isotopic abundance. When substitution coordinates are restricted
to only a subset of carbons, accuracy is retained at 91% on QM9 and 32% on
GEOM. On a test set of experimentally measured substitution coordinates
gathered from the literature, KREED predicts the correct all-atom 3D structure
in 25 of 33 cases, demonstrating experimental applicability for context-free 3D
structure determination with rotational spectroscopy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>added software citations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symmetry-preserving graph attention network to solve routing problems at
  multiple resolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Dao Tran, Thong Bach, Truong Son Hy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Travelling Salesperson Problems (TSPs) and Vehicle Routing Problems (VRPs)
have achieved reasonable improvement in accuracy and computation time with the
adaptation of Machine Learning (ML) methods. However, none of the previous
works completely respects the symmetries arising from TSPs and VRPs including
rotation, translation, permutation, and scaling. In this work, we introduce the
first-ever completely equivariant model and training to solve combinatorial
problems. Furthermore, it is essential to capture the multiscale structure
(i.e. from local to global information) of the input graph, especially for the
cases of large and long-range graphs, while previous methods are limited to
extracting only local information that can lead to a local or sub-optimal
solution. To tackle the above limitation, we propose a Multiresolution scheme
in combination with Equivariant Graph Attention network (mEGAT) architecture,
which can learn the optimal route based on low-level and high-level graph
resolutions in an efficient way. In particular, our approach constructs a
hierarchy of coarse-graining graphs from the input graph, in which we try to
solve the routing problems on simple low-level graphs first, then utilize that
knowledge for the more complex high-level graphs. Experimentally, we have shown
that our model outperforms existing baselines and proved that symmetry
preservation and multiresolution are important recipes for solving
combinatorial problems in a data-driven manner. Our source code is publicly
available at https://github.com/HySonLab/Multires-NP-hard
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Learning with Set-Valued Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06247v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06247v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinod Raman, Unique Subedi, Ambuj Tewari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a variant of online multiclass classification where the learner
predicts a single label but receives a \textit{set of labels} as feedback. In
this model, the learner is penalized for not outputting a label contained in
the revealed set. We show that unlike online multiclass learning with
single-label feedback, deterministic and randomized online learnability are
\textit{not equivalent} in the realizable setting under set-valued feedback. In
addition, we show that deterministic and randomized realizable learnability are
equivalent if the Helly number of the collection of sets that can be revealed
as feedback is finite. In light of this separation, we give two new
combinatorial dimensions, named the Set Littlestone and Measure Shattering
dimension, whose finiteness characterizes deterministic and randomized
realizable learnability respectively. Additionally, these dimensions lower- and
upper bound the deterministic and randomized minimax regret in the realizable
setting. Going beyond the realizable setting, we prove that the Measure
shattering dimension continues to characterize learnability and quantify
minimax regret in the agnostic setting. Finally, we use our results to
establish bounds on the minimax regret for three practical learning settings:
online multilabel ranking, online multilabel classification, and real-valued
prediction with interval-valued response.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Likelihood-Free Frequentist Inference: Bridging Classical Statistics and
  Machine Learning for Reliable Simulator-Based Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.03920v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.03920v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niccolò Dalmasso, Luca Masserano, David Zhao, Rafael Izbicki, Ann B. Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many areas of science make extensive use of computer simulators that
implicitly encode intractable likelihood functions of complex systems.
Classical statistical methods are poorly suited for these so-called
likelihood-free inference (LFI) settings, especially outside asymptotic and
low-dimensional regimes. At the same time, traditional LFI methods - such as
Approximate Bayesian Computation or more recent machine learning techniques -
do not guarantee confidence sets with nominal coverage in general settings
(i.e., with high-dimensional data, finite sample sizes, and for any parameter
value). In addition, there are no diagnostic tools to check the empirical
coverage of confidence sets provided by such methods across the entire
parameter space. In this work, we propose a unified and modular inference
framework that bridges classical statistics and modern machine learning
providing (i) a practical approach to the Neyman construction of confidence
sets with frequentist finite-sample coverage for any value of the unknown
parameters; and (ii) interpretable diagnostics that estimate the empirical
coverage across the entire parameter space. We refer to the general framework
as likelihood-free frequentist inference (LF2I). Any method that defines a test
statistic can leverage LF2I to create valid confidence sets and diagnostics
without costly Monte Carlo samples at fixed parameter settings. We study the
power of two likelihood-based test statistics (ACORE and BFF) and demonstrate
their empirical performance on high-dimensional, complex data. Code is
available at https://github.com/lee-group-cmu/lf2i.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 6 figures, code available at
  https://github.com/lee-group-cmu/lf2i, supplementary material available at
  https://lucamasserano.github.io/data/LF2I_supplementary_material.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEMS: <span class="highlight-title">Pre-train</span>ed Epidemic Time-series Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshavardhan Kamarthi, B. Aditya Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing accurate and reliable predictions about the future of an epidemic
is an important problem for enabling informed public health decisions. Recent
works have shown that leveraging data-driven solutions that utilize advances in
deep learning methods to learn from past data of an epidemic often outperform
traditional mechanistic models. However, in many cases, the past data is sparse
and may not sufficiently capture the underlying dynamics. While there exists a
large amount of data from past epidemics, leveraging prior knowledge from
time-series data of other diseases is a non-trivial challenge. Motivated by the
success of pre-trained models in language and vision tasks, we tackle the
problem of pre-training epidemic time-series models to learn from multiple
datasets from different diseases and epidemics. We introduce Pre-trained
Epidemic Time-Series Models (PEMS) that learn from diverse time-series datasets
of a variety of diseases by formulating pre-training as a set of
self-supervised learning (SSL) tasks. We tackle various important challenges
specific to pre-training for epidemic time-series such as dealing with
heterogeneous dynamics and efficiently capturing useful patterns from multiple
epidemic datasets by carefully designing the SSL tasks to learn important
priors about the epidemic dynamics that can be leveraged for fine-tuning to
multiple downstream tasks. The resultant PEM outperforms previous
state-of-the-art methods in various downstream time-series tasks across
datasets of varying seasonal patterns, geography, and mechanism of contagion
including the novel Covid-19 pandemic unseen in pre-trained data with better
efficiency using smaller fraction of datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond IID weights: sparse and low-rank deep Neural Networks are also
  Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thiziri Nait-Saada, Alireza Naderi, Jared Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The infinitely wide neural network has been proven a useful and manageable
mathematical model that enables the understanding of many phenomena appearing
in deep learning. One example is the convergence of random deep networks to
Gaussian processes that allows a rigorous analysis of the way the choice of
activation function and network weights impacts the training dynamics. In this
paper, we extend the seminal proof of Matthews et al. (2018) to a larger class
of initial weight distributions (which we call PSEUDO-IID), including the
established cases of IID and orthogonal weights, as well as the emerging
low-rank and structured sparse settings celebrated for their computational
speed-up benefits. We show that fully-connected and convolutional networks
initialized with PSEUDO-IID distributions are all effectively equivalent up to
their variance. Using our results, one can identify the Edge-of-Chaos for a
broader class of neural networks and tune them at criticality in order to
enhance their training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting Black-box Machine Learning Models for High Dimensional
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.13405v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.13405v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Rezaul Karim, Md. Shajalal, Alex Graß, Till Döhmen, Sisay Adugna Chala, Christian Beecks, Stefan Decker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have been shown to outperform traditional machine
learning algorithms in a broad variety of application domains due to their
effectiveness in modeling complex problems and handling high-dimensional
datasets. Many real-life datasets, however, are of increasingly high
dimensionality, where a large number of features may be irrelevant for both
supervised and unsupervised learning tasks. The inclusion of such features
would not only introduce unwanted noise but also increase computational
complexity. Furthermore, due to high non-linearity and dependency among a large
number of features, DNN models tend to be unavoidably opaque and perceived as
black-box methods because of their not well-understood internal functioning.
Their algorithmic complexity is often simply beyond the capacities of humans to
understand the interplay among myriads of hyperparameters. A well-interpretable
model can identify statistically significant features and explain the way they
affect the model's outcome. In this paper, we propose an efficient method to
improve the interpretability of black-box models for classification tasks in
the case of high-dimensional datasets. First, we train a black-box model on a
high-dimensional dataset to learn the embeddings on which the classification is
performed. To decompose the inner working principles of the black-box model and
to identify top-k important features, we employ different probing and
perturbing techniques. We then approximate the behavior of the black-box model
by means of an interpretable surrogate model on the top-k feature space.
Finally, we derive decision rules and local explanations from the surrogate
model to explain individual decisions. Our approach outperforms
state-of-the-art methods like TabNet and XGboost when tested on different
datasets with varying dimensionality between 50 and 20,000 w.r.t metrics and
explainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is currently under review in a journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finite element inspired networks: Learning interpretable deformable
  object dynamics from partial observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shamil Mamedov, A. René Geist, Jan Swevers, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate simulation of deformable linear object (DLO) dynamics is challenging
if the task at hand requires a human-interpretable model that also yields fast
predictions. To arrive at such a model, we draw inspiration from the rigid
finite element method (R-FEM) and model a DLO as a serial chain of rigid bodies
whose internal state is unrolled through time by a dynamics network. As this
state is not observed directly, the dynamics network is trained jointly with a
physics-informed encoder which maps observed motion variables to the DLO's
hidden state. To encourage that the state acquires a physically meaningful
representation, we leverage the forward kinematics of the underlying R-FEM
model as a decoder. Through robot experiments we demonstrate that the proposed
architecture provides an easy-to-handle, yet capable DLO dynamics model
yielding physically interpretable predictions from partial observations.
  The project code is available at: \url{https://tinyurl.com/fei-networks}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Many learning agents interacting with an agent-based market model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07393v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07393v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Dicks, Andrew Paskaramoorthy, Tim Gebbie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the dynamics and the interactions of multiple reinforcement
learning optimal execution trading agents interacting with a reactive
Agent-Based Model (ABM) of a financial market in event time. The model
represents a market ecology with 3-trophic levels represented by: optimal
execution learning agents, minimally intelligent liquidity takers, and fast
electronic liquidity providers. The optimal execution agent classes include
buying and selling agents that can either use a combination of limit orders and
market orders, or only trade using market orders. The reward function
explicitly balances trade execution slippage against the penalty of not
executing the order timeously. This work demonstrates how multiple competing
learning agents impact a minimally intelligent market simulation as functions
of the number of agents, the size of agents' initial orders, and the state
spaces used for learning. We use phase space plots to examine the dynamics of
the ABM, when various specifications of learning agents are included. Further,
we examine whether the inclusion of optimal execution agents that can learn is
able to produce dynamics with the same complexity as empirical data. We find
that the inclusion of optimal execution agents changes the stylised facts
produced by ABM to conform more with empirical data, and are a necessary
inclusion for ABMs investigating market micro-structure. However, including
execution agents to chartist-fundamentalist-noise ABMs is insufficient to
recover the complexity observed in empirical data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval
  Score Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, Yingcong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in text-to-3D generation mark a significant milestone
in generative models, unlocking new possibilities for creating imaginative 3D
assets across various real-world scenarios. While recent advancements in
text-to-3D generation have shown promise, they often fall short in rendering
detailed and high-quality 3D models. This problem is especially prevalent as
many methods base themselves on Score Distillation Sampling (SDS). This paper
identifies a notable deficiency in SDS, that it brings inconsistent and
low-quality updating direction for the 3D model, causing the over-smoothing
effect. To address this, we propose a novel approach called Interval Score
Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes
interval-based score matching to counteract over-smoothing. Furthermore, we
incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline.
Extensive experiments show that our model largely outperforms the
state-of-the-art in quality and training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. Our code will
  be available at: https://github.com/EnVision-Research/LucidDreamer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OperARtistry: An AR-based Interactive Application to Assist the Learning
  of Chinese Traditional Opera (Xiqu) Makeup 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Xiong, Shihan Fu, Mingming Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese Traditional Opera (Xiqu) is an important type of intangible cultural
heritage and one key characteristic of Xiqu is its visual effects on face
achieved via makeup. However, Xiqu makeup process, especially the eye-area
makeup process, is complex and time-consuming, which poses a learning challenge
for potential younger inheritors. We introduce OperARtistry, an interactive
application based on Augmented Reality (AR) that offers in-situ Xiqu makeup
guidance for beginners. Our application provides a step-by-step guide for Xiqu
eye-area makeup, incorporating AR effects at each stage. Furthermore, we
conducted an initial user study (n=6) to compare our approach with existing
video-based tutorials to assess the effectiveness and usefulness of our
approach. Our findings show that OperARtisty helped participants achieve
high-quality eye-area makeup effects with less learning time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, In Proceedings of The Eleventh International
  Symposium of Chinese CHI (Chinese CHI 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Real-World Writing Assistance: A Chinese Character Checking
  Benchmark with Faked and Misspelled Characters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Zishan Xu, Shaoshen Chen, Haojing Huang, Yangning Li, Yong Jiang, Zhongli Li, Qingyu Zhou, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing assistance is an application closely related to human life and is
also a fundamental Natural Language Processing (NLP) research field. Its aim is
to improve the correctness and quality of input texts, with character checking
being crucial in detecting and correcting wrong characters. From the
perspective of the real world where handwriting occupies the vast majority,
characters that humans get wrong include faked characters (i.e., untrue
characters created due to writing errors) and misspelled characters (i.e., true
characters used incorrectly due to spelling errors). However, existing datasets
and related studies only focus on misspelled characters mainly caused by
phonological or visual confusion, thereby ignoring faked characters which are
more common and difficult. To break through this dilemma, we present
Visual-C$^3$, a human-annotated Visual Chinese Character Checking dataset with
faked and misspelled Chinese characters. To the best of our knowledge,
Visual-C$^3$ is the first real-world visual and the largest human-crafted
dataset for the Chinese character checking scenario. Additionally, we also
propose and evaluate novel baseline methods on Visual-C$^3$. Extensive
empirical results and analyses show that Visual-C$^3$ is high-quality yet
challenging. The Visual-C$^3$ dataset and the baseline methods will be publicly
available to facilitate further research in the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M$^{2}$UGen: Multi-modal Music Understanding and Generation with the
  Power of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atin Sakkeer Hussain, Shansong Liu, Chenshuo Sun, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current landscape of research leveraging large language models (LLMs) is
experiencing a surge. Many works harness the powerful reasoning capabilities of
these models to comprehend various modalities, such as text, speech, images,
videos, etc. They also utilize LLMs to understand human intention and generate
desired outputs like images, videos, and music. However, research that combines
both understanding and generation using LLMs is still limited and in its
nascent stage. To address this gap, we introduce a Multi-modal Music
Understanding and Generation (M$^{2}$UGen) framework that integrates LLM's
abilities to comprehend and generate music for different modalities. The
M$^{2}$UGen framework is purpose-built to unlock creative potential from
diverse sources of inspiration, encompassing music, image, and video through
the use of pretrained MERT, ViT, and ViViT models, respectively. To enable
music generation, we explore the use of AudioLDM 2 and MusicGen. Bridging
multi-modal understanding and music generation is accomplished through the
integration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA
model to generate extensive datasets that support text/image/video-to-music
generation, facilitating the training of our M$^{2}$UGen framework. We conduct
a thorough evaluation of our proposed framework. The experimental results
demonstrate that our model achieves or surpasses the performance of the current
state-of-the-art models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mulsemedia Communication Research Challenges for Metaverse in 6G
  Wireless Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16359v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16359v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian F. Akyildiz, Hongzhi Guo, Rui Dai, Wolfgang Gerstacker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although humans have five basic senses, sight, hearing, touch, smell, and
taste, most multimedia systems in current systems only capture two of them,
namely, sight and hearing. With the development of the metaverse and related
technologies, there is a growing need for a more immersive media format that
leverages all human senses. Multisensory media(Mulsemedia) that can stimulate
multiple senses will play a critical role in the near future. This paper
provides an overview of the history, background, use cases, existing research,
devices, and standards of mulsemedia. Emerging mulsemedia technologies such as
Extended Reality (XR) and Holographic-Type Communication (HTC) are introduced.
Additionally, the challenges in mulsemedia research from the perspective of
wireless communication and networking are discussed. The potential of 6G
wireless systems to address these challenges is highlighted, and several
research directions that can advance mulsemedia communications are identified.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Multi-view Pedestrian Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyin Liu, Chao Zhu, Shiqi Ren, Xu-Cheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the prosperity of the video surveillance, multiple cameras have been
applied to accurately locate pedestrians in a specific area. However, previous
methods rely on the human-labeled annotations in every video frame and camera
view, leading to heavier burden than necessary camera calibration and
synchronization. Therefore, we propose in this paper an Unsupervised Multi-view
Pedestrian Detection approach (UMPD) to eliminate the need of annotations to
learn a multi-view pedestrian detector via 2D-3D mapping. 1) Firstly,
Semantic-aware Iterative Segmentation (SIS) is proposed to extract unsupervised
representations of multi-view images, which are converted into 2D pedestrian
masks as pseudo labels, via our proposed iterative PCA and zero-shot semantic
classes from vision-language models. 2) Secondly, we propose Geometry-aware
Volume-based Detector (GVD) to end-to-end encode multi-view 2D images into a 3D
volume to predict voxel-wise density and color via 2D-to-3D geometric
projection, trained by 3D-to-2D rendering losses with SIS pseudo labels. 3)
Thirdly, for better detection results, i.e., the 3D density projected on
Birds-Eye-View from GVD, we propose Vertical-aware BEV Regularization (VBR) to
constraint them to be vertical like the natural pedestrian poses. Extensive
experiments on popular multi-view pedestrian detection benchmarks Wildtrack,
Terrace, and MultiviewX, show that our proposed UMPD approach, as the first
fully-unsupervised method to our best knowledge, performs competitively to the
previous state-of-the-art supervised techniques. Code will be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMSum: A <span class="highlight-title">Dataset</span> for Multimodal Summarization and Thumbnail Generation
  of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jielin Qiu, Jiacheng Zhu, William Han, Aditesh Kumar, Karthik Mittal, Claire Jin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Ding Zhao, Bo Li, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal summarization with multimodal output (MSMO) has emerged as a
promising research direction. Nonetheless, numerous limitations exist within
existing public MSMO datasets, including insufficient maintenance, data
inaccessibility, limited size, and the absence of proper categorization, which
pose significant challenges. To address these challenges and provide a
comprehensive dataset for this new direction, we have meticulously curated the
\textbf{MMSum} dataset. Our new dataset features (1) Human-validated summaries
for both video and textual content, providing superior human instruction and
labels for multimodal learning. (2) Comprehensively and meticulously arranged
categorization, spanning 17 principal categories and 170 subcategories to
encapsulate a diverse array of real-world scenarios. (3) Benchmark tests
performed on the proposed dataset to assess various tasks and methods,
including \textit{video summarization}, \textit{text summarization}, and
\textit{multimodal summarization}. To champion accessibility and collaboration,
we will release the \textbf{MMSum} dataset and the data collection tool as
fully open-source resources, fostering transparency and accelerating future
developments. Our project website can be found
at~\url{https://mmsum-dataset.github.io/}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://mmsum-dataset.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-11-18T00:00:00Z">2023-11-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">35</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experts-in-the-Loop: Establishing an Effective Workflow in Crafting
  Privacy Q&A 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Kolagar, Anna Katharina Leschanowsky, Birgit Popp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privacy policies play a vital role in safeguarding user privacy as legal
jurisdictions worldwide emphasize the need for transparent data processing.
While the suitability of privacy policies to enhance transparency has been
critically discussed, employing conversational AI systems presents unique
challenges in informing users effectively. In this position paper, we propose a
dynamic workflow for transforming privacy policies into privacy
question-and-answer (Q&A) pairs to make privacy policies easily accessible
through conversational AI. Thereby, we facilitate interdisciplinary
collaboration among legal experts and conversation designers, while also
considering the utilization of large language models' generative capabilities
and addressing associated challenges. Our proposed workflow underscores
continuous improvement and monitoring throughout the construction of privacy
Q&As, advocating for comprehensive review and refinement through an
experts-in-the-loop approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Position paper presented at CONVERSATIONS 2023 - the 7th
  International Workshop on Chatbot Research and Design, hosted by the
  University of Oslo, Norway, November 22-23, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Inclusiveness of Artificial Intelligence Software in
  Enhancing Project Management Efficiency -- A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasileios Alevizos, Ilias Georgousis, Akebu Simasiku, Sotiria Karypidou, Antonis Messinis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of advanced technology in project management (PM) highlights a
crucial need for inclusiveness. This work examines the enhancement of both
inclusivity and efficiency in PM through technological integration, focusing on
defining and measuring inclusiveness. This approach illuminates how
inclusivity-centered technology can significantly elevate project outcomes. The
research navigates through the challenges of achieving inclusivity, mainly
biases in learning databases and the design process of these technologies,
assessment of transformative potential of these technologies, particularly in
automating tasks like data collection and analysis, thus enabling managers to
prioritize human-centric aspects of projects. However, the integration of such
technology transcends efficiency, indicating a paradigm shift in understanding
their societal roles. This shift necessitates a new approach in the development
of these systems to prevent perpetuating social inequalities. We proposed a
methodology involving criteria development for evaluating the inclusiveness and
effectiveness of these technologies. This methodical approach is vital to
comprehensively address the challenges and limitations inherent in these
systems. Emphasizing the importance of inclusivity, the study advocates for a
balance between technological advancement and ethical considerations, calling
for a holistic understanding and regulation. In conclusion, the paper
underscores that while these technologies can significantly improve outcomes,
their mindful integration, ensuring inclusivity, is paramount. This exploration
into the ethical and practical aspects of technology in PM contributes to a
more informed and balanced approach within the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vashantor: A Large-scale Multilingual Benchmark <span class="highlight-title">Dataset</span> for Automated
  Translation of Bangla Regional Dialects to Bangla Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Bangla linguistic variety is a fascinating mix of regional dialects that
adds to the cultural diversity of the Bangla-speaking community. Despite
extensive study into translating Bangla to English, English to Bangla, and
Banglish to Bangla in the past, there has been a noticeable gap in translating
Bangla regional dialects into standard Bangla. In this study, we set out to
fill this gap by creating a collection of 32,500 sentences, encompassing
Bangla, Banglish, and English, representing five regional Bangla dialects. Our
aim is to translate these regional dialects into standard Bangla and detect
regions accurately. To achieve this, we proposed models known as mT5 and
BanglaT5 for translating regional dialects into standard Bangla. Additionally,
we employed mBERT and Bangla-bert-base to determine the specific regions from
where these dialects originated. Our experimental results showed the highest
BLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score
of 36.75 for Chittagong regional dialects. We also observed the lowest average
word error rate of 0.1548 for Mymensingh regional dialects and the highest of
0.3385 for Chittagong regional dialects. For region detection, we achieved an
accuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first
large-scale investigation of Bangla regional dialects to Bangla machine
translation. We believe our findings will not only pave the way for future work
on Bangla regional dialects to Bangla machine translation, but will also be
useful in solving similar language-related challenges in low-resource language
conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Principled Framework for Knowledge-enhanced Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saizhuo Wang, Zhihan Liu, Zhaoran Wang, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are versatile, yet they often falter in tasks
requiring deep and reliable reasoning due to issues like hallucinations,
limiting their applicability in critical scenarios. This paper introduces a
rigorously designed framework for creating LLMs that effectively anchor
knowledge and employ a closed-loop reasoning process, enhancing their
capability for in-depth analysis. We dissect the framework to illustrate the
contribution of each component to the LLMs' performance, offering a theoretical
assurance of improved reasoning under well-defined assumptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ (Why) Is My <span class="highlight-title">Prompt</span> Getting Worse? Rethinking Regression Testing for
  Evolving LLM APIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanqin Ma, Chenyang Yang, Christian Kästner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly integrated into software
applications. Downstream application developers often access LLMs through APIs
provided as a service. However, LLM APIs are often updated silently and
scheduled to be deprecated, forcing users to continuously adapt to evolving
models. This can cause performance regression and affect prompt design choices,
as evidenced by our case study on toxicity detection. Based on our case study,
we emphasize the need for and re-examine the concept of regression testing for
evolving LLM APIs. We argue that regression testing LLMs requires fundamental
changes to traditional testing approaches, due to different correctness
notions, prompting brittleness, and non-determinism in LLM APIs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Speech Emotion Recognition and Recommender Systems for
  Negative Emotion Handling in Therapy Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farideh Majidi, Marzieh Bahrami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotional well-being significantly influences mental health and overall
quality of life. As therapy chatbots become increasingly prevalent, their
ability to comprehend and respond empathetically to users' emotions remains
limited. This paper addresses this limitation by proposing an approach to
enhance therapy chatbots with auditory perception, enabling them to understand
users' feelings and provide human-like empathy. The proposed method
incorporates speech emotion recognition (SER) techniques using Convolutional
Neural Network (CNN) models and the ShEMO dataset to accurately detect and
classify negative emotions, including anger, fear, and sadness. The SER model
achieves a validation accuracy of 88%, demonstrating its effectiveness in
recognizing emotional states from speech signals. Furthermore, a recommender
system is developed, leveraging the SER model's output to generate personalized
recommendations for managing negative emotions, for which a new bilingual
dataset was generated as well since there is no such dataset available for this
task. The recommender model achieves an accuracy of 98% by employing a
combination of global vectors for word representation (GloVe) and LSTM models.
To provide a more immersive and empathetic user experience, a text-to-speech
model called GlowTTS is integrated, enabling the therapy chatbot to audibly
communicate the generated recommendations to users in both English and Persian.
The proposed approach offers promising potential to enhance therapy chatbots by
providing them with the ability to recognize and respond to users' emotions,
ultimately improving the delivery of mental health support for both English and
Persian-speaking users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the First National Conference of Artificial Intelligence
  and Software Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Responsible AI Considerations in Text Summarization Research: A <span class="highlight-title">Review</span>
  of Current Practices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Lu Liu, Meng Cao, Su Lin Blodgett, Jackie Chi Kit Cheung, Alexandra Olteanu, Adam Trischler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and NLP publication venues have increasingly encouraged researchers to
reflect on possible ethical considerations, adverse impacts, and other
responsible AI issues their work might engender. However, for specific NLP
tasks our understanding of how prevalent such issues are, or when and why these
issues are likely to arise, remains limited. Focusing on text summarization --
a common NLP task largely overlooked by the responsible AI community -- we
examine research and reporting practices in the current literature. We conduct
a multi-round qualitative analysis of 333 summarization papers from the ACL
Anthology published between 2020-2022. We focus on how, which, and when
responsible AI issues are covered, which relevant stakeholders are considered,
and mismatches between stated and realized research goals. We also discuss
current evaluation practices and consider how authors discuss the limitations
of both prior work and their own work. Overall, we find that relatively few
papers engage with possible stakeholders or contexts of use, which limits their
consideration of potential downstream adverse impacts or other responsible AI
issues. Based on our findings, we make recommendations on concrete practices
and research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Radiology Report Generation Using <span class="highlight-title">Transformer</span>s Conditioned with
  Non-imaging Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurbanu Aksoy, Nishant Ravikumar, Alejandro F Frangi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image interpretation is central to most clinical applications such as
disease diagnosis, treatment planning, and prognostication. In clinical
practice, radiologists examine medical images and manually compile their
findings into reports, which can be a time-consuming process. Automated
approaches to radiology report generation, therefore, can reduce radiologist
workload and improve efficiency in the clinical pathway. While recent
deep-learning approaches for automated report generation from medical images
have seen some success, most studies have relied on image-derived features
alone, ignoring non-imaging patient data. Although a few studies have included
the word-level contexts along with the image, the use of patient demographics
is still unexplored. This paper proposes a novel multi-modal transformer
network that integrates chest x-ray (CXR) images and associated patient
demographic information, to synthesise patient-specific radiology reports. The
proposed network uses a convolutional neural network to extract visual features
from CXRs and a transformer-based encoder-decoder network that combines the
visual features with semantic text embeddings of patient demographic
information, to synthesise full-text radiology reports. Data from two public
databases were used to train and evaluate the proposed approach. CXRs and
reports were extracted from the MIMIC-CXR database and combined with
corresponding patients' data MIMIC-IV. Based on the evaluation metrics used
including patient demographic information was found to improve the quality of
reports generated using the proposed approach, relative to a baseline network
trained using CXRs alone. The proposed approach shows potential for enhancing
radiology report generation by leveraging rich patient metadata and combining
semantic text embeddings derived thereof, with medical image-derived visual
features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Images: An Integrative Multi-modal Approach to Chest X-Ray Report
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurbanu Aksoy, Serge Sharoff, Selcuk Baser, Nishant Ravikumar, Alejandro F Frangi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-text radiology report generation aims to automatically produce
radiology reports that describe the findings in medical images. Most existing
methods focus solely on the image data, disregarding the other patient
information accessible to radiologists. In this paper, we present a novel
multi-modal deep neural network framework for generating chest X-rays reports
by integrating structured patient data, such as vital signs and symptoms,
alongside unstructured clinical notes.We introduce a conditioned
cross-multi-head attention module to fuse these heterogeneous data modalities,
bridging the semantic gap between visual and textual data. Experiments
demonstrate substantial improvements from using additional modalities compared
to relying on images alone. Notably, our model achieves the highest reported
performance on the ROUGE-L metric compared to relevant state-of-the-art models
in the literature. Furthermore, we employed both human evaluation and clinical
semantic similarity measurement alongside word-overlap metrics to improve the
depth of quantitative analysis. A human evaluation, conducted by a
board-certified radiologist, confirms the model's accuracy in identifying
high-level findings, however, it also highlights that more improvement is
needed to capture nuanced details and clinical context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining EEG and NLP Features for Predicting Students' Lecture
  Comprehension using Ensemble Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phantharach Natnithikarat, Theerawit Wilaiprasitporn, Supavit Kongwudhikunakorn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography (EEG) and Natural Language Processing (NLP) can be
applied for education to measure students' comprehension in classroom lectures;
currently, the two measures have been used separately. In this work, we propose
a classification framework for predicting students' lecture comprehension in
two tasks: (i) students' confusion after listening to the simulated lecture and
(ii) the correctness of students' responses to the post-lecture assessment. The
proposed framework includes EEG and NLP feature extraction, processing, and
classification. EEG and NLP features are extracted to construct integrated
features obtained from recorded EEG signals and sentence-level syntactic
analysis, which provide information about specific biomarkers and sentence
structures. An ensemble stacking classification method -- a combination of
multiple individual models that produces an enhanced predictive model -- is
studied to learn from the features to make predictions accurately. Furthermore,
we also utilized subjective confusion ratings as another integrated feature to
enhance classification performance. By doing so, experiment results show that
this framework performs better than the baselines, which achieved F1 up to 0.65
for predicting confusion and 0.78 for predicting correctness, highlighting that
utilizing this has helped improve the classification performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapters: A Unified Library for Parameter-Efficient and Modular Transfer
  Learning <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clifton Poth, Hannah Sterz, Indraneil Paul, Sukannya Purkayastha, Leon Engländer, Timo Imhof, Ivan Vulić, Sebastian Ruder, Iryna Gurevych, Jonas Pfeiffer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Adapters, an open-source library that unifies
parameter-efficient and modular transfer learning in large language models. By
integrating 10 diverse adapter methods into a unified interface, Adapters
offers ease of use and flexible configuration. Our library allows researchers
and practitioners to leverage adapter modularity through composition blocks,
enabling the design of complex adapter setups. We demonstrate the library's
efficacy by evaluating its performance against full fine-tuning on various NLP
tasks. Adapters provides a powerful tool for addressing the challenges of
conventional fine-tuning paradigms and promoting more efficient and modular
transfer learning. The library is available via https://adapterhub.ml/adapters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023: Systems Demonstrations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bit Cipher -- A Simple yet Powerful Word Representation System that
  Integrates Efficiently with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhao, Jake Ryland Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) become ever more dominant, classic
pre-trained word embeddings sustain their relevance through computational
efficiency and nuanced linguistic interpretation. Drawing from recent studies
demonstrating that the convergence of GloVe and word2vec optimizations all tend
towards log-co-occurrence matrix variants, we construct a novel word
representation system called Bit-cipher that eliminates the need of
backpropagation while leveraging contextual information and hyper-efficient
dimensionality reduction techniques based on unigram frequency, providing
strong interpretability, alongside efficiency. We use the bit-cipher algorithm
to train word vectors via a two-step process that critically relies on a
hyperparameter -- bits -- that controls the vector dimension. While the first
step trains the bit-cipher, the second utilizes it under two different
aggregation modes -- summation or concatenation -- to produce contextually rich
representations from word co-occurrences. We extend our investigation into
bit-cipher's efficacy, performing probing experiments on part-of-speech (POS)
tagging and named entity recognition (NER) to assess its competitiveness with
classic embeddings like word2vec and GloVe. Additionally, we explore its
applicability in LM training and fine-tuning. By replacing embedding layers
with cipher embeddings, our experiments illustrate the notable efficiency of
cipher in accelerating the training process and attaining better optima
compared to conventional training paradigms. Experiments on the integration of
bit-cipher embedding layers with Roberta, T5, and OPT, prior to or as a
substitute for fine-tuning, showcase a promising enhancement to transfer
learning, allowing rapid model convergence while preserving competitive
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joyful: Joint Modality Fusion and Graph Contrastive Learning for
  Multimodal Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyuan Li, Yusong Wang, Kotaro Funakoshi, Manabu Okumura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal emotion recognition aims to recognize emotions for each utterance
of multiple modalities, which has received increasing attention for its
application in human-machine interaction. Current graph-based methods fail to
simultaneously depict global contextual features and local diverse uni-modal
features in a dialogue. Furthermore, with the number of graph layers
increasing, they easily fall into over-smoothing. In this paper, we propose a
method for joint modality fusion and graph contrastive learning for multimodal
emotion recognition (Joyful), where multimodality fusion, contrastive learning,
and emotion recognition are jointly optimized. Specifically, we first design a
new multimodal fusion mechanism that can provide deep interaction and fusion
between the global contextual and uni-modal specific features. Then, we
introduce a graph contrastive learning framework with inter-view and intra-view
contrastive losses to learn more distinguishable representations for samples
with different sentiments. Extensive experiments on three benchmark datasets
indicate that Joyful achieved state-of-the-art (SOTA) performance compared to
all baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gendec: A Machine Learning-based Framework for Gender Detection from
  Japanese Names 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duong Tien Pham, Luan Thanh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Every human has their own name, a fundamental aspect of their identity and
cultural heritage. The name often conveys a wealth of information, including
details about an individual's background, ethnicity, and, especially, their
gender. By detecting gender through the analysis of names, researchers can
unlock valuable insights into linguistic patterns and cultural norms, which can
be applied to practical applications. Hence, this work presents a novel dataset
for Japanese name gender detection comprising 64,139 full names in romaji,
hiragana, and kanji forms, along with their biological genders. Moreover, we
propose Gendec, a framework for gender detection from Japanese names that
leverages diverse approaches, including traditional machine learning techniques
or cutting-edge transfer learning models, to predict the gender associated with
Japanese names accurately. Through a thorough investigation, the proposed
framework is expected to be effective and serve potential applications in
various domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted for presentation at ISDA'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Behavior Optimized Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Khurana, Yaman K Singla, Jayakumar Subramanian, Rajiv Ratn Shah, Changyou Chen, Zhiqiang Xu, Balaji Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The last few years have witnessed great success on image generation, which
has crossed the acceptance thresholds of aesthetics, making it directly
applicable to personal and commercial applications. However, images, especially
in marketing and advertising applications, are often created as a means to an
end as opposed to just aesthetic concerns. The goal can be increasing sales,
getting more clicks, likes, or image sales (in the case of stock businesses).
Therefore, the generated images need to perform well on these key performance
indicators (KPIs), in addition to being aesthetically good. In this paper, we
make the first endeavor to answer the question of "How can one infuse the
knowledge of the end-goal within the image generation process itself to create
not just better-looking images but also "better-performing'' images?''. We
propose BoigLLM, an LLM that understands both image content and user behavior.
BoigLLM knows how an image should look to get a certain required KPI. We show
that BoigLLM outperforms 13x larger models such as GPT-3.5 and GPT-4 in this
task, demonstrating that while these state-of-the-art models can understand
images, they lack information on how these images perform in the real world. To
generate actual pixels of behavior-conditioned images, we train a
diffusion-based model (BoigSD) to align with a proposed BoigLLM-defined reward.
We show the performance of the overall pipeline on two datasets covering two
different behaviors: a stock dataset with the number of forward actions as the
KPI and a dataset containing tweets with the total likes as the KPI, denoted as
BoigBench. To advance research in the direction of utility-driven image
generation and understanding, we release BoigBench, a benchmark dataset
containing 168 million enterprise tweets with their media, brand account names,
time of post, and total likes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Journey of Hallucination-minimized Generative AI Solutions for Financial
  Decision Makers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sohini Roychowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI has significantly reduced the entry barrier to the domain of AI
owing to the ease of use and core capabilities of automation, translation, and
intelligent actions in our day to day lives. Currently, Large language models
(LLMs) that power such chatbots are being utilized primarily for their
automation capabilities for software monitoring, report generation etc. and for
specific personalized question answering capabilities, on a limited scope and
scale. One major limitation of the currently evolving family of LLMs is
'hallucinations', wherein inaccurate responses are reported as factual.
Hallucinations are primarily caused by biased training data, ambiguous prompts
and inaccurate LLM parameters, and they majorly occur while combining
mathematical facts with language-based context. Thus, monitoring and
controlling for hallucinations becomes necessary when designing solutions that
are meant for decision makers. In this work we present the three major stages
in the journey of designing hallucination-minimized LLM-based solutions that
are specialized for the decision makers of the financial domain, namely:
prototyping, scaling and LLM evolution using human feedback. These three stages
and the novel data to answer generation modules presented in this work are
necessary to ensure that the Generative AI chatbots, autonomous reports and
alerts are reliable and high-quality to aid key decision-making processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Bayes Framework for Open-Domain Dialogue Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yang Lee, Kong Aik Lee, Woon-Seng Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To engage human users in meaningful conversation, open-domain dialogue agents
are required to generate diverse and contextually coherent dialogue. Despite
recent advancements, which can be attributed to the usage of pretrained
language models, the generation of diverse and coherent dialogue remains an
open research problem. A popular approach to address this issue involves the
adaptation of variational frameworks. However, while these approaches
successfully improve diversity, they tend to compromise on contextual
coherence. Hence, we propose the Bayesian Open-domain Dialogue with Empirical
Bayes (BODEB) framework, an empirical bayes framework for constructing an
Bayesian open-domain dialogue agent by leveraging pretrained parameters to
inform the prior and posterior parameter distributions. Empirical results show
that BODEB achieves better results in terms of both diversity and coherence
compared to variational frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deception Detection from Linguistic and Physiological Data Streams Using
  Bimodal Convolutional Neural Networks <span class="chip">NAACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Li, Mohamed Abouelenien, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deception detection is gaining increasing interest due to ethical and
security concerns. This paper explores the application of convolutional neural
networks for the purpose of multimodal deception detection. We use a dataset
built by interviewing 104 subjects about two topics, with one truthful and one
falsified response from each subject about each topic. In particular, we make
three main contributions. First, we extract linguistic and physiological
features from this data to train and construct the neural network models.
Second, we propose a fused convolutional neural network model using both
modalities in order to achieve an improved overall performance. Third, we
compare our new approach with earlier methods designed for multimodal deception
detection. We find that our system outperforms regular classification methods;
our results indicate the feasibility of using neural networks for deception
detection even in the presence of limited amounts of data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NAACL HLT 2018</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partially Randomizing <span class="highlight-title">Transformer</span> Weights for Dialogue Response
  Diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yang Lee, Kong Aik Lee, Woon-Seng Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress in generative open-domain dialogue, the issue of low
response diversity persists. Prior works have addressed this issue via either
novel objective functions, alternative learning approaches such as variational
frameworks, or architectural extensions such as the Randomized Link (RL)
Transformer. However, these approaches typically entail either additional
difficulties during training/inference, or a significant increase in model size
and complexity. Hence, we propose the \underline{Pa}rtially
\underline{Ra}ndomized trans\underline{Former} (PaRaFormer), a simple extension
of the transformer which involves freezing the weights of selected layers after
random initialization. Experimental results reveal that the performance of the
PaRaformer is comparable to that of the aforementioned approaches, despite not
entailing any additional training difficulty or increase in model complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing visual classification as a linear combination of words 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shobhit Agarwal, Yevgeniy R. Semenov, William Lotter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability is a longstanding challenge in deep learning, especially in
high-stakes domains like healthcare. Common explainability methods highlight
image regions that drive an AI model's decision. Humans, however, heavily rely
on language to convey explanations of not only "where" but "what".
Additionally, most explainability approaches focus on explaining individual AI
predictions, rather than describing the features used by an AI model in
general. The latter would be especially useful for model and dataset auditing,
and potentially even knowledge generation as AI is increasingly being used in
novel tasks. Here, we present an explainability strategy that uses a
vision-language model to identify language-based descriptors of a visual
classification task. By leveraging a pre-trained joint embedding space between
images and text, our approach estimates a new classification task as a linear
combination of words, resulting in a weight for each word that indicates its
alignment with the vision-based classifier. We assess our approach using two
medical imaging classification tasks, where we find that the resulting
descriptors largely align with clinical knowledge despite a lack of
domain-specific language training. However, our approach also identifies the
potential for 'shortcut connections' in the public datasets used. Towards a
functional measure of explainability, we perform a pilot reader study where we
find that the AI-identified words can enable non-expert humans to perform a
specialized medical task at a non-trivial level. Altogether, our results
emphasize the potential of using multimodal foundational models to deliver
intuitive, language-based explanations of visual tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the Proceedings of the 3rd Machine Learning for
  Health symposium, Proceedings of Machine Learning Research (PMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAMRA: Copilot for AMR Annotation <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jon Z. Cai, Shafiuddin Rehan Ahmed, Julia Bonn, Kristin Wright-Bettner, Martha Palmer, James H. Martin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce CAMRA (Copilot for AMR Annotatations), a
cutting-edge web-based tool designed for constructing Abstract Meaning
Representation (AMR) from natural language text. CAMRA offers a novel approach
to deep lexical semantics annotation such as AMR, treating AMR annotation akin
to coding in programming languages. Leveraging the familiarity of programming
paradigms, CAMRA encompasses all essential features of existing AMR editors,
including example lookup, while going a step further by integrating Propbank
roleset lookup as an autocomplete feature within the tool. Notably, CAMRA
incorporates AMR parser models as coding co-pilots, greatly enhancing the
efficiency and accuracy of AMR annotators. To demonstrate the tool's
capabilities, we provide a live demo accessible at: https://camra.colorado.edu
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 System Demonstration</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Product Classification for Customs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunji Lee, Sihyeon Kim, Sundong Kim, Soyeon Jung, Heeja Kim, Meeyoung Cha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of assigning internationally accepted commodity codes (aka HS codes)
to traded goods is a critical function of customs offices. Like court decisions
made by judges, this task follows the doctrine of precedent and can be
nontrivial even for experienced officers. Together with the Korea Customs
Service (KCS), we propose a first-ever explainable decision supporting model
that suggests the most likely subheadings (i.e., the first six digits) of the
HS code. The model also provides reasoning for its suggestion in the form of a
document that is interpretable by customs officers. We evaluated the model
using 5,000 cases that recently received a classification request. The results
showed that the top-3 suggestions made by our model had an accuracy of 93.9\%
when classifying 925 challenging subheadings. A user study with 32 customs
experts further confirmed that our algorithmic suggestions accompanied by
explainable reasonings, can substantially reduce the time and effort taken by
customs officers for classification reviews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, Accepted to ACM Transactions on Intelligent Systems and
  Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding and Mitigating Classification Errors Through Interpretable
  Token Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael A. Hedderich, Jonas Fischer, Dietrich Klakow, Jilles Vreeken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art NLP methods achieve human-like performance on many tasks,
but make errors nevertheless. Characterizing these errors in easily
interpretable terms gives insight into whether a classifier is prone to making
systematic errors, but also gives a way to act and improve the classifier. We
propose to discover those patterns of tokens that distinguish correct and
erroneous predictions as to obtain global and interpretable descriptions for
arbitrary NLP classifiers. We formulate the problem of finding a succinct and
non-redundant set of such patterns in terms of the Minimum Description Length
principle. Through an extensive set of experiments, we show that our method,
Premise, performs well in practice. Unlike existing solutions, it recovers
ground truth, even on highly imbalanced data over large vocabularies. In VQA
and NER case studies, we confirm that it gives clear and actionable insight
into the systematic errors made by NLP classifiers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended abstract at BlackboxNLP'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ing LLMs using human-like development data corpus <span class="chip">CoNLL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khushi Bhardwaj, Raj Sanjay Shah, Sashank Varma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Large Language Models (LLMs) have shown success in a diverse set
of language inference and understanding tasks. The pre-training stage of LLMs
looks at a large corpus of raw textual data. The BabyLM shared task compares
LLM pre-training to human language acquisition, where the number of tokens seen
by 13-year-old kids is magnitudes smaller than the number of tokens seen by
LLMs. In this work, we pre-train and evaluate LLMs on their ability to learn
contextual word representations using roughly the same number of tokens as seen
by children. We provide a strong set of baselines; with different
architectures, evaluation of changes in performance across epochs, and reported
pre-training metrics for the strict small and strict tracks of the task. We
also try to loosely replicate the RoBERTa baseline given by the task organizers
to observe the training robustness to hyperparameter selection and
replicability. We provide the submission details to the strict and strict-small
tracks in this report.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoNLL and CMCL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Selection for Language Models via Importance Resampling <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sang Michael Xie, Shibani Santurkar, Tengyu Ma, Percy Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting a suitable pretraining dataset is crucial for both general-domain
(e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We
formalize this problem as selecting a subset of a large raw unlabeled dataset
to match a desired target distribution given unlabeled target samples. Due to
the scale and dimensionality of the raw text data, existing methods use simple
heuristics or require human experts to manually curate data. Instead, we extend
the classic importance resampling approach used in low-dimensions for LM data
selection. We propose Data Selection with Importance Resampling (DSIR), an
efficient and scalable framework that estimates importance weights in a reduced
feature space for tractability and selects data with importance resampling
according to these weights. We instantiate the DSIR framework with hashed
n-gram features for efficiency, enabling the selection of 100M documents from
the full Pile dataset in 4.5 hours. To measure whether hashed n-gram features
preserve the aspects of the data that are relevant to the target, we define KL
reduction, a data metric that measures the proximity between the selected
pretraining data and the target on some feature space. Across 8 data selection
methods (including expert selection), KL reduction on hashed n-gram features
highly correlates with average downstream accuracy (r=0.82). When selecting
data for continued pretraining on a specific domain, DSIR performs comparably
to expert curation across 8 target distributions. When pretraining
general-domain models (target is Wikipedia and books), DSIR improves over
random selection and heuristic filtering baselines by 2-2.5% on the GLUE
benchmark. Code is available at https://github.com/p-lambda/dsir.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-context Learning and Gradient Descent Revisited 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07772v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07772v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilad Deutch, Nadav Magar, Tomer Bar Natan, Guy Dar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) has shown impressive results in few-shot learning
tasks, yet its underlying mechanism is still not fully understood. Recent works
suggest that ICL can be thought of as a gradient descent (GD) based
optimization process. While promising, these results mainly focus on simplified
settings of ICL and provide only a preliminary evaluation of the similarities
between the two methods. In this work, we revisit the comparison between ICL
and GD-based finetuning and study what properties of ICL an equivalent process
must follow. We highlight a major difference in the flow of information between
ICL and standard finetuning. Namely, ICL can only rely on information from
lower layers at every point, while finetuning depends on loss gradients from
deeper layers. We refer to this discrepancy as Layer Causality and show that a
layer causal variant of the finetuning process aligns with ICL on par with
vanilla finetuning and is even better in most cases across relevant metrics. To
the best of our knowledge, this is the first work to discuss this discrepancy
explicitly and suggest a solution that tackles this problem with minimal
changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Chat<span class="highlight-title">GPT</span> a game changer for geocoding -- a benchmark for geocoding
  address parsing techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14360v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14360v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengcong Yin, Diya Li, Daniel W. Goldberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable success of GPT models across various tasks, including toponymy
recognition motivates us to assess the performance of the GPT-3 model in the
geocoding address parsing task. To ensure that the evaluation more accurately
mirrors performance in real-world scenarios with diverse user input qualities
and resolve the pressing need for a 'gold standard' evaluation dataset for
geocoding systems, we introduce a benchmark dataset of low-quality address
descriptions synthesized based on human input patterns mining from actual input
logs of a geocoding system in production. This dataset has 21 different input
errors and variations; contains over 239,000 address records that are uniquely
selected from streets across all U.S. 50 states and D.C.; and consists of three
subsets to be used as training, validation, and testing sets. Building on this,
we train and gauge the performance of the GPT-3 model in extracting address
components, contrasting its performance with transformer-based and LSTM-based
models. The evaluation results indicate that Bidirectional LSTM-CRF model has
achieved the best performance over these transformer-based models and GPT-3
model. Transformer-based models demonstrate very comparable results compared to
the Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in
performance, showcases potential in the address parsing task with few-shot
examples, exhibiting room for improvement with additional fine-tuning. We open
source the code and data of this presented benchmark so that researchers can
utilize it for future model development or extend it to evaluate similar tasks,
such as document geocoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Influenza A Viral Host Using PSSM and Word Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.01140v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.01140v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanhua Xu, Dominik Wojtczak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid mutation of the influenza virus threatens public health.
Reassortment among viruses with different hosts can lead to a fatal pandemic.
However, it is difficult to detect the original host of the virus during or
after an outbreak as influenza viruses can circulate between different species.
Therefore, early and rapid detection of the viral host would help reduce the
further spread of the virus. We use various machine learning models with
features derived from the position-specific scoring matrix (PSSM) and features
learned from word embedding and word encoding to infer the origin host of
viruses. The results show that the performance of the PSSM-based model reaches
the MCC around 95%, and the F1 around 96%. The MCC obtained using the model
with word embedding is around 96%, and the F1 is around 97%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at CIBCB 2021. V1: accepted version + minor
  correction to table 1; V2: corrected a minor typo; V3: update the formula of
  error rate; V4: replacing 'nested cv' with 'nested k-fold cv' for better
  clarity</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Medical Prescriptions with Conditional <span class="highlight-title">Transformer</span> <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Warren Del-Pinto, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Access to real-world medication prescriptions is essential for medical
research and healthcare quality improvement. However, access to real medication
prescriptions is often limited due to the sensitive nature of the information
expressed. Additionally, manually labelling these instructions for training and
fine-tuning Natural Language Processing (NLP) models can be tedious and
expensive. We introduce a novel task-specific model architecture,
Label-To-Text-Transformer (\textbf{LT3}), tailored to generate synthetic
medication prescriptions based on provided labels, such as a vocabulary list of
medications and their attributes. LT3 is trained on a set of around 2K lines of
medication prescriptions extracted from the MIMIC-III database, allowing the
model to produce valuable synthetic medication prescriptions. We evaluate LT3's
performance by contrasting it with a state-of-the-art Pre-trained Language
Model (PLM), T5, analysing the quality and diversity of generated texts. We
deploy the generated synthetic data to train the SpacyNER model for the Named
Entity Recognition (NER) task over the n2c2-2018 dataset. The experiments show
that the model trained on synthetic data can achieve a 96-98\% F1 score at
Label Recognition on Drug, Frequency, Route, Strength, and Form. LT3 codes and
data will be shared at
\url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to: Workshop on Synthetic Data Generation with Generative AI
  (SyntheticData4ML Workshop) at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Song Describer <span class="highlight-title">Dataset</span>: a Corpus of Audio Captions for
  Music-and-Language Evaluation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilaria Manco, Benno Weck, SeungHeon Doh, Minz Won, Yixiao Zhang, Dmitry Bodganov, Yusong Wu, Ke Chen, Philip Tovstogan, Emmanouil Benetos, Elio Quinton, György Fazekas, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Song Describer dataset (SDD), a new crowdsourced corpus of
high-quality audio-caption pairs, designed for the evaluation of
music-and-language models. The dataset consists of 1.1k human-written natural
language descriptions of 706 music recordings, all publicly accessible and
released under Creative Common licenses. To showcase the use of our dataset, we
benchmark popular models on three key music-and-language tasks (music
captioning, text-to-music generation and music-language retrieval). Our
experiments highlight the importance of cross-dataset evaluation and offer
insights into how researchers can use SDD to gain a broader understanding of
model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023 Workshop on Machine Learning for Audio</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning
  Framework for Emotion Recognition in Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Shi, Xiao Liang, Yaoyuan Liang, Xinyi Tong, Shao-Lun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion recognition in conversations (ERC) is a rapidly evolving task within
the natural language processing community, which aims to detect the emotions
expressed by speakers during a conversation. Recently, a growing number of ERC
methods have focused on leveraging supervised contrastive learning (SCL) to
enhance the robustness and generalizability of learned features. However,
current SCL-based approaches in ERC are impeded by the constraint of large
batch sizes and the lack of compatibility with most existing ERC models. To
address these challenges, we propose an efficient and model-agnostic SCL
framework named Supervised Sample-Label Contrastive Learning with Soft-HGR
Maximal Correlation (SSLCL), which eliminates the need for a large batch size
and can be seamlessly integrated with existing ERC models without introducing
any model-specific assumptions. Specifically, we introduce a novel perspective
on utilizing label representations by projecting discrete labels into dense
embeddings through a shallow multilayer perceptron, and formulate the training
objective to maximize the similarity between sample features and their
corresponding ground-truth label embeddings, while minimizing the similarity
between sample features and label embeddings of disparate classes. Moreover, we
innovatively adopt the Soft-HGR maximal correlation as a measure of similarity
between sample features and label embeddings, leading to significant
performance improvements over conventional similarity measures. Additionally,
multimodal cues of utterances are effectively leveraged by SSLCL as data
augmentations to boost model performances. Extensive experiments on two ERC
benchmark datasets, IEMOCAP and MELD, demonstrate the compatibility and
superiority of our proposed SSLCL framework compared to existing
state-of-the-art SCL methods. Our code is available at
\url{https://github.com/TaoShi1998/SSLCL}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Robustness of Adaptation Methods on <span class="highlight-title">Pre-train</span>ed
  Vision-Language Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02080v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02080v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Chen, Jindong Gu, Zhen Han, Yunpu Ma, Philip Torr, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various adaptation methods, such as LoRA, prompts, and adapters, have been
proposed to enhance the performance of pre-trained vision-language models in
specific domains. The robustness of these adaptation methods against
distribution shifts have not been studied. In this study, we assess the
robustness of 11 widely-used adaptation methods across 4 vision-language
datasets under multimodal corruptions. Concretely, we introduce 7 benchmark
datasets, including 96 visual and 87 textual corruptions, to investigate the
robustness of different adaptation methods, the impact of available adaptation
examples, and the influence of trainable parameter size during adaptation. Our
analysis reveals that: 1) Adaptation methods are more sensitive to text
corruptions than visual corruptions. 2) Full fine-tuning does not consistently
provide the highest robustness; instead, adapters can achieve better robustness
with comparable clean performance. 3) Contrary to expectations, our findings
indicate that increasing the number of adaptation data and parameters does not
guarantee enhanced robustness; instead it results in even lower robustness. We
hope this study could benefit future research in the development of robust
multimodal adaptation methods. The benchmark, code, and dataset used in this
study can be accessed at https://adarobustness.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023 Datasets and Benchmarks Track; 9 pages, 5
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talk the Walk: Synthetic Data Generation for Conversational Music
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11489v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11489v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan Leszczynski, Shu Zhang, Ravi Ganti, Krisztian Balog, Filip Radlinski, Fernando Pereira, Arun Tejasvi Chaganty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are ubiquitous yet often difficult for users to control,
and adjust if recommendation quality is poor. This has motivated conversational
recommender systems (CRSs), with control provided through natural language
feedback. However, as with most application domains, building robust CRSs
requires training data that reflects system usage$\unicode{x2014}$here
conversations with user utterances paired with items that cover a wide range of
preferences. This has proved challenging to collect scalably using conventional
methods. We address the question of whether it can be generated synthetically,
building on recent advances in natural language. We evaluate in the setting of
item set recommendation, noting the increasing attention to this task motivated
by use cases like music, news, and recipe recommendation. We present
TalkTheWalk, which synthesizes realistic high-quality conversational data by
leveraging domain expertise encoded in widely available curated item
collections, generating a sequence of hypothetical yet plausible item sets,
then using a language model to produce corresponding user utterances. We
generate over one million diverse playlist curation conversations in the music
domain, and show these contain consistent utterances with relevant item sets
nearly matching the quality of an existing but small human-collected dataset
for this task. We demonstrate the utility of the generated synthetic dataset on
a conversational item retrieval task and show that it improves over both
unsupervised baselines and systems trained on a real dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using <span class="highlight-title">GPT</span>-4 to Augment Unbalanced Data for Automatic Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyang Fang, Gyeong-Geon Lee, Xiaoming Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning-based automatic scoring can be challenging if students'
responses are unbalanced across scoring categories, as it introduces
uncertainty in the machine training process. To meet this challenge, we
introduce a novel text data augmentation framework using GPT-4, a generative
large language model, specifically tailored for unbalanced datasets in
automatic scoring. Our experimental dataset comprised student-written responses
to two science items. We crafted prompts for GPT-4 to generate responses
resembling student-written answers, particularly for the minority scoring
classes, to augment the data. We then finetuned DistillBERT for automatic
scoring based on the augmented and original datasets. Model performance was
assessed using accuracy, precision, recall, and F1 score. We incorporate varied
amounts of augmented data to examine scoring performance, and our findings
revealed remarkedly improved model performance. The average maximum increase
observed across two items is: 3.5% for accuracy, 30.6% for precision, 21.1% for
recall, and 24.2% for F1 score. Notably, using just 5% of the augmented data
led to substantial improvements: 2.6%, 29.2%, 15.1%, and 19.6%. Interestingly,
the extent of improvement varied depending on specific datasets. Moreover, we
found that a varying amount of augmented data (5%-40%) was needed to obtain a
stable improvement. We also compare models trained with GPT-4 augmented data
and those trained with additional student-written responses. The findings
indicate that former ones match or even exceed the performance of the latter.
Specifically, there is an average difference of 1.7%, 1.9%, 11.0%, and 7.8% for
four metrics separately. This research underscores the potential and
effectiveness of data augmentation techniques utilizing GPT-4 in addressing
unbalanced datasets within automated assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Generation Capabilities of Large Chinese Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04823v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04823v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Zeng, Jingyuan Xue, Meng Hao, Chen Sun, Bin Ning, Na Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents CG-Eval, the first comprehensive evaluation of the
generation capabilities of large Chinese language models across a wide range of
academic disciplines. The models' performance was assessed based on their
ability to generate accurate and relevant responses to different types of
questions in six disciplines, namely, Science and Engineering, Humanities and
Social Sciences, Mathematical Calculations, Medical Practitioner Qualification
Examination, Judicial Examination, and Certified Public Accountant Examination.
This paper also presents Gscore, a composite index derived from the weighted
sum of multiple metrics to measure the quality of model's generation against a
reference. The test data and test results can be found at
http://cgeval.besteasy.com/.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextualizing Internet Memes Across Social Media Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurav Joshi, Filip Ilievski, Luca Luceri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Internet memes have emerged as a novel format for communication and
expressing ideas on the web. Their fluidity and creative nature are reflected
in their widespread use, often across platforms and occasionally for unethical
or harmful purposes. While computational work has already analyzed their
high-level virality over time and developed specialized classifiers for hate
speech detection, there have been no efforts to date that aim to holistically
track, identify, and map internet memes posted on social media. To bridge this
gap, we investigate whether internet memes across social media platforms can be
contextualized by using a semantic repository of knowledge, namely, a knowledge
graph. We collect thousands of potential internet meme posts from two social
media platforms, namely Reddit and Discord, and perform an
extract-transform-load procedure to create a data lake with candidate meme
posts. By using vision transformer-based similarity, we match these candidates
against the memes cataloged in a recently released knowledge graph of internet
memes, IMKG. We provide evidence that memes published online can be identified
by mapping them to IMKG. We leverage this grounding to study the prevalence of
memes on different platforms, discover popular memes, and select common meme
channels and subreddits. Finally, we illustrate how the grounding can enable
users to get context about memes on social media thanks to their link to the
knowledge graph.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SBTRec- A <span class="highlight-title">Transformer</span> Framework for Personalized Tour Recommendation
  Problem with Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ngai Lam Ho, Roy Ka-Wei Lee, Kwan Hui Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When traveling to an unfamiliar city for holidays, tourists often rely on
guidebooks, travel websites, or recommendation systems to plan their daily
itineraries and explore popular points of interest (POIs). However, these
approaches may lack optimization in terms of time feasibility, localities, and
user preferences. In this paper, we propose the SBTRec algorithm: a BERT-based
Trajectory Recommendation with sentiment analysis, for recommending
personalized sequences of POIs as itineraries. The key contributions of this
work include analyzing users' check-ins and uploaded photos to understand the
relationship between POI visits and distance. We introduce SBTRec, which
encompasses sentiment analysis to improve recommendation accuracy by
understanding users' preferences and satisfaction levels from reviews and
comments about different POIs. Our proposed algorithms are evaluated against
other sequence prediction methods using datasets from 8 cities. The results
demonstrate that SBTRec achieves an average F1 score of 61.45%, outperforming
baseline algorithms.
  The paper further discusses the flexibility of the SBTRec algorithm, its
ability to adapt to different scenarios and cities without modification, and
its potential for extension by incorporating additional information for more
reliable predictions. Overall, SBTRec provides personalized and relevant POI
recommendations, enhancing tourists' overall trip experiences. Future work
includes fine-tuning personalized embeddings for users, with evaluation of
users' comments on POIs,~to further enhance prediction accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RecExplainer: Aligning Large Language Models for Recommendation Model
  Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely used in various online services, with
embedding-based models being particularly popular due to their expressiveness
in representing complex signals. However, these models often lack
interpretability, making them less reliable and transparent for both users and
developers. With the emergence of large language models (LLMs), we find that
their capabilities in language expression, knowledge-aware reasoning, and
instruction following are exceptionally powerful. Based on this, we propose a
new model interpretation approach for recommender systems, by using LLMs as
surrogate models and learn to mimic and comprehend target recommender models.
Specifically, we introduce three alignment methods: behavior alignment,
intention alignment, and hybrid alignment. Behavior alignment operates in the
language space, representing user preferences and item information as text to
learn the recommendation model's behavior; intention alignment works in the
latent space of the recommendation model, using user and item representations
to understand the model's behavior; hybrid alignment combines both language and
latent spaces for alignment training. To demonstrate the effectiveness of our
methods, we conduct evaluation from two perspectives: alignment effect, and
explanation generation ability on three public datasets. Experimental results
indicate that our approach effectively enables LLMs to comprehend the patterns
of recommendation models and generate highly credible recommendation
explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Product Classification for Customs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunji Lee, Sihyeon Kim, Sundong Kim, Soyeon Jung, Heeja Kim, Meeyoung Cha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of assigning internationally accepted commodity codes (aka HS codes)
to traded goods is a critical function of customs offices. Like court decisions
made by judges, this task follows the doctrine of precedent and can be
nontrivial even for experienced officers. Together with the Korea Customs
Service (KCS), we propose a first-ever explainable decision supporting model
that suggests the most likely subheadings (i.e., the first six digits) of the
HS code. The model also provides reasoning for its suggestion in the form of a
document that is interpretable by customs officers. We evaluated the model
using 5,000 cases that recently received a classification request. The results
showed that the top-3 suggestions made by our model had an accuracy of 93.9\%
when classifying 925 challenging subheadings. A user study with 32 customs
experts further confirmed that our algorithmic suggestions accompanied by
explainable reasonings, can substantially reduce the time and effort taken by
customs officers for classification reviews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, Accepted to ACM Transactions on Intelligent Systems and
  Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Pooling Bias in E-commerce Search via False Negative
  Estimation <span class="chip">WWW'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Wang, Xiao Xiao, Ruhan Zhang, Xuan Zhang, Taesik Na, Tejaswi Tenneti, Haixun Wang, Fenglong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and accurate product relevance assessment is critical for user
experiences and business success. Training a proficient relevance assessment
model requires high-quality query-product pairs, often obtained through
negative sampling strategies. Unfortunately, current methods introduce pooling
bias by mistakenly sampling false negatives, diminishing performance and
business impact. To address this, we present Bias-mitigating Hard Negative
Sampling (BHNS), a novel negative sampling strategy tailored to identify and
adjust for false negatives, building upon our original False Negative
Estimation algorithm. Our experiments in the Instacart search setting confirm
BHNS as effective for practical e-commerce use. Furthermore, comparative
analyses on public dataset showcase its domain-agnostic potential for diverse
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to WWW'24 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perspectives on Large Language Models for Relevance Judgment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guglielmo Faggioli, Laura Dietz, Charles Clarke, Gianluca Demartini, Matthias Hagen, Claudia Hauff, Noriko Kando, Evangelos Kanoulas, Martin Potthast, Benno Stein, Henning Wachsmuth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When asked, large language models (LLMs) like ChatGPT claim that they can
assist with relevance judgments but it is not clear whether automated judgments
can reliably be used in evaluations of retrieval systems. In this perspectives
paper, we discuss possible ways for LLMs to support relevance judgments along
with concerns and issues that arise. We devise a human--machine collaboration
spectrum that allows to categorize different relevance judgment strategies,
based on how much humans rely on machines. For the extreme point of "fully
automated judgments", we further include a pilot experiment on whether
LLM-based relevance judgments correlate with judgments from trained human
assessors. We conclude the paper by providing opposing perspectives for and
against the use of~LLMs for automatic relevance judgments, and a compromise
perspective, informed by our analyses of the literature, our preliminary
experimental evidence, and our experience as IR researchers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating LLMs on Document-Based QA: Exact Answer Selection and
  Numerical Extraction using Cogtale <span class="highlight-title">dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07878v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07878v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zafaryab Rasool, Scott Barnett, Stefanus Kurniawan, Sherwin Balugo, Rajesh Vasa, Courtney Chesser, Alex Bahar-Fuchs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-based Question-Answering (QA) tasks are crucial for precise
information retrieval. While some existing work focus on evaluating large
language model's performance on retrieving and answering questions from
documents, assessing the LLMs' performance on QA types that require exact
answer selection from predefined options and numerical extraction is yet to be
fully assessed. In this paper, we specifically focus on this underexplored
context and conduct empirical analysis of LLMs (GPT-4 and GPT 3.5) on question
types, including single-choice, yes-no, multiple-choice, and number extraction
questions from documents. We use the Cogtale dataset for evaluation, which
provide human expert-tagged responses, offering a robust benchmark for
precision and factual grounding. We found that LLMs, particularly GPT-4, can
precisely answer many single-choice and yes-no questions given relevant
context, demonstrating their efficacy in information retrieval tasks. However,
their performance diminishes when confronted with multiple-choice and number
extraction formats, lowering the overall performance of the model on this task,
indicating that these models may not be reliable for the task. This limits the
applications of LLMs on applications demanding precise information extraction
from documents, such as meta-analysis tasks. However, these findings hinge on
the assumption that the retrievers furnish pertinent context necessary for
accurate responses, emphasizing the need for further research on the efficacy
of retriever mechanisms in enhancing question-answering performance. Our work
offers a framework for ongoing dataset evaluation, ensuring that LLM
applications for information retrieval and document analysis continue to meet
evolving standards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 1 figure, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talk the Walk: Synthetic Data Generation for Conversational Music
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11489v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11489v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan Leszczynski, Shu Zhang, Ravi Ganti, Krisztian Balog, Filip Radlinski, Fernando Pereira, Arun Tejasvi Chaganty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are ubiquitous yet often difficult for users to control,
and adjust if recommendation quality is poor. This has motivated conversational
recommender systems (CRSs), with control provided through natural language
feedback. However, as with most application domains, building robust CRSs
requires training data that reflects system usage$\unicode{x2014}$here
conversations with user utterances paired with items that cover a wide range of
preferences. This has proved challenging to collect scalably using conventional
methods. We address the question of whether it can be generated synthetically,
building on recent advances in natural language. We evaluate in the setting of
item set recommendation, noting the increasing attention to this task motivated
by use cases like music, news, and recipe recommendation. We present
TalkTheWalk, which synthesizes realistic high-quality conversational data by
leveraging domain expertise encoded in widely available curated item
collections, generating a sequence of hypothetical yet plausible item sets,
then using a language model to produce corresponding user utterances. We
generate over one million diverse playlist curation conversations in the music
domain, and show these contain consistent utterances with relevant item sets
nearly matching the quality of an existing but small human-collected dataset
for this task. We demonstrate the utility of the generated synthetic dataset on
a conversational item retrieval task and show that it improves over both
unsupervised baselines and systems trained on a real dataset.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Persian Piano Corpus: A Collection Of Instrument-Based Feature
  Extracted Data Considering Dastgah 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parsa Rasouli, Azam Bastanfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research in the field of music is rapidly growing, and this trend
emphasizes the need for comprehensive data. Though researchers have made an
effort to contribute their own datasets, many data collections lack the
requisite inclusivity for comprehensive study because they are frequently
focused on particular components of music or other specific topics. We have
endeavored to address data scarcity by employing an instrument-based approach
to provide a complete corpus related to the Persian piano. Our piano corpus
includes relevant labels for Persian music mode (Dastgah) and comprehensive
metadata, allowing for utilization in various popular research areas. The
features extracted from 2022 Persian piano pieces in The Persian Piano Corpus
(PPC) have been collected and made available to researchers, aiming for a more
thorough understanding of Persian music and the role of the piano in it in
subsequent steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>including 11 pages and 6 figures. we want to inform related data PPC
  is submitted to Harvard Dataverse: https://doi.org/10.7910/DVN/YY7SVD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HIDRO-VQA: High Dynamic Range Oracle for Video Quality Assessment <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreshth Saini, Avinab Saha, Alan C. Bovik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HIDRO-VQA, a no-reference (NR) video quality assessment model
designed to provide precise quality evaluations of High Dynamic Range (HDR)
videos. HDR videos exhibit a broader spectrum of luminance, detail, and color
than Standard Dynamic Range (SDR) videos. As HDR content becomes increasingly
popular, there is a growing demand for video quality assessment (VQA)
algorithms that effectively address distortions unique to HDR content. To
address this challenge, we propose a self-supervised contrastive fine-tuning
approach to transfer quality-aware features from the SDR to the HDR domain,
utilizing unlabeled HDR videos. Our findings demonstrate that self-supervised
pre-trained neural networks on SDR content can be further fine-tuned in a
self-supervised setting using limited unlabeled HDR videos to achieve
state-of-the-art performance on the only publicly available VQA database for
HDR content, the LIVE-HDR VQA database. Moreover, our algorithm can be extended
to the Full Reference VQA setting, also achieving state-of-the-art performance.
Our code is available publicly at https://github.com/avinabsaha/HIDRO-VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2024 Workshop Paper. Shreshth Saini, Avinab Saha contributed
  equally to this work. Code coming soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperbolic Space with Hierarchical Margin Boosts Fine-Grained Learning
  from Coarse Labels <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu-Lin Xu, Yifan Sun, Faen Zhang, Anqi Xu, Xiu-Shen Wei, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning fine-grained embeddings from coarse labels is a challenging task due
to limited label granularity supervision, i.e., lacking the detailed
distinctions required for fine-grained tasks. The task becomes even more
demanding when attempting few-shot fine-grained recognition, which holds
practical significance in various applications. To address these challenges, we
propose a novel method that embeds visual embeddings into a hyperbolic space
and enhances their discriminative ability with a hierarchical cosine margins
manner. Specifically, the hyperbolic space offers distinct advantages,
including the ability to capture hierarchical relationships and increased
expressive power, which favors modeling fine-grained objects. Based on the
hyperbolic space, we further enforce relatively large/small similarity margins
between coarse/fine classes, respectively, yielding the so-called hierarchical
cosine margins manner. While enforcing similarity margins in the regular
Euclidean space has become popular for deep embedding learning, applying it to
the hyperbolic space is non-trivial and validating the benefit for
coarse-to-fine generalization is valuable. Extensive experiments conducted on
five benchmark datasets showcase the effectiveness of our proposed method,
yielding state-of-the-art results surpassing competing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Video Anomaly Event Detection: Systematic Taxonomy and
  Comparison of Deep Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Dingkang Yang, Yan Wang, Jing Liu, Jun Liu, Azzedine Boukerche, Peng Sun, Liang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Anomaly Detection (VAD) serves as a pivotal technology in the
intelligent surveillance systems, enabling the temporal or spatial
identification of anomalous events within videos. While existing reviews
predominantly concentrate on conventional unsupervised methods, they often
overlook the emergence of weakly-supervised and fully-unsupervised approaches.
To address this gap, this survey extends the conventional scope of VAD beyond
unsupervised methods, encompassing a broader spectrum termed Generalized Video
Anomaly Event Detection (GVAED). By skillfully incorporating recent
advancements rooted in diverse assumptions and learning frameworks, this survey
introduces an intuitive taxonomy that seamlessly navigates through
unsupervised, weakly-supervised, supervised and fully-unsupervised VAD
methodologies, elucidating the distinctions and interconnections within these
research trajectories. In addition, this survey facilitates prospective
researchers by assembling a compilation of research resources, including public
datasets, available codebases, programming tools, and pertinent literature.
Furthermore, this survey quantitatively assesses model performance, delves into
research challenges and directions, and outlines potential avenues for future
exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised to ACM Computing Surveys, under review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-11-17T00:00:00Z">2023-11-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">47</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the release of T\"ULU [Wang et al., 2023b], open resources for
instruction tuning have developed quickly, from better base models to new
finetuning techniques. We test and incorporate a number of these advances into
T\"ULU, resulting in T\"ULU 2, a suite of improved T\"ULU models for advancing
the understanding and best practices of adapting pretrained language models to
downstream tasks and user preferences. Concretely, we release: (1)
T\"ULU-V2-mix, an improved collection of high-quality instruction datasets; (2)
T\"ULU 2, LLAMA-2 models finetuned on the V2 mixture; (3) T\"ULU 2+DPO, T\"ULU
2 models trained with direct preference optimization (DPO), including the
largest DPO-trained model to date (T\"ULU 2+DPO 70B); (4) CODE T\"ULU 2, CODE
LLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its
instruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple
perspectives shows that the T\"ULU 2 suite achieves state-of-the-art
performance among open models and matches or exceeds the performance of
GPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data,
training and evaluation code to facilitate future open efforts on adapting
large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PEFT-MedAware: Large Language Model for Medical Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keivalya Pandya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chat models are capable of answering a wide range of questions, however, the
accuracy of their responses is highly uncertain. In this research, we propose a
specialized PEFT-MedAware model where we utilize parameter-efficient
fine-tuning (PEFT) to enhance the Falcon-1b large language model on specialized
MedQuAD data consisting of 16,407 medical QA pairs, leveraging only 0.44% of
its trainable parameters to enhance computational efficiency. The paper adopts
data preprocessing and PEFT to optimize model performance, complemented by a
BitsAndBytesConfig for efficient transformer training. The resulting model was
capable of outperforming other LLMs in medical question-answering tasks in
specific domains with greater accuracy utilizing limited computational
resources making it suitable for deployment in resource-constrained
environments. We propose further improvements through expanded datasets, larger
models, and feedback mechanisms for sustained medical relevancy. Our work
highlights the efficiency gains and specialized capabilities of PEFT in medical
AI, outpacing standard models in precision without extensive resource demands.
The proposed model and data are released for research purposes only.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure, submitted to the Artificial Intelligence in
  Medicine Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as
  an Alternative to Attention Layers in <span class="highlight-title">Transformer</span>s <span class="chip">AAAI24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vukasin Bozic, Danilo Dordervic, Daniele Coppola, Joseph Thommes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents an analysis of the effectiveness of using standard shallow
feed-forward networks to mimic the behavior of the attention mechanism in the
original Transformer model, a state-of-the-art architecture for
sequence-to-sequence tasks. We substitute key elements of the attention
mechanism in the Transformer with simple feed-forward networks, trained using
the original components via knowledge distillation. Our experiments, conducted
on the IWSLT2017 dataset, reveal the capacity of these "attentionless
Transformers" to rival the performance of the original architecture. Through
rigorous ablation studies, and experimenting with various replacement network
types and sizes, we offer insights that support the viability of our approach.
This not only sheds light on the adaptability of shallow feed-forward networks
in emulating attention mechanisms but also underscores their potential to
streamline complex architectures for sequence-to-sequence tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI24(https://aaai.org/aaai-conference/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Self-enhancement Approach for Domain-specific Chatbot Training via
  Knowledge Mining and Digest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), despite their great power in language
generation, often encounter challenges when dealing with intricate and
knowledge-demanding queries in specific domains. This paper introduces a novel
approach to enhance LLMs by effectively extracting the relevant knowledge from
domain-specific textual sources, and the adaptive training of a chatbot with
domain-specific inquiries. Our two-step approach starts from training a
knowledge miner, namely LLMiner, which autonomously extracts Question-Answer
pairs from relevant documents through a chain-of-thought reasoning process.
Subsequently, we blend the mined QA pairs with a conversational dataset to
fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise
and conversational capabilities. We also developed a new evaluation benchmark
which comprises four domain-specific text corpora and associated human-crafted
QA pairs for testing. Our model shows remarkable performance improvement over
generally aligned LLM and surpasses domain-adapted models directly fine-tuned
on domain corpus. In particular, LLMiner achieves this with minimal human
intervention, requiring only 600 seed instances, thereby providing a pathway
towards self-improvement of LLMs through model-synthesized training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hashing it Out: Predicting Unhealthy Conversations on Twitter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Leung, Filippos Papapolyzos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personal attacks in the context of social media conversations often lead to
fast-paced derailment, leading to even more harmful exchanges being made.
State-of-the-art systems for the detection of such conversational derailment
often make use of deep learning approaches for prediction purposes. In this
paper, we show that an Attention-based BERT architecture, pre-trained on a
large Twitter corpus and fine-tuned on our task, is efficient and effective in
making such predictions. This model shows clear advantages in performance to
the existing LSTM model we use as a baseline. Additionally, we show that this
impressive performance can be attained through fine-tuning on a relatively
small, novel dataset, particularly after mitigating overfitting issues through
synthetic oversampling techniques. By introducing the first transformer based
model for forecasting conversational events on Twitter, this work lays the
foundation for a practical tool to encourage better interactions on one of the
most ubiquitous social media platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, academic</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Countering Misinformation via Emotional Response Generation <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Russo, Shane Peter Kaszefski-Yaschuk, Jacopo Staiano, Marco Guerini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of misinformation on social media platforms (SMPs) poses a
significant danger to public health, social cohesion and ultimately democracy.
Previous research has shown how social correction can be an effective way to
curb misinformation, by engaging directly in a constructive dialogue with users
who spread -- often in good faith -- misleading messages. Although professional
fact-checkers are crucial to debunking viral claims, they usually do not engage
in conversations on social media. Thereby, significant effort has been made to
automate the use of fact-checker material in social correction; however, no
previous work has tried to integrate it with the style and pragmatics that are
commonly employed in social media communication. To fill this gap, we present
VerMouth, the first large-scale dataset comprising roughly 12 thousand
claim-response pairs (linked to debunking articles), accounting for both
SMP-style and basic emotions, two factors which have a significant role in
misinformation credibility and spreading. To collect this dataset we used a
technique based on an author-reviewer pipeline, which efficiently combines LLMs
and human annotators to obtain high-quality data. We also provide comprehensive
experiments showing how models trained on our proposed dataset have significant
improvements in terms of output quality and generalization capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of Offensive and Threatening Online Content in a Low Resource
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatima Muhammad Adam, Abubakar Yakubu Zandam, Isa Inuwa-Dutse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hausa is a major Chadic language, spoken by over 100 million people in
Africa. However, from a computational linguistic perspective, it is considered
a low-resource language, with limited resources to support Natural Language
Processing (NLP) tasks. Online platforms often facilitate social interactions
that can lead to the use of offensive and threatening language, which can go
undetected due to the lack of detection systems designed for Hausa. This study
aimed to address this issue by (1) conducting two user studies (n=308) to
investigate cyberbullying-related issues, (2) collecting and annotating the
first set of offensive and threatening datasets to support relevant downstream
tasks in Hausa, (3) developing a detection system to flag offensive and
threatening content, and (4) evaluating the detection system and the efficacy
of the Google-based translation engine in detecting offensive and threatening
terms in Hausa. We found that offensive and threatening content is quite
common, particularly when discussing religion and politics. Our detection
system was able to detect more than 70% of offensive and threatening content,
although many of these were mistranslated by Google's translation engine. We
attribute this to the subtle relationship between offensive and threatening
content and idiomatic expressions in the Hausa language. We recommend that
diverse stakeholders participate in understanding local conventions and
demographics in order to develop a more effective detection system. These
insights are essential for implementing targeted moderation strategies to
create a safe and inclusive online environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 5 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When a Language Question Is at Stake. A Revisited Approach to Label
  Sensitive Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stetsenko Daria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many under-resourced languages require high-quality datasets for specific
tasks such as offensive language detection, disinformation, or misinformation
identification. However, the intricacies of the content may have a detrimental
effect on the annotators. The article aims to revisit an approach of
pseudo-labeling sensitive data on the example of Ukrainian tweets covering the
Russian-Ukrainian war. Nowadays, this acute topic is in the spotlight of
various language manipulations that cause numerous disinformation and profanity
on social media platforms. The conducted experiment highlights three main
stages of data annotation and underlines the main obstacles during machine
annotation. Ultimately, we provide a fundamental statistical analysis of the
obtained data, evaluation of models used for pseudo-labelling, and set further
guidelines on how the scientists can leverage the corpus to execute more
advanced research and extend the existing data samples without annotators'
engagement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ukrainian language, pseudo-labelling, dataset, offensive-language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CNL2ASP: converting controlled natural language sentences into ASP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Caruso, Carmine Dodaro, Marco Maratea, Marco Mochi, Francesco Riccio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answer Set Programming (ASP) is a popular declarative programming language
for solving hard combinatorial problems. Although ASP has gained widespread
acceptance in academic and industrial contexts, there are certain user groups
who may find it more advantageous to employ a higher-level language that
closely resembles natural language when specifying ASP programs. In this paper,
we propose a novel tool, called CNL2ASP, for translating English sentences
expressed in a controlled natural language (CNL) form into ASP. In particular,
we first provide a definition of the type of sentences allowed by our CNL and
their translation as ASP rules, and then exemplify the usage of the CNL for the
specification of both synthetic and real-world combinatorial problems. Finally,
we report the results of an experimental analysis conducted on the real-world
problems to compare the performance of automatically generated encodings with
the ones written by ASP practitioners, showing that our tool can obtain
satisfactory performance on these benchmarks. Under consideration in Theory and
Practice of Logic Programming (TPLP).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under consideration in Theory and Practice of Logic Programming
  (TPLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sinhala-English Word Embedding Alignment: Introducing <span class="highlight-title">Dataset</span>s and
  Benchmark for a Low Resource Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasun Wickramasinghe, Nisansa de Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since their inception, embeddings have become a primary ingredient in many
flavours of Natural Language Processing (NLP) tasks supplanting earlier types
of representation. Even though multilingual embeddings have been used for the
increasing number of multilingual tasks, due to the scarcity of parallel
training data, low-resource languages such as Sinhala, tend to focus more on
monolingual embeddings. Then when it comes to the aforementioned multi-lingual
tasks, it is challenging to utilize these monolingual embeddings given that
even if the embedding spaces have a similar geometric arrangement due to an
identical training process, the embeddings of the languages considered are not
aligned. This is solved by the embedding alignment task. Even in this,
high-resource language pairs are in the limelight while low-resource languages
such as Sinhala which is in dire need of help seem to have fallen by the
wayside. In this paper, we try to align Sinhala and English word embedding
spaces based on available alignment techniques and introduce a benchmark for
Sinhala language embedding alignment. In addition to that, to facilitate the
supervised alignment, as an intermediate task, we also introduce
Sinhala-English alignment datasets. These datasets serve as our anchor datasets
for supervised word embedding alignment. Even though we do not obtain results
comparable to the high-resource languages such as French, German, or Chinese,
we believe our work lays the groundwork for more specialized alignment between
English and Sinhala embeddings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Graph in Language Model Rediscovers Cortical Hierarchy in Human
  Narrative Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengqi He, Taro Toyoizumi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how humans process natural language has long been a vital
research direction. The field of natural language processing (NLP) has recently
experienced a surge in the development of powerful language models. These
models have proven to be invaluable tools for studying another complex system
known to process human language: the brain. Previous studies have demonstrated
that the features of language models can be mapped to fMRI brain activity. This
raises the question: is there a commonality between information processing in
language models and the human brain? To estimate information flow patterns in a
language model, we examined the causal relationships between different layers.
Drawing inspiration from the workspace framework for consciousness, we
hypothesized that features integrating more information would more accurately
predict higher hierarchical brain activity. To validate this hypothesis, we
classified language model features into two categories based on causal network
measures: 'low in-degree' and 'high in-degree'. We subsequently compared the
brain prediction accuracy maps for these two groups. Our results reveal that
the difference in prediction accuracy follows a hierarchical pattern,
consistent with the cortical hierarchy map revealed by activity time constants.
This finding suggests a parallel between how language models and the human
brain process linguistic information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias A-head? Analyzing Bias in <span class="highlight-title">Transformer</span>-Based Language Model
  Attention Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yang, Hanyu Duan, Ahmed Abbasi, John P. Lalor, Kar Yan Tam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based pretrained large language models (PLM) such as BERT and GPT
have achieved remarkable success in NLP tasks. However, PLMs are prone to
encoding stereotypical biases. Although a burgeoning literature has emerged on
stereotypical bias mitigation in PLMs, such as work on debiasing gender and
racial stereotyping, how such biases manifest and behave internally within PLMs
remains largely unknown. Understanding the internal stereotyping mechanisms may
allow better assessment of model fairness and guide the development of
effective mitigation strategies. In this work, we focus on attention heads, a
major component of the Transformer architecture, and propose a bias analysis
framework to explore and identify a small set of biased heads that are found to
contribute to a PLM's stereotypical bias. We conduct extensive experiments to
validate the existence of these biased heads and to better understand how they
behave. We investigate gender and racial bias in the English language in two
types of Transformer-based PLMs: the encoder-based BERT model and the
decoder-based autoregressive GPT model. Overall, the results shed light on
understanding the bias behavior in pretrained language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOAL: Fine-grained Contrastive Learning for Cross-domain Aspect
  Sentiment Triplet Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting Xu, Zhen Wu, Huiyun Yang, Xinyu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect Sentiment Triplet Extraction (ASTE) has achieved promising results
while relying on sufficient annotation data in a specific domain. However, it
is infeasible to annotate data for each individual domain. We propose to
explore ASTE in the cross-domain setting, which transfers knowledge from a
resource-rich source domain to a resource-poor target domain, thereby
alleviating the reliance on labeled data in the target domain. To effectively
transfer the knowledge across domains and extract the sentiment triplets
accurately, we propose a method named Fine-grained cOntrAstive Learning (FOAL)
to reduce the domain discrepancy and preserve the discriminability of each
category. Experiments on six transfer pairs show that FOAL achieves 6%
performance gains and reduces the domain discrepancy significantly compared
with strong baselines. Our code will be publicly available once accepted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Relationship between In-Context Learning and Instruction
  Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyu Duan, Yixuan Tang, Yi Yang, Ahmed Abbasi, Kar Yan Tam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Context Learning (ICL) and Instruction Tuning (IT) are two primary
paradigms of adopting Large Language Models (LLMs) to downstream applications.
However, they are significantly different. In ICL, a set of demonstrations are
provided at inference time but the LLM's parameters are not updated. In IT, a
set of demonstrations are used to tune LLM's parameters in training time but no
demonstrations are used at inference time. Although a growing body of
literature has explored ICL and IT, studies on these topics have largely been
conducted in isolation, leading to a disconnect between these two paradigms. In
this work, we explore the relationship between ICL and IT by examining how the
hidden states of LLMs change in these two paradigms. Through carefully designed
experiments conducted with LLaMA-2 (7B and 13B), we find that ICL is implicit
IT. In other words, ICL changes an LLM's hidden states as if the demonstrations
were used to instructionally tune the model. Furthermore, the convergence
between ICL and IT is largely contingent upon several factors related to the
provided demonstrations. Overall, this work offers a unique perspective to
explore the connection between ICL and IT and sheds light on understanding the
behaviors of LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complementary Advantages of Chat<span class="highlight-title">GPT</span>s and Human Readers in Reasoning:
  Evidence from English Text Reading Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongquan Zhou, Yao Zhang, Siyi Cao, Yulu Li, Tao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT has shown its great power in text processing, including its reasoning
ability from text reading. However, there has not been any direct comparison
between human readers and ChatGPT in reasoning ability related to text reading.
This study was undertaken to investigate how ChatGPTs (i.e., ChatGPT and
ChatGPT Plus) and Chinese senior school students as ESL learners exhibited
their reasoning ability from English narrative texts. Additionally, we compared
the two ChatGPTs in the reasoning performances when commands were updated
elaborately. The whole study was composed of three reasoning tests: Test 1 for
commonsense inference, Test 2 for emotional inference, and Test 3 for causal
inference. The results showed that in Test 1, the students outdid the two
ChatGPT versions in local-culture-related inferences but performed worse than
the chatbots in daily-life inferences. In Test 2, ChatGPT Plus excelled whereas
ChatGPT lagged behind in accuracy. In association with both accuracy and
frequency of correct responses, the students were inferior to the two chatbots.
Compared with ChatGPTs' better performance in positive emotions, the students
showed their superiority in inferring negative emotions. In Test 3, the
students demonstrated better logical analysis, outdoing both chatbots. In
updating command condition, ChatGPT Plus displayed good causal reasoning
ability while ChatGPT kept unchanged. Our study reveals that human readers and
ChatGPTs have their respective advantages and disadvantages in drawing
inferences from text reading comprehension, unlocking a complementary
relationship in text-based reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> Pool based Class-Incremental Continual Learning for Dialog State
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Liu, Yucheng Cai, Yuan Zhou, Zhijian Ou, Yi Huang, Junlan Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning is crucial for dialog state tracking (DST) in dialog
systems, since requirements from users for new functionalities are often
encountered. However, most of existing continual learning methods for DST
require task identities during testing, which is a severe limit in real-world
applications. In this paper, we aim to address continual learning of DST in the
class-incremental scenario (namely the task identity is unknown in testing).
Inspired by the recently emerging prompt tuning method that performs well on
dialog systems, we propose to use the prompt pool method, where we maintain a
pool of key-value paired prompts and select prompts from the pool according to
the distance between the dialog history and the prompt keys. The proposed
method can automatically identify tasks and select appropriate prompts during
testing. We conduct experiments on Schema-Guided Dialog dataset (SGD) and
another dataset collected from a real-world dialog application. Experiment
results show that the prompt pool method achieves much higher joint goal
accuracy than the baseline. After combining with a rehearsal buffer, the model
performance can be further improved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy and Carbon Considerations of Fine-Tuning <span class="highlight-title">BERT</span> <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaorong Wang, Clara Na, Emma Strubell, Sorelle Friedler, Sasha Luccioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP
community, existing work quantifying energy costs and associated carbon
emissions has largely focused on language model pre-training. Although a single
pre-training run draws substantially more energy than fine-tuning, fine-tuning
is performed more frequently by many more individual actors, and thus must be
accounted for when considering the energy and carbon footprint of NLP. In order
to better characterize the role of fine-tuning in the landscape of energy and
carbon emissions in NLP, we perform a careful empirical study of the
computational costs of fine-tuning across tasks, datasets, hardware
infrastructure and measurement modalities. Our experimental results allow us to
place fine-tuning energy and carbon costs into perspective with respect to
pre-training and inference, and outline recommendations to NLP researchers and
practitioners who wish to improve their fine-tuning energy efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings; First two authors contributed equally; 12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnosing and Debiasing Corpus-Based Political Bias and Insults in <span class="highlight-title">GPT</span>2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ambri Ma, Arnav Kumar, Brett Zeligson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The training of large language models (LLMs) on extensive, unfiltered corpora
sourced from the internet is a common and advantageous practice. Consequently,
LLMs have learned and inadvertently reproduced various types of biases,
including violent, offensive, and toxic language. However, recent research
shows that generative pretrained transformer (GPT) language models can
recognize their own biases and detect toxicity in generated content, a process
referred to as self-diagnosis. In response, researchers have developed a
decoding algorithm that allows LLMs to self-debias, or reduce their likelihood
of generating harmful text. This study investigates the efficacy of the
diagnosing-debiasing approach in mitigating two additional types of biases:
insults and political bias. These biases are often used interchangeably in
discourse, despite exhibiting potentially dissimilar semantic and syntactic
properties. We aim to contribute to the ongoing effort of investigating the
ethical and social implications of human-AI interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexible Model Interpretability through Natural Language Model Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karel D'Oosterlinck, Thomas Demeester, Chris Develder, Christopher Potts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model interpretability and model editing are crucial goals in the age of
large language models. Interestingly, there exists a link between these two
goals: if a method is able to systematically edit model behavior with regard to
a human concept of interest, this editor method can help make internal
representations more interpretable by pointing towards relevant representations
and systematically manipulating them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended Abstract -- work in progress. BlackboxNLP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extraction and Summarization of Explicit Video Content using Multi-Modal
  Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaunak Joshi, Raghav Gaggar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increase in video-sharing platforms across the internet, it is
difficult for humans to moderate the data for explicit content. Hence, an
automated pipeline to scan through video data for explicit content has become
the need of the hour. We propose a novel pipeline that uses multi-modal deep
learning to first extract the explicit segments of input videos and then
summarize their content using text to determine its age appropriateness and age
rating. We also evaluate our pipeline's effectiveness in the end using standard
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Labeling Indoor Scenes with Fusion of Out-of-the-Box Perception Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimeng Li, Navid Rajabi, Sulabh Shrestha, Md Alimoor Reza, Jana Kosecka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The image annotation stage is a critical and often the most time-consuming
part required for training and evaluating object detection and semantic
segmentation models. Deployment of the existing models in novel environments
often requires detecting novel semantic classes not present in the training
data. Furthermore, indoor scenes contain significant viewpoint variations,
which need to be handled properly by trained perception models. We propose to
leverage the recent advancements in state-of-the-art models for bottom-up
segmentation (SAM), object detection (Detic), and semantic segmentation
(MaskFormer), all trained on large-scale datasets. We aim to develop a
cost-effective labeling approach to obtain pseudo-labels for semantic
segmentation and object instance detection in indoor environments, with the
ultimate goal of facilitating the training of lightweight models for various
downstream tasks. We also propose a multi-view labeling fusion stage, which
considers the setting where multiple views of the scenes are available and can
be used to identify and rectify single-view inconsistencies. We demonstrate the
effectiveness of the proposed approach on the Active Vision dataset and the
ADE20K dataset. We evaluate the quality of our labeling process by comparing it
with human annotations. Also, we demonstrate the effectiveness of the obtained
labels in downstream tasks such as object goal navigation and part discovery.
In the context of object goal navigation, we depict enhanced performance using
this fusion approach compared to a zero-shot baseline that utilizes large
monolithic vision-language pre-trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formal concept analysis for evaluating intrinsic dimension of a natural
  language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergei O. Kuznetsov, Vasilii A. Gromov, Nikita S. Borodin, Andrei M. Divavin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Some results of a computational experiment for determining the intrinsic
dimension of linguistic varieties for the Bengali and Russian languages are
presented. At the same time, both sets of words and sets of bigrams in these
languages were considered separately. The method used to solve this problem was
based on formal concept analysis algorithms. It was found that the intrinsic
dimensions of these languages are significantly less than the dimensions used
in popular neural network models in natural language processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, 10th International Conference on Pattern Recognition and
  Machine Intelligence (PReMI 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token-level Adaptation of LoRA Adapters for Downstream Task
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Belofsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a method for adapting LoRA adapters in smaller-sized
language models to arbitrary downstream tasks. Unlike standard
mixture-of-expert architectures, our method employs a gradient-free routing
function to choose a weighted combination of experts without increasing the
compute requirements for training or inference. The results show that
token-level adaptation of LoRA adapters outperforms the base Llama-2-7b model
across mathematical (GSM8K), scientific (ARC-Challenge), reading comprehension
(SQuAD), and coding (CodeAlpaca-20k) tasks. Further evaluations also show that
the average performance of token-level adaptation outperforms individual models
fine-tuned for each of the tasks with the best performance observed in
adaptation of every-other token during inference. The code for this study is
made available through a public repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Language Agent for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-level driving is an ultimate goal of autonomous driving. Conventional
approaches formulate autonomous driving as a perception-prediction-planning
framework, yet their systems do not capitalize on the inherent reasoning
ability and experiential knowledge of humans. In this paper, we propose a
fundamental paradigm shift from current pipelines, exploiting Large Language
Models (LLMs) as a cognitive agent to integrate human-like intelligence into
autonomous driving systems. Our approach, termed Agent-Driver, transforms the
traditional autonomous driving pipeline by introducing a versatile tool library
accessible via function calls, a cognitive memory of common sense and
experiential knowledge for decision-making, and a reasoning engine capable of
chain-of-thought reasoning, task planning, motion planning, and
self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive
common sense and robust reasoning capabilities, thus enabling a more nuanced,
human-like approach to autonomous driving. We evaluate our approach on the
large-scale nuScenes benchmark, and extensive experiments substantiate that our
Agent-Driver significantly outperforms the state-of-the-art driving methods by
a large margin. Our approach also demonstrates superior interpretability and
few-shot learning ability to these methods. Project page:
\href{https://github.com/USC-GVL/Agent-Driver/blob/main/index.html}{here}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Use <span class="highlight-title">GPT</span>-J <span class="highlight-title">Prompt</span> Generation with Ro<span class="highlight-title">BERT</span>a for NER Models on Diagnosis
  Extraction of Periodontal Diagnosis from Electronic Dental Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao-Shun Chuang, Xiaoqian Jiang, Chun-Teh Lee, Ryan Brandon, Duong Tran, Oluwabunmi Tokede, Muhammad F. Walji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explored the usability of prompt generation on named entity
recognition (NER) tasks and the performance in different settings of the
prompt. The prompt generation by GPT-J models was utilized to directly test the
gold standard as well as to generate the seed and further fed to the RoBERTa
model with the spaCy package. In the direct test, a lower ratio of negative
examples with higher numbers of examples in prompt achieved the best results
with a F1 score of 0.72. The performance revealed consistency, 0.92-0.97 in the
F1 score, in all settings after training with the RoBERTa model. The study
highlighted the importance of seed quality rather than quantity in feeding NER
models. This research reports on an efficient and accurate way to mine clinical
notes for periodontal diagnoses, allowing researchers to easily and quickly
build a NER model with the prompt generation approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2023 AMIA Annual Symposium, see
  https://amia.org/education-events/amia-2023-annual-symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as
  an Alternative to Attention Layers in <span class="highlight-title">Transformer</span>s <span class="chip">AAAI24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vukasin Bozic, Danilo Dordevic, Daniele Coppola, Joseph Thommes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents an analysis of the effectiveness of using standard shallow
feed-forward networks to mimic the behavior of the attention mechanism in the
original Transformer model, a state-of-the-art architecture for
sequence-to-sequence tasks. We substitute key elements of the attention
mechanism in the Transformer with simple feed-forward networks, trained using
the original components via knowledge distillation. Our experiments, conducted
on the IWSLT2017 dataset, reveal the capacity of these "attentionless
Transformers" to rival the performance of the original architecture. Through
rigorous ablation studies, and experimenting with various replacement network
types and sizes, we offer insights that support the viability of our approach.
This not only sheds light on the adaptability of shallow feed-forward networks
in emulating attention mechanisms but also underscores their potential to
streamline complex architectures for sequence-to-sequence tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI24(https://aaai.org/aaai-conference/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study on Altering the Latent Space of <span class="highlight-title">Pretrain</span>ed Text to Speech Models
  for Improved Expressiveness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Vogel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report explores the challenge of enhancing expressiveness control in
Text-to-Speech (TTS) models by augmenting a frozen pretrained model with a
Diffusion Model that is conditioned on joint semantic audio/text embeddings.
The paper identifies the challenges encountered when working with a VAE-based
TTS model and evaluates different image-to-image methods for altering latent
speech features. Our results offer valuable insights into the complexities of
adding expressiveness control to TTS systems and open avenues for future
research in this direction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TaCo: Enhancing Cross-Lingual Transfer for Low-Resource Languages in
  LLMs through Translation-Assisted Chain-of-Thought Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bibek Upadhayay, Vahid Behzadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs such as ChatGPT and PaLM can be utilized to train on a new language and
revitalize low-resource languages. However, it is evidently very costly to
pretrain pr fine-tune LLMs to adopt new languages. Another challenge is the
limitation of benchmark datasets and the metrics used to measure the
performance of models in multilingual settings. This paper proposes
cost-effective solutions to both of the aforementioned challenges. We introduce
the Multilingual Instruction-Tuning Dataset (MITS), which is comprised of the
translation of Alpaca-52K, Dolly-15K, and Vicuna Benchmark in 132 languages.
Also, we propose a new method called \emph{TaCo: Translation-Assisted
Cross-Linguality}, which make uses of translation in a chain-of-thought process
to instruction-tune LLMs on a new languages through a curriculum learning
process. As a proof of concept, we experimented with the instruction-tuned
Guanaco-33B model and performed further instruction tuning using the TaCo
method in three low-resource languages and one high-resource language. Our
results show that the TaCo method impresses the GPT-4 with 82% for a
low-resource language in the Vicuna Benchmark dataset, and boosts performance
by double in contrast to the performance of instruction tuning only. Our
results show that TaCo is a promising method for creating multilingual LLMs,
even for low-resource languages. We have released our datasets and the model
adapters, and encourage the research community to make use of these resources
towards advancing work on multilingual LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisIT-Bench: A Benchmark for Vision-Language Instruction Following
  Inspired by Real-World Use 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06595v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06595v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, Ludwig Schmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for
evaluation of instruction-following vision-language models for real-world use.
Our starting point is curating 70 'instruction families' that we envision
instruction tuned vision-language models should be able to address. Extending
beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to
game playing and creative generation. Following curation, our dataset comprises
592 test queries, each with a human-authored instruction-conditioned caption.
These descriptions surface instruction-specific factors, e.g., for an
instruction asking about the accessibility of a storefront for wheelchair
users, the instruction-conditioned caption describes ramps/potential obstacles.
These descriptions enable 1) collecting human-verified reference outputs for
each instance; and 2) automatic evaluation of candidate multimodal generations
using a text-only LLM, aligning with human judgment. We quantify quality gaps
between models and references using both human and automatic evaluations; e.g.,
the top-performing instruction-following model wins against the GPT-4 reference
in just 27% of the comparison. VisIT-Bench is dynamic to participate,
practitioners simply submit their model's response on the project website;
Data, code and leaderboard is available at visit-bench.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration
  in Improving the Performance of Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishani Mondal, Michelle Yuan, Anandhavelu N, Aparna Garimella, Francis Ferraro, Andrew Blair-Stanek, Benjamin Van Durme, Jordan Boyd-Graber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning template based information extraction from documents is a crucial
yet difficult task. Prior template-based IE approaches assume foreknowledge of
the domain templates; however, real-world IE do not have pre-defined schemas
and it is a figure-out-as you go phenomena. To quickly bootstrap templates in a
real-world setting, we need to induce template slots from documents with zero
or minimal supervision. Since the purpose of question answering intersect with
the goal of information extraction, we use automatic question generation to
induce template slots from the documents and investigate how a tiny amount of a
proxy human-supervision on-the-fly (termed as InteractiveIE) can further boost
the performance. Extensive experiments on biomedical and legal documents, where
obtaining training data is expensive, reveal encouraging trends of performance
improvement using InteractiveIE over AI-only baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't Say What You Don't Know: Improving the Consistency of Abstractive
  Summarization by Constraining Beam Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.08436v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.08436v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel King, Zejiang Shen, Nishant Subramani, Daniel S. Weld, Iz Beltagy, Doug Downey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstractive summarization systems today produce fluent and relevant output,
but often "hallucinate" statements not supported by the source text. We analyze
the connection between hallucinations and training data, and find evidence that
models hallucinate because they train on target summaries that are unsupported
by the source. Based on our findings, we present PINOCCHIO, a new decoding
method that improves the consistency of a transformer-based abstractive
summarizer by constraining beam search to avoid hallucinations. Given the model
states and outputs at a given step, PINOCCHIO detects likely model
hallucinations based on various measures of attribution to the source text.
PINOCCHIO backtracks to find more consistent output, and can opt to produce no
summary at all when no consistent generation can be found. In experiments, we
find that PINOCCHIO improves the consistency of generation (in terms of F1) by
an average of~67% on two abstractive summarization datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 2 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Targeted Image Data Augmentation Increases Basic Skills Captioning
  Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Barriere, Felipe del Rio, Andres Carvallo De Ferari, Carlos Aspillaga, Eugenio Herrera-Berg, Cristian Buc Calderon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial neural networks typically struggle in generalizing to
out-of-context examples. One reason for this limitation is caused by having
datasets that incorporate only partial information regarding the potential
correlational structure of the world. In this work, we propose TIDA (Targeted
Image-editing Data Augmentation), a targeted data augmentation method focused
on improving models' human-like abilities (e.g., gender recognition) by filling
the correlational structure gap using a text-to-image generative model. More
specifically, TIDA identifies specific skills in captions describing images
(e.g., the presence of a specific gender in the image), changes the caption
(e.g., "woman" to "man"), and then uses a text-to-image model to edit the image
in order to match the novel caption (e.g., uniquely changing a woman to a man
while maintaining the context identical). Based on the Flickr30K benchmark, we
show that, compared with the original data set, a TIDA-enhanced dataset related
to gender, color, and counting abilities induces better performance in several
image captioning metrics. Furthermore, on top of relying on the classical BLEU
metric, we conduct a fine-grained analysis of the improvements of our models
against the baseline in different ways. We compared text-to-image generative
models and found different behaviors of the image captioning models in terms of
encoding visual encoding and textual decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Fair and In-Depth Evaluation of Existing End-to-End Entity Linking
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Bast, Matthias Hertel, Natalie Prange
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing evaluations of entity linking systems often say little about how the
system is going to perform for a particular application. There are two
fundamental reasons for this. One is that many evaluations only use aggregate
measures (like precision, recall, and F1 score), without a detailed error
analysis or a closer look at the results. The other is that all of the widely
used benchmarks have strong biases and artifacts, in particular: a strong focus
on named entities, an unclear or missing specification of what else counts as
an entity mention, poor handling of ambiguities, and an over- or
underrepresentation of certain kinds of entities.
  We provide a more meaningful and fair in-depth evaluation of a variety of
existing end-to-end entity linkers. We characterize their strengths and
weaknesses and also report on reproducibility aspects. The detailed results of
our evaluation can be inspected under
https://elevant.cs.uni-freiburg.de/emnlp2023 . Our evaluation is based on
several widely used benchmarks, which exhibit the problems mentioned above to
various degrees, as well as on two new benchmarks, which address the problems
mentioned above. The new benchmarks can be found under
https://github.com/ad-freiburg/fair-entity-linking-benchmarks .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncovering Intermediate Variables in <span class="highlight-title">Transformer</span>s using Circuit Probing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04354v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04354v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael A. Lepori, Thomas Serre, Ellie Pavlick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network models have achieved high performance on a wide variety of
complex tasks, but the algorithms that they implement are notoriously difficult
to interpret. In order to understand these algorithms, it is often necessary to
hypothesize intermediate variables involved in the network's computation. For
example, does a language model depend on particular syntactic properties when
generating a sentence? However, existing analysis tools make it difficult to
test hypotheses of this type. We propose a new analysis technique -- circuit
probing -- that automatically uncovers low-level circuits that compute
hypothesized intermediate variables. This enables causal analysis through
targeted ablation at the level of model parameters. We apply this method to
models trained on simple arithmetic tasks, demonstrating its effectiveness at
(1) deciphering the algorithms that models have learned, (2) revealing modular
structure within a model, and (3) tracking the development of circuits over
training. We compare circuit probing to other methods across these three
experiments, and find it on par or more effective than existing analysis
methods. Finally, we demonstrate circuit probing on a real-world use case,
uncovering circuits that are responsible for subject-verb agreement and
reflexive anaphora in GPT2-Small and Medium.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Dark Side of the Language: <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">Transformer</span>s in the DarkNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Ranaldi, Aria Nourbakhsh, Arianna Patrizi, Elena Sofia Ruzzetti, Dario Onorati, Francesca Fallucchi, Fabio Massimo Zanzotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Transformers are challenging human performances in many NLP
tasks. The massive datasets used for pre-training seem to be the key to their
success on existing tasks. In this paper, we explore how a range of pre-trained
Natural Language Understanding models perform on definitely unseen sentences
provided by classification tasks over a DarkNet corpus. Surprisingly, results
show that syntactic and lexical neural networks perform on par with pre-trained
Transformers even after fine-tuning. Only after what we call extreme domain
adaptation, that is, retraining with the masked language model task on all the
novel corpus, pre-trained Transformers reach their standard high results. This
suggests that huge pre-training corpora may give Transformers unexpected help
since they are exposed to many of the possible sentences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classifying COVID-19 vaccine narratives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Li, Carolina Scarton, Xingyi Song, Kalina Bontcheva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vaccine hesitancy is widespread, despite the government's information
campaigns and the efforts of the World Health Organisation (WHO). Categorising
the topics within vaccine-related narratives is crucial to understand the
concerns expressed in discussions and identify the specific issues that
contribute to vaccine hesitancy. This paper addresses the need for monitoring
and analysing vaccine narratives online by introducing a novel vaccine
narrative classification task, which categorises COVID-19 vaccine claims into
one of seven categories. Following a data augmentation approach, we first
construct a novel dataset for this new classification task, focusing on the
minority classes. We also make use of fact-checker annotated data. The paper
also presents a neural vaccine narrative classifier that achieves an accuracy
of 84% under cross-validation. The classifier is publicly available for
researchers and journalists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 14th International Conference on Recent
  Advances in Natural Language Processing, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18075v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18075v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Tian, Liangyu Chen, Na Liu, Yaxuan Liu, Wei Zou, Kaijiang Chen, Ming Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the dual-process theory of human cognition, we introduce DUMA, a
novel conversational agent framework that embodies a dual-mind mechanism
through the utilization of two generative Large Language Models (LLMs)
dedicated to fast and slow thinking respectively. The fast thinking model
serves as the primary interface for external interactions and initial response
generation, evaluating the necessity for engaging the slow thinking model based
on the complexity of the complete response. When invoked, the slow thinking
model takes over the conversation, engaging in meticulous planning, reasoning,
and tool utilization to provide a well-analyzed response. This dual-mind
configuration allows for a seamless transition between intuitive responses and
deliberate problem-solving processes based on the situation. We have
constructed a conversational agent to handle online inquiries in the real
estate industry. The experiment proves that our method balances effectiveness
and efficiency, and has a significant improvement compared to the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Insights Into the Nutritional Prevention of Macular Degeneration based
  on a Comparative Topic Modeling Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00312v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00312v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Cassiel Jacaruso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling and text mining are subsets of Natural Language Processing
(NLP) with relevance for conducting meta-analysis (MA) and systematic review
(SR). For evidence synthesis, the above NLP methods are conventionally used for
topic-specific literature searches or extracting values from reports to
automate essential phases of SR and MA. Instead, this work proposes a
comparative topic modeling approach to analyze reports of contradictory results
on the same general research question. Specifically, the objective is to
identify topics exhibiting distinct associations with significant results for
an outcome of interest by ranking them according to their proportional
occurrence in (and consistency of distribution across) reports of significant
effects. The proposed method was tested on broad-scope studies addressing
whether supplemental nutritional compounds significantly benefit macular
degeneration (MD). Four of these were further supported in terms of
effectiveness upon conducting a follow-up literature search for validation
(omega-3 fatty acids, copper, zeaxanthin, and nitrates). The two not supported
by the follow-up literature search (niacin and molybdenum) also had scores in
the lowest range under the proposed scoring system, suggesting that the
proposed methods score for a given topic may be a viable proxy for its degree
of association with the outcome of interest and can be helpful in the search
for potentially causal relationships. These results underpin the proposed
methods potential to add specificity in understanding effects from broad-scope
reports, elucidate topics of interest for future research, and guide evidence
synthesis in a systematic and scalable way. All of this is accomplished while
yielding valuable insights into the prevention of MD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Who Wrote this Code? Watermarking for Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, Gunhee Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the remarkable generation performance of large language models, ethical
and legal concerns about using them have been raised, such as plagiarism and
copyright issues. For such concerns, several approaches to watermark and detect
LLM-generated text have been proposed very recently. However, we discover that
the previous methods fail to function appropriately with code generation tasks
because of the syntactic and semantic characteristics of code. Based on
\citet{Kirchenbauer2023watermark}, we propose a new watermarking method,
Selective WatErmarking via Entropy Thresholding (SWEET), that promotes "green"
tokens only at the position with high entropy of the token distribution during
generation, thereby preserving the correctness of the generated code. The
watermarked code is detected by the statistical test and Z-score based on the
entropy information. Our experiments on HumanEval and MBPP show that SWEET
significantly improves the Pareto Frontier between the code correctness and
watermark detection performance. We also show that notable post-hoc detection
methods (e.g. DetectGPT) fail to work well in this task. Finally, we show that
setting a reasonable entropy threshold is not much of a challenge. Code is
available at https://github.com/hongcheki/sweet-watermark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PsyBench: a balanced and in-depth Psychological Chinese Evaluation
  Benchmark for Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlei Zhang, Hongliang He, Nirui Song, Shuyuan He, Shuai Zhang, Huachuan Qiu, Anqi Li, Lizhi Ma, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) are becoming prevalent in various fields,
there is an urgent need for improved NLP benchmarks that encompass all the
necessary knowledge of individual discipline. Many contemporary benchmarks for
foundational models emphasize a broad range of subjects but often fall short in
presenting all the critical subjects and encompassing necessary professional
knowledge of them. This shortfall has led to skewed results, given that LLMs
exhibit varying performance across different subjects and knowledge areas. To
address this issue, we present psybench, the first comprehensive Chinese
evaluation suite that covers all the necessary knowledge required for graduate
entrance exams. psybench offers a deep evaluation of a model's strengths and
weaknesses in psychology through multiple-choice questions. Our findings show
significant differences in performance across different sections of a subject,
highlighting the risk of skewed results when the knowledge in test sets is not
balanced. Notably, only the ChatGPT model reaches an average accuracy above
$70\%$, indicating that there is still plenty of room for improvement. We
expect that psybench will help to conduct thorough evaluations of base models'
strengths and weaknesses and assist in practical application in the field of
psychology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code
  Completion <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, Bing Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code completion models have made significant progress in recent years, yet
current popular evaluation datasets, such as HumanEval and MBPP, predominantly
focus on code completion tasks within a single file. This over-simplified
setting falls short of representing the real-world software development
scenario where repositories span multiple files with numerous cross-file
dependencies, and accessing and understanding cross-file context is often
required to complete the code correctly.
  To fill in this gap, we propose CrossCodeEval, a diverse and multilingual
code completion benchmark that necessitates an in-depth cross-file contextual
understanding to complete the code accurately. CrossCodeEval is built on a
diverse set of real-world, open-sourced, permissively-licensed repositories in
four popular programming languages: Python, Java, TypeScript, and C#. To create
examples that strictly require cross-file context for accurate completion, we
propose a straightforward yet efficient static-analysis-based approach to
pinpoint the use of cross-file context within the current file.
  Extensive experiments on state-of-the-art code language models like CodeGen
and StarCoder demonstrate that CrossCodeEval is extremely challenging when the
relevant cross-file context is absent, and we see clear improvements when
adding these context into the prompt. However, despite such improvements, the
pinnacle of performance remains notably unattained even with the
highest-performing model, indicating that CrossCodeEval is also capable of
assessing model's capability in leveraging extensive context to make better
code completion. Finally, we benchmarked various methods in retrieving
cross-file context, and show that CrossCodeEval can also be used to measure the
capability of code retrievers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at NeurIPS 2023 (Datasets and Benchmarks Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Catalogue Generation for Literature <span class="highlight-title">Review</span>: A Benchmark <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Zhu, Xiaocheng Feng, Xiachong Feng, Yingsheng Wu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific literature review generation aims to extract and organize
important information from an abundant collection of reference papers and
produces corresponding reviews while lacking a clear and logical hierarchy. We
observe that a high-quality catalogue-guided generation process can effectively
alleviate this problem. Therefore, we present an atomic and challenging task
named Hierarchical Catalogue Generation for Literature Review as the first step
for review generation, which aims to produce a hierarchical catalogue of a
review paper given various references. We construct a novel English
Hierarchical Catalogues of Literature Reviews Dataset with 7.6k literature
review catalogues and 389k reference papers. To accurately assess the model
performance, we design two evaluation metrics for informativeness and
similarity to ground truth from semantics and structure.Our extensive analyses
verify the high quality of our dataset and the effectiveness of our evaluation
metrics. We further benchmark diverse experiments on state-of-the-art
summarization models like BART and large language models like ChatGPT to
evaluate their capabilities. We further discuss potential directions for this
task to motivate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-4 can pass the Korean National Licensing Examination for Korean
  Medicine Doctors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyeop Jang, Tae-Rim Yun, Choong-Yeol Lee, Young-Kyu Kwon, Chang-Eop Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Korean medicine (TKM) emphasizes individualized diagnosis and
treatment. This uniqueness makes AI modeling difficult due to limited data and
implicit processes. Large language models (LLMs) have demonstrated impressive
medical inference, even without advanced training in medical texts. This study
assessed the capabilities of GPT-4 in TKM, using the Korean National Licensing
Examination for Korean Medicine Doctors (K-NLEKMD) as a benchmark. The
K-NLEKMD, administered by a national organization, encompasses 12 major
subjects in TKM. We optimized prompts with Chinese-term annotation, English
translation for questions and instruction, exam-optimized instruction, and
self-consistency. GPT-4 with optimized prompts achieved 66.18% accuracy,
surpassing both the examination's average pass mark of 60% and the 40% minimum
for each subject. The gradual introduction of language-related prompts and
prompting techniques enhanced the accuracy from 51.82% to its maximum accuracy.
GPT-4 showed low accuracy in subjects including public health &
medicine-related law, internal medicine (2) which are localized in Korea and
TKM. The model's accuracy was lower for questions requiring TKM-specialized
knowledge. It exhibited higher accuracy in diagnosis-based and recall-based
questions than in intervention-based questions. A positive correlation was
observed between the consistency and accuracy of GPT-4's responses. This study
unveils both the potential and challenges of applying LLMs to TKM. These
findings underline the potential of LLMs like GPT-4 in culturally adapted
medicine, especially TKM, for tasks such as clinical assistance, medical
education, and research. But they also point towards the necessity for the
development of methods to mitigate cultural bias inherent in large language
models and validate their efficacy in real-world clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Contamination Quiz: A Tool to Detect and Estimate Contamination in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahriar Golchin, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the Data Contamination Quiz, a simple and effective approach to
detect data contamination in large language models (LLMs) and estimate the
amount of it. Specifically, we frame data contamination detection as a series
of multiple-choice questions. We devise a quiz format wherein three perturbed
versions of each dataset instance are created. These changes only include
word-level perturbations, replacing words with their contextual synonyms,
ensuring both the semantic and sentence structure remain exactly the same as
the original instance. Together with the original instance, these perturbed
versions constitute the choices in the quiz. Given that the only distinguishing
signal among these choices is the exact wording, an LLM, when tasked with
identifying the original instance from the choices, opts for the original if it
has memorized it in its pre-training phase--a trait intrinsic to LLMs. A
dataset partition is then marked as contaminated if the LLM's performance on
the quiz surpasses what random chance suggests. Our evaluation spans seven
datasets and their respective splits (train and test/validation) on two
state-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the
pre-training data, our results suggest that our approach not only enhances the
detection of data contamination but also provides an accurate estimation of its
extent, even when the contamination signal is weak.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v1.1 preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inferring the Reader: Guiding Automated Story Generation with
  Commonsense Reasoning <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.01311v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.01311v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Peng, Siyan Li, Sarah Wiegreffe, Mark Riedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based language model approaches to automated story generation
currently provide state-of-the-art results. However, they still suffer from
plot incoherence when generating narratives over time, and critically lack
basic commonsense reasoning. Furthermore, existing methods generally focus only
on single-character stories, or fail to track characters at all. To improve the
coherence of generated narratives and to expand the scope of character-centric
narrative generation, we introduce Commonsense-inference Augmented neural
StoryTelling (CAST), a framework for introducing commonsense reasoning into the
generation process with the option to model the interaction between multiple
characters. We find that our CAST method produces significantly more coherent,
on-topic, enjoyable and fluent stories than existing models in both the
single-character and two-character settings in three storytelling domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2022. For conference video and anthology version,
  see https://aclanthology.org/2022.findings-emnlp.520/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DocGen: Generating Detailed Parameter Docstrings in Python 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06453v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06453v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vatsal Venkatkrishna, Durga Shree Nagabushanam, Emmanuel Iko-Ojo Simon, Melina Vidoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Documentation debt hinders the effective utilization of open-source software.
Although code summarization tools have been helpful for developers, most would
prefer a detailed account of each parameter in a function rather than a
high-level summary. However, generating such a summary is too intricate for a
single generative model to produce reliably due to the lack of high-quality
training data. Thus, we propose a multi-step approach that combines multiple
task-specific models, each adept at producing a specific section of a
docstring. The combination of these models ensures the inclusion of each
section in the final docstring. We compared the results from our approach with
existing generative models using both automatic metrics and a human-centred
evaluation with 17 participating developers, which proves the superiority of
our approach over existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Decision <span class="highlight-title">Transformer</span>s with Exponential Tilt for Interactive
  Text Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Gontier, Pau Rodriguez, Issam Laradji, David Vazquez, Christopher Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based game environments are challenging because agents must deal with
long sequences of text, execute compositional actions using text and learn from
sparse rewards. We address these challenges by proposing Language Decision
Transformers (LDTs), a framework that is based on transformer language models
and decision transformers (DTs). Our LDTs extend DTs with 3 components: (1)
exponential tilt to guide the agent towards high obtainable goals, (2) novel
goal conditioning methods yielding better results than the traditional
return-to-go (sum of all future rewards), and (3) a model of future
observations that improves agent performance. LDTs are the first to address
offline RL with DTs on these challenging games. Our experiments show that LDTs
achieve the highest scores among many different types of agents on some of the
most challenging Jericho games, such as Enchanter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">88</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emu Video: Factorizing Text-to-Video Generation by Explicit Image
  Conditioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, Ishan Misra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Emu Video, a text-to-video generation model that factorizes the
generation into two steps: first generating an image conditioned on the text,
and then generating a video conditioned on the text and the generated image. We
identify critical design decisions--adjusted noise schedules for diffusion, and
multi-stage training--that enable us to directly generate high quality and high
resolution videos, without requiring a deep cascade of models as in prior work.
In human evaluations, our generated videos are strongly preferred in quality
compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's
PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial
solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing
approach naturally lends itself to animating images based on a user's text
prompt, where our generations are preferred 96% over prior work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://emu-video.metademolab.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfEval: Leveraging the discriminative nature of generative models for
  evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Saketh Rambhatla, Ishan Misra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we show that text-to-image generative models can be 'inverted'
to assess their own text-image understanding capabilities in a completely
automated manner.
  Our method, called SelfEval, uses the generative model to compute the
likelihood of real images given text prompts, making the generative model
directly applicable to discriminative tasks.
  Using SelfEval, we repurpose standard datasets created for evaluating
multimodal text-image discriminative models to evaluate generative models in a
fine-grained manner: assessing their performance on attribute binding, color
recognition, counting, shape recognition, spatial understanding.
  To the best of our knowledge SelfEval is the first automated metric to show a
high degree of agreement for measuring text-faithfulness with the gold-standard
human evaluations across multiple models and benchmarks.
  Moreover, SelfEval enables us to evaluate generative models on challenging
tasks such as Winoground image-score where they demonstrate competitive
performance to discriminative models.
  We also show severe drawbacks of standard automated metrics such as
CLIP-score to measure text faithfulness on benchmarks such as DrawBench, and
how SelfEval sidesteps these issues.
  We hope SelfEval enables easy and reliable automated evaluation for diffusion
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Representation Learning by Alternating Unimodal Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohui Zhang, Jaehong Yoon, Mohit Bansal, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning, which integrates data from diverse sensory modes, plays
a pivotal role in artificial intelligence. However, existing multimodal
learning methods often struggle with challenges where some modalities appear
more dominant than others during multimodal learning, resulting in suboptimal
performance. To address this challenge, we propose MLA (Multimodal Learning
with Alternating Unimodal Adaptation). MLA reframes the conventional joint
multimodal learning process by transforming it into an alternating unimodal
learning process, thereby minimizing interference between modalities.
Simultaneously, it captures cross-modal interactions through a shared head,
which undergoes continuous optimization across different modalities. This
optimization process is controlled by a gradient modification mechanism to
prevent the shared head from losing previously acquired information. During the
inference phase, MLA utilizes a test-time uncertainty-based model fusion
mechanism to integrate multimodal information. Extensive experiments are
conducted on five diverse datasets, encompassing scenarios with complete
modalities and scenarios with missing modalities. These experiments demonstrate
the superiority of MLA over competing prior approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpACNN-LDVAE: Spatial Attention Convolutional Latent Dirichlet
  Variational Autoencoder for Hyperspectral Pixel Unmixing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soham Chitnis, Kiran Mantripragada, Faisal Z. Qureshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Hyperspectral Unxming problem is to find the pure spectral signal of the
underlying materials (endmembers) and their proportions (abundances). The
proposed method builds upon the recently proposed method, Latent Dirichlet
Variational Autoencoder (LDVAE). It assumes that abundances can be encoded as
Dirichlet Distributions while mixed pixels and endmembers are represented by
Multivariate Normal Distributions. However, LDVAE does not leverage spatial
information present in an HSI; we propose an Isotropic CNN encoder with spatial
attention to solve the hyperspectral unmixing problem. We evaluated our model
on Samson, Hydice Urban, Cuprite, and OnTech-HSI-Syn-21 datasets. Our model
also leverages the transfer learning paradigm for Cuprite Dataset, where we
train the model on synthetic data and evaluate it on real-world data. We are
able to observe the improvement in the results for the endmember extraction and
abundance estimation by incorporating the spatial information. Code can be
found at https://github.com/faisalqureshi/cnn-ldvae
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using linear initialisation to improve speed of convergence and
  fully-trained error in Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Marais, Mate Hartstein, George Cevora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Good weight initialisation is an important step in successful training of
Artificial Neural Networks. Over time a number of improvements have been
proposed to this process. In this paper we introduce a novel weight
initialisation technique called the Straddled Matrix Initialiser. This
initialisation technique is motivated by our assumption that major,
global-scale relationships in data are linear with only smaller effects
requiring complex non-linearities. Combination of Straddled Matrix and ReLU
activation function initialises a Neural Network as a de facto linear model,
which we postulate should be a better starting point for optimisation given our
assumptions. We test this by training autoencoders on three datasets using
Straddled Matrix and seven other state-of-the-art weight initialisation
techniques. In all our experiments the Straddeled Matrix Initialiser clearly
outperforms all other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Versatile Medical Image Segmentation Learned from Multi-Source <span class="highlight-title">Dataset</span>s
  via Model Self-Disambiguation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyang Chen, Hao Zheng, Yuemeng Li, Yuncong Ma, Liang Ma, Hongming Li, Yong Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A versatile medical image segmentation model applicable to imaging data
collected with diverse equipment and protocols can facilitate model deployment
and maintenance. However, building such a model typically requires a large,
diverse, and fully annotated dataset, which is rarely available due to the
labor-intensive and costly data curation. In this study, we develop a
cost-efficient method by harnessing readily available data with partially or
even sparsely annotated segmentation labels. We devise strategies for model
self-disambiguation, prior knowledge incorporation, and imbalance mitigation to
address challenges associated with inconsistently labeled data from various
sources, including label ambiguity and imbalances across modalities, datasets,
and segmentation labels. Experimental results on a multi-modal dataset compiled
from eight different sources for abdominal organ segmentation have demonstrated
our method's effectiveness and superior performance over alternative
state-of-the-art methods, highlighting its potential for optimizing the use of
existing annotated data and reducing the annotation efforts for new data to
further enhance model capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-TexSeg: Unsupervised Segmentation of 3D Texture using Mutual
  <span class="highlight-title">Transformer</span> Learning <span class="chip">3DV-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iyyakutti Iyappan Ganapathi, Fayaz Ali, Sajid Javed, Syed Sadaf Ali, Naoufel Werghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analysis of the 3D Texture is indispensable for various tasks, such as
retrieval, segmentation, classification, and inspection of sculptures, knitted
fabrics, and biological tissues. A 3D texture is a locally repeated surface
variation independent of the surface's overall shape and can be determined
using the local neighborhood and its characteristics. Existing techniques
typically employ computer vision techniques that analyze a 3D mesh globally,
derive features, and then utilize the obtained features for retrieval or
classification. Several traditional and learning-based methods exist in the
literature, however, only a few are on 3D texture, and nothing yet, to the best
of our knowledge, on the unsupervised schemes. This paper presents an original
framework for the unsupervised segmentation of the 3D texture on the mesh
manifold. We approach this problem as binary surface segmentation, partitioning
the mesh surface into textured and non-textured regions without prior
annotation. We devise a mutual transformer-based system comprising a label
generator and a cleaner. The two models take geometric image representations of
the surface mesh facets and label them as texture or non-texture across an
iterative mutual learning scheme. Extensive experiments on three publicly
available datasets with diverse texture patterns demonstrate that the proposed
framework outperforms standard and SOTA unsupervised techniques and competes
reasonably with supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted in 3DV-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-trained Panoptic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shourya Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoptic segmentation is an important computer vision task which combines
semantic and instance segmentation. It plays a crucial role in domains of
medical image analysis, self-driving vehicles, and robotics by providing a
comprehensive understanding of visual environments. Traditionally, deep
learning panoptic segmentation models have relied on dense and accurately
annotated training data, which is expensive and time consuming to obtain.
Recent advancements in self-supervised learning approaches have shown great
potential in leveraging synthetic and unlabelled data to generate pseudo-labels
using self-training to improve the performance of instance and semantic
segmentation models. The three available methods for self-supervised panoptic
segmentation use proposal-based transformer architectures which are
computationally expensive, complicated and engineered for specific tasks. The
aim of this work is to develop a framework to perform embedding-based
self-supervised panoptic segmentation using self-training in a
synthetic-to-real domain adaptation problem setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Astronomical Images Quality Assessment with Automated Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Parisot, Pierrick Bruneau, Patrik Hitzelberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronically Assisted Astronomy consists in capturing deep sky images with
a digital camera coupled to a telescope to display views of celestial objects
that would have been invisible through direct observation. This practice
generates a large quantity of data, which may then be enhanced with dedicated
image editing software after observation sessions. In this study, we show how
Image Quality Assessment can be useful for automatically rating astronomical
images, and we also develop a dedicated model by using Automated Machine
Learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, accepted at DATA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CA-Jaccard: Camera-aware Jaccard Distance for Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyu Chen, Zheyi Fan, Zhaoru Chen, Yixuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person re-identification (re-ID) is a challenging task that aims to learn
discriminative features for person retrieval. In person re-ID, Jaccard distance
is a widely used distance metric, especially in re-ranking and clustering
scenarios. However, we discover that camera variation has a significant
negative impact on the reliability of Jaccard distance. In particular, Jaccard
distance calculates the distance based on the overlap of relevant neighbors.
Due to camera variation, intra-camera samples dominate the relevant neighbors,
which reduces the reliability of the neighbors by introducing intra-camera
negative samples and excluding inter-camera positive samples. To overcome this
problem, we propose a novel camera-aware Jaccard (CA-Jaccard) distance that
leverages camera information to enhance the reliability of Jaccard distance.
Specifically, we introduce camera-aware k-reciprocal nearest neighbors (CKRNNs)
to find k-reciprocal nearest neighbors on the intra-camera and inter-camera
ranking lists, which improves the reliability of relevant neighbors and
guarantees the contribution of inter-camera samples in the overlap. Moreover,
we propose a camera-aware local query expansion (CLQE) to exploit camera
variation as a strong constraint to mine reliable samples in relevant neighbors
and assign these samples higher weights in overlap to further improve the
reliability. Our CA-Jaccard distance is simple yet effective and can serve as a
general distance metric for person re-ID methods with high reliability and low
computational cost. Extensive experiments demonstrate the effectiveness of our
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Indoor Localization Using Crowdsourced Radio Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoguang Yi, Xiangyu Wen, Qiyue Xia, Peize Li, Francisco Zampella, Firas Alsehly, Chris Xiaoxuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indoor Positioning Systems (IPS) traditionally rely on odometry and building
infrastructures like WiFi, often supplemented by building floor plans for
increased accuracy. However, the limitation of floor plans in terms of
availability and timeliness of updates challenges their wide applicability. In
contrast, the proliferation of smartphones and WiFi-enabled robots has made
crowdsourced radio maps - databases pairing locations with their corresponding
Received Signal Strengths (RSS) - increasingly accessible. These radio maps not
only provide WiFi fingerprint-location pairs but encode movement regularities
akin to the constraints imposed by floor plans. This work investigates the
possibility of leveraging these radio maps as a substitute for floor plans in
multimodal IPS. We introduce a new framework to address the challenges of radio
map inaccuracies and sparse coverage. Our proposed system integrates an
uncertainty-aware neural network model for WiFi localization and a bespoken
Bayesian fusion technique for optimal fusion. Extensive evaluations on multiple
real-world sites indicate a significant performance enhancement, with results
showing ~ 25% improvement over the best baseline
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Détection d'objets célestes dans des images astronomiques par IA
  explicable 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Parisot, Mahmoud Jaziri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amateur and professional astronomers can easily capture a large number of
deep sky images with recent smart telescopes. However, afterwards verification
is still required to check whether the celestial objects targeted are actually
visible in the images produced. Depending on the magnitude of the targets, the
observation conditions and the time during which the data is captured, it is
possible that only stars are present in the images. In this study, we propose
an approach based on explainable Artificial Intelligence to automatically
detect the presence and position of captured objects. -- --
  Gr\^ace \`a l'apport des t\'elescopes automatis\'es grand public, les
astronomes amateurs et professionnels peuvent capturer facilement une grande
quantit\'e d'images du ciel profond (comme par exemple les galaxies,
n\'ebuleuses, ou amas globulaires). N\'eanmoins, une v\'erification reste
n\'ecessaire \`a post\'eriori pour v\'erifier si les objets c\'elestes vis\'es
sont effectivement visibles dans les images produites: cela d\'epend notamment
de la magnitude des cibles, des conditions d'observation mais aussi de la
dur\'ee pendant laquelle les donn\'ees sont captur\'ees. Dans cette \'etude,
nous proposons une approche bas\'ee sur l'IA explicable pour d\'etecter
automatiquement la pr\'esence et la position des objets captur\'es.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, in French, accepted in short version for EGC2024 (24\`eme
  conf\'erence francophone sur l'Extraction et la Gestion des Connaissances)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOCAL: A Cost-Aware Video <span class="highlight-title">Dataset</span> for Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Kokilepersaud, Yash-Yee Logan, Ryan Benkert, Chen Zhou, Mohit Prabhushankar, Ghassan AlRegib, Enrique Corona, Kunjan Singh, Mostafa Parchami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce the FOCAL (Ford-OLIVES Collaboration on Active
Learning) dataset which enables the study of the impact of annotation-cost
within a video active learning setting. Annotation-cost refers to the time it
takes an annotator to label and quality-assure a given video sequence. A
practical motivation for active learning research is to minimize
annotation-cost by selectively labeling informative samples that will maximize
performance within a given budget constraint. However, previous work in video
active learning lacks real-time annotation labels for accurately assessing cost
minimization and instead operates under the assumption that annotation-cost
scales linearly with the amount of data to annotate. This assumption does not
take into account a variety of real-world confounding factors that contribute
to a nonlinear cost such as the effect of an assistive labeling tool and the
variety of interactions within a scene such as occluded objects, weather, and
motion of objects. FOCAL addresses this discrepancy by providing real
annotation-cost labels for 126 video sequences across 69 unique city scenes
with a variety of weather, lighting, and seasonal conditions. We also introduce
a set of conformal active learning algorithms that take advantage of the
sequential structure of video data in order to achieve a better trade-off
between annotation-cost and performance while also reducing floating point
operations (FLOPS) overhead by at least 77.67%. We show how these approaches
better reflect how annotations on videos are done in practice through a
sequence selection framework. We further demonstrate the advantage of these
approaches by introducing two performance-cost metrics and show that the best
conformal active learning method is cheaper than the best traditional active
learning method by 113 hours.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted as a main conference paper at the IEEE
  International Conference on Big Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human motion trajectory prediction using the Social Force Model for
  real-time and low computational cost applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Gil, Alberto Sanfeliu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion trajectory prediction is a very important functionality for
human-robot collaboration, specifically in accompanying, guiding, or
approaching tasks, but also in social robotics, self-driving vehicles, or
security systems. In this paper, a novel trajectory prediction model, Social
Force Generative Adversarial Network (SoFGAN), is proposed. SoFGAN uses a
Generative Adversarial Network (GAN) and Social Force Model (SFM) to generate
different plausible people trajectories reducing collisions in a scene.
Furthermore, a Conditional Variational Autoencoder (CVAE) module is added to
emphasize the destination learning. We show that our method is more accurate in
making predictions in UCY or BIWI datasets than most of the current
state-of-the-art models and also reduces collisions in comparison to other
approaches. Through real-life experiments, we demonstrate that the model can be
used in real-time without GPU's to perform good quality predictions with a low
computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSB: Simple but Strong Baseline for Boosting Performance of Open-Set
  Semi-Supervised Learning <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Fan, Anna Kukleva, Dengxin Dai, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods effectively leverage unlabeled data to
improve model generalization. However, SSL models often underperform in
open-set scenarios, where unlabeled data contain outliers from novel categories
that do not appear in the labeled set. In this paper, we study the challenging
and realistic open-set SSL setting, where the goal is to both correctly
classify inliers and to detect outliers. Intuitively, the inlier classifier
should be trained on inlier data only. However, we find that inlier
classification performance can be largely improved by incorporating
high-confidence pseudo-labeled data, regardless of whether they are inliers or
outliers. Also, we propose to utilize non-linear transformations to separate
the features used for inlier classification and outlier detection in the
multi-task learning framework, preventing adverse effects between them.
Additionally, we introduce pseudo-negative mining, which further boosts outlier
detection performance. The three ingredients lead to what we call Simple but
Strong Baseline (SSB) for open-set SSL. In experiments, SSB greatly improves
both inlier classification and outlier detection performance, outperforming
existing methods by a large margin. Our code will be released at
https://github.com/YUE-FAN/SSB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted in ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phase Guided Light Field for Spatial-Depth High Resolution 3D Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geyou Zhang, Ce Zhu, Kai Liu, Yipeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On 3D imaging, light field cameras typically are of single shot, and however,
they heavily suffer from low spatial resolution and depth accuracy. In this
paper, by employing an optical projector to project a group of single
high-frequency phase-shifted sinusoid patterns, we propose a phase guided light
field algorithm to significantly improve both the spatial and depth resolutions
for off-the-shelf light field cameras. First, for correcting the axial
aberrations caused by the main lens of our light field camera, we propose a
deformed cone model to calibrate our structured light field system. Second,
over wrapped phases computed from patterned images, we propose a stereo
matching algorithm, i.e. phase guided sum of absolute difference, to robustly
obtain the correspondence for each pair of neighbored two lenslets. Finally, by
introducing a virtual camera according to the basic geometrical optics of light
field imaging, we propose a reorganization strategy to reconstruct 3D point
clouds with spatial-depth high resolution. Experimental results show that,
compared with the state-of-the-art active light field methods, the proposed
reconstructs 3D point clouds with a spatial resolution of 1280$\times$720 with
factors 10$\times$ increased, while maintaining the same high depth resolution
and needing merely a single group of high-frequency patterns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Archtree: on-the-fly tree-structured exploration for latency-aware
  pruning of deep neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rémi Ouazan Reboul, Edouard Yvinec, Arnaud Dapogny, Kevin Bailly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have become ubiquitous in addressing a number of
problems, particularly in computer vision. However, DNN inference is
computationally intensive, which can be prohibitive e.g. when considering edge
devices. To solve this problem, a popular solution is DNN pruning, and more so
structured pruning, where coherent computational blocks (e.g. channels for
convolutional networks) are removed: as an exhaustive search of the space of
pruned sub-models is intractable in practice, channels are typically removed
iteratively based on an importance estimation heuristic. Recently, promising
latency-aware pruning methods were proposed, where channels are removed until
the network reaches a target budget of wall-clock latency pre-emptively
estimated on specific hardware. In this paper, we present Archtree, a novel
method for latency-driven structured pruning of DNNs. Archtree explores
multiple candidate pruned sub-models in parallel in a tree-like fashion,
allowing for a better exploration of the search space. Furthermore, it involves
on-the-fly latency estimation on the target hardware, accounting for closer
latencies as compared to the specified budget. Empirical results on several DNN
architectures and target hardware show that Archtree better preserves the
original model accuracy while better fitting the latency budget as compared to
existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint covariance property under geometric image transformations for
  spatio-temporal receptive fields according to generalized Gaussian model for
  receptive fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Lindeberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The influence of natural image transformations on receptive field responses
is crucial for modelling visual operations in computer vision and biological
vision. In this regard, covariance properties with respect to geometric image
transformations in the earliest layers of the visual hierarchy are essential
for expressing robust image operations and for formulating invariant visual
operations at higher levels. This paper defines and proves a joint covariance
property under compositions of spatial scaling transformations, spatial affine
transformations, Galilean transformations and temporal scaling transformations,
which makes it possible to characterize how different types of image
transformations interact with each other. Specifically, the derived relations
show the receptive field parameters need to be transformed, in order to match
the output from spatio-temporal receptive fields with the underlying
spatio-temporal image transformations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segment Anything Model with Uncertainty Rectification for Auto-<span class="highlight-title">Prompt</span>ing
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Shiyao Hu, Chen Jiang, Yuan Cheng, Yuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of the Segment Anything Model (SAM) has marked a significant
advancement in prompt-driven image segmentation. However, SAM's application to
medical image segmentation requires manual prompting of target structures to
obtain acceptable performance, which is still labor-intensive. Despite attempts
of auto-prompting to turn SAM into a fully automatic manner, it still exhibits
subpar performance and lacks of reliability in the field of medical imaging. In
this paper, we propose UR-SAM, an uncertainty rectified SAM framework to
enhance the robustness and reliability for auto-prompting medical image
segmentation. Our method incorporates a prompt augmentation module to estimate
the distribution of predictions and generate uncertainty maps, and an
uncertainty-based rectification module to further enhance the performance of
SAM. Extensive experiments on two public 3D medical datasets covering the
segmentation of 35 organs demonstrate that without supplementary training or
fine-tuning, our method further improves the segmentation performance with up
to 10.7 % and 13.8 % in dice similarity coefficient, demonstrating efficiency
and broad capabilities for medical image segmentation without manual prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Removing Adverse Volumetric Effects From Trained Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas L. Teigen, Mauhing Yip, Victor P. Hamran, Vegard Skui, Annette Stahl, Rudolf Mester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the use of neural radiance fields (NeRFs) in different challenging
settings has been explored, only very recently have there been any
contributions that focus on the use of NeRF in foggy environments. We argue
that the traditional NeRF models are able to replicate scenes filled with fog
and propose a method to remove the fog when synthesizing novel views. By
calculating the global contrast of a scene, we can estimate a density threshold
that, when applied, removes all visible fog. This makes it possible to use NeRF
as a way of rendering clear views of objects of interest located in fog-filled
environments. Additionally, to benchmark performance on such scenes, we
introduce a new dataset that expands some of the original synthetic NeRF scenes
through the addition of fog and natural environments. The code, dataset, and
video results can be found on our project page: https://vegardskui.com/fognerf/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Object Coherence in Layout-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel's generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind the map! Accounting for existing map information when estimating
  online HDMaps from sensor data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rémy Sun, Li Yang, Diane Lingrand, Frédéric Precioso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online High Definition Map (HDMap) estimation from sensors offers a low-cost
alternative to manually acquired HDMaps. As such, it promises to lighten costs
for already HDMap-reliant Autonomous Driving systems, and potentially even
spread their use to new systems. In this paper, we propose to improve online
HDMap estimation by accounting for already existing maps. We identify 3
reasonable types of useful existing maps (minimalist, noisy, and outdated). We
also introduce MapEX, a novel online HDMap estimation framework that accounts
for existing maps. MapEX achieves this by encoding map elements into query
tokens and by refining the matching algorithm used to train classic query based
map estimation models. We demonstrate that MapEX brings significant
improvements on the nuScenes dataset. For instance, MapEX - given noisy maps -
improves by 38% over the MapTRv2 detector it is based on and by 16% over the
current SOTA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Framework of Landsat-8 Band Selection based on UMDA for Deforestation
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo B. Neto, Paulo R. C. Pedro, Alvaro Fazenda, Fabio A. Faria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conservation of tropical forests is a current subject of social and
ecological relevance due to their crucial role in the global ecosystem.
Unfortunately, millions of hectares are deforested and degraded each year.
Therefore, government or private initiatives are needed for monitoring tropical
forests. In this sense, this work proposes a novel framework, which uses of
distribution estimation algorithm (UMDA) to select spectral bands from
Landsat-8 that yield a better representation of deforestation areas to guide a
semantic segmentation architecture called DeepLabv3+. In performed experiments,
it was possible to find several compositions that reach balanced accuracy
superior to 90% in segment classification tasks. Furthermore, the best
composition (651) found by UMDA algorithm fed the DeepLabv3+ architecture and
surpassed in efficiency and effectiveness all compositions compared in this
work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in Portuguese language. Best Paper Award at the Workshop of
  Undergraduate Works (WUW), SIBGRAPI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Relay System for Semantic Image Transmission based on Shared Feature
  Extraction and Hyperprior Entropy Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wannian An, Zhicheng Bao, Haotai Liang, Chen Dong,  Xiaodong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, the need for high-quality image reconstruction and restoration is
more and more urgent. However, most image transmission systems may suffer from
image quality degradation or transmission interruption in the face of
interference such as channel noise and link fading. To solve this problem, a
relay communication network for semantic image transmission based on shared
feature extraction and hyperprior entropy compression (HEC) is proposed, where
the shared feature extraction technology based on Pearson correlation is
proposed to eliminate partial shared feature of extracted semantic latent
feature. In addition, the HEC technology is used to resist the effect of
channel noise and link fading and carried out respectively at the source node
and the relay node. Experimental results demonstrate that compared with other
recent research methods, the proposed system has lower transmission overhead
and higher semantic image transmission performance. Particularly, under the
same conditions, the multi-scale structural similarity (MS-SSIM) of this system
is superior to the comparison method by approximately 0.2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRCSyn Challenge at WACV 2024:Face Recognition Challenge in the Era of
  Synthetic Data <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Melzi, Ruben Tolosana, Ruben Vera-Rodriguez, Minchul Kim, Christian Rathgeb, Xiaoming Liu, Ivan DeAndres-Tame, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia, Weisong Zhao, Xiangyu Zhu, Zheyu Yan, Xiao-Yu Zhang, Jinlin Wu, Zhen Lei, Suvidha Tripathi, Mahak Kothari, Md Haider Zama, Debayan Deb, Bernardo Biesseck, Pedro Vidal, Roger Granada, Guilherme Fickel, Gustavo Führ, David Menotti, Alexander Unnervik, Anjith George, Christophe Ecabert, Hatef Otroshi Shahreza, Parsa Rahimi, Sébastien Marcel, Ioannis Sarridis, Christos Koutlis, Georgia Baltsou, Symeon Papadopoulos, Christos Diou, Nicolò Di Domenico, Guido Borghi, Lorenzo Pellegrini, Enrique Mas-Candela, Ángela Sánchez-Pérez, Andrea Atzori, Fadi Boutros, Naser Damer, Gianni Fenu, Mirko Marras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread adoption of face recognition technology around the
world, and its remarkable performance on current benchmarks, there are still
several challenges that must be covered in more detail. This paper offers an
overview of the Face Recognition Challenge in the Era of Synthetic Data
(FRCSyn) organized at WACV 2024. This is the first international challenge
aiming to explore the use of synthetic data in face recognition to address
existing limitations in the technology. Specifically, the FRCSyn Challenge
targets concerns related to data privacy issues, demographic biases,
generalization to unseen scenarios, and performance limitations in challenging
scenarios, including significant age disparities between enrollment and
testing, pose variations, and occlusions. The results achieved in the FRCSyn
Challenge, together with the proposed benchmark, contribute significantly to
the application of synthetic data to improve face recognition technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure, WACV 2024 Workshops</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-end autoencoding architecture for the simultaneous generation of
  medical images and corresponding segmentation masks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aghiles Kebaili, Jérôme Lapuyade-Lahorgue, Pierre Vera, Su Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the increasing use of deep learning in medical image segmentation,
acquiring sufficient training data remains a challenge in the medical field. In
response, data augmentation techniques have been proposed; however, the
generation of diverse and realistic medical images and their corresponding
masks remains a difficult task, especially when working with insufficient
training sets. To address these limitations, we present an end-to-end
architecture based on the Hamiltonian Variational Autoencoder (HVAE). This
approach yields an improved posterior distribution approximation compared to
traditional Variational Autoencoders (VAE), resulting in higher image
generation quality. Our method outperforms generative adversarial architectures
under data-scarce conditions, showcasing enhancements in image quality and
precise tumor mask synthesis. We conduct experiments on two publicly available
datasets, MICCAI's Brain Tumor Segmentation Challenge (BRATS), and Head and
Neck Tumor Segmentation Challenge (HECKTOR), demonstrating the effectiveness of
our method on different medical imaging modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correlation-Distance Graph Learning for Treatment Response Prediction
  from rs-fMRI <span class="chip">ICONIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiatian Zhang, Sisi Zheng, Hubert P. H. Shum, Haozheng Zhang, Nan Song, Mingkang Song, Hongxiao Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resting-state fMRI (rs-fMRI) functional connectivity (FC) analysis provides
valuable insights into the relationships between different brain regions and
their potential implications for neurological or psychiatric disorders.
However, specific design efforts to predict treatment response from rs-fMRI
remain limited due to difficulties in understanding the current brain state and
the underlying mechanisms driving the observed patterns, which limited the
clinical application of rs-fMRI. To overcome that, we propose a graph learning
framework that captures comprehensive features by integrating both correlation
and distance-based similarity measures under a contrastive loss. This approach
results in a more expressive framework that captures brain dynamic features at
different scales and enables more accurate prediction of treatment response.
Our experiments on the chronic pain and depersonalization disorder datasets
demonstrate that our proposed method outperforms current methods in different
scenarios. To the best of our knowledge, we are the first to explore the
integration of distance-based and correlation-based neural similarity into
graph learning for treatment response prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 2023 International Conference on Neural
  Information Processing (ICONIP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepClean: Machine Unlearning on the Cheap by Resetting Privacy
  Sensitive Weights using the Fisher Diagonal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaeli Shi, Najah Ghalyan, Kostis Gourgoulias, John Buford, Sean Moran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models trained on sensitive or private data can
inadvertently memorize and leak that information. Machine unlearning seeks to
retroactively remove such details from model weights to protect privacy. We
contribute a lightweight unlearning algorithm that leverages the Fisher
Information Matrix (FIM) for selective forgetting. Prior work in this area
requires full retraining or large matrix inversions, which are computationally
expensive. Our key insight is that the diagonal elements of the FIM, which
measure the sensitivity of log-likelihood to changes in weights, contain
sufficient information for effective forgetting. Specifically, we compute the
FIM diagonal over two subsets -- the data to retain and forget -- for all
trainable weights. This diagonal representation approximates the complete FIM
while dramatically reducing computation. We then use it to selectively update
weights to maximize forgetting of the sensitive subset while minimizing impact
on the retained subset. Experiments show that our algorithm can successfully
forget any randomly selected subsets of training data across neural network
architectures. By leveraging the FIM diagonal, our approach provides an
interpretable, lightweight, and efficient solution for machine unlearning with
practical privacy benefits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DUA-DA: Distillation-based Unbiased Alignment for Domain Adaptive Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Feng, Shiwei Li, Yingjie Gao, Ziyue Huang, Yanan Zhang, Qingjie Liu, Yunhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though feature-alignment based Domain Adaptive Object Detection (DAOD) have
achieved remarkable progress, they ignore the source bias issue, i.e. the
aligned features are more favorable towards the source domain, leading to a
sub-optimal adaptation. Furthermore, the presence of domain shift between the
source and target domains exacerbates the problem of inconsistent
classification and localization in general detection pipelines. To overcome
these challenges, we propose a novel Distillation-based Unbiased Alignment
(DUA) framework for DAOD, which can distill the source features towards a more
balanced position via a pre-trained teacher model during the training process,
alleviating the problem of source bias effectively. In addition, we design a
Target-Relevant Object Localization Network (TROLN), which can mine
target-related knowledge to produce two classification-free metrics (IoU and
centerness). Accordingly, we implement a Domain-aware Consistency Enhancing
(DCE) strategy that utilizes these two metrics to further refine classification
confidences, achieving a harmonization between classification and localization
in cross-domain scenarios. Extensive experiments have been conducted to
manifest the effectiveness of this method, which consistently improves the
strong baseline by large margins, outperforming existing alignment-based works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Residual CNN for Multi-Class Chest Infection Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Donghan Kwon, Dohyun Lim, Yoonha Lee, Seung Won Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of deep learning has significantly propelled the capabilities of
automated medical image diagnosis, providing valuable tools and resources in
the realm of healthcare and medical diagnostics. This research delves into the
development and evaluation of a Deep Residual Convolutional Neural Network
(CNN) for the multi-class diagnosis of chest infections, utilizing chest X-ray
images. The implemented model, trained and validated on a dataset amalgamated
from diverse sources, demonstrated a robust overall accuracy of 93%. However,
nuanced disparities in performance across different classes, particularly
Fibrosis, underscored the complexity and challenges inherent in automated
medical image diagnosis. The insights derived pave the way for future research,
focusing on enhancing the model's proficiency in classifying conditions that
present more subtle and nuanced visual features in the images, as well as
optimizing and refining the model architecture and training process. This paper
provides a comprehensive exploration into the development, implementation, and
evaluation of the model, offering insights and directions for future research
and development in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning based CNN Model for Classification and Detection of
  Individuals Wearing Face Mask 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Chinnaiyan, Iyyappan M, Al Raiyan Shariff A, Kondaveeti Sai, Mallikarjunaiah B M, P Bharath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to the global COVID-19 pandemic, there has been a critical demand
for protective measures, with face masks emerging as a primary safeguard. The
approach involves a two-fold strategy: first, recognizing the presence of a
face by detecting faces, and second, identifying masks on those faces. This
project utilizes deep learning to create a model that can detect face masks in
real-time streaming video as well as images. Face detection, a facet of object
detection, finds applications in diverse fields such as security, biometrics,
and law enforcement. Various detector systems worldwide have been developed and
implemented, with convolutional neural networks chosen for their superior
performance accuracy and speed in object detection. Experimental results attest
to the model's excellent accuracy on test data. The primary focus of this
research is to enhance security, particularly in sensitive areas. The research
paper proposes a rapid image pre-processing method with masks centred on faces.
Employing feature extraction and Convolutional Neural Network, the system
classifies and detects individuals wearing masks. The research unfolds in three
stages: image pre-processing, image cropping, and image classification,
collectively contributing to the identification of masked faces. Continuous
surveillance through webcams or CCTV cameras ensures constant monitoring,
triggering a security alert if a person is detected without a mask.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages , 6 figures , 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimized Deep Learning Models for AUV Seabed Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajesh Sharma R, Akey Sungheetha, Chinnaiyan R
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using autonomous underwater vehicles, or AUVs, has completely changed how we
gather data from the ocean floor. AUV innovation has advanced significantly,
especially in the analysis of images, due to the increasing need for accurate
and efficient seafloor mapping. This blog post provides a detailed summary and
comparison of the most current advancements in AUV seafloor image processing.
We will go into the realm of undersea technology, covering everything through
computer and algorithmic advancements to advances in sensors and cameras. After
reading this page through to the end, you will have a solid understanding of
the most up-to-date techniques and tools for using AUVs to process seabed
photos and how they could further our comprehension of the ocean floor
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages , 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-Factor Authentication Approach Based on Behavior Patterns for
  Defeating Puppet Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Wang, Guyue Li, Zhiming Chu, Haobo Li, Daniele Faccio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fingerprint traits are widely recognized for their unique qualities and
security benefits. Despite their extensive use, fingerprint features can be
vulnerable to puppet attacks, where attackers manipulate a reluctant but
genuine user into completing the authentication process. Defending against such
attacks is challenging due to the coexistence of a legitimate identity and an
illegitimate intent. In this paper, we propose PUPGUARD, a solution designed to
guard against puppet attacks. This method is based on user behavioral patterns,
specifically, the user needs to press the capture device twice successively
with different fingers during the authentication process. PUPGUARD leverages
both the image features of fingerprints and the timing characteristics of the
pressing intervals to establish two-factor authentication. More specifically,
after extracting image features and timing characteristics, and performing
feature selection on the image features, PUPGUARD fuses these two features into
a one-dimensional feature vector, and feeds it into a one-class classifier to
obtain the classification result. This two-factor authentication method
emphasizes dynamic behavioral patterns during the authentication process,
thereby enhancing security against puppet attacks. To assess PUPGUARD's
effectiveness, we conducted experiments on datasets collected from 31 subjects,
including image features and timing characteristics. Our experimental results
demonstrate that PUPGUARD achieves an impressive accuracy rate of 97.87% and a
remarkably low false positive rate (FPR) of 1.89%. Furthermore, we conducted
comparative experiments to validate the superiority of combining image features
and timing characteristics within PUPGUARD for enhancing resistance against
puppet attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-Shot and Multi-Shot Feature Learning for Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Li, Sanping Zhou, Zheng Qin, Le Wang, Jinjun Wang, Nanning Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Object Tracking (MOT) remains a vital component of intelligent video
analysis, which aims to locate targets and maintain a consistent identity for
each target throughout a video sequence. Existing works usually learn a
discriminative feature representation, such as motion and appearance, to
associate the detections across frames, which are easily affected by mutual
occlusion and background clutter in practice. In this paper, we propose a
simple yet effective two-stage feature learning paradigm to jointly learn
single-shot and multi-shot features for different targets, so as to achieve
robust data association in the tracking process. For the detections without
being associated, we design a novel single-shot feature learning module to
extract discriminative features of each detection, which can efficiently
associate targets between adjacent frames. For the tracklets being lost several
frames, we design a novel multi-shot feature learning module to extract
discriminative features of each tracklet, which can accurately refind these
lost targets after a long period. Once equipped with a simple data association
logic, the resulting VisualTracker can perform robust MOT based on the
single-shot and multi-shot feature representations. Extensive experimental
results demonstrate that our method has achieved significant improvements on
MOT17 and MOT20 datasets while reaching state-of-the-art performance on
DanceTrack dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSE-Nets: Multi-annotated Semi-supervised Ensemble Networks for
  Improving Segmentation of Medical Image with Ambiguous Boundaries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Wang, Tengjin Weng, Jingyi Wang, Yang Shen, Zhidong Zhao, Yixiu Liu, Pengfei Jiao, Zhiming Cheng, Yaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation annotations exhibit variations among experts due
to the ambiguous boundaries of segmented objects and backgrounds in medical
images. Although using multiple annotations for each image in the
fully-supervised has been extensively studied for training deep models,
obtaining a large amount of multi-annotated data is challenging due to the
substantial time and manpower costs required for segmentation annotations,
resulting in most images lacking any annotations. To address this, we propose
Multi-annotated Semi-supervised Ensemble Networks (MSE-Nets) for learning
segmentation from limited multi-annotated and abundant unannotated data.
Specifically, we introduce the Network Pairwise Consistency Enhancement (NPCE)
module and Multi-Network Pseudo Supervised (MNPS) module to enhance MSE-Nets
for the segmentation task by considering two major factors: (1) to optimize the
utilization of all accessible multi-annotated data, the NPCE separates
(dis)agreement annotations of multi-annotated data at the pixel level and
handles agreement and disagreement annotations in different ways, (2) to
mitigate the introduction of imprecise pseudo-labels, the MNPS extends the
training data by leveraging consistent pseudo-labels from unannotated data.
Finally, we improve confidence calibration by averaging the predictions of base
networks. Experiments on the ISIC dataset show that we reduced the demand for
multi-annotated data by 97.75\% and narrowed the gap with the best
fully-supervised baseline to just a Jaccard index of 4\%. Furthermore, compared
to other semi-supervised methods that rely only on a single annotation or a
combined fusion approach, the comprehensive experimental results on ISIC and
RIGA datasets demonstrate the superior performance of our proposed method in
medical image segmentation with ambiguous boundaries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Temporal Consistency: Generating Video Universal Adversarial
  Perturbations Using Image Models <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee-Seon Kim, Minji Son, Minbeom Kim, Myung-Joon Kwon, Changick Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As video analysis using deep learning models becomes more widespread, the
vulnerability of such models to adversarial attacks is becoming a pressing
concern. In particular, Universal Adversarial Perturbation (UAP) poses a
significant threat, as a single perturbation can mislead deep learning models
on entire datasets. We propose a novel video UAP using image data and image
model. This enables us to take advantage of the rich image data and image
model-based studies available for video applications. However, there is a
challenge that image models are limited in their ability to analyze the
temporal aspects of videos, which is crucial for a successful video attack. To
address this challenge, we introduce the Breaking Temporal Consistency (BTC)
method, which is the first attempt to incorporate temporal information into
video attacks using image models. We aim to generate adversarial videos that
have opposite patterns to the original. Specifically, BTC-UAP minimizes the
feature similarity between neighboring frames in videos. Our approach is simple
but effective at attacking unseen video models. Additionally, it is applicable
to videos of varying lengths and invariant to temporal shifts. Our approach
surpasses existing methods in terms of effectiveness on various datasets,
including ImageNet, UCF-101, and Kinetics-400.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dates Fruit Disease Recognition using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ghassen Ben Brahim, Jaafar Alghazo, Ghazanfar Latif, Khalid Alnujaidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many countries such as Saudi Arabia, Morocco and Tunisia are among the top
exporters and consumers of palm date fruits. Date fruit production plays a
major role in the economies of the date fruit exporting countries. Date fruits
are susceptible to disease just like any fruit and early detection and
intervention can end up saving the produce. However, with the vast farming
lands, it is nearly impossible for farmers to observe date trees on a frequent
basis for early disease detection. In addition, even with human observation the
process is prone to human error and increases the date fruit cost. With the
recent advances in computer vision, machine learning, drone technology, and
other technologies; an integrated solution can be proposed for the automatic
detection of date fruit disease. In this paper, a hybrid features based method
with the standard classifiers is proposed based on the extraction of L*a*b
color features, statistical features, and Discrete Wavelet Transform (DWT)
texture features for the early detection and classification of date fruit
disease. A dataset was developed for this work consisting of 871 images divided
into the following classes; Healthy date, Initial stage of disease,
Malnourished date, and Parasite infected. The extracted features were input to
common classifiers such as the Random Forest (RF), Multilayer Perceptron (MLP),
Na\"ive Bayes (NB), and Fuzzy Decision Trees (FDT). The highest average
accuracy was achieved when combining the L*a*b, Statistical, and DWT Features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video-based Sequential Bayesian Homography Estimation for Soccer Field
  Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul J. Claasen, J. P. de Villiers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel Bayesian framework is proposed, which explicitly relates the
homography of one video frame to the next through an affine transformation
while explicitly modelling keypoint uncertainty. The literature has previously
used differential homography between subsequent frames, but not in a Bayesian
setting. In cases where Bayesian methods have been applied, camera motion is
not adequately modelled, and keypoints are treated as deterministic. The
proposed method, Bayesian Homography Inference from Tracked Keypoints (BHITK),
employs a two-stage Kalman filter and significantly improves existing methods.
Existing keypoint detection methods may be easily augmented with BHITK. It
enables less sophisticated and less computationally expensive methods to
outperform the state-of-the-art approaches in most homography evaluation
metrics. Furthermore, the homography annotations of the WorldCup and
TS-WorldCup datasets have been refined using a custom homography annotation
tool released for public use. The refined datasets are consolidated and
released as the consolidated and refined WorldCup (CARWC) dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Expert Systems with Applications and currently under
  review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Garment Recovery with Shape and Deformation Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ren Li, Corentin Dumery, Benoît Guillard, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While modeling people wearing tight-fitting clothing has made great strides
in recent years, loose-fitting clothing remains a challenge. We propose a
method that delivers realistic garment models from real-world images,
regardless of garment shape or deformation. To this end, we introduce a fitting
approach that utilizes shape and deformation priors learned from synthetic data
to accurately capture garment shapes and deformations, including large ones.
Not only does our approach recover the garment geometry accurately, it also
yields models that can be directly used by downstream applications such as
animation and simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pseudo Label-Guided Data Fusion and Output Consistency for
  Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wang, Yuanbin Chen, Xinlin Zhang, Yuanbo Zhou, Junlin Lan, Bizhe Bai, Tao Tan, Min Du, Qinquan Gao, Tong Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised learning algorithms based on Convolutional Neural Networks have
become the benchmark for medical image segmentation tasks, but their
effectiveness heavily relies on a large amount of labeled data. However,
annotating medical image datasets is a laborious and time-consuming process.
Inspired by semi-supervised algorithms that use both labeled and unlabeled data
for training, we propose the PLGDF framework, which builds upon the mean
teacher network for segmenting medical images with less annotation. We propose
a novel pseudo-label utilization scheme, which combines labeled and unlabeled
data to augment the dataset effectively. Additionally, we enforce the
consistency between different scales in the decoder module of the segmentation
network and propose a loss function suitable for evaluating the consistency.
Moreover, we incorporate a sharpening operation on the predicted results,
further enhancing the accuracy of the segmentation.
  Extensive experiments on three publicly available datasets demonstrate that
the PLGDF framework can largely improve performance by incorporating the
unlabeled data. Meanwhile, our framework yields superior performance compared
to six state-of-the-art semi-supervised learning methods. The codes of this
study are available at https://github.com/ortonwang/PLGDF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Student Engagement in Online Learning through Facial
  Expression Analysis and Complex Emotion Recognition using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rekha R Nair, Tina Babu, Pavithra K
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to the COVID-19 pandemic, traditional physical classrooms have
transitioned to online environments, necessitating effective strategies to
ensure sustained student engagement. A significant challenge in online teaching
is the absence of real-time feedback from teachers on students learning
progress. This paper introduces a novel approach employing deep learning
techniques based on facial expressions to assess students engagement levels
during online learning sessions. Human emotions cannot be adequately conveyed
by a student using only the basic emotions, including anger, disgust, fear,
joy, sadness, surprise, and neutrality. To address this challenge, proposed a
generation of four complex emotions such as confusion, satisfaction,
disappointment, and frustration by combining the basic emotions. These complex
emotions are often experienced simultaneously by students during the learning
session. To depict these emotions dynamically,utilized a continuous stream of
image frames instead of discrete images. The proposed work utilized a
Convolutional Neural Network (CNN) model to categorize the fundamental
emotional states of learners accurately. The proposed CNN model demonstrates
strong performance, achieving a 95% accuracy in precise categorization of
learner emotions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Face emotion recognition work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A2XP: Towards Private Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geunhyeok Yu, Hyoseok Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) have become pivotal in various fields, especially
in computer vision, outperforming previous methodologies. A critical challenge
in their deployment is the bias inherent in data across different domains, such
as image style, and environmental conditions, leading to domain gaps. This
necessitates techniques for learning general representations from biased
training data, known as domain generalization. This paper presents Attend to
eXpert Prompts (A2XP), a novel approach for domain generalization that
preserves the privacy and integrity of the network architecture. A2XP consists
of two phases: Expert Adaptation and Domain Generalization. In the first phase,
prompts for each source domain are optimized to guide the model towards the
optimal direction. In the second phase, two embedder networks are trained to
effectively amalgamate these expert prompts, aiming for an optimal output. Our
extensive experiments demonstrate that A2XP achieves state-of-the-art results
over existing non-private domain generalization methods. The experimental
results validate that the proposed approach not only tackles the domain
generalization challenge in DNNs but also offers a privacy-preserving,
efficient solution to the broader field of computer vision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages (8 pages except for references), 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooperative Perception with Learning-Based V2V communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenguang Liu, Yunfei Chen, Jianjun Chen, Ryan Payton, Michael Riley, Shuang-Hua Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperative perception has been widely used in autonomous driving to
alleviate the inherent limitation of single automated vehicle perception. To
enable cooperation, vehicle-to-vehicle (V2V) communication plays an
indispensable role. This work analyzes the performance of cooperative
perception accounting for communications channel impairments. Different fusion
methods and channel impairments are evaluated. A new late fusion scheme is
proposed to leverage the robustness of intermediate features. In order to
compress the data size incurred by cooperation, a convolution neural
network-based autoencoder is adopted. Numerical results demonstrate that
intermediate fusion is more robust to channel impairments than early fusion and
late fusion, when the SNR is greater than 0 dB. Also, the proposed fusion
scheme outperforms the conventional late fusion using detection outputs, and
autoencoder provides a good compromise between detection accuracy and bandwidth
usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Multimodal Fusion for Enhanced Diagnosis of Multiple Retinal
  Diseases in Ultra-wide OCTA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wei, Peilun Shi, Guitao Bai, Minqing Zhang, Shuangle Li, Wu Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultra-wide optical coherence tomography angiography (UW-OCTA) is an emerging
imaging technique that offers significant advantages over traditional OCTA by
providing an exceptionally wide scanning range of up to 24 x 20 $mm^{2}$,
covering both the anterior and posterior regions of the retina. However, the
currently accessible UW-OCTA datasets suffer from limited comprehensive
hierarchical information and corresponding disease annotations. To address this
limitation, we have curated the pioneering M3OCTA dataset, which is the first
multimodal (i.e., multilayer), multi-disease, and widest field-of-view UW-OCTA
dataset. Furthermore, the effective utilization of multi-layer ultra-wide
ocular vasculature information from UW-OCTA remains underdeveloped. To tackle
this challenge, we propose the first cross-modal fusion framework that
leverages multi-modal information for diagnosing multiple diseases. Through
extensive experiments conducted on our openly available M3OCTA dataset, we
demonstrate the effectiveness and superior performance of our method, both in
fixed and varying modalities settings. The construction of the M3OCTA dataset,
the first multimodal OCTA dataset encompassing multiple diseases, aims to
advance research in the ophthalmic image analysis community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-fidelity Person-centric Subject-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current subject-driven image generation methods encounter significant
challenges in person-centric image generation. The reason is that they learn
the semantic scene and person generation by fine-tuning a common pre-trained
diffusion, which involves an irreconcilable training imbalance. Precisely, to
generate realistic persons, they need to sufficiently tune the pre-trained
model, which inevitably causes the model to forget the rich semantic scene
prior and makes scene generation over-fit to the training data. Moreover, even
with sufficient fine-tuning, these methods can still not generate high-fidelity
persons since joint learning of the scene and person generation also lead to
quality compromise. In this paper, we propose Face-diffuser, an effective
collaborative generation pipeline to eliminate the above training imbalance and
quality compromise. Specifically, we first develop two specialized pre-trained
diffusion models, i.e., Text-driven Diffusion Model (TDM) and Subject-augmented
Diffusion Model (SDM), for scene and person generation, respectively. The
sampling process is divided into three sequential stages, i.e., semantic scene
construction, subject-scene fusion, and subject enhancement. The first and last
stages are performed by TDM and SDM respectively. The subject-scene fusion
stage, that is the collaboration achieved through a novel and highly effective
mechanism, Saliency-adaptive Noise Fusion (SNF). Specifically, it is based on
our key observation that there exists a robust link between classifier-free
guidance responses and the saliency of generated images. In each time step, SNF
leverages the unique strengths of each model and allows for the spatial
blending of predicted noises from both models automatically in a saliency-aware
manner. Extensive experiments confirm the impressive effectiveness and
robustness of the Face-diffuser.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransONet: Automatic Segmentation of Vasculature in Computed Tomographic
  Angiograms Using Deep Learning <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Bagheri Rajeoni, Breanna Pederson, Ali Firooz, Hamed Abdollahi, Andrew K. Smith, Daniel G. Clair, Susan M. Lessner, Homayoun Valafar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathological alterations in the human vascular system underlie many chronic
diseases, such as atherosclerosis and aneurysms. However, manually analyzing
diagnostic images of the vascular system, such as computed tomographic
angiograms (CTAs) is a time-consuming and tedious process. To address this
issue, we propose a deep learning model to segment the vascular system in CTA
images of patients undergoing surgery for peripheral arterial disease (PAD).
Our study focused on accurately segmenting the vascular system (1) from the
descending thoracic aorta to the iliac bifurcation and (2) from the descending
thoracic aorta to the knees in CTA images using deep learning techniques. Our
approach achieved average Dice accuracies of 93.5% and 80.64% in test dataset
for (1) and (2), respectively, highlighting its high accuracy and potential
clinical utility. These findings demonstrate the use of deep learning
techniques as a valuable tool for medical professionals to analyze the health
of the vascular system efficiently and accurately. Please visit the GitHub page
for this paper at https://github.com/pip-alireza/TransOnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 2023 International Conference on Computational
  Science and Computational Intelligence (CSCI), Las Vegas, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning <span class="highlight-title">transformer</span>-based heterogeneously salient graph representation
  for multimodal fusion classification of hyperspectral image and LiDAR data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Yang, Bo Du, Liangpei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data collected by different modalities can provide a wealth of complementary
information, such as hyperspectral image (HSI) to offer rich spectral-spatial
properties, synthetic aperture radar (SAR) to provide structural information
about the Earth's surface, and light detection and ranging (LiDAR) to cover
altitude information about ground elevation. Therefore, a natural idea is to
combine multimodal images for refined and accurate land-cover interpretation.
Although many efforts have been attempted to achieve multi-source remote
sensing image classification, there are still three issues as follows: 1)
indiscriminate feature representation without sufficiently considering modal
heterogeneity, 2) abundant features and complex computations associated with
modeling long-range dependencies, and 3) overfitting phenomenon caused by
sparsely labeled samples. To overcome the above barriers, a transformer-based
heterogeneously salient graph representation (THSGR) approach is proposed in
this paper. First, a multimodal heterogeneous graph encoder is presented to
encode distinctively non-Euclidean structural features from heterogeneous data.
Then, a self-attention-free multi-convolutional modulator is designed for
effective and efficient long-term dependency modeling. Finally, a mean forward
is put forward in order to avoid overfitting. Based on the above structures,
the proposed model is able to break through modal gaps to obtain differentiated
graph representation with competitive time cost, even for a small fraction of
training samples. Experiments and analyses on three benchmark datasets with
various state-of-the-art (SOTA) methods show the performance of the proposed
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shifting to Machine Supervision: Annotation-Efficient Semi and
  <span class="highlight-title">Self-Supervised</span> Learning for Automatic Medical Image Segmentation and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Singh, Raviteja Chukkapalli, Shravan Chaudhari, Luoyao Chen, Mei Chen, Jinqian Pan, Craig Smuda, Jacopo Cirrone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in clinical treatment and research are limited by supervised
learning techniques that rely on large amounts of annotated data, an expensive
task requiring many hours of clinical specialists' time. In this paper, we
propose using self-supervised and semi-supervised learning. These techniques
perform an auxiliary task that is label-free, scaling up machine-supervision is
easier compared with fully-supervised techniques. This paper proposes S4MI
(Self-Supervision and Semi-Supervision for Medical Imaging), our pipeline to
leverage advances in self and semi-supervision learning. We benchmark them on
three medical imaging datasets to analyze their efficacy for classification and
segmentation. This advancement in self-supervised learning with 10% annotation
performed better than 100% annotation for the classification of most datasets.
The semi-supervised approach yielded favorable outcomes for segmentation,
outperforming the fully-supervised approach by using 50% fewer labels in all
three datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Seventeen pages (incl. references), five figures, and one table.
  (Under Review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonparametric Teaching for Multiple Learners <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhang, Xiaofeng Cao, Weiyang Liu, Ivor Tsang, James Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of teaching multiple learners simultaneously in the
nonparametric iterative teaching setting, where the teacher iteratively
provides examples to the learner for accelerating the acquisition of a target
concept. This problem is motivated by the gap between current single-learner
teaching setting and the real-world scenario of human instruction where a
teacher typically imparts knowledge to multiple students. Under the new problem
formulation, we introduce a novel framework -- Multi-learner Nonparametric
Teaching (MINT). In MINT, the teacher aims to instruct multiple learners, with
each learner focusing on learning a scalar-valued target model. To achieve
this, we frame the problem as teaching a vector-valued target model and extend
the target model space from a scalar-valued reproducing kernel Hilbert space
used in single-learner scenarios to a vector-valued space. Furthermore, we
demonstrate that MINT offers significant teaching speed-up over repeated
single-learner teaching, particularly when the multiple learners can
communicate with each other. Lastly, we conduct extensive experiments to
validate the practicality and efficiency of MINT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 (31 pages, 20 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPSeg : Multi-Phase strategy for coronary artery Segmentation <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonghoe Ku, Yong-Hee Lee, Junsup Shin, In Kyu Lee, Hyun-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of coronary arteries is a pivotal process in assessing
cardiovascular diseases. However, the intricate structure of the cardiovascular
system presents significant challenges for automatic segmentation, especially
when utilizing methodologies like the SYNTAX Score, which relies extensively on
detailed structural information for precise risk stratification. To address
these difficulties and cater to this need, we present MPSeg, an innovative
multi-phase strategy designed for coronary artery segmentation. Our approach
specifically accommodates these structural complexities and adheres to the
principles of the SYNTAX Score. Initially, our method segregates vessels into
two categories based on their unique morphological characteristics: Left
Coronary Artery (LCA) and Right Coronary Artery (RCA). Specialized ensemble
models are then deployed for each category to execute the challenging
segmentation task. Due to LCA's higher complexity over RCA, a refinement model
is utilized to scrutinize and correct initial class predictions on segmented
areas. Notably, our approach demonstrated exceptional effectiveness when
evaluated in the Automatic Region-based Coronary Artery Disease diagnostics
using x-ray angiography imagEs (ARCADE) Segmentation Detection Algorithm
challenge at MICCAI 2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023 Conference ARCADE Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised ViT knowledge distillation network with style transfer
  normalization for colorectal liver metastases survival prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed El Amine Elforaici, Emmanuel Montagnon, Francisco Perdigon Romero, William Trung Le, Feryel Azzi, Dominique Trudel, Bich Nguyen, Simon Turcotte, An Tang, Samuel Kadoury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colorectal liver metastases (CLM) significantly impact colon cancer patients,
influencing survival based on systemic chemotherapy response. Traditional
methods like tumor grading scores (e.g., tumor regression grade - TRG) for
prognosis suffer from subjectivity, time constraints, and expertise demands.
Current machine learning approaches often focus on radiological data, yet the
relevance of histological images for survival predictions, capturing intricate
tumor microenvironment characteristics, is gaining recognition. To address
these limitations, we propose an end-to-end approach for automated prognosis
prediction using histology slides stained with H&E and HPS. We first employ a
Generative Adversarial Network (GAN) for slide normalization to reduce staining
variations and improve the overall quality of the images that are used as input
to our prediction pipeline. We propose a semi-supervised model to perform
tissue classification from sparse annotations, producing feature maps. We use
an attention-based approach that weighs the importance of different slide
regions in producing the final classification results. We exploit the extracted
features for the metastatic nodules and surrounding tissue to train a prognosis
model. In parallel, we train a vision Transformer (ViT) in a knowledge
distillation framework to replicate and enhance the performance of the
prognosis prediction. In our evaluation on a clinical dataset of 258 patients,
our approach demonstrates superior performance with c-indexes of 0.804 (0.014)
for OS and 0.733 (0.014) for TTR. Achieving 86.9% to 90.3% accuracy in
predicting TRG dichotomization and 78.5% to 82.1% accuracy for the 3-class TRG
classification task, our approach outperforms comparative methods. Our proposed
pipeline can provide automated prognosis for pathologists and oncologists, and
can greatly promote precision medicine progress in managing CLM patients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures and 7 tables. Submitted to Medical Journal
  Analysis (MedIA) journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BiHRNet: A Binary high-resolution network for Human Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Zhang, Xueyao Sun, Yonghao Dang, Jianqin Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Pose Estimation (HPE) plays a crucial role in computer vision
applications. However, it is difficult to deploy state-of-the-art models on
resouce-limited devices due to the high computational costs of the networks. In
this work, a binary human pose estimator named BiHRNet(Binary HRNet) is
proposed, whose weights and activations are expressed as $\pm$1. BiHRNet
retains the keypoint extraction ability of HRNet, while using fewer computing
resources by adapting binary neural network (BNN). In order to reduce the
accuracy drop caused by network binarization, two categories of techniques are
proposed in this work. For optimizing the training process for binary pose
estimator, we propose a new loss function combining KL divergence loss with
AWing loss, which makes the binary network obtain more comprehensive output
distribution from its real-valued counterpart to reduce information loss caused
by binarization. For designing more binarization-friendly structures, we
propose a new information reconstruction bottleneck called IR Bottleneck to
retain more information in the initial stage of the network. In addition, we
also propose a multi-scale basic block called MS-Block for information
retention. Our work has less computation cost with few precision drop.
Experimental results demonstrate that BiHRNet achieves a PCKh of 87.9 on the
MPII dataset, which outperforms all binary pose estimation networks. On the
challenging of COCO dataset, the proposed method enables the binary neural
network to achieve 70.8 mAP, which is better than most tested lightweight
full-precision networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Pruning of Deep Ensembles with Focal Diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzhao Wu, Ka-Ho Chow, Wenqi Wei, Ling Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural network ensembles combine the wisdom of multiple deep neural
networks to improve the generalizability and robustness over individual
networks. It has gained increasing popularity to study deep ensemble techniques
in the deep learning community. Some mission-critical applications utilize a
large number of deep neural networks to form deep ensembles to achieve desired
accuracy and resilience, which introduces high time and space costs for
ensemble execution. However, it still remains a critical challenge whether a
small subset of the entire deep ensemble can achieve the same or better
generalizability and how to effectively identify these small deep ensembles for
improving the space and time efficiency of ensemble execution. This paper
presents a novel deep ensemble pruning approach, which can efficiently identify
smaller deep ensembles and provide higher ensemble accuracy than the entire
deep ensemble of a large number of member networks. Our hierarchical ensemble
pruning approach (HQ) leverages three novel ensemble pruning techniques. First,
we show that the focal diversity metrics can accurately capture the
complementary capacity of the member networks of an ensemble, which can guide
ensemble pruning. Second, we design a focal diversity based hierarchical
pruning approach, which will iteratively find high quality deep ensembles with
low cost and high accuracy. Third, we develop a focal diversity consensus
method to integrate multiple focal diversity metrics to refine ensemble pruning
results, where smaller deep ensembles can be effectively identified to offer
high accuracy, high robustness and high efficiency. Evaluated using popular
benchmark datasets, we demonstrate that the proposed hierarchical ensemble
pruning approach can effectively identify high quality deep ensembles with
better generalizability while being more time and space efficient in ensemble
decision making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear on ACM Transactions on Intelligent Systems and Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSASS: Semi-Supervised Approach for Stenosis Segmentation <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        In Kyu Lee, Junsup Shin, Yong-Hee Lee, Jonghoe Ku, Hyun-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coronary artery stenosis is a critical health risk, and its precise
identification in Coronary Angiography (CAG) can significantly aid medical
practitioners in accurately evaluating the severity of a patient's condition.
The complexity of coronary artery structures combined with the inherent noise
in X-ray images poses a considerable challenge to this task. To tackle these
obstacles, we introduce a semi-supervised approach for cardiovascular stenosis
segmentation. Our strategy begins with data augmentation, specifically tailored
to replicate the structural characteristics of coronary arteries. We then apply
a pseudo-label-based semi-supervised learning technique that leverages the data
generated through our augmentation process. Impressively, our approach
demonstrated an exceptional performance in the Automatic Region-based Coronary
Artery Disease diagnostics using x-ray angiography imagEs (ARCADE) Stenosis
Detection Algorithm challenge by utilizing a single model instead of relying on
an ensemble of multiple models. This success emphasizes our method's capability
and efficiency in providing an automated solution for accurately assessing
stenosis severity from medical imaging data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023 Conference ARCADE Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human fingerprints serve as one unique and powerful characteristic for each
person, from which policemen can recognize the identity. Similar to humans,
many natural bodies and intrinsic mechanical qualities can also be uniquely
identified from surface characteristics. To measure the elasto-plastic
properties of one material, one formally sharp indenter is pushed into the
measured body under constant force and retracted, leaving a unique residual
imprint of the minute size from several micrometers to nanometers. However, one
great challenge is how to map the optical image of this residual imprint into
the real wanted mechanical properties, i.e., the tensile force curve. In this
paper, we propose a novel method to use multi-fidelity neural networks (MFNN)
to solve this inverse problem. We first actively train the NN model via pure
simulation data, and then bridge the sim-to-real gap via transfer learning. The
most innovative part is that we use NN to dig out the unknown physics and also
implant the known physics into the transfer learning framework, thus highly
improving the model stability and decreasing the data requirement. This work
serves as one great example of applying machine learning into the real
experimental research, especially under the constraints of data limitation and
fidelity variance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, NeurIPS 2023 Workshop on Adaptive Experimental
  Design and Active Learning in the Real World</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable pap smear cell representation for cervical cancer
  screening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ando, Nora Jee-Young Park and, Gun Oh Chong, Seokhwan Ko, Donghyeon Lee, Junghwan Cho, Hyungsoo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Screening is critical for prevention and early detection of cervical cancer
but it is time-consuming and laborious. Supervised deep convolutional neural
networks have been developed to automate pap smear screening and the results
are promising. However, the interest in using only normal samples to train deep
neural networks has increased owing to class imbalance problems and
high-labeling costs that are both prevalent in healthcare. In this study, we
introduce a method to learn explainable deep cervical cell representations for
pap smear cytology images based on one class classification using variational
autoencoders. Findings demonstrate that a score can be calculated for cell
abnormality without training models with abnormal samples and localize
abnormality to interpret our results with a novel metric based on absolute
difference in cross entropy in agglomerative clustering. The best model that
discriminates squamous cell carcinoma (SCC) from normals gives 0.908 +- 0.003
area under operating characteristic curve (AUC) and one that discriminates
high-grade epithelial lesion (HSIL) 0.920 +- 0.002 AUC. Compared to other
clustering methods, our method enhances the V-measure and yields higher
homogeneity scores, which more effectively isolate different abnormality
regions, aiding in the interpretation of our results. Evaluation using in-house
and additional open dataset show that our model can discriminate abnormality
without the need of additional training of deep models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision meets mmWave Radar: 3D Object Perception Benchmark for Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Wang, Jen-Hao Cheng, Jui-Te Huang, Sheng-Yao Kuan, Qiqian Fu, Chiming Ni, Shengyu Hao, Gaoang Wang, Guanbin Xing, Hui Liu, Jenq-Neng Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sensor fusion is crucial for an accurate and robust perception system on
autonomous vehicles. Most existing datasets and perception solutions focus on
fusing cameras and LiDAR. However, the collaboration between camera and radar
is significantly under-exploited. The incorporation of rich semantic
information from the camera, and reliable 3D information from the radar can
potentially achieve an efficient, cheap, and portable solution for 3D object
perception tasks. It can also be robust to different lighting or all-weather
driving scenarios due to the capability of mmWave radars. In this paper, we
introduce the CRUW3D dataset, including 66K synchronized and well-calibrated
camera, radar, and LiDAR frames in various driving scenarios. Unlike other
large-scale autonomous driving datasets, our radar data is in the format of
radio frequency (RF) tensors that contain not only 3D location information but
also spatio-temporal semantic information. This kind of radar format can enable
machine learning models to generate more reliable object perception results
after interacting and fusing the information or features between the camera and
radar.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniMOS: A Universal Framework For Multi-Organ Segmentation Over
  Label-Constrained <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Li, Sheng Shao, Junyi Qu, Shuchao Pang, Mehmet A. Orgun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models for medical images can help physicians diagnose and
manage diseases. However, due to the fact that medical image annotation
requires a great deal of manpower and expertise, as well as the fact that
clinical departments perform image annotation based on task orientation, there
is the problem of having fewer medical image annotation data with more
unlabeled data and having many datasets that annotate only a single organ. In
this paper, we present UniMOS, the first universal framework for achieving the
utilization of fully and partially labeled images as well as unlabeled images.
Specifically, we construct a Multi-Organ Segmentation (MOS) module over
fully/partially labeled data as the basenet and designed a new target adaptive
loss. Furthermore, we incorporate a semi-supervised training module that
combines consistent regularization and pseudolabeling techniques on unlabeled
data, which significantly improves the segmentation of unlabeled data.
Experiments show that the framework exhibits excellent performance in several
medical image segmentation tasks compared to other advanced methods, and also
significantly improves data utilization and reduces annotation cost. Code and
models are available at: https://github.com/lw8807001/UniMOS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segment Anything in Defect Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bozhen Hu, Bin Gao, Cheng Tan, Tongle Wu, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defect detection plays a crucial role in infrared non-destructive testing
systems, offering non-contact, safe, and efficient inspection capabilities.
However, challenges such as low resolution, high noise, and uneven heating in
infrared thermal images hinder comprehensive and accurate defect detection. In
this study, we propose DefectSAM, a novel approach for segmenting defects on
highly noisy thermal images based on the widely adopted model, Segment Anything
(SAM)\cite{kirillov2023segany}. Harnessing the power of a meticulously curated
dataset generated through labor-intensive lab experiments and valuable prompts
from experienced experts, DefectSAM surpasses existing state-of-the-art
segmentation algorithms and achieves significant improvements in defect
detection rates. Notably, DefectSAM excels in detecting weaker and smaller
defects on complex and irregular surfaces, reducing the occurrence of missed
detections and providing more accurate defect size estimations. Experimental
studies conducted on various materials have validated the effectiveness of our
solutions in defect detection, which hold significant potential to expedite the
evolution of defect detection tools, enabling enhanced inspection capabilities
and accuracy in defect identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SHAMSUL: Systematic Holistic Analysis to investigate Medical
  Significance Utilizing Local interpretability methods in deep learning for
  chest radiography pathology prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahbub Ul Alam, Jaakko Hollmén, Jón Rúnar Baldvinsson, Rahim Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interpretability of deep neural networks has become a subject of great
interest within the medical and healthcare domain. This attention stems from
concerns regarding transparency, legal and ethical considerations, and the
medical significance of predictions generated by these deep neural networks in
clinical decision support systems. To address this matter, our study delves
into the application of four well-established interpretability methods: Local
Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations
(SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise
Relevance Propagation (LRP). Leveraging the approach of transfer learning with
a multi-label-multi-class chest radiography dataset, we aim to interpret
predictions pertaining to specific pathology classes. Our analysis encompasses
both single-label and multi-label predictions, providing a comprehensive and
unbiased assessment through quantitative and qualitative investigations, which
are compared against human expert annotation. Notably, Grad-CAM demonstrates
the most favorable performance in quantitative evaluation, while the LIME
heatmap score segmentation visualization exhibits the highest level of medical
significance. Our research underscores both the outcomes and the challenges
faced in the holistic approach adopted for assessing these interpretability
methods and suggests that a multimodal-based approach, incorporating diverse
sources of information beyond chest radiography images, could offer additional
insights for enhancing interpretability in the medical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version contains extensive modifications compared to the
  previous version. The published version of this article can be obtained using
  the following link: https://doi.org/10.5617/nmi.10471 Code Repository:
  https://github.com/anondo1969/SHAMSUL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisIT-Bench: A Benchmark for Vision-Language Instruction Following
  Inspired by Real-World Use 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06595v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06595v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, Ludwig Schmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for
evaluation of instruction-following vision-language models for real-world use.
Our starting point is curating 70 'instruction families' that we envision
instruction tuned vision-language models should be able to address. Extending
beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to
game playing and creative generation. Following curation, our dataset comprises
592 test queries, each with a human-authored instruction-conditioned caption.
These descriptions surface instruction-specific factors, e.g., for an
instruction asking about the accessibility of a storefront for wheelchair
users, the instruction-conditioned caption describes ramps/potential obstacles.
These descriptions enable 1) collecting human-verified reference outputs for
each instance; and 2) automatic evaluation of candidate multimodal generations
using a text-only LLM, aligning with human judgment. We quantify quality gaps
between models and references using both human and automatic evaluations; e.g.,
the top-performing instruction-following model wins against the GPT-4 reference
in just 27% of the comparison. VisIT-Bench is dynamic to participate,
practitioners simply submit their model's response on the project website;
Data, code and leaderboard is available at visit-bench.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What User Behaviors Make the Differences During the Process of Visual
  Analytics? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahin Doroudian, Zekun Wu, Aidong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The understanding of visual analytics process can benefit visualization
researchers from multiple aspects, including improving visual designs and
developing advanced interaction functions. However, the log files of user
behaviors are still hard to analyze due to the complexity of sensemaking and
our lack of knowledge on the related user behaviors. This work presents a study
on a comprehensive data collection of user behaviors, and our analysis approach
with time-series classification methods. We have chosen a classical
visualization application, Covid-19 data analysis, with common analysis tasks
covering geo-spatial, time-series and multi-attributes. Our user study collects
user behaviors on a diverse set of visualization tasks with two comparable
systems, desktop and immersive visualizations. We summarize the classification
results with three time-series machine learning algorithms at two scales, and
explore the influences of behavior features. Our results reveal that user
behaviors can be distinguished during the process of visual analytics and there
is a potentially strong association between the physical behaviors of users and
the visualization tasks they perform. We also demonstrate the usage of our
models by interpreting open sessions of visual analytics, which provides an
automatic way to study sensemaking without tedious manual annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The authors have decided to withdraw the paper due to identified
  critical errors. These errors were deemed substantial enough to compromise
  the integrity and reliability of the research findings presented in the
  paper. As a result, the authors have chosen to retract the paper to maintain
  academic standards and transparency in the dissemination of scientific
  knowledge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning-based Compressed Domain Multimedia for Man and Machine: A
  Taxonomy and Application to Point Cloud Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18849v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18849v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Seleem, André F. R. Guarda, Nuno M. M. Rodrigues, Fernando Pereira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current golden age of multimedia, human visualization is no longer the
single main target, with the final consumer often being a machine which
performs some processing or computer vision tasks. In both cases, deep learning
plays a undamental role in extracting features from the multimedia
representation data, usually producing a compressed representation referred to
as latent representation. The increasing development and adoption of deep
learning-based solutions in a wide area of multimedia applications have opened
an exciting new vision where a common compressed multimedia representation is
used for both man and machine. The main benefits of this vision are two-fold:
i) improved performance for the computer vision tasks, since the effects of
coding artifacts are mitigated; and ii) reduced computational complexity, since
prior decoding is not required. This paper proposes the first taxonomy for
designing compressed domain computer vision solutions driven by the
architecture and weights compatibility with an available spatio-temporal
computer vision processor. The potential of the proposed taxonomy is
demonstrated for the specific case of point cloud classification by designing
novel compressed domain processors using the JPEG Pleno Point Cloud Coding
standard under development and adaptations of the PointGrid classifier.
Experimental results show that the designed compressed domain point cloud
classification solutions can significantly outperform the spatial-temporal
domain classification benchmarks when applied to the decompressed data,
containing coding artifacts, and even surpass their performance when applied to
the original uncompressed data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Targeted Image Data Augmentation Increases Basic Skills Captioning
  Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Barriere, Felipe del Rio, Andres Carvallo De Ferari, Carlos Aspillaga, Eugenio Herrera-Berg, Cristian Buc Calderon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial neural networks typically struggle in generalizing to
out-of-context examples. One reason for this limitation is caused by having
datasets that incorporate only partial information regarding the potential
correlational structure of the world. In this work, we propose TIDA (Targeted
Image-editing Data Augmentation), a targeted data augmentation method focused
on improving models' human-like abilities (e.g., gender recognition) by filling
the correlational structure gap using a text-to-image generative model. More
specifically, TIDA identifies specific skills in captions describing images
(e.g., the presence of a specific gender in the image), changes the caption
(e.g., "woman" to "man"), and then uses a text-to-image model to edit the image
in order to match the novel caption (e.g., uniquely changing a woman to a man
while maintaining the context identical). Based on the Flickr30K benchmark, we
show that, compared with the original data set, a TIDA-enhanced dataset related
to gender, color, and counting abilities induces better performance in several
image captioning metrics. Furthermore, on top of relying on the classical BLEU
metric, we conduct a fine-grained analysis of the improvements of our models
against the baseline in different ways. We compared text-to-image generative
models and found different behaviors of the image captioning models in terms of
encoding visual encoding and textual decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Israa Fahmy, Marwah Sulaiman, Zahraa Shehabeldin, Mohammed Barakat, Dareen Hussein, Mohammed El-Naggar, Hesham Eraqi, Moustafa Youssef
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, video super resolution (VSR) has become a very impactful task in
the area of Computer Vision due to its various applications. In this paper, we
propose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for
VSR in an attempt to generate temporally coherent solutions while preserving
spatial details. RBPGAN integrates two state-of-the-art models to get the best
in both worlds without compromising the accuracy of produced video. The
generator of the model is inspired by RBPN system, while the discriminator is
inspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal
consistency over time. Our contribution together results in a model that
outperforms earlier work in terms of temporally consistent details, as we will
demonstrate qualitatively and quantitatively using different datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tree Variational Autoencoders <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08984v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08984v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Manduchi, Moritz Vandenhirtz, Alain Ryser, Julia Vogt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Tree Variational Autoencoder (TreeVAE), a new generative
hierarchical clustering model that learns a flexible tree-based posterior
distribution over latent variables. TreeVAE hierarchically divides samples
according to their intrinsic characteristics, shedding light on hidden
structures in the data. It adapts its architecture to discover the optimal tree
for encoding dependencies between latent variables. The proposed tree-based
generative architecture enables lightweight conditional inference and improves
generative performance by utilizing specialized leaf decoders. We show that
TreeVAE uncovers underlying clusters in the data and finds meaningful
hierarchical relations between the different groups on a variety of datasets,
including real-world imaging data. We present empirically that TreeVAE provides
a more competitive log-likelihood lower bound than the sequential counterparts.
Finally, due to its generative nature, TreeVAE is able to generate new samples
from the discovered clusters via conditional sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as Spotlight to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bespoke: A Block-Level Neural Network Optimization Framework for
  Low-Cost Deployment <span class="chip">AAAI-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jong-Ryul Lee, Yong-Hyuk Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep learning models become popular, there is a lot of need for deploying
them to diverse device environments. Because it is costly to develop and
optimize a neural network for every single environment, there is a line of
research to search neural networks for multiple target environments
efficiently. However, existing works for such a situation still suffer from
requiring many GPUs and expensive costs. Motivated by this, we propose a novel
neural network optimization framework named Bespoke for low-cost deployment.
Our framework searches for a lightweight model by replacing parts of an
original model with randomly selected alternatives, each of which comes from a
pretrained neural network or the original model. In the practical sense,
Bespoke has two significant merits. One is that it requires near zero cost for
designing the search space of neural networks. The other merit is that it
exploits the sub-networks of public pretrained neural networks, so the total
cost is minimal compared to the existing works. We conduct experiments
exploring Bespoke's the merits, and the results show that it finds efficient
models for multiple targets with meager cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the extended version of our AAAI-2023 paper
  (https://ojs.aaai.org/index.php/AAAI/article/view/26020)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Edit-A-Video: Single Video Editing with Object-Aware Consistency <span class="chip">ACML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07945v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07945v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the fact that text-to-video (TTV) model has recently achieved
remarkable success, there have been few approaches on TTV for its extension to
video editing. Motivated by approaches on TTV models adapting from
diffusion-based text-to-image (TTI) models, we suggest the video editing
framework given only a pretrained TTI model and a single <text, video> pair,
which we term Edit-A-Video. The framework consists of two stages: (1) inflating
the 2D model into the 3D model by appending temporal modules and tuning on the
source video (2) inverting the source video into the noise and editing with
target text prompt and attention map injection. Each stage enables the temporal
modeling and preservation of semantic attributes of the source video. One of
the key challenges for video editing include a background inconsistency
problem, where the regions not included for the edit suffer from undesirable
and inconsistent temporal alterations. To mitigate this issue, we also
introduce a novel mask blending method, termed as sparse-causal blending (SC
Blending). We improve previous mask blending methods to reflect the temporal
consistency so that the area where the editing is applied exhibits smooth
transition while also achieving spatio-temporal consistency of the unedited
regions. We present extensive experimental results over various types of text
and videos, and demonstrate the superiority of the proposed method compared to
baselines in terms of background consistency, text alignment, and video editing
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACML 2023 Best Paper Award</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating and Mitigating the Side Effects of Noisy Views for
  <span class="highlight-title">Self-Supervised</span> Clustering Algorithms in Practical Multi-View Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17245v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17245v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Xu, Yazhou Ren, Xiaolong Wang, Lei Feng, Zheng Zhang, Gang Niu, Xiaofeng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view clustering (MVC) aims at exploring category structures among
multi-view data in self-supervised manners. Multiple views provide more
information than single views and thus existing MVC methods can achieve
satisfactory performance. However, their performance might seriously degenerate
when the views are noisy in practical multi-view scenarios. In this paper, we
first formally investigate the drawback of noisy views and then propose a
theoretically grounded deep MVC method (namely MVCAN) to address this issue.
Specifically, we propose a novel MVC objective that enables un-shared
parameters and inconsistent clustering predictions across multiple views to
reduce the side effects of noisy views. Furthermore, a two-level multi-view
iterative optimization is designed to generate robust learning targets for
refining individual views' representation learning. Theoretical analysis
reveals that MVCAN works by achieving the multi-view consistency,
complementarity, and noise robustness. Finally, experiments on extensive public
datasets demonstrate that MVCAN outperforms state-of-the-art methods and is
robust against the existence of noisy views.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual-Query Multiple Instance Learning for Dynamic Meta-Embedding based
  Tumor Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Holdenried-Krafft, Peter Somers, Ivonne A. Montes-Majarro, Diana Silimon, Cristina Tarín, Falko Fend, Hendrik P. A. Lensch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole slide image (WSI) assessment is a challenging and crucial step in
cancer diagnosis and treatment planning. WSIs require high magnifications to
facilitate sub-cellular analysis. Precise annotations for patch- or even
pixel-level classifications in the context of gigapixel WSIs are tedious to
acquire and require domain experts. Coarse-grained labels, on the other hand,
are easily accessible, which makes WSI classification an ideal use case for
multiple instance learning (MIL). In our work, we propose a novel
embedding-based Dual-Query MIL pipeline (DQ-MIL). We contribute to both the
embedding and aggregation steps. Since all-purpose visual feature
representations are not yet available, embedding models are currently limited
in terms of generalizability. With our work, we explore the potential of
dynamic meta-embedding based on cutting-edge self-supervised pre-trained models
in the context of MIL. Moreover, we propose a new MIL architecture capable of
combining MIL-attention with correlated self-attention. The Dual-Query
Perceiver design of our approach allows us to leverage the concept of
self-distillation and to combine the advantages of a small model in the context
of a low data regime with the rich feature representation of a larger model. We
demonstrate the superior performance of our approach on three histopathological
datasets, where we show improvement of up to 10% over state-of-the-art
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structured Prediction Problem Archive 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03574v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03574v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Swoboda, Bjoern Andres, Andrea Hornakova, Florian Bernard, Jannik Irmai, Paul Roetzer, Bogdan Savchynskyy, David Stein, Ahmed Abbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured prediction problems are one of the fundamental tools in machine
learning. In order to facilitate algorithm development for their numerical
solution, we collect in one place a large number of datasets in easy to read
formats for a diverse set of problem classes. We provide archival links to
datasets, description of the considered problems and problem formats, and a
short summary of problem characteristics including size, number of instances
etc. For reference we also give a non-exhaustive selection of algorithms
proposed in the literature for their solution. We hope that this central
repository will make benchmarking and comparison to established works easier.
We welcome submission of interesting new datasets and algorithms for inclusion
in our archive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added multicast instances from Andres group</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigSmall: Efficient Multi-Task Learning for Disparate Spatial and
  Temporal Physiological Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Girish Narayanswamy, Yujia Liu, Yuzhe Yang, Chengqian Ma, Xin Liu, Daniel McDuff, Shwetak Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding of human visual perception has historically inspired the design
of computer vision architectures. As an example, perception occurs at different
scales both spatially and temporally, suggesting that the extraction of salient
visual information may be made more effective by paying attention to specific
features at varying scales. Visual changes in the body due to physiological
processes also occur at different scales and with modality-specific
characteristic properties. Inspired by this, we present BigSmall, an efficient
architecture for physiological and behavioral measurement. We present the first
joint camera-based facial action, cardiac, and pulmonary measurement model. We
propose a multi-branch network with wrapping temporal shift modules that yields
both accuracy and efficiency gains. We observe that fusing low-level features
leads to suboptimal performance, but that fusing high level features enables
efficiency gains with negligible loss in accuracy. Experimental results
demonstrate that BigSmall significantly reduces the computational costs.
Furthermore, compared to existing task-specific models, BigSmall achieves
comparable or better results on multiple physiological measurement tasks
simultaneously with a unified model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Semantic-aware Attention and Visual Shielding Network for
  Cloth-changing Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zan Gao, Hongwei Wei, Weili Guan, Jie Nie, Meng Wang, Shenyong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth-changing person reidentification (ReID) is a newly emerging research
topic that aims to retrieve pedestrians whose clothes are changed. Since the
human appearance with different clothes exhibits large variations, it is very
difficult for existing approaches to extract discriminative and robust feature
representations. Current works mainly focus on body shape or contour sketches,
but the human semantic information and the potential consistency of pedestrian
features before and after changing clothes are not fully explored or are
ignored. To solve these issues, in this work, a novel semantic-aware attention
and visual shielding network for cloth-changing person ReID (abbreviated as
SAVS) is proposed where the key idea is to shield clues related to the
appearance of clothes and only focus on visual semantic information that is not
sensitive to view/posture changes. Specifically, a visual semantic encoder is
first employed to locate the human body and clothing regions based on human
semantic segmentation information. Then, a human semantic attention module
(HSA) is proposed to highlight the human semantic information and reweight the
visual feature map. In addition, a visual clothes shielding module (VCS) is
also designed to extract a more robust feature representation for the
cloth-changing task by covering the clothing regions and focusing the model on
the visual semantic information unrelated to the clothes. Most importantly,
these two modules are jointly explored in an end-to-end unified framework.
Extensive experiments demonstrate that the proposed method can significantly
outperform state-of-the-art methods, and more robust features can be extracted
for cloth-changing persons. Compared with FSAM (published in CVPR 2021), this
method can achieve improvements of 32.7% (16.5%) and 14.9% (-) on the LTCC and
PRCC datasets in terms of mAP (rank-1), respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2108.04527</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identity-Guided Collaborative Learning for Cloth-Changing Person
  Reidentification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zan Gao, Shenxun Wei, Weili Guan, Lei Zhu, Meng Wang, Shenyong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth-changing person reidentification (ReID) is a newly emerging research
topic that is aimed at addressing the issues of large feature variations due to
cloth-changing and pedestrian view/pose changes. Although significant progress
has been achieved by introducing extra information (e.g., human contour
sketching information, human body keypoints, and 3D human information),
cloth-changing person ReID is still challenging due to impressionable
pedestrian representations. Moreover, human semantic information and pedestrian
identity information are not fully explored. To solve these issues, we propose
a novel identity-guided collaborative learning scheme (IGCL) for cloth-changing
person ReID, where the human semantic is fully utilized and the identity is
unchangeable to guide collaborative learning. First, we design a novel clothing
attention degradation stream to reasonably reduce the interference caused by
clothing information where clothing attention and mid-level collaborative
learning are employed. Second, we propose a human semantic attention and body
jigsaw stream to highlight the human semantic information and simulate
different poses of the same identity. In this way, the extraction features not
only focus on human semantic information that is unrelated to the background
but also are suitable for pedestrian pose variations. Moreover, a pedestrian
identity enhancement stream is further proposed to enhance the identity
importance and extract more favorable identity robust features. Most
importantly, all these streams are jointly explored in an end-to-end unified
framework, and the identity is utilized to guide the optimization. Extensive
experiments on five public clothing person ReID datasets demonstrate that the
proposed IGCL significantly outperforms SOTA methods and that the extracted
feature is more robust, discriminative, and clothing-irrelevant.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOTUS: Continual Imitation Learning for Robot Manipulation Through
  Unsupervised Skill Discovery <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02058v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02058v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikang Wan, Yifeng Zhu, Rutav Shah, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LOTUS, a continual imitation learning algorithm that empowers a
physical robot to continuously and efficiently learn to solve new manipulation
tasks throughout its lifespan. The core idea behind LOTUS is constructing an
ever-growing skill library from a sequence of new tasks with a small number of
human demonstrations. LOTUS starts with a continual skill discovery process
using an open-vocabulary vision model, which extracts skills as recurring
patterns presented in unsegmented demonstrations. Continual skill discovery
updates existing skills to avoid catastrophic forgetting of previous tasks and
adds new skills to solve novel tasks. LOTUS trains a meta-controller that
flexibly composes various skills to tackle vision-based manipulation tasks in
the lifelong learning process. Our comprehensive experiments show that LOTUS
outperforms state-of-the-art baselines by over 11% in success rate, showing its
superior knowledge transfer ability compared to prior methods. More results and
videos can be found on the project website:
https://ut-austin-rpl.github.io/Lotus/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Normalization Layers Are All That Sharpness-Aware Minimization Needs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Mueller, Tiffany Vlaar, David Rolnick, Matthias Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima
and has been shown to enhance generalization performance in various settings.
In this work we show that perturbing only the affine normalization parameters
(typically comprising 0.1% of the total parameters) in the adversarial step of
SAM can outperform perturbing all of the parameters.This finding generalizes to
different SAM variants and both ResNet (Batch Normalization) and Vision
Transformer (Layer Normalization) architectures. We consider alternative sparse
perturbation approaches and find that these do not achieve similar performance
enhancement at such extreme sparsity levels, showing that this behaviour is
unique to the normalization layers. Although our findings reaffirm the
effectiveness of SAM in improving generalization performance, they cast doubt
on whether this is solely caused by reduced sharpness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOCATE: <span class="highlight-title">Self-supervised</span> Object Discovery via Flow-guided Graph-cut and
  Bootstrapped Self-training <span class="chip">BMVC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silky Singh, Shripad Deshmukh, Mausoom Sarkar, Balaji Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning object segmentation in image and video datasets without human
supervision is a challenging problem. Humans easily identify moving salient
objects in videos using the gestalt principle of common fate, which suggests
that what moves together belongs together. Building upon this idea, we propose
a self-supervised object discovery approach that leverages motion and
appearance information to produce high-quality object segmentation masks.
Specifically, we redesign the traditional graph cut on images to include motion
information in a linear combination with appearance information to produce edge
weights. Remarkably, this step produces object segmentation masks comparable to
the current state-of-the-art on multiple benchmarks. To further improve
performance, we bootstrap a segmentation network trained on these preliminary
masks as pseudo-ground truths to learn from its own outputs via self-training.
We demonstrate the effectiveness of our approach, named LOCATE, on multiple
standard video object segmentation, image saliency detection, and object
segmentation benchmarks, achieving results on par with and, in many cases
surpassing state-of-the-art methods. We also demonstrate the transferability of
our approach to novel domains through a qualitative study on in-the-wild
images. Additionally, we present extensive ablation analysis to support our
design choices and highlight the contribution of each component of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to British Machine Vision Conference (BMVC) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FD-Align: Feature Discrimination Alignment for Fine-tuning <span class="highlight-title">Pre-Train</span>ed
  Models in Few-Shot Learning <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15105v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15105v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Song, Huimin Ma, Bochao Zou, Huishuai Zhang, Weiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the limited availability of data, existing few-shot learning methods
trained from scratch fail to achieve satisfactory performance. In contrast,
large-scale pre-trained models such as CLIP demonstrate remarkable few-shot and
zero-shot capabilities. To enhance the performance of pre-trained models for
downstream tasks, fine-tuning the model on downstream data is frequently
necessary. However, fine-tuning the pre-trained model leads to a decrease in
its generalizability in the presence of distribution shift, while the limited
number of samples in few-shot learning makes the model highly susceptible to
overfitting. Consequently, existing methods for fine-tuning few-shot learning
primarily focus on fine-tuning the model's classification head or introducing
additional structure. In this paper, we introduce a fine-tuning approach termed
Feature Discrimination Alignment (FD-Align). Our method aims to bolster the
model's generalizability by preserving the consistency of spurious features
across the fine-tuning process. Extensive experimental results validate the
efficacy of our approach for both ID and OOD tasks. Once fine-tuned, the model
can seamlessly integrate with existing methods, leading to performance
improvements. Our code can be found in https://github.com/skingorz/FD-Align.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does Fine-Tuning Impact Out-of-Distribution Detection for
  Vision-Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Ming, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large vision-language models such as CLIP have shown remarkable
out-of-distribution (OOD) detection and generalization performance. However,
their zero-shot in-distribution (ID) accuracy is often limited for downstream
datasets. Recent CLIP-based fine-tuning methods such as prompt learning have
demonstrated significant improvements in ID classification and OOD
generalization where OOD labels are available. Nonetheless, it remains unclear
whether the model is reliable to semantic shifts without OOD labels. In this
paper, we aim to bridge the gap and present a comprehensive study to understand
how fine-tuning impact OOD detection for few-shot downstream tasks. By framing
OOD detection as multi-modal concept matching, we establish a connection
between fine-tuning methods and various OOD scores. Our results suggest that a
proper choice of OOD scores is essential for CLIP-based fine-tuning. In
particular, the maximum concept matching (MCM) score provides a promising
solution consistently. We also show that prompt learning demonstrates the
state-of-the-art OOD detection performance over the zero-shot counterpart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Recognition of Oil Leakage Area Based on Logical Semantic
  Discrimination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiying Lin, Che Liu, Xin Zhang, Zhen Wei, Sizhe Li, Xun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implementing precise detection of oil leaks in peak load equipment through
image analysis can significantly enhance inspection quality and ensure the
system's safety and reliability. However, challenges such as varying shapes of
oil-stained regions, background noise, and fluctuating lighting conditions
complicate the detection process. To address this, the integration of logical
rule-based discrimination into image recognition has been proposed. This
approach involves recognizing the spatial relationships among objects to
semantically segment images of oil spills using a Mask RCNN network. The
process begins with histogram equalization to enhance the original image,
followed by the use of Mask RCNN to identify the preliminary positions and
outlines of oil tanks, the ground, and areas of potential oil contamination.
Subsequent to this identification, the spatial relationships between these
objects are analyzed. Logical rules are then applied to ascertain whether the
suspected areas are indeed oil spills. This method's effectiveness has been
confirmed by testing on images captured from peak power equipment in the field.
The results indicate that this approach can adeptly tackle the challenges in
identifying oil-contaminated areas, showing a substantial improvement in
accuracy compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSVQ: <span class="highlight-title">Self-Supervised</span> Learning with Multiple Sample Views and Queues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Peng, Xianzhong Long, Yun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised methods based on contrastive learning have achieved great
success in unsupervised visual representation learning. However, most methods
under this framework suffer from the problem of false negative samples.
Inspired by the mean shift for self-supervised learning, we propose a new
simple framework, namely Multiple Sample Views and Queues (MSVQ). We jointly
construct three soft labels on-the-fly by utilizing two complementary and
symmetric approaches: multiple augmented positive views and two momentum
encoders that generate various semantic features for negative samples. Two
teacher networks perform similarity relationship calculations with negative
samples and then transfer this knowledge to the student network. Let the
student network mimic the similarity relationships between the samples, thus
giving the student network a more flexible ability to identify false negative
samples in the dataset. The classification results on four benchmark image
datasets demonstrate the high effectiveness and efficiency of our approach
compared to some classical methods. Source code and pretrained models are
available \href{https://github.com/pc-cp/MSVQ}{here}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in KBS(Knowledge-Based Systems)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CtxMIM: Context-Enhanced Masked Image Modeling for Remote Sensing Image
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingming Zhang, Qingjie Liu, Yunhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning representations through self-supervision on unlabeled data has
proven highly effective for understanding diverse images. However, remote
sensing images often have complex and densely populated scenes with multiple
land objects and no clear foreground objects. This intrinsic property generates
high object density, resulting in false positive pairs or missing contextual
information in self-supervised learning. To address these problems, we propose
a context-enhanced masked image modeling method (CtxMIM), a simple yet
efficient MIM-based self-supervised learning for remote sensing image
understanding. CtxMIM formulates original image patches as a reconstructive
template and employs a Siamese framework to operate on two sets of image
patches. A context-enhanced generative branch is introduced to provide
contextual information through context consistency constraints in the
reconstruction. With the simple and elegant design, CtxMIM encourages the
pre-training model to learn object-level or pixel-level features on a
large-scale dataset without specific temporal or geographical constraints.
Finally, extensive experiments show that features learned by CtxMIM outperform
fully supervised and state-of-the-art self-supervised learning methods on
various downstream tasks, including land cover classification, semantic
segmentation, object detection, and instance segmentation. These results
demonstrate that CtxMIM learns impressive remote sensing representations with
high generalization and transferability. Code and data will be made public
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple Framework for 3D Occupancy Estimation in Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10076v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10076v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanshui Gan, Ningkai Mo, Hongbin Xu, Naoto Yokoya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of estimating 3D occupancy from surrounding-view images is an
exciting development in the field of autonomous driving, following the success
of Bird's Eye View (BEV) perception. This task provides crucial 3D attributes
of the driving environment, enhancing the overall understanding and perception
of the surrounding space. In this work, we present a simple framework for 3D
occupancy estimation, which is a CNN-based framework designed to reveal several
key factors for 3D occupancy estimation, such as network design, optimization,
and evaluation. In addition, we explore the relationship between 3D occupancy
estimation and other related tasks, such as monocular depth estimation and 3D
reconstruction, which could advance the study of 3D perception in autonomous
driving. For evaluation, we propose a simple sampling strategy to define the
metric for occupancy evaluation, which is flexible for current public datasets.
Moreover, we establish the benchmark in terms of the depth estimation metric,
where we compare our proposed method with monocular depth estimation methods on
the DDAD and Nuscenes datasets and achieve competitive performance. The
relevant code will be updated in https://github.com/GANWANSHUI/SimpleOccupancy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion with Forward Models: Solving Stochastic Inverse Problems
  Without Direct Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B. Tenenbaum, Frédo Durand, William T. Freeman, Vincent Sitzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models are a powerful type of generative models used to
capture complex distributions of real-world signals. However, their
applicability is limited to scenarios where training samples are readily
available, which is not always the case in real-world applications. For
example, in inverse graphics, the goal is to generate samples from a
distribution of 3D scenes that align with a given image, but ground-truth 3D
scenes are unavailable and only 2D images are accessible. To address this
limitation, we propose a novel class of denoising diffusion probabilistic
models that learn to sample from distributions of signals that are never
directly observed. Instead, these signals are measured indirectly through a
known differentiable forward model, which produces partial observations of the
unknown signal. Our approach involves integrating the forward model directly
into the denoising process. This integration effectively connects the
generative modeling of observations with the generative modeling of the
underlying signals, allowing for end-to-end training of a conditional
generative model over signals. During inference, our approach enables sampling
from the distribution of underlying signals that are consistent with a given
partial observation. We demonstrate the effectiveness of our method on three
challenging computer vision tasks. For instance, in the context of inverse
graphics, our model enables direct sampling from the distribution of 3D scenes
that align with a single 2D input image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://diffusion-with-forward-models.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic-aware Consistency Network for Cloth-changing Person
  Re-Identification <span class="chip">ACM MM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14113v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14113v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peini Guo, Hong Liu, Jianbing Wu, Guoquan Wang, Tao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth-changing Person Re-Identification (CC-ReID) is a challenging task that
aims to retrieve the target person across multiple surveillance cameras when
clothing changes might happen. Despite recent progress in CC-ReID, existing
approaches are still hindered by the interference of clothing variations since
they lack effective constraints to keep the model consistently focused on
clothing-irrelevant regions. To address this issue, we present a Semantic-aware
Consistency Network (SCNet) to learn identity-related semantic features by
proposing effective consistency constraints. Specifically, we generate the
black-clothing image by erasing pixels in the clothing area, which explicitly
mitigates the interference from clothing variations. In addition, to fully
exploit the fine-grained identity information, a head-enhanced attention module
is introduced, which learns soft attention maps by utilizing the proposed
part-based matching loss to highlight head information. We further design a
semantic consistency loss to facilitate the learning of high-level
identity-related semantic features, forcing the model to focus on semantically
consistent cloth-irrelevant regions. By using the consistency constraint, our
model does not require any extra auxiliary segmentation module to generate the
black-clothing image or locate the head region during the inference stage.
Extensive experiments on four cloth-changing person Re-ID datasets (LTCC, PRCC,
Vc-Clothes, and DeepChange) demonstrate that our proposed SCNet makes
significant improvements over prior state-of-the-art approaches. Our code is
available at: https://github.com/Gpn-star/SCNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical
  Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05836v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05836v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Hu, Qinrui Fan, Shu Hu, Siwei Lyu, Xi Wu, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of clinical medicine, computed tomography (CT) is an effective
medical imaging modality for the diagnosis of various pathologies. Compared
with X-ray images, CT images can provide more information, including
multi-planar slices and three-dimensional structures for clinical diagnosis.
However, CT imaging requires patients to be exposed to large doses of ionizing
radiation for a long time, which may cause irreversible physical harm. In this
paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on
generated radiation fields. The network can learn a continuous representation
of CT projections from 2D X-ray images by obtaining the internal structure and
depth information and using adaptive loss weights to ensure the quality of the
generated images. Our model is trained on publicly available knee and chest
datasets, and we show the results of CT projection rendering with a single
X-ray and compare our method with other methods based on generated radiation
fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Environment Assessment for Safe Autonomous Quadrotor Landing <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10065v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10065v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Secchiero, Nishanth Bobbili, Yang Zhou, Giuseppe Loianno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous identification and evaluation of safe landing zones are of
paramount importance for ensuring the safety and effectiveness of aerial robots
in the event of system failures, low battery, or the successful completion of
specific tasks. In this paper, we present a novel approach for detection and
assessment of potential landing sites for safe quadrotor landing. Our solution
efficiently integrates 2D and 3D environmental information, eliminating the
need for external aids such as GPS and computationally intensive elevation
maps. The proposed pipeline combines semantic data derived from a Neural
Network (NN), to extract environmental features, with geometric data obtained
from a disparity map, to extract critical geometric attributes such as slope,
flatness, and roughness. We define several cost metrics based on these
attributes to evaluate safety, stability, and suitability of regions in the
environments and identify the most suitable landing area. Our approach runs in
real-time on quadrotors equipped with limited computational capabilities.
Experimental results conducted in diverse environments demonstrate that the
proposed method can effectively assess and identify suitable landing areas,
enabling the safe and autonomous landing of a quadrotor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 1 table, submitted to IEEE International
  Conference on Robotics and Automation (ICRA), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LymphoML: An interpretable artificial intelligence-based method
  identifies morphologic features that correlate with lymphoma subtype 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Shankar, Xiaoli Yang, Vrishab Krishna, Brent Tan, Oscar Silva, Rebecca Rojansky, Andrew Ng, Fabiola Valvert, Edward Briercheck, David Weinstock, Yasodha Natkunam, Sebastian Fernandez-Pol, Pranav Rajpurkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML's interpretable models, developed on a
limited volume of H&E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proceedings of the 3rd Machine Learning for Health
  symposium, Proceedings of Machine Learning Research (PMLR)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PEFT-MedAware: Large Language Model for Medical Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keivalya Pandya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chat models are capable of answering a wide range of questions, however, the
accuracy of their responses is highly uncertain. In this research, we propose a
specialized PEFT-MedAware model where we utilize parameter-efficient
fine-tuning (PEFT) to enhance the Falcon-1b large language model on specialized
MedQuAD data consisting of 16,407 medical QA pairs, leveraging only 0.44% of
its trainable parameters to enhance computational efficiency. The paper adopts
data preprocessing and PEFT to optimize model performance, complemented by a
BitsAndBytesConfig for efficient transformer training. The resulting model was
capable of outperforming other LLMs in medical question-answering tasks in
specific domains with greater accuracy utilizing limited computational
resources making it suitable for deployment in resource-constrained
environments. We propose further improvements through expanded datasets, larger
models, and feedback mechanisms for sustained medical relevancy. Our work
highlights the efficiency gains and specialized capabilities of PEFT in medical
AI, outpacing standard models in precision without extensive resource demands.
The proposed model and data are released for research purposes only.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure, submitted to the Artificial Intelligence in
  Medicine Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ex2Vec: Characterizing Users and Items from the Mere Exposure Effect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Sguerra, Viet-Anh Tran, Romain Hennequin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The traditional recommendation framework seeks to connect user and content,
by finding the best match possible based on users past interaction. However, a
good content recommendation is not necessarily similar to what the user has
chosen in the past. As humans, users naturally evolve, learn, forget, get
bored, they change their perspective of the world and in consequence, of the
recommendable content. One well known mechanism that affects user interest is
the Mere Exposure Effect: when repeatedly exposed to stimuli, users' interest
tends to rise with the initial exposures, reaching a peak, and gradually
decreasing thereafter, resulting in an inverted-U shape. Since previous
research has shown that the magnitude of the effect depends on a number of
interesting factors such as stimulus complexity and familiarity, leveraging
this effect is a way to not only improve repeated recommendation but to gain a
more in-depth understanding of both users and stimuli. In this work we present
(Mere) Exposure2Vec (Ex2Vec) our model that leverages the Mere Exposure Effect
in repeat consumption to derive user and item characterization and track user
interest evolution. We validate our model through predicting future music
consumption based on repetition and discuss its implications for recommendation
scenarios where repetition is common.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Search and Exploration of Greek Painted Pottery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisabeth Trinkl, Stephan Karl, Stefan Lengauer, Reinhold Preiner, Tobias Schreck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on digitally-supported research methods for an important
group of cultural heritage objects, the Greek pottery, especially with figured
decoration. The design, development and application of new digital methods for
searching, comparing, and visually exploring these vases needs an
interdisciplinary approach to effectively analyse the various features of the
vases, like shape, decoration, and manufacturing techniques, and relationships
between the vases. We motivate the need and opportunities by a multimodal
representation of the objects, including 3D shape, material, and painting. We
then illustrate a range of innovative methods for these representations,
including quantified surface and capacity comparison, material analysis, image
flattening from 3D objects, retrieval and comparison of shapes and paintings,
and multidimensional data visualization. We also discuss challenges and future
work in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures, preprint for a book chapter, supplementary
  video available at https://youtu.be/x_Xg0vy3nJY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Word-based <span class="highlight-title">Pre-train</span>ed Item Representation for
  Transferable Recommendation <span class="chip">ICDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghao Yang, Chenyang Wang, Yankai Liu, Kangping Xu, Weizhi Ma, Yiqun Liu, Min Zhang, Haitao Zeng, Junlan Feng, Chao Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Item representation learning (IRL) plays an essential role in recommender
systems, especially for sequential recommendation. Traditional sequential
recommendation models usually utilize ID embeddings to represent items, which
are not shared across different domains and lack the transferable ability.
Recent studies use pre-trained language models (PLM) for item text embeddings
(text-based IRL) that are universally applicable across domains. However, the
existing text-based IRL is unaware of the important collaborative filtering
(CF) information. In this paper, we propose CoWPiRec, an approach of
Collaborative Word-based Pre-trained item representation for Recommendation. To
effectively incorporate CF information into text-based IRL, we convert the
item-level interaction data to a word graph containing word-level
collaborations. Subsequently, we design a novel pre-training task to align the
word-level semantic- and CF-related item representation. Extensive experimental
results on multiple public datasets demonstrate that compared to
state-of-the-art transferable sequential recommenders, CoWPiRec achieves
significantly better performances in both fine-tuning and zero-shot settings
for cross-scenario recommendation and effectively alleviates the cold-start
issue. The code is available at: https://github.com/ysh-1998/CoWPiRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Analysis of Retrievability and PageRank Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Sinha, Priyanshu Raj Mall, Dwaipayan Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accessibility of documents within a collection holds a pivotal role in
Information Retrieval, signifying the ease of locating specific content in a
collection of documents. This accessibility can be achieved via two distinct
avenues. The first is through some retrieval model using a keyword or other
feature-based search, and the other is where a document can be navigated using
links associated with them, if available. Metrics such as PageRank, Hub, and
Authority illuminate the pathways through which documents can be discovered
within the network of content while the concept of Retrievability is used to
quantify the ease with which a document can be found by a retrieval model. In
this paper, we compare these two perspectives, PageRank and retrievability, as
they quantify the importance and discoverability of content in a corpus.
Through empirical experimentation on benchmark datasets, we demonstrate a
subtle similarity between retrievability and PageRank particularly
distinguishable for larger datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at FIRE 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion-Aware Music Recommendation System: Enhancing User Experience
  Through Real-Time Emotional Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tina Babu, Rekha R Nair, Geetha A
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the deficiency in conventional music recommendation
systems by focusing on the vital role of emotions in shaping users music
choices. These systems often disregard the emotional context, relying
predominantly on past listening behavior and failing to consider the dynamic
and evolving nature of users emotional preferences. This gap leads to several
limitations. Users may receive recommendations that do not match their current
mood, which diminishes the quality of their music experience. Furthermore,
without accounting for emotions, the systems might overlook undiscovered or
lesser-known songs that have a profound emotional impact on users. To combat
these limitations, this research introduces an AI model that incorporates
emotional context into the song recommendation process. By accurately detecting
users real-time emotions, the model can generate personalized song
recommendations that align with the users emotional state. This approach aims
to enhance the user experience by offering music that resonates with their
current mood, elicits the desired emotions, and creates a more immersive and
meaningful listening experience. By considering emotional context in the song
recommendation process, the proposed model offers an opportunity for a more
personalized and emotionally resonant musical journey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Privacy Preserving System for Movie Recommendations Using Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Neumann, Andreas Lutz, Karsten Müller, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become ubiquitous in the past years. They solve the
tyranny of choice problem faced by many users, and are utilized by many online
businesses to drive engagement and sales. Besides other criticisms, like
creating filter bubbles within social networks, recommender systems are often
reproved for collecting considerable amounts of personal data. However, to
personalize recommendations, personal information is fundamentally required. A
recent distributed learning scheme called federated learning has made it
possible to learn from personal user data without its central collection.
Consequently, we present a recommender system for movie recommendations, which
provides privacy and thus trustworthiness on multiple levels: First and
foremost, it is trained using federated learning and thus, by its very nature,
privacy-preserving, while still enabling users to benefit from global insights.
Furthermore, a novel federated learning scheme, called FedQ, is employed, which
not only addresses the problem of non-i.i.d.-ness and small local datasets, but
also prevents input data reconstruction attacks by aggregating client updates
early. Finally, to reduce the communication overhead, compression is applied,
which significantly compresses the exchanged neural network parametrizations to
a fraction of their original size. We conjecture that this may also improve
data privacy through its lossy quantization stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the ACM TORS Special Issue on Trustworthy Recommender
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trustworthy Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.06265v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.06265v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoujin Wang, Xiuzhen Zhang, Yan Wang, Huan Liu, Francesco Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RSs) aim to help users to effectively retrieve items of
their interests from a large catalogue. For a quite long period of time,
researchers and practitioners have been focusing on developing accurate RSs.
Recent years have witnessed an increasing number of threats to RSs, coming from
attacks, system and user generated noise, system bias. As a result, it has
become clear that a strict focus on RS accuracy is limited and the research
must consider other important factors, e.g., trustworthiness. For end users, a
trustworthy RS (TRS) should not only be accurate, but also transparent,
unbiased and fair as well as robust to noise or attacks. These observations
actually led to a paradigm shift of the research on RSs: from accuracy-oriented
RSs to TRSs. However, researchers lack a systematic overview and discussion of
the literature in this novel and fast developing field of TRSs. To this end, in
this paper, we provide an overview of TRSs, including a discussion of the
motivation and basic concepts of TRSs, a presentation of the challenges in
building TRSs, and a perspective on the future directions in this area. We also
provide a novel conceptual framework to support the construction of TRSs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Enhanced <span class="highlight-title">BERT</span> for Query Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juanhui Li, Yao Ma, Wei Zeng, Suqi Cheng, Jiliang Tang, Shuaiqiang Wang, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query understanding plays a key role in exploring users' search intents and
facilitating users to locate their most desired information. However, it is
inherently challenging since it needs to capture semantic information from
short and ambiguous queries and often requires massive task-specific labeled
data. In recent years, pre-trained language models (PLMs) have advanced various
natural language processing tasks because they can extract general semantic
information from large-scale corpora. Therefore, there are unprecedented
opportunities to adopt PLMs for query understanding. However, there is a gap
between the goal of query understanding and existing pre-training strategies --
the goal of query understanding is to boost search performance while existing
strategies rarely consider this goal. Thus, directly applying them to query
understanding is sub-optimal. On the other hand, search logs contain user
clicks between queries and urls that provide rich users' search behavioral
information on queries beyond their content. Therefore, in this paper, we aim
to fill this gap by exploring search logs. In particular, to incorporate search
logs into pre-training, we first construct a query graph where nodes are
queries and two queries are connected if they lead to clicks on the same urls.
Then we propose a novel graph-enhanced pre-training framework, GE-BERT, which
can leverage both query content and the query graph. In other words, GE-BERT
can capture both the semantic information and the users' search behavioral
information of queries. Extensive experiments on various query understanding
tasks have demonstrated the effectiveness of the proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> Tuning on Graph-augmented Low-resource Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Wen, Yuan Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text classification is a fundamental problem in information retrieval with
many real-world applications, such as predicting the topics of online articles
and the categories of e-commerce product descriptions. However, low-resource
text classification, with no or few labeled samples, presents a serious concern
for supervised learning. Meanwhile, many text data are inherently grounded on a
network structure, such as a hyperlink/citation network for online articles,
and a user-item purchase network for e-commerce products. These graph
structures capture rich semantic relationships, which can potentially augment
low-resource text classification. In this paper, we propose a novel model
called Graph-Grounded Pre-training and Prompting (G2P2) to address low-resource
text classification in a two-pronged approach. During pre-training, we propose
three graph interaction-based contrastive strategies to jointly pre-train a
graph-text model; during downstream classification, we explore handcrafted
discrete prompts and continuous prompt tuning for the jointly pre-trained model
to achieve zero- and few-shot classification, respectively. Besides, for
generalizing continuous prompts to unseen classes, we propose conditional
prompt tuning on graphs (G2P2$^*$). Extensive experiments on four real-world
datasets demonstrate the strength of G2P2 in zero- and few-shot low-resource
text classification tasks, and illustrate the advantage of G2P2$^*$ in dealing
with unseen classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, journal under review. arXiv admin note: substantial text
  overlap with arXiv:2305.03324</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMRec: Large Language Models with Graph Augmentation for Recommendation <span class="chip">WSDM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00423v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00423v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of data sparsity has long been a challenge in recommendation
systems, and previous studies have attempted to address this issue by
incorporating side information. However, this approach often introduces side
effects such as noise, availability issues, and low data quality, which in turn
hinder the accurate modeling of user preferences and adversely impact
recommendation performance. In light of the recent advancements in large
language models (LLMs), which possess extensive knowledge bases and strong
reasoning capabilities, we propose a novel framework called LLMRec that
enhances recommender systems by employing three simple yet effective LLM-based
graph augmentation strategies. Our approach leverages the rich content
available within online platforms (e.g., Netflix, MovieLens) to augment the
interaction graph in three ways: (i) reinforcing user-item interaction egde,
(ii) enhancing the understanding of item node attributes, and (iii) conducting
user node profiling, intuitively from the natural language perspective. By
employing these strategies, we address the challenges posed by sparse implicit
feedback and low-quality side information in recommenders. Besides, to ensure
the quality of the augmentation, we develop a denoised data robustification
mechanism that includes techniques of noisy implicit feedback pruning and
MAE-based feature enhancement that help refine the augmented data and improve
its reliability. Furthermore, we provide theoretical analysis to support the
effectiveness of LLMRec and clarify the benefits of our method in facilitating
model optimization. Experimental results on benchmark datasets demonstrate the
superiority of our LLM-based augmentation approach over state-of-the-art
techniques. To ensure reproducibility, we have made our code and augmented data
publicly available at: https://github.com/HKUDS/LLMRec.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WSDM 2024 Oral Presentation</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">119</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine learning phase transitions: Connections to the Fisher
  information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Arnold, Niels Lörch, Flemming Holtorf, Frank Schäfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread use and success of machine-learning techniques for
detecting phase transitions from data, their working principle and fundamental
limits remain elusive. Here, we explain the inner workings and identify
potential failure modes of these techniques by rooting popular machine-learning
indicators of phase transitions in information-theoretic concepts. Using tools
from information geometry, we prove that several machine-learning indicators of
phase transitions approximate the square root of the system's (quantum) Fisher
information from below -- a quantity that is known to indicate phase
transitions but is often difficult to compute from data. We numerically
demonstrate the quality of these bounds for phase transitions in classical and
quantum systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7+11 pages, 2+3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emu Video: Factorizing Text-to-Video Generation by Explicit Image
  Conditioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, Ishan Misra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Emu Video, a text-to-video generation model that factorizes the
generation into two steps: first generating an image conditioned on the text,
and then generating a video conditioned on the text and the generated image. We
identify critical design decisions--adjusted noise schedules for diffusion, and
multi-stage training--that enable us to directly generate high quality and high
resolution videos, without requiring a deep cascade of models as in prior work.
In human evaluations, our generated videos are strongly preferred in quality
compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's
PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial
solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing
approach naturally lends itself to animating images based on a user's text
prompt, where our generations are preferred 96% over prior work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://emu-video.metademolab.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfEval: Leveraging the discriminative nature of generative models for
  evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Saketh Rambhatla, Ishan Misra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we show that text-to-image generative models can be 'inverted'
to assess their own text-image understanding capabilities in a completely
automated manner.
  Our method, called SelfEval, uses the generative model to compute the
likelihood of real images given text prompts, making the generative model
directly applicable to discriminative tasks.
  Using SelfEval, we repurpose standard datasets created for evaluating
multimodal text-image discriminative models to evaluate generative models in a
fine-grained manner: assessing their performance on attribute binding, color
recognition, counting, shape recognition, spatial understanding.
  To the best of our knowledge SelfEval is the first automated metric to show a
high degree of agreement for measuring text-faithfulness with the gold-standard
human evaluations across multiple models and benchmarks.
  Moreover, SelfEval enables us to evaluate generative models on challenging
tasks such as Winoground image-score where they demonstrate competitive
performance to discriminative models.
  We also show severe drawbacks of standard automated metrics such as
CLIP-score to measure text faithfulness on benchmarks such as DrawBench, and
how SelfEval sidesteps these issues.
  We hope SelfEval enables easy and reliable automated evaluation for diffusion
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Representation Learning by Alternating Unimodal Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohui Zhang, Jaehong Yoon, Mohit Bansal, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning, which integrates data from diverse sensory modes, plays
a pivotal role in artificial intelligence. However, existing multimodal
learning methods often struggle with challenges where some modalities appear
more dominant than others during multimodal learning, resulting in suboptimal
performance. To address this challenge, we propose MLA (Multimodal Learning
with Alternating Unimodal Adaptation). MLA reframes the conventional joint
multimodal learning process by transforming it into an alternating unimodal
learning process, thereby minimizing interference between modalities.
Simultaneously, it captures cross-modal interactions through a shared head,
which undergoes continuous optimization across different modalities. This
optimization process is controlled by a gradient modification mechanism to
prevent the shared head from losing previously acquired information. During the
inference phase, MLA utilizes a test-time uncertainty-based model fusion
mechanism to integrate multimodal information. Extensive experiments are
conducted on five diverse datasets, encompassing scenarios with complete
modalities and scenarios with missing modalities. These experiments demonstrate
the superiority of MLA over competing prior approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpACNN-LDVAE: Spatial Attention Convolutional Latent Dirichlet
  Variational Autoencoder for Hyperspectral Pixel Unmixing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soham Chitnis, Kiran Mantripragada, Faisal Z. Qureshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Hyperspectral Unxming problem is to find the pure spectral signal of the
underlying materials (endmembers) and their proportions (abundances). The
proposed method builds upon the recently proposed method, Latent Dirichlet
Variational Autoencoder (LDVAE). It assumes that abundances can be encoded as
Dirichlet Distributions while mixed pixels and endmembers are represented by
Multivariate Normal Distributions. However, LDVAE does not leverage spatial
information present in an HSI; we propose an Isotropic CNN encoder with spatial
attention to solve the hyperspectral unmixing problem. We evaluated our model
on Samson, Hydice Urban, Cuprite, and OnTech-HSI-Syn-21 datasets. Our model
also leverages the transfer learning paradigm for Cuprite Dataset, where we
train the model on synthetic data and evaluate it on real-world data. We are
able to observe the improvement in the results for the endmember extraction and
abundance estimation by incorporating the spatial information. Code can be
found at https://github.com/faisalqureshi/cnn-ldvae
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using linear initialisation to improve speed of convergence and
  fully-trained error in Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Marais, Mate Hartstein, George Cevora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Good weight initialisation is an important step in successful training of
Artificial Neural Networks. Over time a number of improvements have been
proposed to this process. In this paper we introduce a novel weight
initialisation technique called the Straddled Matrix Initialiser. This
initialisation technique is motivated by our assumption that major,
global-scale relationships in data are linear with only smaller effects
requiring complex non-linearities. Combination of Straddled Matrix and ReLU
activation function initialises a Neural Network as a de facto linear model,
which we postulate should be a better starting point for optimisation given our
assumptions. We test this by training autoencoders on three datasets using
Straddled Matrix and seven other state-of-the-art weight initialisation
techniques. In all our experiments the Straddeled Matrix Initialiser clearly
outperforms all other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PEFT-MedAware: Large Language Model for Medical Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keivalya Pandya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chat models are capable of answering a wide range of questions, however, the
accuracy of their responses is highly uncertain. In this research, we propose a
specialized PEFT-MedAware model where we utilize parameter-efficient
fine-tuning (PEFT) to enhance the Falcon-1b large language model on specialized
MedQuAD data consisting of 16,407 medical QA pairs, leveraging only 0.44% of
its trainable parameters to enhance computational efficiency. The paper adopts
data preprocessing and PEFT to optimize model performance, complemented by a
BitsAndBytesConfig for efficient transformer training. The resulting model was
capable of outperforming other LLMs in medical question-answering tasks in
specific domains with greater accuracy utilizing limited computational
resources making it suitable for deployment in resource-constrained
environments. We propose further improvements through expanded datasets, larger
models, and feedback mechanisms for sustained medical relevancy. Our work
highlights the efficiency gains and specialized capabilities of PEFT in medical
AI, outpacing standard models in precision without extensive resource demands.
The proposed model and data are released for research purposes only.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure, submitted to the Artificial Intelligence in
  Medicine Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Embedding Dimension for Sparse Subspace Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shabarish Chenakkod, Michał Dereziński, Xiaoyu Dong, Mark Rudelson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A random $m\times n$ matrix $S$ is an oblivious subspace embedding (OSE) with
parameters $\epsilon>0$, $\delta\in(0,1/3)$ and $d\leq m\leq n$, if for any
$d$-dimensional subspace $W\subseteq R^n$,
  $P\big(\,\forall_{x\in W}\ (1+\epsilon)^{-1}\|x\|\leq\|Sx\|\leq
(1+\epsilon)\|x\|\,\big)\geq 1-\delta.$
  It is known that the embedding dimension of an OSE must satisfy $m\geq d$,
and for any $\theta > 0$, a Gaussian embedding matrix with $m\geq (1+\theta) d$
is an OSE with $\epsilon = O_\theta(1)$. However, such optimal embedding
dimension is not known for other embeddings. Of particular interest are sparse
OSEs, having $s\ll m$ non-zeros per column, with applications to problems such
as least squares regression and low-rank approximation.
  We show that, given any $\theta > 0$, an $m\times n$ random matrix $S$ with
$m\geq (1+\theta)d$ consisting of randomly sparsified $\pm1/\sqrt s$ entries
and having $s= O(\log^4(d))$ non-zeros per column, is an oblivious subspace
embedding with $\epsilon = O_{\theta}(1)$. Our result addresses the main open
question posed by Nelson and Nguyen (FOCS 2013), who conjectured that sparse
OSEs can achieve $m=O(d)$ embedding dimension, and it improves on
$m=O(d\log(d))$ shown by Cohen (SODA 2016). We use this to construct the first
oblivious subspace embedding with $O(d)$ embedding dimension that can be
applied faster than current matrix multiplication time, and to obtain an
optimal single-pass algorithm for least squares regression. We further extend
our results to construct even sparser non-oblivious embeddings, leading to the
first subspace embedding with low distortion $\epsilon=o(1)$ and optimal
embedding dimension $m=O(d/\epsilon^2)$ that can be applied in current matrix
multiplication time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling and Retrieving Generalizable Knowledge for Robot Manipulation
  via Language Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihan Zha, Yuchen Cui, Li-Heng Lin, Minae Kwon, Montserrat Gonzalez Arenas, Andy Zeng, Fei Xia, Dorsa Sadigh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's robot policies exhibit subpar performance when faced with the
challenge of generalizing to novel environments. Human corrective feedback is a
crucial form of guidance to enable such generalization. However, adapting to
and learning from online human corrections is a non-trivial endeavor: not only
do robots need to remember human feedback over time to retrieve the right
information in new settings and reduce the intervention rate, but also they
would need to be able to respond to feedback that can be arbitrary corrections
about high-level human preferences to low-level adjustments to skill
parameters. In this work, we present Distillation and Retrieval of Online
Corrections (DROC), a large language model (LLM)-based system that can respond
to arbitrary forms of language feedback, distill generalizable knowledge from
corrections, and retrieve relevant past experiences based on textual and visual
similarity for improving performance in novel settings. DROC is able to respond
to a sequence of online language corrections that address failures in both
high-level task plans and low-level skill primitives. We demonstrate that DROC
effectively distills the relevant information from the sequence of online
corrections in a knowledge base and retrieves that knowledge in settings with
new task or object instances. DROC outperforms other techniques that directly
generate robot code via LLMs by using only half of the total number of
corrections needed in the first round and requires little to no corrections
after two iterations. We show further results, videos, prompts and code on
https://sites.google.com/stanford.edu/droc .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, videos and code links on website
  https://sites.google.com/stanford.edu/droc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Schmitt, Stefan T. Radev, Paul-Christian Bürkner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present multimodal neural posterior estimation (MultiNPE), a method to
integrate heterogeneous data from different sources in simulation-based
inference with neural networks. Inspired by advances in attention-based deep
fusion learning, it empowers researchers to analyze data from different domains
and infer the parameters of complex mathematical models with increased
accuracy. We formulate different multimodal fusion approaches for MultiNPE
(early, late, and hybrid) and evaluate their performance in three challenging
numerical experiments. MultiNPE not only outperforms na\"ive baselines on a
benchmark model, but also achieves superior inference on representative
scientific models from neuroscience and cardiology. In addition, we
systematically investigate the impact of partially missing data on the
different fusion strategies. Across our different experiments, late and hybrid
fusion techniques emerge as the methods of choice for practical applications of
multimodal simulation-based inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Calibration of Deep Learning Sub-Models for Hybrid Numerical
  Modeling Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Said Ouala, Bertrand Chapron, Fabrice Collard, Lucile Gaultier, Ronan Fablet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence and deep learning are currently reshaping numerical
simulation frameworks by introducing new modeling capabilities. These
frameworks are extensively investigated in the context of model correction and
parameterization where they demonstrate great potential and often outperform
traditional physical models. Most of these efforts in defining hybrid dynamical
systems follow {offline} learning strategies in which the neural
parameterization (called here sub-model) is trained to output an ideal
correction. Yet, these hybrid models can face hard limitations when defining
what should be a relevant sub-model response that would translate into a good
forecasting performance. End-to-end learning schemes, also referred to as
online learning, could address such a shortcoming by allowing the deep learning
sub-models to train on historical data. However, defining end-to-end training
schemes for the calibration of neural sub-models in hybrid systems requires
working with an optimization problem that involves the solver of the physical
equations. Online learning methodologies thus require the numerical model to be
differentiable, which is not the case for most modeling systems. To overcome
this difficulty and bypass the differentiability challenge of physical models,
we present an efficient and practical online learning approach for hybrid
systems. The method, called EGA for Euler Gradient Approximation, assumes an
additive neural correction to the physical model, and an explicit Euler
approximation of the gradients. We demonstrate that the EGA converges to the
exact gradients in the limit of infinitely small time steps. Numerical
experiments are performed on various case studies, including prototypical
ocean-atmosphere dynamics. Results show significant improvements over offline
learning, highlighting the potential of end-to-end online learning for hybrid
modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Realistic Joint Space Boundaries for Range of Motion Analysis
  of Healthy and Impaired Human Arms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shafagh Keyvanian, Michelle J. Johnson, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A realistic human kinematic model that satisfies anatomical constraints is
essential for human-robot interaction, biomechanics and robot-assisted
rehabilitation. Modeling realistic joint constraints, however, is challenging
as human arm motion is constrained by joint limits, inter- and intra-joint
dependencies, self-collisions, individual capabilities and muscular or
neurological constraints which are difficult to represent. Hence, physicians
and researchers have relied on simple box-constraints, ignoring important
anatomical factors. In this paper, we propose a data-driven method to learn
realistic anatomically constrained upper-limb range of motion (RoM) boundaries
from motion capture data. This is achieved by fitting a one-class support
vector machine to a dataset of upper-limb joint space exploration motions with
an efficient hyper-parameter tuning scheme. Our approach outperforms similar
works focused on valid RoM learning. Further, we propose an impairment index
(II) metric that offers a quantitative assessment of capability/impairment when
comparing healthy and impaired arms. We validate the metric on healthy subjects
physically constrained to emulate hemiplegia and different disability levels as
stroke patients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-trained Panoptic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shourya Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoptic segmentation is an important computer vision task which combines
semantic and instance segmentation. It plays a crucial role in domains of
medical image analysis, self-driving vehicles, and robotics by providing a
comprehensive understanding of visual environments. Traditionally, deep
learning panoptic segmentation models have relied on dense and accurately
annotated training data, which is expensive and time consuming to obtain.
Recent advancements in self-supervised learning approaches have shown great
potential in leveraging synthetic and unlabelled data to generate pseudo-labels
using self-training to improve the performance of instance and semantic
segmentation models. The three available methods for self-supervised panoptic
segmentation use proposal-based transformer architectures which are
computationally expensive, complicated and engineered for specific tasks. The
aim of this work is to develop a framework to perform embedding-based
self-supervised panoptic segmentation using self-training in a
synthetic-to-real domain adaptation problem setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as
  an Alternative to Attention Layers in <span class="highlight-title">Transformer</span>s <span class="chip">AAAI24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vukasin Bozic, Danilo Dordervic, Daniele Coppola, Joseph Thommes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents an analysis of the effectiveness of using standard shallow
feed-forward networks to mimic the behavior of the attention mechanism in the
original Transformer model, a state-of-the-art architecture for
sequence-to-sequence tasks. We substitute key elements of the attention
mechanism in the Transformer with simple feed-forward networks, trained using
the original components via knowledge distillation. Our experiments, conducted
on the IWSLT2017 dataset, reveal the capacity of these "attentionless
Transformers" to rival the performance of the original architecture. Through
rigorous ablation studies, and experimenting with various replacement network
types and sizes, we offer insights that support the viability of our approach.
This not only sheds light on the adaptability of shallow feed-forward networks
in emulating attention mechanisms but also underscores their potential to
streamline complex architectures for sequence-to-sequence tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI24(https://aaai.org/aaai-conference/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-delay arterial spin-labeled perfusion estimation with biophysics
  simulation and deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjiu Hu, Qihao Zhang, Pascal Spincemaille, Thanh D. Nguyen, Yi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To develop biophysics-based method for estimating perfusion Q from
arterial spin labeling (ASL) images using deep learning. Methods: A 3D U-Net
(QTMnet) was trained to estimate perfusion from 4D tracer propagation images.
The network was trained and tested on simulated 4D tracer concentration data
based on artificial vasculature structure generated by constrained constructive
optimization (CCO) method. The trained network was further tested in a
synthetic brain ASL image based on vasculature network extracted from magnetic
resonance (MR) angiography. The estimations from both trained network and a
conventional kinetic model were compared in ASL images acquired from eight
healthy volunteers. Results: QTMnet accurately reconstructed perfusion Q from
concentration data. Relative error of the synthetic brain ASL image was 7.04%
for perfusion Q, lower than the error using single-delay ASL model: 25.15% for
Q, and multi-delay ASL model: 12.62% for perfusion Q. Conclusion: QTMnet
provides accurate estimation on perfusion parameters and is a promising
approach as a clinical ASL MRI image processing pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concept-free Causal Disentanglement with Variational Graph Auto-Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyun Feng, Lin Zhang, Lili Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In disentangled representation learning, the goal is to achieve a compact
representation that consists of all interpretable generative factors in the
observational data. Learning disentangled representations for graphs becomes
increasingly important as graph data rapidly grows. Existing approaches often
rely on Variational Auto-Encoder (VAE) or its causal structure learning-based
refinement, which suffer from sub-optimality in VAEs due to the independence
factor assumption and unavailability of concept labels, respectively. In this
paper, we propose an unsupervised solution, dubbed concept-free causal
disentanglement, built on a theoretically provable tight upper bound
approximating the optimal factor. This results in an SCM-like causal structure
modeling that directly learns concept structures from data. Based on this idea,
we propose Concept-free Causal VGAE (CCVGAE) by incorporating a novel causal
disentanglement layer into Variational Graph Auto-Encoder. Furthermore, we
prove concept consistency under our concept-free causal disentanglement
framework, hence employing it to enhance the meta-learning framework, called
concept-free causal Meta-Graph (CC-Meta-Graph). We conduct extensive
experiments to demonstrate the superiority of the proposed models: CCVGAE and
CC-Meta-Graph, reaching up to $29\%$ and $11\%$ absolute improvements over
baselines in terms of AUC, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting the Probability of Collision of a Satellite with Space
  Debris: A Bayesian Machine Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Simões Catulo, Cláudia Soares, Marta Guimarães
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Space is becoming more crowded in Low Earth Orbit due to increased space
activity. Such a dense space environment increases the risk of collisions
between space objects endangering the whole space population. Therefore, the
need to consider collision avoidance as part of routine operations is evident
to satellite operators. Current procedures rely on the analysis of multiple
collision warnings by human analysts. However, with the continuous growth of
the space population, this manual approach may become unfeasible, highlighting
the importance of automation in risk assessment. In 2019, ESA launched a
competition to study the feasibility of applying machine learning in collision
risk estimation and released a dataset that contained sequences of Conjunction
Data Messages (CDMs) in support of real close encounters. The competition
results showed that the naive forecast and its variants are strong predictors
for this problem, which suggests that the CDMs may follow the Markov property.
The proposed work investigates this theory by benchmarking Hidden Markov Models
(HMM) in predicting the risk of collision between two resident space objects by
using one feature of the entire dataset: the sequence of the probability in the
CDMs. In addition, Bayesian statistics are used to infer a joint distribution
for the parameters of the models, which allows the development of robust and
reliable probabilistic predictive models that can incorporate physical or prior
knowledge about the problem within a rigorous theoretical framework and
provides prediction uncertainties that nicely reflect the accuracy of the
predicted risk. This work shows that the implemented HMM outperforms the naive
solution in some metrics, which further adds to the idea that the collision
warnings may be Markovian and suggests that this is a powerful method to be
further explored.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Poincaré Inequality and Consistency Results for Signal Sampling on
  Large Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thien Le, Luana Ruiz, Stefanie Jegelka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale graph machine learning is challenging as the complexity of
learning models scales with the graph size. Subsampling the graph is a viable
alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.
Existing graph sampling techniques require not only computing the spectra of
large matrices but also repeating these computations when the graph changes,
e.g., grows. In this paper, we introduce a signal sampling theory for a type of
graph limit -- the graphon. We prove a Poincar\'e inequality for graphon
signals and show that complements of node subsets satisfying this inequality
are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting
connections with spectral clustering and Gaussian elimination, we prove that
such sampling sets are consistent in the sense that unique sampling sets on a
convergent graph sequence converge to unique sampling sets on the graphon. We
then propose a related graphon signal sampling algorithm for large graphs, and
demonstrate its good empirical performance on graph machine learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data
  Fitted Networks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Feuer, Chinmay Hegde, Niv Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular classification has traditionally relied on supervised algorithms,
which estimate the parameters of a prediction model using its training data.
Recently, Prior-Data Fitted Networks (PFNs) such as TabPFN have successfully
learned to classify tabular data in-context: the model parameters are designed
to classify new samples based on labelled training samples given after the
model training. While such models show great promise, their applicability to
real-world data remains limited due to the computational scale needed. Here we
study the following question: given a pre-trained PFN for tabular data, what is
the best way to summarize the labelled training samples before feeding them to
the model? We conduct an initial investigation of sketching and
feature-selection methods for TabPFN, and note certain key differences between
it and conventionally fitted tabular models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2nd Table Representation Learning Workshop: 37th Conference on Neural
  Information Processing Systems (NeurIPS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Inference on the Edge: A Design Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Sedlak, Victor Casamayor Pujol, Praveen Kumar Donta, Schahram Dustdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) is a common tool to interpret and predict the behavior
of distributed computing systems, e.g., to optimize the task distribution
between devices. As more and more data is created by Internet of Things (IoT)
devices, data processing and ML training are carried out by edge devices in
close proximity. To ensure Quality of Service (QoS) throughout these
operations, systems are supervised and dynamically adapted with the help of ML.
However, as long as ML models are not retrained, they fail to capture gradual
shifts in the variable distribution, leading to an inaccurate view of the
system state. Moreover, as the prediction accuracy decreases, the reporting
device should actively resolve uncertainties to improve the model's precision.
Such a level of self-determination could be provided by Active Inference (ACI)
-- a concept from neuroscience that describes how the brain constantly predicts
and evaluates sensory information to decrease long-term surprise. We
encompassed these concepts in a single action-perception cycle, which we
implemented for distributed agents in a smart manufacturing use case. As a
result, we showed how our ACI agent was able to quickly and traceably solve an
optimization problem while fulfilling QoS requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing Reconfigurable Intelligent Systems with Markov Blankets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Sedlak, Victor Casamayor Pujol, Praveen Kumar Donta, Schahram Dustdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compute Continuum (CC) systems comprise a vast number of devices distributed
over computational tiers. Evaluating business requirements, i.e., Service Level
Objectives (SLOs), requires collecting data from all those devices; if SLOs are
violated, devices must be reconfigured to ensure correct operation. If done
centrally, this dramatically increases the number of devices and variables that
must be considered, while creating an enormous communication overhead. To
address this, we (1) introduce a causality filter based on Markov blankets (MB)
that limits the number of variables that each device must track, (2) evaluate
SLOs decentralized on a device basis, and (3) infer optimal device
configuration for fulfilling SLOs. We evaluated our methodology by analyzing
video stream transformations and providing device configurations that ensure
the Quality of Service (QoS). The devices thus perceived their environment and
acted accordingly -- a form of decentralized intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EduGym: An Environment Suite for Reinforcement Learning Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas M. Moerland, Matthias Müller-Brockhausen, Zhao Yang, Andrius Bernatavicius, Koen Ponse, Tom Kouwenhoven, Andreas Sauter, Michiel van der Meer, Bram Renting, Aske Plaat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the empirical success of reinforcement learning, an increasing number
of students study the subject. However, from our practical teaching experience,
we see students entering the field (bachelor, master and early PhD) often
struggle. On the one hand, textbooks and (online) lectures provide the
fundamentals, but students find it hard to translate between equations and
code. On the other hand, public codebases do provide practical examples, but
the implemented algorithms tend to be complex, and the underlying test
environments contain multiple reinforcement learning challenges at once.
Although this is realistic from a research perspective, it often hinders
educational conceptual understanding. To solve this issue we introduce EduGym,
a set of educational reinforcement learning environments and associated
interactive notebooks tailored for education. Each EduGym environment is
specifically designed to illustrate a certain aspect/challenge of reinforcement
learning (e.g., exploration, partial observability, stochasticity, etc.), while
the associated interactive notebook explains the challenge and its possible
solution approaches, connecting equations and code in a single document. An
evaluation among RL students and researchers shows 86% of them think EduGym is
a useful tool for reinforcement learning education. All notebooks are available
from https://sites.google.com/view/edu-gym/home, while the full software
package can be installed from https://github.com/RLG-Leiden/edugym.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Maximum a Posteriori Filtering via Adaptive Optimization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca M. Bencomo, Jake C. Snell, Thomas L. Griffiths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian filtering approximates the true underlying behavior of a
time-varying system by inverting an explicit generative model to convert noisy
measurements into state estimates. This process typically requires either
storage, inversion, and multiplication of large matrices or Monte Carlo
estimation, neither of which are practical in high-dimensional state spaces
such as the weight spaces of artificial neural networks. Here, we frame the
standard Bayesian filtering problem as optimization over a time-varying
objective. Instead of maintaining matrices for the filtering equations or
simulating particles, we specify an optimizer that defines the Bayesian filter
implicitly. In the linear-Gaussian setting, we show that every Kalman filter
has an equivalent formulation using K steps of gradient descent. In the
nonlinear setting, our experiments demonstrate that our framework results in
filters that are effective, robust, and scalable to high-dimensional systems,
comparing well against the standard toolbox of Bayesian filtering solutions. We
suggest that it is easier to fine-tune an optimizer than it is to specify the
correct filtering equations, making our framework an attractive option for
high-dimensional filtering problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Networks for Pressure Estimation in Water Distribution
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Truong, Andrés Tello, Alexander Lazovik, Victoria Degeler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pressure and flow estimation in Water Distribution Networks (WDN) allows
water management companies to optimize their control operations. For many
years, mathematical simulation tools have been the most common approach to
reconstructing an estimate of the WDN hydraulics. However, pure physics-based
simulations involve several challenges, e.g. partially observable data, high
uncertainty, and extensive manual configuration. Thus, data-driven approaches
have gained traction to overcome such limitations. In this work, we combine
physics-based modeling and Graph Neural Networks (GNN), a data-driven approach,
to address the pressure estimation problem. First, we propose a new data
generation method using a mathematical simulation but not considering temporal
patterns and including some control parameters that remain untouched in
previous works; this contributes to a more diverse training data. Second, our
training strategy relies on random sensor placement making our GNN-based
estimation model robust to unexpected sensor location changes. Third, a
realistic evaluation protocol considers real temporal patterns and additionally
injects the uncertainties intrinsic to real-world scenarios. Finally, a
multi-graph pre-training strategy allows the model to be reused for pressure
estimation in unseen target WDNs. Our GNN-based model estimates the pressure of
a large-scale WDN in The Netherlands with a MAE of 1.94mH$_2$O and a MAPE of
7%, surpassing the performance of previous studies. Likewise, it outperformed
previous approaches on other WDN benchmarks, showing a reduction of absolute
error up to approximately 52% in the best cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Water Resources Research. Huy Truong and Andr\'es Tello
  contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSB: Simple but Strong Baseline for Boosting Performance of Open-Set
  Semi-Supervised Learning <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Fan, Anna Kukleva, Dengxin Dai, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods effectively leverage unlabeled data to
improve model generalization. However, SSL models often underperform in
open-set scenarios, where unlabeled data contain outliers from novel categories
that do not appear in the labeled set. In this paper, we study the challenging
and realistic open-set SSL setting, where the goal is to both correctly
classify inliers and to detect outliers. Intuitively, the inlier classifier
should be trained on inlier data only. However, we find that inlier
classification performance can be largely improved by incorporating
high-confidence pseudo-labeled data, regardless of whether they are inliers or
outliers. Also, we propose to utilize non-linear transformations to separate
the features used for inlier classification and outlier detection in the
multi-task learning framework, preventing adverse effects between them.
Additionally, we introduce pseudo-negative mining, which further boosts outlier
detection performance. The three ingredients lead to what we call Simple but
Strong Baseline (SSB) for open-set SSL. In experiments, SSB greatly improves
both inlier classification and outlier detection performance, outperforming
existing methods by a large margin. Our code will be released at
https://github.com/YUE-FAN/SSB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted in ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Amortized Likelihood Ratio Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam D. Cobb, Brian Matejek, Daniel Elenius, Anirban Roy, Susmit Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new amortized likelihood ratio estimator for likelihood-free
simulation-based inference (SBI). Our estimator is simple to train and
estimates the likelihood ratio using a single forward pass of the neural
estimator. Our approach directly computes the likelihood ratio between two
competing parameter sets which is different from the previous approach of
comparing two neural network output values. We refer to our model as the direct
neural ratio estimator (DNRE). As part of introducing the DNRE, we derive a
corresponding Monte Carlo estimate of the posterior. We benchmark our new ratio
estimator and compare to previous ratio estimators in the literature. We show
that our new ratio estimator often outperforms these previous approaches. As a
further contribution, we introduce a new derivative estimator for likelihood
ratio estimators that enables us to compare likelihood-free Hamiltonian Monte
Carlo (HMC) with random-walk Metropolis-Hastings (MH). We show that HMC is
equally competitive, which has not been previously shown. Finally, we include a
novel real-world application of SBI by using our neural ratio estimator to
design a quadcopter. Code is available at https://github.com/SRI-CSL/dnre.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 Pages, 10 Figures, GitHub: https://github.com/SRI-CSL/dnre</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RONAALP: Reduced-Order Nonlinear Approximation with Active Learning
  Procedure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Scherding, Georgios Rigas, Denis Sipp, Peter J Schmid, Taraneh Sayadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many engineering applications rely on the evaluation of expensive, non-linear
high-dimensional functions. In this paper, we propose the RONAALP algorithm
(Reduced Order Nonlinear Approximation with Active Learning Procedure) to
incrementally learn a fast and accurate reduced-order surrogate model of a
target function on-the-fly as the application progresses. First, the
combination of nonlinear auto-encoder, community clustering and radial basis
function networks allows to learn an efficient and compact surrogate model with
limited training data. Secondly, the active learning procedure overcome any
extrapolation issue when evaluating the surrogate model outside of its initial
training range during the online stage. This results in generalizable, fast and
accurate reduced-order models of high-dimensional functions. The method is
demonstrated on three direct numerical simulations of hypersonic flows in
chemical nonequilibrium. Accurate simulations of these flows rely on detailed
thermochemical gas models that dramatically increase the cost of such
calculations. Using RONAALP to learn a reduced-order thermodynamic model
surrogate on-the-fly, the cost of such simulation was reduced by up to 75%
while maintaining an error of less than 10% on relevant quantities of interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing VQ-VAE for End-to-End Health Indicator Generation in
  Predicting Rolling Bearing RUL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junliang Wang, Qinghua Zhang, Guanhua Zhu, Guoxi Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prediction of the remaining useful life (RUL) of rolling bearings is a
pivotal issue in industrial production. A crucial approach to tackling this
issue involves transforming vibration signals into health indicators (HI) to
aid model training. This paper presents an end-to-end HI construction method,
vector quantised variational autoencoder (VQ-VAE), which addresses the need for
dimensionality reduction of latent variables in traditional unsupervised
learning methods such as autoencoder. Moreover, concerning the inadequacy of
traditional statistical metrics in reflecting curve fluctuations accurately,
two novel statistical metrics, mean absolute distance (MAD) and mean variance
(MV), are introduced. These metrics accurately depict the fluctuation patterns
in the curves, thereby indicating the model's accuracy in discerning similar
features. On the PMH2012 dataset, methods employing VQ-VAE for label
construction achieved lower values for MAD and MV. Furthermore, the ASTCN
prediction model trained with VQ-VAE labels demonstrated commendable
performance, attaining the lowest values for MAD and MV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind the map! Accounting for existing map information when estimating
  online HDMaps from sensor data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rémy Sun, Li Yang, Diane Lingrand, Frédéric Precioso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online High Definition Map (HDMap) estimation from sensors offers a low-cost
alternative to manually acquired HDMaps. As such, it promises to lighten costs
for already HDMap-reliant Autonomous Driving systems, and potentially even
spread their use to new systems. In this paper, we propose to improve online
HDMap estimation by accounting for already existing maps. We identify 3
reasonable types of useful existing maps (minimalist, noisy, and outdated). We
also introduce MapEX, a novel online HDMap estimation framework that accounts
for existing maps. MapEX achieves this by encoding map elements into query
tokens and by refining the matching algorithm used to train classic query based
map estimation models. We demonstrate that MapEX brings significant
improvements on the nuScenes dataset. For instance, MapEX - given noisy maps -
improves by 38% over the MapTRv2 detector it is based on and by 16% over the
current SOTA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Fairness-Guided <span class="highlight-title">Dataset</span> Reweighting using Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Zhao, Klaus Broelemann, Salvatore Ruggieri, Gjergji Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of achieving fairness in machine learning models cannot be
overstated. Recent research has pointed out that fairness should be examined
from a causal perspective, and several fairness notions based on the on Pearl's
causal framework have been proposed. In this paper, we construct a reweighting
scheme of datasets to address causal fairness. Our approach aims at mitigating
bias by considering the causal relationships among variables and incorporating
them into the reweighting process. The proposed method adopts two neural
networks, whose structures are intentionally used to reflect the structures of
a causal graph and of an interventional graph. The two neural networks can
approximate the causal model of the data, and the causal model of
interventions. Furthermore, reweighting guided by a discriminator is applied to
achieve various fairness notions. Experiments on real-world datasets show that
our method can achieve causal fairness on the data while remaining close to the
original data for downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the proceedings of 2023 IEEE International
  Conference on Big Data (IEEE BigData 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Principle to Practice: Vertical Data Minimization for Machine
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Staab, Nikola Jovanović, Mislav Balunović, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming to train and deploy predictive models, organizations collect large
amounts of detailed client data, risking the exposure of private information in
the event of a breach. To mitigate this, policymakers increasingly demand
compliance with the data minimization (DM) principle, restricting data
collection to only that data which is relevant and necessary for the task.
Despite regulatory pressure, the problem of deploying machine learning models
that obey DM has so far received little attention. In this work, we address
this challenge in a comprehensive manner. We propose a novel vertical DM (vDM)
workflow based on data generalization, which by design ensures that no
full-resolution client data is collected during training and deployment of
models, benefiting client privacy by reducing the attack surface in case of a
breach. We formalize and study the corresponding problem of finding
generalizations that both maximize data utility and minimize empirical privacy
risk, which we quantify by introducing a diverse set of policy-aligned
adversarial scenarios. Finally, we propose a range of baseline vDM algorithms,
as well as Privacy-aware Tree (PAT), an especially effective vDM algorithm that
outperforms all baselines across several settings. We plan to release our code
as a publicly available library, helping advance the standardization of DM for
machine learning. Overall, we believe our work can help lay the foundation for
further exploration and adoption of DM principles in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE S&P 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handling Overlapping Asymmetric <span class="highlight-title">Dataset</span>s -- A Twice Penalized P-Spline
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew McTeer, Robin Henderson, Quentin M Anstee, Paolo Missier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overlapping asymmetric datasets are common in data science and pose questions
of how they can be incorporated together into a predictive analysis. In
healthcare datasets there is often a small amount of information that is
available for a larger number of patients such as an electronic health record,
however a small number of patients may have had extensive further testing.
Common solutions such as missing imputation can often be unwise if the smaller
cohort is significantly different in scale to the larger sample, therefore the
aim of this research is to develop a new method which can model the smaller
cohort against a particular response, whilst considering the larger cohort
also. Motivated by non-parametric models, and specifically flexible smoothing
techniques via generalized additive models, we model a twice penalized P-Spline
approximation method to firstly prevent over/under-fitting of the smaller
cohort and secondly to consider the larger cohort. This second penalty is
created through discrepancies in the marginal value of covariates that exist in
both the smaller and larger cohorts. Through data simulations, parameter
tunings and model adaptations to consider a continuous and binary response, we
find our twice penalized approach offers an enhanced fit over a linear B-Spline
and once penalized P-Spline approximation. Applying to a real-life dataset
relating to a person's risk of developing Non-Alcoholic Steatohepatitis, we see
an improved model fit performance of over 65%. Areas for future work within
this space include adapting our method to not require dimensionality reduction
and also consider parametric modelling methods. However, to our knowledge this
is the first work to propose additional marginal penalties in a flexible
regression of which we can report a vastly improved model fit that is able to
consider asymmetric datasets, without the need for missing data imputation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 17 figures, 8 tables, 34 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regions are Who Walk Them: a Large <span class="highlight-title">Pre-train</span>ed Spatiotemporal Model
  Based on Human Mobility for Ubiquitous Urban Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixing Zhang, Liangzhe Han, Leilei Sun, Yunqi Liu, Jibin Wang, Weifeng Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User profiling and region analysis are two tasks of significant commercial
value. However, in practical applications, modeling different features
typically involves four main steps: data preparation, data processing, model
establishment, evaluation, and optimization. This process is time-consuming and
labor-intensive. Repeating this workflow for each feature results in abundant
development time for tasks and a reduced overall volume of task development.
Indeed, human mobility data contains a wealth of information. Several
successful cases suggest that conducting in-depth analysis of population
movement data could potentially yield meaningful profiles about users and
areas. Nonetheless, most related works have not thoroughly utilized the
semantic information within human mobility data and trained on a fixed number
of the regions. To tap into the rich information within population movement,
based on the perspective that Regions Are Who walk them, we propose a large
spatiotemporal model based on trajectories (RAW). It possesses the following
characteristics: 1) Tailored for trajectory data, introducing a GPT-like
structure with a parameter count of up to 1B; 2) Introducing a spatiotemporal
fine-tuning module, interpreting trajectories as collection of users to derive
arbitrary region embedding. This framework allows rapid task development based
on the large spatiotemporal model. We conducted extensive experiments to
validate the effectiveness of our proposed large spatiotemporal model. It's
evident that our proposed method, relying solely on human mobility data without
additional features, exhibits a certain level of relevance in user profiling
and region analysis. Moreover, our model showcases promising predictive
capabilities in trajectory generation tasks based on the current state,
offering the potential for further innovative work utilizing this large
spatiotemporal model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Cooperative Game Theory to Prune Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mauricio Diaz-Ortiz Jr, Benjamin Kempinski, Daphne Cornelisse, Yoram Bachrach, Tal Kachman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show how solution concepts from cooperative game theory can be used to
tackle the problem of pruning neural networks.
  The ever-growing size of deep neural networks (DNNs) increases their
performance, but also their computational requirements. We introduce a method
called Game Theory Assisted Pruning (GTAP), which reduces the neural network's
size while preserving its predictive accuracy. GTAP is based on eliminating
neurons in the network based on an estimation of their joint impact on the
prediction quality through game theoretic solutions. Specifically, we use a
power index akin to the Shapley value or Banzhaf index, tailored using a
procedure similar to Dropout (commonly used to tackle overfitting problems in
machine learning).
  Empirical evaluation of both feedforward networks and convolutional neural
networks shows that this method outperforms existing approaches in the achieved
tradeoff between the number of parameters and model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate and Fast Fischer-Tropsch Reaction Microkinetics using PINNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshil Patel, Aniruddha Panda, Tymofii Nikolaienko, Stanislav Jaso, Alejandro Lopez, Kaushic Kalyanaraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microkinetics allows detailed modelling of chemical transformations occurring
in many industrially relevant reactions. Traditional way of solving the
microkinetics model for Fischer-Tropsch synthesis (FTS) becomes inefficient
when it comes to more advanced real-time applications. In this work, we address
these challenges by using physics-informed neural networks(PINNs) for modelling
FTS microkinetics. We propose a computationally efficient and accurate method,
enabling the ultra-fast solution of the existing microkinetics models in
realistic process conditions. The proposed PINN model computes the fraction of
vacant catalytic sites, a key quantity in FTS microkinetics, with median
relative error (MRE) of 0.03%, and the FTS product formation rates with MRE of
0.1%. Compared to conventional equation solvers, the model achieves up to 1E+06
times speed-up when running on GPUs, thus being fast enough for multi-scale and
multi-physics reactor modelling and enabling its applications in real-time
process control and optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepClean: Machine Unlearning on the Cheap by Resetting Privacy
  Sensitive Weights using the Fisher Diagonal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaeli Shi, Najah Ghalyan, Kostis Gourgoulias, John Buford, Sean Moran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models trained on sensitive or private data can
inadvertently memorize and leak that information. Machine unlearning seeks to
retroactively remove such details from model weights to protect privacy. We
contribute a lightweight unlearning algorithm that leverages the Fisher
Information Matrix (FIM) for selective forgetting. Prior work in this area
requires full retraining or large matrix inversions, which are computationally
expensive. Our key insight is that the diagonal elements of the FIM, which
measure the sensitivity of log-likelihood to changes in weights, contain
sufficient information for effective forgetting. Specifically, we compute the
FIM diagonal over two subsets -- the data to retain and forget -- for all
trainable weights. This diagonal representation approximates the complete FIM
while dramatically reducing computation. We then use it to selectively update
weights to maximize forgetting of the sensitive subset while minimizing impact
on the retained subset. Experiments show that our algorithm can successfully
forget any randomly selected subsets of training data across neural network
architectures. By leveraging the FIM diagonal, our approach provides an
interpretable, lightweight, and efficient solution for machine unlearning with
practical privacy benefits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Residual CNN for Multi-Class Chest Infection Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Donghan Kwon, Dohyun Lim, Yoonha Lee, Seung Won Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of deep learning has significantly propelled the capabilities of
automated medical image diagnosis, providing valuable tools and resources in
the realm of healthcare and medical diagnostics. This research delves into the
development and evaluation of a Deep Residual Convolutional Neural Network
(CNN) for the multi-class diagnosis of chest infections, utilizing chest X-ray
images. The implemented model, trained and validated on a dataset amalgamated
from diverse sources, demonstrated a robust overall accuracy of 93%. However,
nuanced disparities in performance across different classes, particularly
Fibrosis, underscored the complexity and challenges inherent in automated
medical image diagnosis. The insights derived pave the way for future research,
focusing on enhancing the model's proficiency in classifying conditions that
present more subtle and nuanced visual features in the images, as well as
optimizing and refining the model architecture and training process. This paper
provides a comprehensive exploration into the development, implementation, and
evaluation of the model, offering insights and directions for future research
and development in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maintenance Techniques for Anomaly Detection AIOps Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorena Poenaru-Olaru, Natalia Karpova, Luis Cruz, Jan Rellermeyer, Arie van Deursen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection techniques are essential in automating the monitoring of IT
systems and operations. These techniques imply that machine learning algorithms
are trained on operational data corresponding to a specific period of time and
that they are continuously evaluated on newly emerging data. Operational data
is constantly changing over time, which affects the performance of deployed
anomaly detection models. Therefore, continuous model maintenance is required
to preserve the performance of anomaly detectors over time. In this work, we
analyze two different anomaly detection model maintenance techniques in terms
of the model update frequency, namely blind model retraining and informed model
retraining. We further investigate the effects of updating the model by
retraining it on all the available data (full-history approach) and on only the
newest data (sliding window approach). Moreover, we investigate whether a data
change monitoring tool is capable of determining when the anomaly detection
model needs to be updated through retraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynaPipe: Optimizing Multi-task Training through Dynamic Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Jiang, Zhen Jia, Shuai Zheng, Yida Wang, Chuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task model training has been adopted to enable a single deep neural
network model (often a large language model) to handle multiple tasks (e.g.,
question answering and text summarization). Multi-task training commonly
receives input sequences of highly different lengths due to the diverse
contexts of different tasks. Padding (to the same sequence length) or packing
(short examples into long sequences of the same length) is usually adopted to
prepare input samples for model training, which is nonetheless not space or
computation efficient. This paper proposes a dynamic micro-batching approach to
tackle sequence length variation and enable efficient multi-task model
training. We advocate pipeline-parallel training of the large model with
variable-length micro-batches, each of which potentially comprises a different
number of samples. We optimize micro-batch construction using a dynamic
programming-based approach, and handle micro-batch execution time variation
through dynamic pipeline and communication scheduling, enabling highly
efficient pipeline training. Extensive evaluation on the FLANv2 dataset
demonstrates up to 4.39x higher training throughput when training T5, and 3.25x
when training GPT, as compared with packing-based baselines. DynaPipe's source
code is publicly available at
https://github.com/awslabs/optimizing-multitask-training-through-dynamic-pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Energy Marketplace via NFTs and AI-based Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasoul Nikbakht, Farhana Javed, Farhad Rezazadeh, Nikolaos Bartzoudis, Josep Mangues-Bafalluy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper introduces an advanced Decentralized Energy Marketplace (DEM)
integrating blockchain technology and artificial intelligence to manage energy
exchanges among smart homes with energy storage systems. The proposed framework
uses Non-Fungible Tokens (NFTs) to represent unique energy profiles in a
transparent and secure trading environment. Leveraging Federated Deep
Reinforcement Learning (FDRL), the system promotes collaborative and adaptive
energy management strategies, maintaining user privacy. A notable innovation is
the use of smart contracts, ensuring high efficiency and integrity in energy
transactions. Extensive evaluations demonstrate the system's scalability and
the effectiveness of the FDRL method in optimizing energy distribution. This
research significantly contributes to developing sophisticated decentralized
smart grid infrastructures. Our approach broadens potential blockchain and AI
applications in sustainable energy systems and addresses incentive alignment
and transparency challenges in traditional energy trading mechanisms. The
implementation of this paper is publicly accessible at
\url{https://github.com/RasoulNik/DEM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Bridge between Dynamical Systems and Machine Learning: Engineered
  Ordinary Differential Equations as Classification Algorithm (EODECA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raffaele Marino, Lorenzo Giambagli, Lorenzo Chicchi, Lorenzo Buffoni, Duccio Fanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a world increasingly reliant on machine learning, the interpretability of
these models remains a substantial challenge, with many equating their
functionality to an enigmatic black box. This study seeks to bridge machine
learning and dynamical systems. Recognizing the deep parallels between dense
neural networks and dynamical systems, particularly in the light of
non-linearities and successive transformations, this manuscript introduces the
Engineered Ordinary Differential Equations as Classification Algorithms
(EODECAs). Uniquely designed as neural networks underpinned by continuous
ordinary differential equations, EODECAs aim to capitalize on the
well-established toolkit of dynamical systems. Unlike traditional deep learning
models, which often suffer from opacity, EODECAs promise both high
classification performance and intrinsic interpretability. They are naturally
invertible, granting them an edge in understanding and transparency over their
counterparts. By bridging these domains, we hope to usher in a new era of
machine learning models where genuine comprehension of data processes
complements predictive prowess.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delete My Account: Impact of Data Deletion on Machine Learning
  Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Dam, Maximilian Henzl, Lukas Daniel Klausner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users are more aware than ever of the importance of their own data, thanks to
reports about security breaches and leaks of private, often sensitive data in
recent years. Additionally, the GDPR has been in effect in the European Union
for over three years and many people have encountered its effects in one way or
another. Consequently, more and more users are actively protecting their
personal data. One way to do this is to make of the right to erasure guaranteed
in the GDPR, which has potential implications for a number of different fields,
such as big data and machine learning.
  Our paper presents an in-depth analysis about the impact of the use of the
right to erasure on the performance of machine learning models on
classification tasks. We conduct various experiments utilising different
datasets as well as different machine learning algorithms to analyse a variety
of deletion behaviour scenarios. Due to the lack of credible data on actual
user behaviour, we make reasonable assumptions for various deletion modes and
biases and provide insight into the effects of different plausible scenarios
for right to erasure usage on data quality of machine learning. Our results
show that the impact depends strongly on the amount of data deleted, the
particular characteristics of the dataset and the bias chosen for deletion and
assumptions on user behaviour.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Message-Enhanced Contrastive Learning for Graph Anomaly
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Xu, Nan Wang, Xuezhi Wen, Meiqi Gao, Chaoqun Guo, Xibin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph anomaly detection plays a crucial role in identifying exceptional
instances in graph data that deviate significantly from the majority. It has
gained substantial attention in various domains of information security,
including network intrusion, financial fraud, and malicious comments, et al.
Existing methods are primarily developed in an unsupervised manner due to the
challenge in obtaining labeled data. For lack of guidance from prior knowledge
in unsupervised manner, the identified anomalies may prove to be data noise or
individual data instances. In real-world scenarios, a limited batch of labeled
anomalies can be captured, making it crucial to investigate the few-shot
problem in graph anomaly detection. Taking advantage of this potential, we
propose a novel few-shot Graph Anomaly Detection model called FMGAD (Few-shot
Message-Enhanced Contrastive-based Graph Anomaly Detector). FMGAD leverages a
self-supervised contrastive learning strategy within and across views to
capture intrinsic and transferable structural representations. Furthermore, we
propose the Deep-GNN message-enhanced reconstruction module, which extensively
exploits the few-shot label information and enables long-range propagation to
disseminate supervision signals to deeper unlabeled nodes. This module in turn
assists in the training of self-supervised contrastive learning. Comprehensive
experimental results on six real-world datasets demonstrate that FMGAD can
achieve better performance than other state-of-the-art methods, regardless of
artificially injected anomalies or domain-organic anomalies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel
  Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqing Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Highly parallelized workloads like machine learning training, inferences and
general HPC tasks are greatly accelerated using GPU devices. In a cloud
computing cluster, serving a GPU's computation power through multi-tasks
sharing is highly demanded since there are always more task requests than the
number of GPU available. Existing GPU sharing solutions focus on reducing
task-level waiting time or task-level switching costs when multiple jobs
competing for a single GPU. Non-stopped computation requests come with
different priorities, having non-symmetric impact on QoS for sharing a GPU
device. Existing work missed the kernel-level optimization opportunity brought
by this setting. To address this problem, we present a novel kernel-level
scheduling strategy called FIKIT: Filling Inter-kernel Idle Time. FIKIT
incorporates task-level priority information, fine-grained kernel
identification, and kernel measurement, allowing low priorities task's
execution during high priority task's inter-kernel idle time. Thereby, filling
the GPU's device runtime fully, and reduce overall GPU sharing impact to cloud
services. Across a set of ML models, the FIKIT based inference system
accelerated high priority tasks by 1.33 to 14.87 times compared to the JCT in
GPU sharing mode, and more than half of the cases are accelerated by more than
3.5 times. Alternatively, under preemptive sharing, the low-priority tasks have
a comparable to default GPU sharing mode JCT, with a 0.84 to 1 times ratio. We
further limit the kernel measurement and runtime fine-grained kernel scheduling
overhead to less than 10%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pseudo Label-Guided Data Fusion and Output Consistency for
  Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wang, Yuanbin Chen, Xinlin Zhang, Yuanbo Zhou, Junlin Lan, Bizhe Bai, Tao Tan, Min Du, Qinquan Gao, Tong Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised learning algorithms based on Convolutional Neural Networks have
become the benchmark for medical image segmentation tasks, but their
effectiveness heavily relies on a large amount of labeled data. However,
annotating medical image datasets is a laborious and time-consuming process.
Inspired by semi-supervised algorithms that use both labeled and unlabeled data
for training, we propose the PLGDF framework, which builds upon the mean
teacher network for segmenting medical images with less annotation. We propose
a novel pseudo-label utilization scheme, which combines labeled and unlabeled
data to augment the dataset effectively. Additionally, we enforce the
consistency between different scales in the decoder module of the segmentation
network and propose a loss function suitable for evaluating the consistency.
Moreover, we incorporate a sharpening operation on the predicted results,
further enhancing the accuracy of the segmentation.
  Extensive experiments on three publicly available datasets demonstrate that
the PLGDF framework can largely improve performance by incorporating the
unlabeled data. Meanwhile, our framework yields superior performance compared
to six state-of-the-art semi-supervised learning methods. The codes of this
study are available at https://github.com/ortonwang/PLGDF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Knowledge Graph Completion via Latent Embedding Sharing and
  Tensor Factorization <span class="chip">ICDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maolin Wang, Dun Zeng, Zenglin Xu, Ruocheng Guo, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs (KGs), which consist of triples, are inherently incomplete
and always require completion procedure to predict missing triples. In
real-world scenarios, KGs are distributed across clients, complicating
completion tasks due to privacy restrictions. Many frameworks have been
proposed to address the issue of federated knowledge graph completion. However,
the existing frameworks, including FedE, FedR, and FEKG, have certain
limitations. = FedE poses a risk of information leakage, FedR's optimization
efficacy diminishes when there is minimal overlap among relations, and FKGE
suffers from computational costs and mode collapse issues. To address these
issues, we propose a novel method, i.e., Federated Latent Embedding Sharing
Tensor factorization (FLEST), which is a novel approach using federated tensor
factorization for KG completion. FLEST decompose the embedding matrix and
enables sharing of latent dictionary embeddings to lower privacy risks.
Empirical results demonstrate FLEST's effectiveness and efficiency, offering a
balanced solution between performance and privacy. FLEST expands the
application of federated tensor factorization in KG completion tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransONet: Automatic Segmentation of Vasculature in Computed Tomographic
  Angiograms Using Deep Learning <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Bagheri Rajeoni, Breanna Pederson, Ali Firooz, Hamed Abdollahi, Andrew K. Smith, Daniel G. Clair, Susan M. Lessner, Homayoun Valafar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathological alterations in the human vascular system underlie many chronic
diseases, such as atherosclerosis and aneurysms. However, manually analyzing
diagnostic images of the vascular system, such as computed tomographic
angiograms (CTAs) is a time-consuming and tedious process. To address this
issue, we propose a deep learning model to segment the vascular system in CTA
images of patients undergoing surgery for peripheral arterial disease (PAD).
Our study focused on accurately segmenting the vascular system (1) from the
descending thoracic aorta to the iliac bifurcation and (2) from the descending
thoracic aorta to the knees in CTA images using deep learning techniques. Our
approach achieved average Dice accuracies of 93.5% and 80.64% in test dataset
for (1) and (2), respectively, highlighting its high accuracy and potential
clinical utility. These findings demonstrate the use of deep learning
techniques as a valuable tool for medical professionals to analyze the health
of the vascular system efficiently and accurately. Please visit the GitHub page
for this paper at https://github.com/pip-alireza/TransOnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 2023 International Conference on Computational
  Science and Computational Intelligence (CSCI), Las Vegas, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering Techniques for Stable Linear Dynamical Systems with
  applications to Hard Disk Drives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Potu Surya Prakash, Joohwan Seo, Jongeun Choi, Roberto Horowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Robust Control and Data Driven Robust Control design methodologies,
multiple plant transfer functions or a family of transfer functions are
considered and a common controller is designed such that all the plants that
fall into this family are stabilized. Though the plants are stabilized, the
controller might be sub-optimal for each of the plants when the variations in
the plants are large. This paper presents a way of clustering stable linear
dynamical systems for the design of robust controllers within each of the
clusters such that the controllers are optimal for each of the clusters. First
a k-medoids algorithm for hard clustering will be presented for stable Linear
Time Invariant (LTI) systems and then a Gaussian Mixture Models (GMM)
clustering for a special class of LTI systems, common for Hard Disk Drive
plants, will be presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Machine Learning-based Quantitative Hyperspectral Image Guidance
  for Brain Tumor Resection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Black, Declan Byrne, Anna Walke, Sidong Liu, Antonio Di leva, Sadahiro Kaneko, Walter Stummer, Septimiu Salcudean, Eric Suero Molina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complete resection of malignant gliomas is hampered by the difficulty in
distinguishing tumor cells at the infiltration zone. Fluorescence guidance with
5-ALA assists in reaching this goal. Using hyperspectral imaging, previous work
characterized five fluorophores' emission spectra in most human brain tumors.
In this paper, the effectiveness of these five spectra was explored for
different tumor and tissue classification tasks in 184 patients (891
hyperspectral measurements) harboring low- (n=30) and high-grade gliomas
(n=115), non-glial primary brain tumors (n=19), radiation necrosis (n=2),
miscellaneous (n=10) and metastases (n=8). Four machine learning models were
trained to classify tumor type, grade, glioma margins and IDH mutation. Using
random forests and multi-layer perceptrons, the classifiers achieved average
test accuracies of 74-82%, 79%, 81%, and 93% respectively. All five fluorophore
abundances varied between tumor margin types and tumor grades (p < 0.01). For
tissue type, at least four of the five fluorophore abundances were found to be
significantly different (p < 0.01) between all classes. These results
demonstrate the fluorophores' differing abundances in different tissue classes,
as well as the value of the five fluorophores as potential optical biomarkers,
opening new opportunities for intraoperative classification systems in
fluorescence-guided neurosurgery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonparametric Teaching for Multiple Learners <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhang, Xiaofeng Cao, Weiyang Liu, Ivor Tsang, James Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of teaching multiple learners simultaneously in the
nonparametric iterative teaching setting, where the teacher iteratively
provides examples to the learner for accelerating the acquisition of a target
concept. This problem is motivated by the gap between current single-learner
teaching setting and the real-world scenario of human instruction where a
teacher typically imparts knowledge to multiple students. Under the new problem
formulation, we introduce a novel framework -- Multi-learner Nonparametric
Teaching (MINT). In MINT, the teacher aims to instruct multiple learners, with
each learner focusing on learning a scalar-valued target model. To achieve
this, we frame the problem as teaching a vector-valued target model and extend
the target model space from a scalar-valued reproducing kernel Hilbert space
used in single-learner scenarios to a vector-valued space. Furthermore, we
demonstrate that MINT offers significant teaching speed-up over repeated
single-learner teaching, particularly when the multiple learners can
communicate with each other. Lastly, we conduct extensive experiments to
validate the practicality and efficiency of MINT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 (31 pages, 20 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Sparsifications using Neural Network Assisted Monte Carlo Tree
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvin Chiu, Mithun Ghosh, Reyan Ahmed, Kwang-Sung Jun, Stephen Kobourov, Michael T. Goodrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks have been successful for machine learning, as well as
for combinatorial and graph problems such as the Subgraph Isomorphism Problem
and the Traveling Salesman Problem. We describe an approach for computing graph
sparsifiers by combining a graph neural network and Monte Carlo Tree Search. We
first train a graph neural network that takes as input a partial solution and
proposes a new node to be added as output. This neural network is then used in
a Monte Carlo search to compute a sparsifier. The proposed method consistently
outperforms several standard approximation algorithms on different types of
graphs and often finds the optimal solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2305.00535</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Modeling of Single-cell perturbation Responses to Novel
  Drugs Using Cycle Consistence Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Huang, Aichun Zhu, Hui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phenotype-based screening has attracted much attention for identifying
cell-active compounds. Transcriptional and proteomic profiles of cell
population or single cells are informative phenotypic measures of cellular
responses to perturbations. In this paper, we proposed a deep learning
framework based on encoder-decoder architecture that maps the initial cellular
states to a latent space, in which we assume the effects of drug perturbation
on cellular states follow linear additivity. Next, we introduced the cycle
consistency constraints to enforce that initial cellular state subjected to
drug perturbations would produce the perturbed cellular responses, and,
conversely, removal of drug perturbation from the perturbed cellular states
would restore the initial cellular states. The cycle consistency constraints
and linear modeling in latent space enable to learn interpretable and
transferable drug perturbation representations, so that our model can predict
cellular response to unseen drugs. We validated our model on three different
types of datasets, including bulk transcriptional responses, bulk proteomic
responses, and single-cell transcriptional responses to drug perturbations. The
experimental results show that our model achieves better performance than
existing state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imagination-augmented Hierarchical Reinforcement Learning for Safe and
  Interactive Autonomous Driving in Urban Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sang-Hyun Lee, Yoonjae Jung, Seung-Woo Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical reinforcement learning (HRL) has led to remarkable achievements
in diverse fields. However, existing HRL algorithms still cannot be applied to
real-world navigation tasks. These tasks require an agent to perform
safety-aware behaviors and interact with surrounding objects in dynamic
environments. In addition, an agent in these tasks should perform consistent
and structured exploration as they are long-horizon and have complex structures
with diverse objects and task-specific rules. Designing HRL agents that can
handle these challenges in real-world navigation tasks is an open problem. In
this paper, we propose imagination-augmented HRL (IAHRL), a new and general
navigation algorithm that allows an agent to learn safe and interactive
behaviors in real-world navigation tasks. Our key idea is to train a
hierarchical agent in which a high-level policy infers interactions by
interpreting behaviors imagined with low-level policies. Specifically, the
high-level policy is designed with a permutation-invariant attention mechanism
to determine which low-level policy generates the most interactive behavior,
and the low-level policies are implemented with an optimization-based behavior
planner to generate safe and structured behaviors following task-specific
rules. To evaluate our algorithm, we introduce five complex urban driving
tasks, which are among the most challenging real-world navigation tasks. The
experimental results indicate that our hierarchical agent performs safety-aware
behaviors and properly interacts with surrounding vehicles, achieving higher
success rates and lower average episode steps than baselines in urban driving
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPSeg : Multi-Phase strategy for coronary artery Segmentation <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonghoe Ku, Yong-Hee Lee, Junsup Shin, In Kyu Lee, Hyun-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of coronary arteries is a pivotal process in assessing
cardiovascular diseases. However, the intricate structure of the cardiovascular
system presents significant challenges for automatic segmentation, especially
when utilizing methodologies like the SYNTAX Score, which relies extensively on
detailed structural information for precise risk stratification. To address
these difficulties and cater to this need, we present MPSeg, an innovative
multi-phase strategy designed for coronary artery segmentation. Our approach
specifically accommodates these structural complexities and adheres to the
principles of the SYNTAX Score. Initially, our method segregates vessels into
two categories based on their unique morphological characteristics: Left
Coronary Artery (LCA) and Right Coronary Artery (RCA). Specialized ensemble
models are then deployed for each category to execute the challenging
segmentation task. Due to LCA's higher complexity over RCA, a refinement model
is utilized to scrutinize and correct initial class predictions on segmented
areas. Notably, our approach demonstrated exceptional effectiveness when
evaluated in the Automatic Region-based Coronary Artery Disease diagnostics
using x-ray angiography imagEs (ARCADE) Segmentation Detection Algorithm
challenge at MICCAI 2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023 Conference ARCADE Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised structure learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl J. Friston, Lancelot Da Costa, Alexander Tschantz, Alex Kiefer, Tommaso Salvatori, Victorita Neacsu, Magnus Koudahl, Conor Heins, Noor Sajid, Dimitrije Markovic, Thomas Parr, Tim Verbelen, Christopher L Buckley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper concerns structure learning or discovery of discrete generative
models. It focuses on Bayesian model selection and the assimilation of training
data or content, with a special emphasis on the order in which data are
ingested. A key move - in the ensuing schemes - is to place priors on the
selection of models, based upon expected free energy. In this setting, expected
free energy reduces to a constrained mutual information, where the constraints
inherit from priors over outcomes (i.e., preferred outcomes). The resulting
scheme is first used to perform image classification on the MNIST dataset to
illustrate the basic idea, and then tested on a more challenging problem of
discovering models with dynamics, using a simple sprite-based visual
disentanglement paradigm and the Tower of Hanoi (cf., blocks world) problem. In
these examples, generative models are constructed autodidactically to recover
(i.e., disentangle) the factorial structure of latent states - and their
characteristic paths or dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Pruning of Deep Ensembles with Focal Diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzhao Wu, Ka-Ho Chow, Wenqi Wei, Ling Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural network ensembles combine the wisdom of multiple deep neural
networks to improve the generalizability and robustness over individual
networks. It has gained increasing popularity to study deep ensemble techniques
in the deep learning community. Some mission-critical applications utilize a
large number of deep neural networks to form deep ensembles to achieve desired
accuracy and resilience, which introduces high time and space costs for
ensemble execution. However, it still remains a critical challenge whether a
small subset of the entire deep ensemble can achieve the same or better
generalizability and how to effectively identify these small deep ensembles for
improving the space and time efficiency of ensemble execution. This paper
presents a novel deep ensemble pruning approach, which can efficiently identify
smaller deep ensembles and provide higher ensemble accuracy than the entire
deep ensemble of a large number of member networks. Our hierarchical ensemble
pruning approach (HQ) leverages three novel ensemble pruning techniques. First,
we show that the focal diversity metrics can accurately capture the
complementary capacity of the member networks of an ensemble, which can guide
ensemble pruning. Second, we design a focal diversity based hierarchical
pruning approach, which will iteratively find high quality deep ensembles with
low cost and high accuracy. Third, we develop a focal diversity consensus
method to integrate multiple focal diversity metrics to refine ensemble pruning
results, where smaller deep ensembles can be effectively identified to offer
high accuracy, high robustness and high efficiency. Evaluated using popular
benchmark datasets, we demonstrate that the proposed hierarchical ensemble
pruning approach can effectively identify high quality deep ensembles with
better generalizability while being more time and space efficient in ensemble
decision making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear on ACM Transactions on Intelligent Systems and Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Function Space Aggregation for Federated Learning at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Dhawan, Nicole Mitchell, Zachary Charles, Zachary Garrett, Gintare Karolina Dziugaite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The federated learning paradigm has motivated the development of methods for
aggregating multiple client updates into a global server model, without sharing
client data. Many federated learning algorithms, including the canonical
Federated Averaging (FedAvg), take a direct (possibly weighted) average of the
client parameter updates, motivated by results in distributed optimization. In
this work, we adopt a function space perspective and propose a new algorithm,
FedFish, that aggregates local approximations to the functions learned by
clients, using an estimate based on their Fisher information. We evaluate
FedFish on realistic, large-scale cross-device benchmarks. While the
performance of FedAvg can suffer as client models drift further apart, we
demonstrate that FedFish is more robust to longer local training. Our
evaluation across several settings in image and language benchmarks shows that
FedFish outperforms FedAvg as local training epochs increase. Further, FedFish
results in global networks that are more amenable to efficient personalization
via local fine-tuning on the same or shifted data distributions. For instance,
federated pretraining on the C4 dataset, followed by few-shot personalization
on Stack Overflow, results in a 7% improvement in next-token prediction by
FedFish over FedAvg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human fingerprints serve as one unique and powerful characteristic for each
person, from which policemen can recognize the identity. Similar to humans,
many natural bodies and intrinsic mechanical qualities can also be uniquely
identified from surface characteristics. To measure the elasto-plastic
properties of one material, one formally sharp indenter is pushed into the
measured body under constant force and retracted, leaving a unique residual
imprint of the minute size from several micrometers to nanometers. However, one
great challenge is how to map the optical image of this residual imprint into
the real wanted mechanical properties, i.e., the tensile force curve. In this
paper, we propose a novel method to use multi-fidelity neural networks (MFNN)
to solve this inverse problem. We first actively train the NN model via pure
simulation data, and then bridge the sim-to-real gap via transfer learning. The
most innovative part is that we use NN to dig out the unknown physics and also
implant the known physics into the transfer learning framework, thus highly
improving the model stability and decreasing the data requirement. This work
serves as one great example of applying machine learning into the real
experimental research, especially under the constraints of data limitation and
fidelity variance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, NeurIPS 2023 Workshop on Adaptive Experimental
  Design and Active Learning in the Real World</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sobol Sequence Optimization for Hardware-Efficient Vector Symbolic
  Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sercan Aygun, M. Hassan Najafi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperdimensional computing (HDC) is an emerging computing paradigm with
significant promise for efficient and robust learning. In HDC, objects are
encoded with high-dimensional vector symbolic sequences called hypervectors.
The quality of hypervectors, defined by their distribution and independence,
directly impacts the performance of HDC systems. Despite a large body of work
on the processing parts of HDC systems, little to no attention has been paid to
data encoding and the quality of hypervectors. Most prior studies have
generated hypervectors using inherent random functions, such as MATLAB`s or
Python`s random function. This work introduces an optimization technique for
generating hypervectors by employing quasi-random sequences. These sequences
have recently demonstrated their effectiveness in achieving accurate and
low-discrepancy data encoding in stochastic computing systems. The study
outlines the optimization steps for utilizing Sobol sequences to produce
high-quality hypervectors in HDC systems. An optimization algorithm is proposed
to select the most suitable Sobol sequences for generating minimally correlated
hypervectors, particularly in applications related to symbol-oriented
architectures. The performance of the proposed technique is evaluated in
comparison to two traditional approaches of generating hypervectors based on
linear-feedback shift registers and MATLAB random function. The evaluation is
conducted for two applications: (i) language and (ii) headline classification.
Our experimental results demonstrate accuracy improvements of up to 10.79%,
depending on the vector size. Additionally, the proposed encoding hardware
exhibits reduced energy consumption and a superior area-delay product.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiscale Hodge Scattering Networks for Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Saito, Stefan C. Schonsheck, Eugene Shvarts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose new scattering networks for signals measured on simplicial
complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs).
Our construction is based on multiscale basis dictionaries on simplicial
complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently
developed for simplices of dimension $\kappa \in \N$ in a given simplicial
complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT)
and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and
the $\kk$-HGLET both form redundant sets (i.e., dictionaries) of multiscale
basis vectors and the corresponding expansion coefficients of a given signal.
Our MHSNs use a layered structure analogous to a convolutional neural network
(CNN) to cascade the moments of the modulus of the dictionary coefficients. The
resulting features are invariant to reordering of the simplices (i.e., node
permutation of the underlying graphs). Importantly, the use of multiscale basis
dictionaries in our MHSNs admits a natural pooling operation that is akin to
local pooling in CNNs, and which may be performed either locally or per-scale.
These pooling operations are harder to define in both traditional scattering
networks based on Morlet wavelets, and geometric scattering networks based on
Diffusion Wavelets. As a result, we are able to extract a rich set of
descriptive yet robust features that can be used along with very simple machine
learning methods (i.e., logistic regression or support vector machines) to
achieve high-accuracy classification systems with far fewer parameters to train
than most modern graph neural networks. Finally, we demonstrate the usefulness
of our MHSNs in three distinct types of problems: signal classification, domain
(i.e., graph/simplex) classification, and molecular dynamics prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 Pages, Comments Welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy and Carbon Considerations of Fine-Tuning <span class="highlight-title">BERT</span> <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaorong Wang, Clara Na, Emma Strubell, Sorelle Friedler, Sasha Luccioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP
community, existing work quantifying energy costs and associated carbon
emissions has largely focused on language model pre-training. Although a single
pre-training run draws substantially more energy than fine-tuning, fine-tuning
is performed more frequently by many more individual actors, and thus must be
accounted for when considering the energy and carbon footprint of NLP. In order
to better characterize the role of fine-tuning in the landscape of energy and
carbon emissions in NLP, we perform a careful empirical study of the
computational costs of fine-tuning across tasks, datasets, hardware
infrastructure and measurement modalities. Our experimental results allow us to
place fine-tuning energy and carbon costs into perspective with respect to
pre-training and inference, and outline recommendations to NLP researchers and
practitioners who wish to improve their fine-tuning energy efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings; First two authors contributed equally; 12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Differentiable Causal Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achille Nazaret, Justin Hong, Elham Azizi, David Blei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inferring causal relationships as directed acyclic graphs (DAGs) is an
important but challenging problem. Differentiable Causal Discovery (DCD) is a
promising approach to this problem, framing the search as a continuous
optimization. But existing DCD methods are numerically unstable, with poor
performance beyond tens of variables. In this paper, we propose Stable
Differentiable Causal Discovery (SDCD), a new method that improves previous DCD
methods in two ways: (1) It employs an alternative constraint for acyclicity;
this constraint is more stable, both theoretically and empirically, and fast to
compute. (2) It uses a training procedure tailored for sparse causal graphs,
which are common in real-world scenarios. We first derive SDCD and prove its
stability and correctness. We then evaluate it with both observational and
interventional data and on both small-scale and large-scale settings. We find
that SDCD outperforms existing methods in both convergence speed and accuracy
and can scale to thousands of variables.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FREE: The Foundational Semantic Recognition for Modeling Environmental
  Ecosystems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Luo, Juntong Ni, Shengyu Chen, Runlong Yu, Yiqun Xie, Licheng Liu, Zhenong Jin, Huaxiu Yao, Xiaowei Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling environmental ecosystems is critical for the sustainability of our
planet, but is extremely challenging due to the complex underlying processes
driven by interactions amongst a large number of physical variables. As many
variables are difficult to measure at large scales, existing works often
utilize a combination of observable features and locally available measurements
or modeled values as input to build models for a specific study region and time
period. This raises a fundamental question in advancing the modeling of
environmental ecosystems: how to build a general framework for modeling the
complex relationships amongst various environmental data over space and time?
In this paper, we introduce a new framework, FREE, which maps available
environmental data into a text space and then converts the traditional
predictive modeling task in environmental science to the semantic recognition
problem. The proposed FREE framework leverages recent advances in Large
Language Models (LLMs) to supplement the original input features with natural
language descriptions. This facilitates capturing the data semantics and also
allows harnessing the irregularities of input features. When used for long-term
prediction, FREE has the flexibility to incorporate newly collected
observations to enhance future prediction. The efficacy of FREE is evaluated in
the context of two societally important real-world applications, predicting
stream water temperature in the Delaware River Basin and predicting annual corn
yield in Illinois and Iowa. Beyond the superior predictive performance over
multiple baseline methods, FREE is shown to be more data- and
computation-efficient as it can be pre-trained on simulated data generated by
physics-based models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniMOS: A Universal Framework For Multi-Organ Segmentation Over
  Label-Constrained <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Li, Sheng Shao, Junyi Qu, Shuchao Pang, Mehmet A. Orgun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models for medical images can help physicians diagnose and
manage diseases. However, due to the fact that medical image annotation
requires a great deal of manpower and expertise, as well as the fact that
clinical departments perform image annotation based on task orientation, there
is the problem of having fewer medical image annotation data with more
unlabeled data and having many datasets that annotate only a single organ. In
this paper, we present UniMOS, the first universal framework for achieving the
utilization of fully and partially labeled images as well as unlabeled images.
Specifically, we construct a Multi-Organ Segmentation (MOS) module over
fully/partially labeled data as the basenet and designed a new target adaptive
loss. Furthermore, we incorporate a semi-supervised training module that
combines consistent regularization and pseudolabeling techniques on unlabeled
data, which significantly improves the segmentation of unlabeled data.
Experiments show that the framework exhibits excellent performance in several
medical image segmentation tasks compared to other advanced methods, and also
significantly improves data utilization and reduces annotation cost. Code and
models are available at: https://github.com/lw8807001/UniMOS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedTruth: Byzantine-Robust and Backdoor-Resilient Federated Learning
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheldon C. Ebron Jr., Kan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables collaborative machine learning model training
across multiple parties without sharing raw data. However, FL's distributed
nature allows malicious clients to impact model training through Byzantine or
backdoor attacks, using erroneous model updates. Existing defenses measure the
deviation of each update from a 'ground-truth model update.' They often rely on
a benign root dataset on the server or use trimmed mean or median for clipping,
both methods having limitations.
  We introduce FedTruth, a robust defense against model poisoning in FL.
FedTruth doesn't assume specific data distributions nor requires a benign root
dataset. It estimates a global model update with dynamic aggregation weights,
considering contributions from all benign clients. Empirical studies
demonstrate FedTruth's efficacy in mitigating the impacts of poisoned updates
from both Byzantine and backdoor attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amartya Banerjee, Christopher J. Hazard, Jacob Beel, Cade Mack, Jack Xia, Michael Resnick, Will Goddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonparametric learning is a fundamental concept in machine learning that aims
to capture complex patterns and relationships in data without making strong
assumptions about the underlying data distribution. Owing to simplicity and
familiarity, one of the most well-known algorithms under this paradigm is the
$k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine
learning in safety-critical applications, in this work, we shed new light on
the traditional nearest neighbors algorithm from the perspective of information
theory and propose a robust and interpretable framework for tasks such as
classification, regression, and anomaly detection using a single model. Instead
of using a traditional distance measure which needs to be scaled and
contextualized, we use a novel formulation of \textit{surprisal} (amount of
information required to explain the difference between the observed and
expected result). Finally, we demonstrate this architecture's capability to
perform at-par or above the state-of-the-art on classification, regression, and
anomaly detection tasks using a single model with enhanced interpretability by
providing novel concepts for characterizing data and predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancements in Generative AI: A Comprehensive <span class="highlight-title">Review</span> of GANs, <span class="highlight-title">GPT</span>,
  Autoencoders, Diffusion Model, and <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Staphord Bengesi, Hoda El-Sayed, Md Kamruzzaman Sarker, Yao Houkpati, John Irungu, Timothy Oladunni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The launch of ChatGPT has garnered global attention, marking a significant
milestone in the field of Generative Artificial Intelligence. While Generative
AI has been in effect for the past decade, the introduction of ChatGPT has
ignited a new wave of research and innovation in the AI domain. This surge in
interest has led to the development and release of numerous cutting-edge tools,
such as Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox,
among others. These tools exhibit remarkable capabilities, encompassing tasks
ranging from text generation and music composition, image creation, video
production, code generation, and even scientific work. They are built upon
various state-of-the-art models, including Stable Diffusion, transformer models
like GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial
networks. This advancement in Generative AI presents a wealth of exciting
opportunities and, simultaneously, unprecedented challenges. Throughout this
paper, we have explored these state-of-the-art models, the diverse array of
tasks they can accomplish, the challenges they pose, and the promising future
of Generative Artificial Intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JaxMARL: Multi-Agent RL Environments in JAX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Gardar Ingvarsson, Timon Willi, Akbir Khan, Christian Schroeder de Witt, Alexandra Souly, Saptarashmi Bandyopadhyay, Mikayel Samvelyan, Minqi Jiang, Robert Tjarko Lange, Shimon Whiteson, Bruno Lacerda, Nick Hawes, Tim Rocktaschel, Chris Lu, Jakob Nicolaus Foerster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarks play an important role in the development of machine learning
algorithms. For example, research in reinforcement learning (RL) has been
heavily influenced by available environments and benchmarks. However, RL
environments are traditionally run on the CPU, limiting their scalability with
typical academic compute. Recent advancements in JAX have enabled the wider use
of hardware acceleration to overcome these computational hurdles, enabling
massively parallel RL training pipelines and environments. This is particularly
useful for multi-agent reinforcement learning (MARL) research. First of all,
multiple agents must be considered at each environment step, adding
computational burden, and secondly, the sample complexity is increased due to
non-stationarity, decentralised partial observability, or other MARL
challenges. In this paper, we present JaxMARL, the first open-source code base
that combines ease-of-use with GPU enabled efficiency, and supports a large
number of commonly used MARL environments as well as popular baseline
algorithms. When considering wall clock time, our experiments show that per-run
our JAX-based training pipeline is up to 12500x faster than existing
approaches. This enables efficient and thorough evaluations, with the potential
to alleviate the evaluation crisis of the field. We also introduce and
benchmark SMAX, a vectorised, simplified version of the popular StarCraft
Multi-Agent Challenge, which removes the need to run the StarCraft II game
engine. This not only enables GPU acceleration, but also provides a more
flexible MARL environment, unlocking the potential for self-play,
meta-learning, and other future applications in MARL. We provide code at
https://github.com/flairox/jaxmarl.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What User Behaviors Make the Differences During the Process of Visual
  Analytics? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahin Doroudian, Zekun Wu, Aidong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The understanding of visual analytics process can benefit visualization
researchers from multiple aspects, including improving visual designs and
developing advanced interaction functions. However, the log files of user
behaviors are still hard to analyze due to the complexity of sensemaking and
our lack of knowledge on the related user behaviors. This work presents a study
on a comprehensive data collection of user behaviors, and our analysis approach
with time-series classification methods. We have chosen a classical
visualization application, Covid-19 data analysis, with common analysis tasks
covering geo-spatial, time-series and multi-attributes. Our user study collects
user behaviors on a diverse set of visualization tasks with two comparable
systems, desktop and immersive visualizations. We summarize the classification
results with three time-series machine learning algorithms at two scales, and
explore the influences of behavior features. Our results reveal that user
behaviors can be distinguished during the process of visual analytics and there
is a potentially strong association between the physical behaviors of users and
the visualization tasks they perform. We also demonstrate the usage of our
models by interpreting open sessions of visual analytics, which provides an
automatic way to study sensemaking without tedious manual annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The authors have decided to withdraw the paper due to identified
  critical errors. These errors were deemed substantial enough to compromise
  the integrity and reliability of the research findings presented in the
  paper. As a result, the authors have chosen to retract the paper to maintain
  academic standards and transparency in the dissemination of scientific
  knowledge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In search of dispersed memories: Generative diffusion models are
  associative memory networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17290v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17290v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Ambrogioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncovering the mechanisms behind long-term memory is one of the most
fascinating open problems in neuroscience and artificial intelligence.
Artificial associative memory networks have been used to formalize important
aspects of biological memory. Generative diffusion models are a type of
generative machine learning techniques that have shown great performance in
many tasks. Like associative memory systems, these networks define a dynamical
system that converges to a set of target states. In this work we show that
generative diffusion models can be interpreted as energy-based models and that,
when trained on discrete patterns, their energy function is (asymptotically)
identical to that of modern Hopfield networks. This equivalence allows us to
interpret the supervised training of diffusion models as a synaptic learning
process that encodes the associative dynamics of a modern Hopfield network in
the weight structure of a deep neural network. Leveraging this connection, we
formulate a generalized framework for understanding the formation of long-term
memory, where creative generation and memory recall can be seen as parts of a
unified continuum.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closed Drafting as a Case Study for First-Principle Interpretability,
  Memory, and Generalizability in Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20654v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20654v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Rezai, Jason Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Closed drafting or "pick and pass" is a popular game mechanic where each
round players select a card or other playable element from their hand and pass
the rest to the next player. In this paper, we establish first-principle
methods for studying the interpretability, generalizability, and memory of Deep
Q-Network (DQN) models playing closed drafting games. In particular, we use a
popular family of closed drafting games called "Sushi Go Party", in which we
achieve state-of-the-art performance. We fit decision rules to interpret the
decision-making strategy of trained DRL agents by comparing them to the ranking
preferences of different types of human players. As Sushi Go Party can be
expressed as a set of closely-related games based on the set of cards in play,
we quantify the generalizability of DRL models trained on various sets of
cards, establishing a method to benchmark agent performance as a function of
environment unfamiliarity. Using the explicitly calculable memory of other
player's hands in closed drafting games, we create measures of the ability of
DRL models to learn memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures, equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Privacy Preserving System for Movie Recommendations Using Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Neumann, Andreas Lutz, Karsten Müller, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become ubiquitous in the past years. They solve the
tyranny of choice problem faced by many users, and are utilized by many online
businesses to drive engagement and sales. Besides other criticisms, like
creating filter bubbles within social networks, recommender systems are often
reproved for collecting considerable amounts of personal data. However, to
personalize recommendations, personal information is fundamentally required. A
recent distributed learning scheme called federated learning has made it
possible to learn from personal user data without its central collection.
Consequently, we present a recommender system for movie recommendations, which
provides privacy and thus trustworthiness on multiple levels: First and
foremost, it is trained using federated learning and thus, by its very nature,
privacy-preserving, while still enabling users to benefit from global insights.
Furthermore, a novel federated learning scheme, called FedQ, is employed, which
not only addresses the problem of non-i.i.d.-ness and small local datasets, but
also prevents input data reconstruction attacks by aggregating client updates
early. Finally, to reduce the communication overhead, compression is applied,
which significantly compresses the exchanged neural network parametrizations to
a fraction of their original size. We conjecture that this may also improve
data privacy through its lossy quantization stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the ACM TORS Special Issue on Trustworthy Recommender
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing Deep Reinforcement Learning Algorithms in Two-Echelon Supply
  Chains <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.09603v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.09603v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Stranieri, Fabio Stella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we analyze and compare the performance of state-of-the-art
deep reinforcement learning algorithms for solving the supply chain inventory
management problem. This complex sequential decision-making problem consists of
determining the optimal quantity of products to be produced and shipped across
different warehouses over a given time horizon. In particular, we present a
mathematical formulation of a two-echelon supply chain environment with
stochastic and seasonal demand, which allows managing an arbitrary number of
warehouses and product types. Through a rich set of numerical experiments, we
compare the performance of different deep reinforcement learning algorithms
under various supply chain structures, topologies, demands, capacities, and
costs. The results of the experimental plan indicate that deep reinforcement
learning algorithms outperform traditional inventory management strategies,
such as the static (s, Q)-policy. Furthermore, this study provides detailed
insight into the design and development of an open-source software library that
provides a customizable environment for solving the supply chain inventory
management problem using a wide range of data-driven approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted for presentation and inclusion in the
  proceedings of the AI for Manufacturing workshop (AI4M), co-located with the
  ECML PKDD 2023 (European Conference on Machine Learning and Principles and
  Practice of Knowledge Discovery in Databases). For supplementary material and
  source code, please visit https://github.com/frenkowski/SCIMAI-Gym</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inferential Moments of Uncertain Multivariable Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.01841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.01841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Vanslette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article expands the framework of Bayesian inference and provides direct
probabilistic methods for approaching inference tasks that are typically
handled with information theory. We treat Bayesian probability updating as a
random process and uncover intrinsic quantitative features of joint probability
distributions called inferential moments. Inferential moments quantify shape
information about how a prior distribution is expected to update in response to
yet to be obtained information. Further, we quantify the unique probability
distribution whose statistical moments are the inferential moments in question.
We find a power series expansion of the mutual information in terms of
inferential moments, which implies a connection between inferential theoretic
logic and elements of information theory. Of particular interest is the
inferential deviation, which is the expected variation of the probability of
one variable in response to an inferential update of another. We explore two
applications that analyze the inferential deviations of a Bayesian network to
improve decision-making. We implement simple greedy algorithms for exploring
sensor tasking using inferential deviations that generally outperform similar
greedy mutual information algorithms in terms of root mean squared error
between epistemic probability estimates and the ground truth probabilities they
are estimating.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximately Equivariant Graph Networks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10436v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10436v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyuan Huang, Ron Levie, Soledad Villar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) are commonly described as being permutation
equivariant with respect to node relabeling in the graph. This symmetry of GNNs
is often compared to the translation equivariance of Euclidean convolution
neural networks (CNNs). However, these two symmetries are fundamentally
different: The translation equivariance of CNNs corresponds to symmetries of
the fixed domain acting on the image signals (sometimes known as active
symmetries), whereas in GNNs any permutation acts on both the graph signals and
the graph domain (sometimes described as passive symmetries). In this work, we
focus on the active symmetries of GNNs, by considering a learning setting where
signals are supported on a fixed graph. In this case, the natural symmetries of
GNNs are the automorphisms of the graph. Since real-world graphs tend to be
asymmetric, we relax the notion of symmetries by formalizing approximate
symmetries via graph coarsening. We present a bias-variance formula that
quantifies the tradeoff between the loss in expressivity and the gain in the
regularity of the learned estimator, depending on the chosen symmetry group. To
illustrate our approach, we conduct extensive experiments on image inpainting,
traffic flow prediction, and human pose estimation with different choices of
symmetries. We show theoretically and empirically that the best generalization
performance can be achieved by choosing a suitably larger group than the graph
automorphism, but smaller than the permutation group.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Quantum Eigensolver with Constraints (VQEC): Solving
  Constrained Optimization Problems via VQE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thinh Viet Le, Vassilis Kekatos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational quantum approaches have shown great promise in finding
near-optimal solutions to computationally challenging tasks. Nonetheless,
enforcing constraints in a disciplined fashion has been largely unexplored. To
address this gap, this work proposes a hybrid quantum-classical algorithmic
paradigm termed VQEC that extends the celebrated VQE to handle optimization
with constraints. As with the standard VQE, the vector of optimization
variables is captured by the state of a variational quantum circuit (VQC). To
deal with constraints, VQEC optimizes a Lagrangian function classically over
both the VQC parameters as well as the dual variables associated with
constraints. To comply with the quantum setup, variables are updated via a
perturbed primal-dual method leveraging the parameter shift rule. Among a wide
gamut of potential applications, we showcase how VQEC can approximately solve
quadratically-constrained binary optimization (QCBO) problems, find stochastic
binary policies satisfying quadratic constraints on the average and in
probability, and solve large-scale linear programs (LP) over the probability
simplex. Under an assumption on the error for the VQC to approximate an
arbitrary probability mass function (PMF), we provide bounds on the optimality
gap attained by a VQC. Numerical tests on a quantum simulator investigate the
effect of various parameters and corroborate that VQEC can generate
high-quality solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 13 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring and Interacting with the Set of Good Sparse Generalized
  Additive Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16047v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16047v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chudi Zhong, Zhi Chen, Jiachang Liu, Margo Seltzer, Cynthia Rudin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real applications, interaction between machine learning models and domain
experts is critical; however, the classical machine learning paradigm that
usually produces only a single model does not facilitate such interaction.
Approximating and exploring the Rashomon set, i.e., the set of all near-optimal
models, addresses this practical challenge by providing the user with a
searchable space containing a diverse set of models from which domain experts
can choose. We present algorithms to efficiently and accurately approximate the
Rashomon set of sparse, generalized additive models with ellipsoids for fixed
support sets and use these ellipsoids to approximate Rashomon sets for many
different support sets. The approximated Rashomon set serves as a cornerstone
to solve practical challenges such as (1) studying the variable importance for
the model class; (2) finding models under user-specified constraints
(monotonicity, direct editing); and (3) investigating sudden changes in the
shape functions. Experiments demonstrate the fidelity of the approximated
Rashomon set and its effectiveness in solving practical challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GCondNet: A Novel Method for Improving Neural Networks on Small
  High-Dimensional Tabular Data <span class="chip">NeurIPS
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06302v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06302v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Margeloiu, Nikola Simidjievski, Pietro Lio, Mateja Jamnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network models often struggle with high-dimensional but small
sample-size tabular datasets. One reason is that current weight initialisation
methods assume independence between weights, which can be problematic when
there are insufficient samples to estimate the model's parameters accurately.
In such small data scenarios, leveraging additional structures can improve the
model's performance and training stability. To address this, we propose
GCondNet, a general approach to enhance neural networks by leveraging implicit
structures present in tabular data. We create a graph between samples for each
data dimension, and utilise Graph Neural Networks (GNNs) for extracting this
implicit structure, and for conditioning the parameters of the first layer of
an underlying predictor network. By creating many small graphs, GCondNet
exploits the data's high-dimensionality, and thus improves the performance of
an underlying predictor network. We demonstrate the effectiveness of our method
on 9 real-world datasets, where GCondNet outperforms 15 standard and
state-of-the-art methods. The results show that GCondNet is a versatile
framework for injecting graph-regularisation into various types of neural
networks, including MLPs and tabular Transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2nd Table Representation Learning Workshop at NeurIPS
  2023 [selected for oral presentation]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rates of Convergence in Certain Native Spaces of Approximations used in
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07383v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07383v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Bouland, Shengyuan Niu, Sai Tej Paruchuri, Andrew Kurdila, John Burns, Eugenio Schuster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies convergence rates for some value function approximations
that arise in a collection of reproducing kernel Hilbert spaces (RKHS)
$H(\Omega)$. By casting an optimal control problem in a specific class of
native spaces, strong rates of convergence are derived for the operator
equation that enables offline approximations that appear in policy iteration.
Explicit upper bounds on error in value function and controller approximations
are derived in terms of power function $\mathcal{P}_{H,N}$ for the space of
finite dimensional approximants $H_N$ in the native space $H(\Omega)$. These
bounds are geometric in nature and refine some well-known, now classical
results concerning convergence of approximations of value functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A finite sample analysis of the benign overfitting phenomenon for ridge
  function estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.12882v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.12882v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanuel Caron, Stephane Chretien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent extensive numerical experiments in high scale machine learning have
allowed to uncover a quite counterintuitive phase transition, as a function of
the ratio between the sample size and the number of parameters in the model. As
the number of parameters $p$ approaches the sample size $n$, the generalisation
error increases, but surprisingly, it starts decreasing again past the
threshold $p=n$. This phenomenon, brought to the theoretical community
attention in \cite{belkin2019reconciling}, has been thoroughly investigated
lately, more specifically for simpler models than deep neural networks, such as
the linear model when the parameter is taken to be the minimum norm solution to
the least-squares problem, firstly in the asymptotic regime when $p$ and $n$
tend to infinity, see e.g. \cite{hastie2019surprises}, and recently in the
finite dimensional regime and more specifically for linear models
\cite{bartlett2020benign}, \cite{tsigler2020benign},
\cite{lecue2022geometrical}. In the present paper, we propose a finite sample
analysis of non-linear models of \textit{ridge} type, where we investigate the
\textit{overparametrised regime} of the double descent phenomenon for both the
\textit{estimation problem} and the \textit{prediction} problem. Our results
provide a precise analysis of the distance of the best estimator from the true
parameter as well as a generalisation bound which complements recent works of
\cite{bartlett2020benign} and \cite{chinot2020benign}. Our analysis is based on
tools closely related to the continuous Newton method
\cite{neuberger2007continuous} and a refined quantitative analysis of the
performance in prediction of the minimum $\ell_2$-norm solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>New section on generalisation added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tree Variational Autoencoders <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08984v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08984v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Manduchi, Moritz Vandenhirtz, Alain Ryser, Julia Vogt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Tree Variational Autoencoder (TreeVAE), a new generative
hierarchical clustering model that learns a flexible tree-based posterior
distribution over latent variables. TreeVAE hierarchically divides samples
according to their intrinsic characteristics, shedding light on hidden
structures in the data. It adapts its architecture to discover the optimal tree
for encoding dependencies between latent variables. The proposed tree-based
generative architecture enables lightweight conditional inference and improves
generative performance by utilizing specialized leaf decoders. We show that
TreeVAE uncovers underlying clusters in the data and finds meaningful
hierarchical relations between the different groups on a variety of datasets,
including real-world imaging data. We present empirically that TreeVAE provides
a more competitive log-likelihood lower bound than the sequential counterparts.
Finally, due to its generative nature, TreeVAE is able to generate new samples
from the discovered clusters via conditional sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as Spotlight to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Dark Side of the Language: <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">Transformer</span>s in the DarkNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Ranaldi, Aria Nourbakhsh, Arianna Patrizi, Elena Sofia Ruzzetti, Dario Onorati, Francesca Fallucchi, Fabio Massimo Zanzotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Transformers are challenging human performances in many NLP
tasks. The massive datasets used for pre-training seem to be the key to their
success on existing tasks. In this paper, we explore how a range of pre-trained
Natural Language Understanding models perform on definitely unseen sentences
provided by classification tasks over a DarkNet corpus. Surprisingly, results
show that syntactic and lexical neural networks perform on par with pre-trained
Transformers even after fine-tuning. Only after what we call extreme domain
adaptation, that is, retraining with the masked language model task on all the
novel corpus, pre-trained Transformers reach their standard high results. This
suggests that huge pre-training corpora may give Transformers unexpected help
since they are exposed to many of the possible sentences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bespoke: A Block-Level Neural Network Optimization Framework for
  Low-Cost Deployment <span class="chip">AAAI-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jong-Ryul Lee, Yong-Hyuk Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep learning models become popular, there is a lot of need for deploying
them to diverse device environments. Because it is costly to develop and
optimize a neural network for every single environment, there is a line of
research to search neural networks for multiple target environments
efficiently. However, existing works for such a situation still suffer from
requiring many GPUs and expensive costs. Motivated by this, we propose a novel
neural network optimization framework named Bespoke for low-cost deployment.
Our framework searches for a lightweight model by replacing parts of an
original model with randomly selected alternatives, each of which comes from a
pretrained neural network or the original model. In the practical sense,
Bespoke has two significant merits. One is that it requires near zero cost for
designing the search space of neural networks. The other merit is that it
exploits the sub-networks of public pretrained neural networks, so the total
cost is minimal compared to the existing works. We conduct experiments
exploring Bespoke's the merits, and the results show that it finds efficient
models for multiple targets with meager cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the extended version of our AAAI-2023 paper
  (https://ojs.aaai.org/index.php/AAAI/article/view/26020)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating and Mitigating the Side Effects of Noisy Views for
  <span class="highlight-title">Self-Supervised</span> Clustering Algorithms in Practical Multi-View Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17245v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17245v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Xu, Yazhou Ren, Xiaolong Wang, Lei Feng, Zheng Zhang, Gang Niu, Xiaofeng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view clustering (MVC) aims at exploring category structures among
multi-view data in self-supervised manners. Multiple views provide more
information than single views and thus existing MVC methods can achieve
satisfactory performance. However, their performance might seriously degenerate
when the views are noisy in practical multi-view scenarios. In this paper, we
first formally investigate the drawback of noisy views and then propose a
theoretically grounded deep MVC method (namely MVCAN) to address this issue.
Specifically, we propose a novel MVC objective that enables un-shared
parameters and inconsistent clustering predictions across multiple views to
reduce the side effects of noisy views. Furthermore, a two-level multi-view
iterative optimization is designed to generate robust learning targets for
refining individual views' representation learning. Theoretical analysis
reveals that MVCAN works by achieving the multi-view consistency,
complementarity, and noise robustness. Finally, experiments on extensive public
datasets demonstrate that MVCAN outperforms state-of-the-art methods and is
robust against the existence of noisy views.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trustworthy Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.06265v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.06265v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoujin Wang, Xiuzhen Zhang, Yan Wang, Huan Liu, Francesco Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RSs) aim to help users to effectively retrieve items of
their interests from a large catalogue. For a quite long period of time,
researchers and practitioners have been focusing on developing accurate RSs.
Recent years have witnessed an increasing number of threats to RSs, coming from
attacks, system and user generated noise, system bias. As a result, it has
become clear that a strict focus on RS accuracy is limited and the research
must consider other important factors, e.g., trustworthiness. For end users, a
trustworthy RS (TRS) should not only be accurate, but also transparent,
unbiased and fair as well as robust to noise or attacks. These observations
actually led to a paradigm shift of the research on RSs: from accuracy-oriented
RSs to TRSs. However, researchers lack a systematic overview and discussion of
the literature in this novel and fast developing field of TRSs. To this end, in
this paper, we provide an overview of TRSs, including a discussion of the
motivation and basic concepts of TRSs, a presentation of the challenges in
building TRSs, and a perspective on the future directions in this area. We also
provide a novel conceptual framework to support the construction of TRSs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Complex Disease Trajectories using Deep Generative Models with
  Semi-Supervised Latent Processes <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08149v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08149v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cécile Trottet, Manuel Schürch, Ahmed Allam, Imon Barua, Liubov Petelytska, Oliver Distler, Anna-Maria Hoffmann-Vold, Michael Krauthammer, the EUSTAR collaborators
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a deep generative time series approach using latent
temporal processes for modeling and holistically analyzing complex disease
trajectories. We aim to find meaningful temporal latent representations of an
underlying generative process that explain the observed disease trajectories in
an interpretable and comprehensive way. To enhance the interpretability of
these latent temporal processes, we develop a semi-supervised approach for
disentangling the latent space using established medical concepts. By combining
the generative approach with medical knowledge, we leverage the ability to
discover novel aspects of the disease while integrating medical concepts into
the model. We show that the learned temporal latent processes can be utilized
for further data analysis and clinical hypothesis testing, including finding
similar patients and clustering the disease into new sub-types. Moreover, our
method enables personalized online monitoring and prediction of multivariate
time series including uncertainty quantification. We demonstrate the
effectiveness of our approach in modeling systemic sclerosis, showcasing the
potential of our machine learning model to capture complex disease trajectories
and acquire new medical knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended Abstract presented at Machine Learning for Health (ML4H)
  symposium 2023, December 10th, 2023, New Orleans, United States, 23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid quantum physics-informed neural networks for simulating
  computational fluid dynamics in complex shapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandr Sedykh, Maninadh Podapaka, Asel Sagingalieva, Karan Pinto, Markus Pflitsch, Alexey Melnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding the distribution of the velocities and pressures of a fluid (by
solving the Navier-Stokes equations) is a principal task in the chemical,
energy, and pharmaceutical industries, as well as in mechanical engineering and
the design of pipeline systems. With existing solvers, such as OpenFOAM and
Ansys, simulations of fluid dynamics in intricate geometries are
computationally expensive and require re-simulation whenever the geometric
parameters or the initial and boundary conditions are altered. Physics-informed
neural networks are a promising tool for simulating fluid flows in complex
geometries, as they can adapt to changes in the geometry and mesh definitions,
allowing for generalization across different shapes. We present a hybrid
quantum physics-informed neural network that simulates laminar fluid flows in
3D Y-shaped mixers. Our approach combines the expressive power of a quantum
model with the flexibility of a physics-informed neural network, resulting in a
21% higher accuracy compared to a purely classical neural network. Our findings
highlight the potential of machine learning approaches, and in particular
hybrid quantum physics-informed neural network, for complex shape optimization
tasks in computational fluid dynamics. By improving the accuracy of fluid
simulations in complex geometries, our research using hybrid quantum models
contributes to the development of more efficient and reliable fluid dynamics
solvers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-Based Reinforcement Learning with Isolated Imaginations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minting Pan, Xiangming Zhu, Yitao Zheng, Yunbo Wang, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models learn the consequences of actions in vision-based interactive
systems. However, in practical scenarios like autonomous driving,
noncontrollable dynamics that are independent or sparsely dependent on action
signals often exist, making it challenging to learn effective world models. To
address this issue, we propose Iso-Dream++, a model-based reinforcement
learning approach that has two main contributions. First, we optimize the
inverse dynamics to encourage the world model to isolate controllable state
transitions from the mixed spatiotemporal variations of the environment.
Second, we perform policy optimization based on the decoupled latent
imaginations, where we roll out noncontrollable states into the future and
adaptively associate them with the current controllable state. This enables
long-horizon visuomotor control tasks to benefit from isolating mixed dynamics
sources in the wild, such as self-driving cars that can anticipate the movement
of other vehicles, thereby avoiding potential risks. On top of our previous
work, we further consider the sparse dependencies between controllable and
noncontrollable states, address the training collapse problem of state
decoupling, and validate our approach in transfer learning setups. Our
empirical study demonstrates that Iso-Dream++ outperforms existing
reinforcement learning models significantly on CARLA and DeepMind Control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2205.13817</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Normalizing flows as approximations of optimal transport maps via
  linear-control neural ODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Scagliotti, Sara Farinelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The term "Normalizing Flows" is related to the task of constructing
invertible transport maps between probability measures by means of deep neural
networks. In this paper, we consider the problem of recovering the
$W_2$-optimal transport map $T$ between absolutely continuous measures
$\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$ as the flow of a linear-control neural
ODE. We first show that, under suitable assumptions on $\mu,\nu$ and on the
controlled vector fields, the optimal transport map is contained in the
$C^0_c$-closure of the flows generated by the system. Assuming that discrete
approximations $\mu_N,\nu_N$ of the original measures $\mu,\nu$ are available,
we use a discrete optimal coupling $\gamma_N$ to define an optimal control
problem. With a $\Gamma$-convergence argument, we prove that its solutions
correspond to flows that approximate the optimal transport map $T$. Finally,
taking advantage of the Pontryagin Maximum Principle, we propose an iterative
numerical scheme for the resolution of the optimal control problem, resulting
in an algorithm for the practical computation of the approximated optimal
transport map.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Correction of typos and new bibliographical references. 32 pages, 1
  figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Addressing caveats of neural persistence with deep graph persistence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leander Girrbach, Anders Christensen, Ole Winther, Zeynep Akata, A. Sophia Koepke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Persistence is a prominent measure for quantifying neural network
complexity, proposed in the emerging field of topological data analysis in deep
learning. In this work, however, we find both theoretically and empirically
that the variance of network weights and spatial concentration of large weights
are the main factors that impact neural persistence. Whilst this captures
useful information for linear classifiers, we find that no relevant spatial
structure is present in later layers of deep neural networks, making neural
persistence roughly equivalent to the variance of weights. Additionally, the
proposed averaging procedure across layers for deep neural networks does not
consider interaction between layers. Based on our analysis, we propose an
extension of the filtration underlying neural persistence to the whole neural
network instead of single layers, which is equivalent to calculating neural
persistence on one particular matrix. This yields our deep graph persistence
measure, which implicitly incorporates persistent paths through the network and
alleviates variance-related issues through standardisation. Code is available
at https://github.com/ExplainableML/Deep-Graph-Persistence .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Transactions on Machine Learning Research (TMLR), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamically Weighted Federated k-Means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Holzer, Tania Jacob, Shubham Kavane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated clustering, an integral aspect of federated machine learning,
enables multiple data sources to collaboratively cluster their data,
maintaining decentralization and preserving privacy. In this paper, we
introduce a novel federated clustering algorithm named Dynamically Weighted
Federated k-means (DWF k-means) based on Lloyd's method for k-means clustering,
to address the challenges associated with distributed data sources and
heterogeneous data. Our proposed algorithm combines the benefits of traditional
clustering techniques with the privacy and scalability benefits offered by
federated learning. The algorithm facilitates collaborative clustering among
multiple data owners, allowing them to cluster their local data collectively
while exchanging minimal information with the central coordinator. The
algorithm optimizes the clustering process by adaptively aggregating cluster
assignments and centroids from each data source, thereby learning a global
clustering solution that reflects the collective knowledge of the entire
federated network. We address the issue of empty clusters, which commonly
arises in the context of federated clustering. We conduct experiments on
multiple datasets and data distribution settings to evaluate the performance of
our algorithm in terms of clustering score, accuracy, and v-measure. The
results demonstrate that our approach can match the performance of the
centralized classical k-means baseline, and outperform existing federated
clustering methods like k-FED in realistic scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking Boundaries: Balancing Performance and Robustness in Deep
  Wireless Traffic Forecasting <span class="chip">CCS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Ilbert, Thai V. Hoang, Zonghua Zhang, Themis Palpanas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Balancing the trade-off between accuracy and robustness is a long-standing
challenge in time series forecasting. While most of existing robust algorithms
have achieved certain suboptimal performance on clean data, sustaining the same
performance level in the presence of data perturbations remains extremely hard.
In this paper, we study a wide array of perturbation scenarios and propose
novel defense mechanisms against adversarial attacks using real-world telecom
data. We compare our strategy against two existing adversarial training
algorithms under a range of maximal allowed perturbations, defined using
$\ell_{\infty}$-norm, $\in [0.1,0.4]$. Our findings reveal that our hybrid
strategy, which is composed of a classifier to detect adversarial examples, a
denoiser to eliminate noise from the perturbed data samples, and a standard
forecaster, achieves the best performance on both clean and perturbed data. Our
optimal model can retain up to $92.02\%$ the performance of the original
forecasting model in terms of Mean Squared Error (MSE) on clean data, while
being more robust than the standard adversarially trained models on perturbed
data. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparing
methods on normal and perturbed data, respectively. In addition, the components
of our models can be trained in parallel, resulting in better computational
efficiency. Our results indicate that we can optimally balance the trade-off
between the performance and robustness of forecasting models by improving the
classifier and denoiser, even in the presence of sophisticated and destructive
poisoning attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the ARTMAN workshop, part of the ACM
  Conference on Computer and Communications Security (CCS), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structured Prediction Problem Archive 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.03574v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.03574v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Swoboda, Bjoern Andres, Andrea Hornakova, Florian Bernard, Jannik Irmai, Paul Roetzer, Bogdan Savchynskyy, David Stein, Ahmed Abbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured prediction problems are one of the fundamental tools in machine
learning. In order to facilitate algorithm development for their numerical
solution, we collect in one place a large number of datasets in easy to read
formats for a diverse set of problem classes. We provide archival links to
datasets, description of the considered problems and problem formats, and a
short summary of problem characteristics including size, number of instances
etc. For reference we also give a non-exhaustive selection of algorithms
proposed in the literature for their solution. We hope that this central
repository will make benchmarking and comparison to established works easier.
We welcome submission of interesting new datasets and algorithms for inclusion
in our archive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added multicast instances from Andres group</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DUET: 2D Structured and Approximately Equivariant Representations <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16058v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16058v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Suau, Federico Danieli, T. Anderson Keller, Arno Blaas, Chen Huang, Jason Ramapuram, Dan Busbridge, Luca Zappella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiview Self-Supervised Learning (MSSL) is based on learning invariances
with respect to a set of input transformations. However, invariance partially
or totally removes transformation-related information from the representations,
which might harm performance for specific downstream tasks that require such
information. We propose 2D strUctured and EquivarianT representations (coined
DUET), which are 2d representations organized in a matrix structure, and
equivariant with respect to transformations acting on the input data. DUET
representations maintain information about an input transformation, while
remaining semantically expressive. Compared to SimCLR (Chen et al., 2020)
(unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured
and equivariant), the structured and equivariant nature of DUET representations
enables controlled generation with lower reconstruction error, while
controllability is not possible with SimCLR or ESSL. DUET also achieves higher
accuracy for several discriminative tasks, and improves transfer learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Monitoring and Retraining Language Models in Real-World
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaykumar Kasundra, Claudia Schulz, Melicaalsadat Mirsafian, Stavroula Skylaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Machine Learning (ML) model development lifecycle, training candidate
models using an offline holdout dataset and identifying the best model for the
given task is only the first step. After the deployment of the selected model,
continuous model monitoring and model retraining is required in many real-world
applications. There are multiple reasons for retraining, including data or
concept drift, which may be reflected on the model performance as monitored by
an appropriate metric. Another motivation for retraining is the acquisition of
increasing amounts of data over time, which may be used to retrain and improve
the model performance even in the absence of drifts. We examine the impact of
various retraining decision points on crucial factors, such as model
performance and resource utilization, in the context of Multilabel
Classification models. We explain our key decision points and propose a
reference framework for designing an effective model retraining strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural approximation of Wasserstein distance via a universal
  architecture for symmetric and factorwise group invariant functions <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samantha Chen, Yusu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning distance functions between complex objects, such as the Wasserstein
distance to compare point sets, is a common goal in machine learning
applications. However, functions on such complex objects (e.g., point sets and
graphs) are often required to be invariant to a wide variety of group actions
e.g. permutation or rigid transformation. Therefore, continuous and symmetric
product functions (such as distance functions) on such complex objects must
also be invariant to the product of such group actions. We call these functions
symmetric and factor-wise group invariant (or SFGI functions in short). In this
paper, we first present a general neural network architecture for approximating
SFGI functions. The main contribution of this paper combines this general
neural network with a sketching idea to develop a specific and efficient neural
network which can approximate the $p$-th Wasserstein distance between point
sets. Very importantly, the required model complexity is independent of the
sizes of input point sets. On the theoretical front, to the best of our
knowledge, this is the first result showing that there exists a neural network
with the capacity to approximate Wasserstein distance with bounded model
complexity. Our work provides an interesting integration of sketching ideas for
geometric problems with universal approximation of symmetric functions. On the
empirical front, we present a range of results showing that our newly proposed
neural network architecture performs comparatively or better than other models
(including a SOTA Siamese Autoencoder based approach). In particular, our
neural network generalizes significantly better and trains much faster than the
SOTA Siamese AE. Finally, this line of investigation could be useful in
exploring effective neural network design for solving a broad range of
geometric optimization problems (e.g., $k$-means in a metric space).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOTUS: Continual Imitation Learning for Robot Manipulation Through
  Unsupervised Skill Discovery <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02058v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02058v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikang Wan, Yifeng Zhu, Rutav Shah, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LOTUS, a continual imitation learning algorithm that empowers a
physical robot to continuously and efficiently learn to solve new manipulation
tasks throughout its lifespan. The core idea behind LOTUS is constructing an
ever-growing skill library from a sequence of new tasks with a small number of
human demonstrations. LOTUS starts with a continual skill discovery process
using an open-vocabulary vision model, which extracts skills as recurring
patterns presented in unsegmented demonstrations. Continual skill discovery
updates existing skills to avoid catastrophic forgetting of previous tasks and
adds new skills to solve novel tasks. LOTUS trains a meta-controller that
flexibly composes various skills to tackle vision-based manipulation tasks in
the lifelong learning process. Our comprehensive experiments show that LOTUS
outperforms state-of-the-art baselines by over 11% in success rate, showing its
superior knowledge transfer ability compared to prior methods. More results and
videos can be found on the project website:
https://ut-austin-rpl.github.io/Lotus/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CDMPP: A Device-Model Agnostic Framework for Latency Prediction of
  Tensor Programs <span class="chip">EuroSys 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanpeng Hu, Junwei Su, Juntao Zhao, Yanghua Peng, Yibo Zhu, Haibin Lin, Chuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) have shown excellent performance in a wide range
of machine learning applications. Knowing the latency of running a DNN model or
tensor program on a specific device is useful in various tasks, such as DNN
graph- or tensor-level optimization and device selection. Considering the large
space of DNN models and devices that impede direct profiling of all
combinations, recent efforts focus on building a predictor to model the
performance of DNN models on different devices. However, none of the existing
attempts have achieved a cost model that can accurately predict the performance
of various tensor programs while supporting both training and inference
accelerators. We propose CDMPP, an efficient tensor program latency prediction
framework for both cross-model and cross-device prediction. We design an
informative but efficient representation of tensor programs, called compact
ASTs, and a pre-order-based positional encoding method, to capture the internal
structure of tensor programs. We develop a domain-adaption-inspired method to
learn domain-invariant representations and devise a KMeans-based sampling
algorithm, for the predictor to learn from different domains (i.e., different
DNN operators and devices). Our extensive experiments on a diverse range of DNN
models and devices demonstrate that CDMPP significantly outperforms
state-of-the-art baselines with 14.03% and 10.85% prediction error for
cross-model and cross-device prediction, respectively, and one order of
magnitude higher training efficiency. The implementation and the expanded
dataset are available at https://github.com/joapolarbear/cdmpp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EuroSys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Normalization Layers Are All That Sharpness-Aware Minimization Needs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Mueller, Tiffany Vlaar, David Rolnick, Matthias Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima
and has been shown to enhance generalization performance in various settings.
In this work we show that perturbing only the affine normalization parameters
(typically comprising 0.1% of the total parameters) in the adversarial step of
SAM can outperform perturbing all of the parameters.This finding generalizes to
different SAM variants and both ResNet (Batch Normalization) and Vision
Transformer (Layer Normalization) architectures. We consider alternative sparse
perturbation approaches and find that these do not achieve similar performance
enhancement at such extreme sparsity levels, showing that this behaviour is
unique to the normalization layers. Although our findings reaffirm the
effectiveness of SAM in improving generalization performance, they cast doubt
on whether this is solely caused by reduced sharpness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random Forest Kernel for High-Dimension Low Sample Size Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucca Portes Cavalheiro, Simon Bernard, Jean Paul Barddal, Laurent Heutte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High dimension, low sample size (HDLSS) problems are numerous among
real-world applications of machine learning. From medical images to text
processing, traditional machine learning algorithms are usually unsuccessful in
learning the best possible concept from such data. In a previous work, we
proposed a dissimilarity-based approach for multi-view classification, the
Random Forest Dissimilarity (RFD), that perfoms state-of-the-art results for
such problems. In this work, we transpose the core principle of this approach
to solving HDLSS classification problems, by using the RF similarity measure as
a learned precomputed SVM kernel (RFSVM). We show that such a learned
similarity measure is particularly suited and accurate for this classification
context. Experiments conducted on 40 public HDLSS classification datasets,
supported by rigorous statistical analyses, show that the RFSVM method
outperforms existing methods for the majority of HDLSS problems and remains at
the same time very competitive for low or non-HDLSS problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages. To be published in statistics and computing (accepted
  September 26, 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Signal Processing Meets SGD: From Momentum to Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02818v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02818v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipeng Yao, Guisong Chang, Jiaqi Zhang, Qi Zhang, Yu Zhang, Dazhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of deep learning, Stochastic Gradient Descent (SGD) and its
momentum-based variants are the predominant choices for optimization
algorithms. Despite all that, these momentum strategies, which accumulate
historical gradients by using a fixed $\beta$ hyperparameter to smooth the
optimization processing, often neglect the potential impact of the variance of
historical gradients on the current gradient estimation. In the gradient
variance during training, fluctuation indicates the objective function does not
meet the Lipschitz continuity condition at all time, which raises the
troublesome optimization problem. This paper aims to explore the potential
benefits of reducing the variance of historical gradients to make optimizer
converge to flat solutions. Moreover, we proposed a new optimization method
based on reducing the variance. We employed the Wiener filter theory to enhance
the first moment estimation of SGD, notably introducing an adaptive weight to
optimizer. Specifically, the adaptive weight dynamically changes along with
temporal fluctuation of gradient variance during deep learning model training.
Experimental results demonstrated our proposed adaptive weight optimizer, SGDF
(Stochastic Gradient Descent With Filter), can achieve satisfactory performance
compared with state-of-the-art optimizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2010.07468 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligent machines work in unstructured environments by differential
  neuromorphic computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08835v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08835v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengbo Wang, Shuo Gao, Chenyu Tang, Edoardo Occhipinti, Cong Li, Shurui Wang, Jiaqi Wang, Hubin Zhao, Guohua Hu, Arokia Nathan, Ravinder Dahiya, Luigi Occhipinti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient operation of intelligent machines in the real world requires
methods that allow them to understand and predict the uncertainties presented
by the unstructured environments with good accuracy, scalability and
generalization, similar to humans. Current methods rely on pretrained networks
instead of continuously learning from the dynamic signal properties of working
environments and suffer inherent limitations, such as data-hungry procedures,
and limited generalization capabilities. Herein, we present a memristor-based
differential neuromorphic computing, perceptual signal processing and learning
method for intelligent machines. The main features of environmental information
such as amplification (>720%) and adaptation (<50%) of mechanical stimuli
encoded in memristors, are extracted to obtain human-like processing in
unstructured environments. The developed method takes advantage of the
intrinsic multi-state property of memristors and exhibits good scalability and
generalization, as confirmed by validation in two different application
scenarios: object grasping and autonomous driving. In the former, a robot hand
experimentally realizes safe and stable grasping through fast learning (in ~1
ms) the unknown object features (e.g., sharp corner and smooth surface) with a
single memristor. In the latter, the decision-making information of 10
unstructured environments in autonomous driving (e.g., overtaking cars,
pedestrians) is accurately (94%) extracted with a 40*25 memristor array. By
mimicking the intrinsic nature of human low-level perception mechanisms, the
electronic memristive neuromorphic circuit-based method, presented here shows
the potential for adapting to diverse sensing technologies and helping
intelligent machines generate smart high-level decisions in the real world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Inverse Problem Solutions with Accurate Surrogate Simulators
  and Promising Candidates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akihiro Fujii, Hideki Tsunashima, Yoshihiro Fukuhara, Koji Shimizu, Satoshi Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep-learning inverse techniques have attracted significant attention in
recent years. Among them, the neural adjoint (NA) method, which employs a
neural network surrogate simulator, has demonstrated impressive performance in
the design tasks of artificial electromagnetic materials (AEM). However, the
impact of the surrogate simulators' accuracy on the solutions in the NA method
remains uncertain. Furthermore, achieving sufficient optimization becomes
challenging in this method when the surrogate simulator is large, and
computational resources are limited. Additionally, the behavior under
constraints has not been studied, despite its importance from the engineering
perspective. In this study, we investigated the impact of surrogate simulators'
accuracy on the solutions and discovered that the more accurate the surrogate
simulator is, the better the solutions become. We then developed an extension
of the NA method, named Neural Lagrangian (NeuLag) method, capable of
efficiently optimizing a sufficient number of solution candidates. We then
demonstrated that the NeuLag method can find optimal solutions even when
handling sufficient candidates is difficult due to the use of a large and
accurate surrogate simulator. The resimulation errors of the NeuLag method were
approximately 1/50 compared to previous methods for three AEM tasks. Finally,
we performed optimization under constraint using NA and NeuLag, and confirmed
their potential in optimization with soft or hard constraints. We believe our
method holds potential in areas that require large and accurate surrogate
simulators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does Fine-Tuning Impact Out-of-Distribution Detection for
  Vision-Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Ming, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large vision-language models such as CLIP have shown remarkable
out-of-distribution (OOD) detection and generalization performance. However,
their zero-shot in-distribution (ID) accuracy is often limited for downstream
datasets. Recent CLIP-based fine-tuning methods such as prompt learning have
demonstrated significant improvements in ID classification and OOD
generalization where OOD labels are available. Nonetheless, it remains unclear
whether the model is reliable to semantic shifts without OOD labels. In this
paper, we aim to bridge the gap and present a comprehensive study to understand
how fine-tuning impact OOD detection for few-shot downstream tasks. By framing
OOD detection as multi-modal concept matching, we establish a connection
between fine-tuning methods and various OOD scores. Our results suggest that a
proper choice of OOD scores is essential for CLIP-based fine-tuning. In
particular, the maximum concept matching (MCM) score provides a promising
solution consistently. We also show that prompt learning demonstrates the
state-of-the-art OOD detection performance over the zero-shot counterpart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-Aware Decision <span class="highlight-title">Transformer</span> for Stochastic Driving
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zenan Li, Fan Nie, Qiao Sun, Fang Da, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Reinforcement Learning (RL) has emerged as a promising framework for
learning policies without active interactions, making it especially appealing
for autonomous driving tasks. Recent successes of Transformers inspire casting
offline RL as sequence modeling, which performs well in long-horizon tasks.
However, they are overly optimistic in stochastic environments with incorrect
assumptions that the same goal can be consistently achieved by identical
actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer
(UNREST) for planning in stochastic driving environments without introducing
additional transition or complex generative models. Specifically, UNREST
estimates state uncertainties by the conditional mutual information between
transitions and returns, and segments sequences accordingly. Discovering the
`uncertainty accumulation' and `temporal locality' properties of driving
environments, UNREST replaces the global returns in decision transformers with
less uncertain truncated returns, to learn from true outcomes of agent actions
rather than environment transitions. We also dynamically evaluate environmental
uncertainty during inference for cautious planning. Extensive experimental
results demonstrate UNREST's superior performance in various driving scenarios
and the power of our uncertainty estimation strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tomography of Quantum States from Structured Measurements via
  quantum-aware <span class="highlight-title">transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05433v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05433v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailan Ma, Zhenhong Sun, Daoyi Dong, Chunlin Chen, Herschel Rabitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum state tomography (QST) is the process of reconstructing the state of
a quantum system (mathematically described as a density matrix) through a
series of different measurements, which can be solved by learning a
parameterized function to translate experimentally measured statistics into
physical density matrices. However, the specific structure of quantum
measurements for characterizing a quantum state has been neglected in previous
work. In this paper, we explore the similarity between highly structured
sentences in natural language and intrinsically structured measurements in QST.
To fully leverage the intrinsic quantum characteristics involved in QST, we
design a quantum-aware transformer (QAT) model to capture the complex
relationship between measured frequencies and density matrices. In particular,
we query quantum operators in the architecture to facilitate informative
representations of quantum data and integrate the Bures distance into the loss
function to evaluate quantum state fidelity, thereby enabling the
reconstruction of quantum states from measured data with high fidelity.
Extensive simulations and experiments (on IBM quantum computers) demonstrate
the superiority of the QAT in reconstructing quantum states with favorable
robustness against experimental noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion with Forward Models: Solving Stochastic Inverse Problems
  Without Direct Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B. Tenenbaum, Frédo Durand, William T. Freeman, Vincent Sitzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models are a powerful type of generative models used to
capture complex distributions of real-world signals. However, their
applicability is limited to scenarios where training samples are readily
available, which is not always the case in real-world applications. For
example, in inverse graphics, the goal is to generate samples from a
distribution of 3D scenes that align with a given image, but ground-truth 3D
scenes are unavailable and only 2D images are accessible. To address this
limitation, we propose a novel class of denoising diffusion probabilistic
models that learn to sample from distributions of signals that are never
directly observed. Instead, these signals are measured indirectly through a
known differentiable forward model, which produces partial observations of the
unknown signal. Our approach involves integrating the forward model directly
into the denoising process. This integration effectively connects the
generative modeling of observations with the generative modeling of the
underlying signals, allowing for end-to-end training of a conditional
generative model over signals. During inference, our approach enables sampling
from the distribution of underlying signals that are consistent with a given
partial observation. We demonstrate the effectiveness of our method on three
challenging computer vision tasks. For instance, in the context of inverse
graphics, our model enables direct sampling from the distribution of 3D scenes
that align with a single 2D input image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://diffusion-with-forward-models.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Causal Deep Learning for Vulnerability Detection <span class="chip">ICSE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07958v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07958v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mahbubur Rahman, Ira Ceka, Chengzhi Mao, Saikat Chakraborty, Baishakhi Ray, Wei Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning vulnerability detection has shown promising results in recent
years. However, an important challenge that still blocks it from being very
useful in practice is that the model is not robust under perturbation and it
cannot generalize well over the out-of-distribution (OOD) data, e.g., applying
a trained model to unseen projects in real world. We hypothesize that this is
because the model learned non-robust features, e.g., variable names, that have
spurious correlations with labels. When the perturbed and OOD datasets no
longer have the same spurious features, the model prediction fails. To address
the challenge, in this paper, we introduced causality into deep learning
vulnerability detection. Our approach CausalVul consists of two phases. First,
we designed novel perturbations to discover spurious features that the model
may use to make predictions. Second, we applied the causal learning algorithms,
specifically, do-calculus, on top of existing deep learning models to
systematically remove the use of spurious features and thus promote causal
based prediction. Our results show that CausalVul consistently improved the
model accuracy, robustness and OOD performance for all the state-of-the-art
models and datasets we experimented. To the best of our knowledge, this is the
first work that introduces do calculus based causal learning to software
engineering models and shows it's indeed useful for improving the model
accuracy, robustness and generalization. Our replication package is located at
https://figshare.com/s/0ffda320dcb96c249ef2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICSE 2024 (not camera-ready version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying the Key Attributes in an Unlabeled Event Log for Automated
  Process Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kentaroh Toyoda, Rachel Gan Kai Ying, Allan NengSheng Zhang, Tan Puay Siew
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process mining discovers and analyzes a process model from historical event
logs. The prior art methods use the key attributes of case-id, activity, and
timestamp hidden in an event log as clues to discover a process model. However,
a user needs to specify them manually, and this can be an exhaustive task. In
this paper, we propose a two-stage key attribute identification method to avoid
such a manual investigation, and thus this is a step toward fully automated
process discovery. One of the challenging tasks is how to avoid exhaustive
computation due to combinatorial explosion. For this, we narrow down candidates
for each key attribute by using supervised machine learning in the first stage
and identify the best combination of the key attributes by discovering process
models and evaluating them in the second stage. Our computational complexity
can be reduced from $\mathcal{O}(N^3)$ to $\mathcal{O}(k^3)$ where $N$ and $k$
are the numbers of columns and candidates we keep in the first stage,
respectively, and usually $k$ is much smaller than $N$. We evaluated our method
with 14 open datasets and showed that our method could identify the key
attributes even with $k = 2$ for about 20 seconds for many datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Services Computing (Early Access version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Faithful Deep Sensitivity Estimation for Accelerated Magnetic
  Resonance Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Wang, Haoming Fang, Chen Qian, Boxuan Shi, Lijun Bao, Liuhong Zhu, Jianjun Zhou, Wenping Wei, Jianzhong Lin, Di Guo, Xiaobo Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic resonance imaging (MRI) is an essential diagnostic tool that suffers
from prolonged scan time. To alleviate this limitation, advanced fast MRI
technology attracts extensive research interests. Recent deep learning has
shown its great potential in improving image quality and reconstruction speed.
Faithful coil sensitivity estimation is vital for MRI reconstruction. However,
most deep learning methods still rely on pre-estimated sensitivity maps and
ignore their inaccuracy, resulting in the significant quality degradation of
reconstructed images. In this work, we propose a Joint Deep Sensitivity
estimation and Image reconstruction network, called JDSI. During the image
artifacts removal, it gradually provides more faithful sensitivity maps with
high-frequency information, leading to improved image reconstructions. To
understand the behavior of the network, the mutual promotion of sensitivity
estimation and image reconstruction is revealed through the visualization of
network intermediate results. Results on in vivo datasets and radiologist
reader study demonstrate that, for both calibration-based and calibrationless
reconstruction, the proposed JDSI achieves the state-of-the-art performance
visually and quantitatively, especially when the acceleration factor is high.
Additionally, JDSI owns nice robustness to patients and autocalibration
signals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 12 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoDiff: combining Auto-encoder and Diffusion model for tabular data
  synthesizing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namjoon Suh, Xiaofeng Lin, Din-Yin Hsieh, Merhdad Honarkhah, Guang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion model has become a main paradigm for synthetic data generation in
many subfields of modern machine learning, including computer vision, language
model, or speech synthesis. In this paper, we leverage the power of diffusion
model for generating synthetic tabular data. The heterogeneous features in
tabular data have been main obstacles in tabular data synthesis, and we tackle
this problem by employing the auto-encoder architecture. When compared with the
state-of-the-art tabular synthesizers, the resulting synthetic tables from our
model show nice statistical fidelities to the real data, and perform well in
downstream tasks for machine learning utilities. We conducted the experiments
over $15$ publicly available datasets. Notably, our model adeptly captures the
correlations among features, which has been a long-standing challenge in
tabular data synthesis. Our code is available at
https://github.com/UCLA-Trustworthy-AI-Lab/AutoDiffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digital Twin Accelerated Deep Reinforcement Learning for Online
  Admission Control of Network Slicing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09299v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09299v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Tao, Wei Xu, Xiaohu You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of diverse wireless services in 5G and beyond has led to
the emergence of network slicing technologies. Among these, admission control
plays a crucial role in achieving service-oriented optimization goals through
the selective acceptance of service requests. Although deep reinforcement
learning (DRL) forms the foundation in many admission control approaches thanks
to its effectiveness and flexibility, initial instability with excessive
convergence delay of DRL models hinders their deployment in real-world
networks. We propose a digital twin (DT) accelerated DRL solution to address
this issue. Specifically, we first formulate the admission decision-making
process as a semi-Markov decision process, which is subsequently simplified
into an equivalent discrete-time Markov decision process to facilitate the
implementation of DRL methods. A neural network-based DT is established with a
customized output layer for queuing systems, trained through supervised
learning, and then employed to assist the training phase of the DRL model.
Extensive simulations show that the DT-accelerated DRL improves resource
utilization by over 40% compared to the directly trained state-of-the-art
dueling deep Q-learning model. This improvement is achieved while preserving
the model's capability to optimize the long-term rewards of the admission
process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code
  Completion <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, Bing Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code completion models have made significant progress in recent years, yet
current popular evaluation datasets, such as HumanEval and MBPP, predominantly
focus on code completion tasks within a single file. This over-simplified
setting falls short of representing the real-world software development
scenario where repositories span multiple files with numerous cross-file
dependencies, and accessing and understanding cross-file context is often
required to complete the code correctly.
  To fill in this gap, we propose CrossCodeEval, a diverse and multilingual
code completion benchmark that necessitates an in-depth cross-file contextual
understanding to complete the code accurately. CrossCodeEval is built on a
diverse set of real-world, open-sourced, permissively-licensed repositories in
four popular programming languages: Python, Java, TypeScript, and C#. To create
examples that strictly require cross-file context for accurate completion, we
propose a straightforward yet efficient static-analysis-based approach to
pinpoint the use of cross-file context within the current file.
  Extensive experiments on state-of-the-art code language models like CodeGen
and StarCoder demonstrate that CrossCodeEval is extremely challenging when the
relevant cross-file context is absent, and we see clear improvements when
adding these context into the prompt. However, despite such improvements, the
pinnacle of performance remains notably unattained even with the
highest-performing model, indicating that CrossCodeEval is also capable of
assessing model's capability in leveraging extensive context to make better
code completion. Finally, we benchmarked various methods in retrieving
cross-file context, and show that CrossCodeEval can also be used to measure the
capability of code retrievers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at NeurIPS 2023 (Datasets and Benchmarks Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical
  Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05836v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05836v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Hu, Qinrui Fan, Shu Hu, Siwei Lyu, Xi Wu, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of clinical medicine, computed tomography (CT) is an effective
medical imaging modality for the diagnosis of various pathologies. Compared
with X-ray images, CT images can provide more information, including
multi-planar slices and three-dimensional structures for clinical diagnosis.
However, CT imaging requires patients to be exposed to large doses of ionizing
radiation for a long time, which may cause irreversible physical harm. In this
paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on
generated radiation fields. The network can learn a continuous representation
of CT projections from 2D X-ray images by obtaining the internal structure and
depth information and using adaptive loss weights to ensure the quality of the
generated images. Our model is trained on publicly available knee and chest
datasets, and we show the results of CT projection rendering with a single
X-ray and compare our method with other methods based on generated radiation
fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concave Utility Reinforcement Learning with Zero-Constraint Violations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.05439v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.05439v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mridul Agarwal, Qinbo Bai, Vaneet Aggarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of tabular infinite horizon concave utility
reinforcement learning (CURL) with convex constraints. For this, we propose a
model-based learning algorithm that also achieves zero constraint violations.
Assuming that the concave objective and the convex constraints have a solution
interior to the set of feasible occupation measures, we solve a tighter
optimization problem to ensure that the constraints are never violated despite
the imprecise model knowledge and model stochasticity. We use Bellman
error-based analysis for tabular infinite-horizon setups which allows analyzing
stochastic policies. Combining the Bellman error-based analysis and tighter
optimization equation, for $T$ interactions with the environment, we obtain a
high-probability regret guarantee for objective which grows as
$\Tilde{O}(1/\sqrt{T})$, excluding other factors. The proposed method can be
applied for optimistic algorithms to obtain high-probability regret bounds and
also be used for posterior sampling algorithms to obtain a loose Bayesian
regret bounds but with significant improvement in computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Transactions on Machine Learning Research, Dec 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-4 can pass the Korean National Licensing Examination for Korean
  Medicine Doctors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyeop Jang, Tae-Rim Yun, Choong-Yeol Lee, Young-Kyu Kwon, Chang-Eop Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Korean medicine (TKM) emphasizes individualized diagnosis and
treatment. This uniqueness makes AI modeling difficult due to limited data and
implicit processes. Large language models (LLMs) have demonstrated impressive
medical inference, even without advanced training in medical texts. This study
assessed the capabilities of GPT-4 in TKM, using the Korean National Licensing
Examination for Korean Medicine Doctors (K-NLEKMD) as a benchmark. The
K-NLEKMD, administered by a national organization, encompasses 12 major
subjects in TKM. We optimized prompts with Chinese-term annotation, English
translation for questions and instruction, exam-optimized instruction, and
self-consistency. GPT-4 with optimized prompts achieved 66.18% accuracy,
surpassing both the examination's average pass mark of 60% and the 40% minimum
for each subject. The gradual introduction of language-related prompts and
prompting techniques enhanced the accuracy from 51.82% to its maximum accuracy.
GPT-4 showed low accuracy in subjects including public health &
medicine-related law, internal medicine (2) which are localized in Korea and
TKM. The model's accuracy was lower for questions requiring TKM-specialized
knowledge. It exhibited higher accuracy in diagnosis-based and recall-based
questions than in intervention-based questions. A positive correlation was
observed between the consistency and accuracy of GPT-4's responses. This study
unveils both the potential and challenges of applying LLMs to TKM. These
findings underline the potential of LLMs like GPT-4 in culturally adapted
medicine, especially TKM, for tasks such as clinical assistance, medical
education, and research. But they also point towards the necessity for the
development of methods to mitigate cultural bias inherent in large language
models and validate their efficacy in real-world clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LymphoML: An interpretable artificial intelligence-based method
  identifies morphologic features that correlate with lymphoma subtype 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Shankar, Xiaoli Yang, Vrishab Krishna, Brent Tan, Oscar Silva, Rebecca Rojansky, Andrew Ng, Fabiola Valvert, Edward Briercheck, David Weinstock, Yasodha Natkunam, Sebastian Fernandez-Pol, Pranav Rajpurkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML's interpretable models, developed on a
limited volume of H&E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proceedings of the 3rd Machine Learning for Health
  symposium, Proceedings of Machine Learning Research (PMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> a Robot to Walk with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) pre-trained on vast internet-scale data have
showcased remarkable capabilities across diverse domains. Recently, there has
been escalating interest in deploying LLMs for robotics, aiming to harness the
power of foundation models in real-world settings. However, this approach faces
significant challenges, particularly in grounding these models in the physical
world and in generating dynamic robot motions. To address these issues, we
introduce a novel paradigm in which we use few-shot prompts collected from the
physical environment, enabling the LLM to autoregressively generate low-level
control commands for robots without task-specific fine-tuning. Experiments
across various robots and environments validate that our method can effectively
prompt a robot to walk. We thus illustrate how LLMs can proficiently function
as low-level feedback controllers for dynamic motion control even in
high-dimensional robotic systems. The project website and source code can be
found at: https://prompt2walk.github.io/ .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Contamination Quiz: A Tool to Detect and Estimate Contamination in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahriar Golchin, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the Data Contamination Quiz, a simple and effective approach to
detect data contamination in large language models (LLMs) and estimate the
amount of it. Specifically, we frame data contamination detection as a series
of multiple-choice questions. We devise a quiz format wherein three perturbed
versions of each dataset instance are created. These changes only include
word-level perturbations, replacing words with their contextual synonyms,
ensuring both the semantic and sentence structure remain exactly the same as
the original instance. Together with the original instance, these perturbed
versions constitute the choices in the quiz. Given that the only distinguishing
signal among these choices is the exact wording, an LLM, when tasked with
identifying the original instance from the choices, opts for the original if it
has memorized it in its pre-training phase--a trait intrinsic to LLMs. A
dataset partition is then marked as contaminated if the LLM's performance on
the quiz surpasses what random chance suggests. Our evaluation spans seven
datasets and their respective splits (train and test/validation) on two
state-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the
pre-training data, our results suggest that our approach not only enhances the
detection of data contamination but also provides an accurate estimation of its
extent, even when the contamination signal is weak.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v1.1 preprint</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emu Video: Factorizing Text-to-Video Generation by Explicit Image
  Conditioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, Ishan Misra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Emu Video, a text-to-video generation model that factorizes the
generation into two steps: first generating an image conditioned on the text,
and then generating a video conditioned on the text and the generated image. We
identify critical design decisions--adjusted noise schedules for diffusion, and
multi-stage training--that enable us to directly generate high quality and high
resolution videos, without requiring a deep cascade of models as in prior work.
In human evaluations, our generated videos are strongly preferred in quality
compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's
PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial
solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing
approach naturally lends itself to animating images based on a user's text
prompt, where our generations are preferred 96% over prior work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://emu-video.metademolab.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User Dynamics-Aware Edge Caching and Computing for Mobile Virtual
  Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mushu Li, Jie Gao, Conghao Zhou, Xuemin Shen, Weihua Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel content caching and delivery approach for
mobile virtual reality (VR) video streaming. The proposed approach aims to
maximize VR video streaming performance, i.e., minimizing video frame missing
rate, by proactively caching popular VR video chunks and adaptively scheduling
computing resources at an edge server based on user and network dynamics.
First, we design a scalable content placement scheme for deciding which video
chunks to cache at the edge server based on tradeoffs between computing and
caching resource consumption. Second, we propose a machine learning-assisted VR
video delivery scheme, which allocates computing resources at the edge server
to satisfy video delivery requests from multiple VR headsets. A Whittle
index-based method is adopted to reduce the video frame missing rate by
identifying network and user dynamics with low signaling overhead. Simulation
results demonstrate that the proposed approach can significantly improve VR
video streaming performance over conventional caching and computing resource
scheduling strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 13 figures, single column double spaced, published in IEEE
  Journal of Selected Topics in Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring User Perceptions of Virtual Reality Scene Design in Metaverse
  Learning Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahatara Ferdousi, Mohammed Faisal, Fedwa Laamarti, Chunsheng Yang, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaverse learning environments allow for a seamless and intuitive transition
between activities compared to Virtual Reality (VR) learning environments, due
to their interconnected design. The design of VR scenes is important for
creating effective learning experiences in the Metaverse. However, there is
limited research on the impact of different design elements on user's learning
experiences in VR scenes. To address this, a study was conducted with 16
participants who interacted with two VR scenes, each with varying design
elements such as style, color, texture, object, and background, while watching
a short tutorial. Participant rankings of the scenes for learning were obtained
using a seven-point Likert scale, and the Mann-Whitney U test was used to
validate differences in preference between the scenes. The results showed a
significant difference in preference between the scenes. Further analysis using
the NASA TLX questionnaire was conducted to examine the impact of this
difference on cognitive load, and participant feedback was also considered. The
study emphasizes the importance of careful VR scene design to improve the
user's learning experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages,3 figures, accepted to present at IEEE 42nd International
  Conference on Consumer Electronics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality-invariant and Specific <span class="highlight-title">Prompt</span>ing for Multimodal Human
  Perception Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Ziwei Niu, Xinyao Yu, Jiaqing Liu, Yen-Wei Chen, Lanfen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding human perceptions presents a formidable multimodal challenge
for computers, encompassing aspects such as sentiment tendencies and sense of
humor. While various methods have recently been introduced to extract
modality-invariant and specific information from diverse modalities, with the
goal of enhancing the efficacy of multimodal learning, few works emphasize this
aspect in large language models. In this paper, we introduce a novel multimodal
prompt strategy tailored for tuning large language models. Our method assesses
the correlation among different modalities and isolates the modality-invariant
and specific components, which are then utilized for prompt tuning. This
approach enables large language models to efficiently and effectively
assimilate information from various modalities. Furthermore, our strategy is
designed with scalability in mind, allowing the integration of features from
any modality into pretrained large language models. Experimental results on
public datasets demonstrate that our proposed method significantly improves
performance compared to previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Review</span> of Intelligent Music Generation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09124v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09124v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Ziyi Zhao, Hanwei Liu, Junwei Pang, Yi Qin, Qidi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the introduction of ChatGPT, the public's perception of AI-generated
content (AIGC) has begun to reshape. Artificial intelligence has significantly
reduced the barrier to entry for non-professionals in creative endeavors,
enhancing the efficiency of content creation. Recent advancements have seen
significant improvements in the quality of symbolic music generation, which is
enabled by the use of modern generative algorithms to extract patterns implicit
in a piece of music based on rule constraints or a musical corpus.
Nevertheless, existing literature reviews tend to present a conventional and
conservative perspective on future development trajectories, with a notable
absence of thorough benchmarking of generative models. This paper provides a
survey and analysis of recent intelligent music generation techniques,
outlining their respective characteristics and discussing existing methods for
evaluation. Additionally, the paper compares the different characteristics of
music generation techniques in the East and West as well as analysing the
field's development prospects.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2023-11-25T05:21:35.623471689Z">
            2023-11-25 05:21:35 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
